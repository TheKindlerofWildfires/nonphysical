//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-33567101
// Cuda compilation tools, release 12.3, V12.3.107
// Based on NVVM 7.0.1
//

.version 8.3
.target sm_52
.address_size 64

	// .globl	__cudaCDP2Malloc
.global .align 4 .f32 M_PI = 0f40490E56;

.visible .func  (.param .b32 func_retval0) __cudaCDP2Malloc(
	.param .b64 __cudaCDP2Malloc_param_0,
	.param .b64 __cudaCDP2Malloc_param_1
)
{
	.reg .b32 	%r<2>;


	mov.u32 	%r1, 999;
	st.param.b32 	[func_retval0+0], %r1;
	ret;

}
	// .globl	_Z4ceilf
.visible .func  (.param .b32 func_retval0) _Z4ceilf(
	.param .b32 _Z4ceilf_param_0
)
{
	.reg .f32 	%f<3>;


	ld.param.f32 	%f1, [_Z4ceilf_param_0];
	cvt.rpi.ftz.f32.f32 	%f2, %f1;
	st.param.f32 	[func_retval0+0], %f2;
	ret;

}
	// .globl	_Z3cosf
.visible .func  (.param .b32 func_retval0) _Z3cosf(
	.param .b32 _Z3cosf_param_0
)
{
	.reg .f32 	%f<3>;


	ld.param.f32 	%f1, [_Z3cosf_param_0];
	cos.approx.ftz.f32 	%f2, %f1;
	st.param.f32 	[func_retval0+0], %f2;
	ret;

}
	// .globl	_Z3sinf
.visible .func  (.param .b32 func_retval0) _Z3sinf(
	.param .b32 _Z3sinf_param_0
)
{
	.reg .f32 	%f<3>;


	ld.param.f32 	%f1, [_Z3sinf_param_0];
	sin.approx.ftz.f32 	%f2, %f1;
	st.param.f32 	[func_retval0+0], %f2;
	ret;

}
	// .globl	_Z17inplace_fft_innerP6float2iiiib
.visible .func _Z17inplace_fft_innerP6float2iiiib(
	.param .b64 _Z17inplace_fft_innerP6float2iiiib_param_0,
	.param .b32 _Z17inplace_fft_innerP6float2iiiib_param_1,
	.param .b32 _Z17inplace_fft_innerP6float2iiiib_param_2,
	.param .b32 _Z17inplace_fft_innerP6float2iiiib_param_3,
	.param .b32 _Z17inplace_fft_innerP6float2iiiib_param_4,
	.param .b32 _Z17inplace_fft_innerP6float2iiiib_param_5
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<25>;
	.reg .b32 	%r<10>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<6>;


	ld.param.s8 	%rs1, [_Z17inplace_fft_innerP6float2iiiib_param_5];
	ld.param.u64 	%rd1, [_Z17inplace_fft_innerP6float2iiiib_param_0];
	ld.param.u32 	%r5, [_Z17inplace_fft_innerP6float2iiiib_param_1];
	ld.param.u32 	%r3, [_Z17inplace_fft_innerP6float2iiiib_param_2];
	ld.param.u32 	%r6, [_Z17inplace_fft_innerP6float2iiiib_param_4];
	ld.param.u32 	%r4, [_Z17inplace_fft_innerP6float2iiiib_param_3];
	add.s32 	%r1, %r3, %r5;
	shr.u32 	%r7, %r4, 31;
	add.s32 	%r8, %r4, %r7;
	shr.s32 	%r2, %r8, 1;
	add.s32 	%r9, %r2, %r1;
	setp.ge.s32 	%p1, %r9, %r6;
	setp.le.s32 	%p2, %r2, %r3;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB4_2;

	ld.global.f32 	%f1, [M_PI];
	add.ftz.f32 	%f2, %f1, %f1;
	cvt.rn.f32.s32 	%f3, %r3;
	mul.ftz.f32 	%f4, %f2, %f3;
	cvt.ftz.f64.f32 	%fd1, %f4;
	setp.eq.s16 	%p4, %rs1, 0;
	selp.f64 	%fd2, 0dBFF0000000000000, 0d3FF0000000000000, %p4;
	cvt.rn.f64.s32 	%fd3, %r4;
	mul.f64 	%fd4, %fd2, %fd3;
	div.rn.f64 	%fd5, %fd1, %fd4;
	cvt.rn.ftz.f32.f64 	%f5, %fd5;
	cos.approx.ftz.f32 	%f6, %f5;
	sin.approx.ftz.f32 	%f7, %f5;
	mul.wide.s32 	%rd2, %r1, 8;
	add.s64 	%rd3, %rd1, %rd2;
	ld.v2.f32 	{%f8, %f9}, [%rd3];
	mul.wide.s32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.v2.f32 	{%f12, %f13}, [%rd5];
	mul.ftz.f32 	%f16, %f6, %f12;
	mul.ftz.f32 	%f17, %f7, %f13;
	sub.ftz.f32 	%f18, %f16, %f17;
	mul.ftz.f32 	%f19, %f7, %f12;
	fma.rn.ftz.f32 	%f20, %f6, %f13, %f19;
	add.ftz.f32 	%f21, %f9, %f20;
	add.ftz.f32 	%f22, %f8, %f18;
	st.v2.f32 	[%rd3], {%f22, %f21};
	sub.ftz.f32 	%f23, %f9, %f20;
	sub.ftz.f32 	%f24, %f8, %f18;
	st.v2.f32 	[%rd5], {%f24, %f23};

$L__BB4_2:
	ret;

}
	// .globl	_Z21inplace_divide_invertP6float2ii
.visible .entry _Z21inplace_divide_invertP6float2ii(
	.param .u64 _Z21inplace_divide_invertP6float2ii_param_0,
	.param .u32 _Z21inplace_divide_invertP6float2ii_param_1,
	.param .u32 _Z21inplace_divide_invertP6float2ii_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd1, [_Z21inplace_divide_invertP6float2ii_param_0];
	ld.param.u32 	%r2, [_Z21inplace_divide_invertP6float2ii_param_1];
	ld.param.u32 	%r3, [_Z21inplace_divide_invertP6float2ii_param_2];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.s32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB5_2;

	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r1, 8;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.v2.f32 	{%f1, %f2}, [%rd4];
	cvt.rn.f32.s32 	%f5, %r2;
	div.approx.ftz.f32 	%f6, %f2, %f5;
	div.approx.ftz.f32 	%f7, %f1, %f5;
	st.global.v2.f32 	[%rd4], {%f7, %f6};

$L__BB5_2:
	ret;

}
	// .globl	_Z14bitrev_reorderP6float2S0_iyi
.visible .entry _Z14bitrev_reorderP6float2S0_iyi(
	.param .u64 _Z14bitrev_reorderP6float2S0_iyi_param_0,
	.param .u64 _Z14bitrev_reorderP6float2S0_iyi_param_1,
	.param .u32 _Z14bitrev_reorderP6float2S0_iyi_param_2,
	.param .u64 _Z14bitrev_reorderP6float2S0_iyi_param_3,
	.param .u32 _Z14bitrev_reorderP6float2S0_iyi_param_4
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<15>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [_Z14bitrev_reorderP6float2S0_iyi_param_0];
	ld.param.u64 	%rd2, [_Z14bitrev_reorderP6float2S0_iyi_param_1];
	ld.param.u32 	%r3, [_Z14bitrev_reorderP6float2S0_iyi_param_2];
	ld.param.u32 	%r4, [_Z14bitrev_reorderP6float2S0_iyi_param_4];
	ld.param.u32 	%r5, [_Z14bitrev_reorderP6float2S0_iyi_param_3];
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r6, %r5, %r7;
	setp.ge.s32 	%p1, %r1, %r4;
	@%p1 bra 	$L__BB6_3;

	brev.b32 	%r8, %r1;
	mov.u32 	%r9, 32;
	sub.s32 	%r10, %r9, %r3;
	shr.u32 	%r2, %r8, %r10;
	setp.ge.u32 	%p2, %r2, %r4;
	@%p2 bra 	$L__BB6_3;

	cvta.to.global.u64 	%rd3, %rd1;
	mul.wide.u32 	%rd4, %r2, 8;
	add.s64 	%rd5, %rd3, %rd4;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.nc.v2.u32 	{%r11, %r12}, [%rd8];
	st.global.v2.u32 	[%rd5], {%r11, %r12};

$L__BB6_3:
	ret;

}
	// .globl	_Z11inplace_fftP6float2iiiib
.visible .entry _Z11inplace_fftP6float2iiiib(
	.param .u64 _Z11inplace_fftP6float2iiiib_param_0,
	.param .u32 _Z11inplace_fftP6float2iiiib_param_1,
	.param .u32 _Z11inplace_fftP6float2iiiib_param_2,
	.param .u32 _Z11inplace_fftP6float2iiiib_param_3,
	.param .u32 _Z11inplace_fftP6float2iiiib_param_4,
	.param .u8 _Z11inplace_fftP6float2iiiib_param_5
)
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<25>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<7>;


	ld.param.s8 	%rs1, [_Z11inplace_fftP6float2iiiib_param_5];
	ld.param.u64 	%rd1, [_Z11inplace_fftP6float2iiiib_param_0];
	ld.param.u32 	%r5, [_Z11inplace_fftP6float2iiiib_param_1];
	ld.param.u32 	%r6, [_Z11inplace_fftP6float2iiiib_param_3];
	ld.param.u32 	%r7, [_Z11inplace_fftP6float2iiiib_param_4];
	ld.param.u32 	%r4, [_Z11inplace_fftP6float2iiiib_param_2];
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r8, %r7, %r9;
	add.s32 	%r2, %r1, %r5;
	shr.u32 	%r10, %r4, 31;
	add.s32 	%r11, %r4, %r10;
	shr.s32 	%r3, %r11, 1;
	add.s32 	%r12, %r2, %r3;
	setp.ge.s32 	%p1, %r12, %r6;
	setp.le.s32 	%p2, %r3, %r1;
	or.pred  	%p3, %p2, %p1;
	@%p3 bra 	$L__BB7_2;

	ld.global.nc.f32 	%f1, [M_PI];
	add.ftz.f32 	%f2, %f1, %f1;
	cvt.rn.f32.s32 	%f3, %r1;
	mul.ftz.f32 	%f4, %f2, %f3;
	cvt.ftz.f64.f32 	%fd1, %f4;
	setp.eq.s16 	%p4, %rs1, 0;
	selp.f64 	%fd2, 0dBFF0000000000000, 0d3FF0000000000000, %p4;
	cvt.rn.f64.s32 	%fd3, %r4;
	mul.f64 	%fd4, %fd2, %fd3;
	div.rn.f64 	%fd5, %fd1, %fd4;
	cvt.rn.ftz.f32.f64 	%f5, %fd5;
	cos.approx.ftz.f32 	%f6, %f5;
	sin.approx.ftz.f32 	%f7, %f5;
	cvta.to.global.u64 	%rd2, %rd1;
	mul.wide.s32 	%rd3, %r2, 8;
	add.s64 	%rd4, %rd2, %rd3;
	ld.global.v2.f32 	{%f8, %f9}, [%rd4];
	mul.wide.s32 	%rd5, %r3, 8;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v2.f32 	{%f12, %f13}, [%rd6];
	mul.ftz.f32 	%f16, %f6, %f12;
	mul.ftz.f32 	%f17, %f7, %f13;
	sub.ftz.f32 	%f18, %f16, %f17;
	mul.ftz.f32 	%f19, %f7, %f12;
	fma.rn.ftz.f32 	%f20, %f6, %f13, %f19;
	add.ftz.f32 	%f21, %f9, %f20;
	add.ftz.f32 	%f22, %f8, %f18;
	st.global.v2.f32 	[%rd4], {%f22, %f21};
	sub.ftz.f32 	%f23, %f9, %f20;
	sub.ftz.f32 	%f24, %f8, %f18;
	st.global.v2.f32 	[%rd6], {%f24, %f23};

$L__BB7_2:
	ret;

}
	// .globl	_Z17inplace_fft_outerP6float2iiib
.visible .entry _Z17inplace_fft_outerP6float2iiib(
	.param .u64 _Z17inplace_fft_outerP6float2iiib_param_0,
	.param .u32 _Z17inplace_fft_outerP6float2iiib_param_1,
	.param .u32 _Z17inplace_fft_outerP6float2iiib_param_2,
	.param .u32 _Z17inplace_fft_outerP6float2iiib_param_3,
	.param .u8 _Z17inplace_fft_outerP6float2iiib_param_4
)
{
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<2>;
	.reg .f32 	%f<113>;
	.reg .b32 	%r<50>;
	.reg .f64 	%fd<14>;
	.reg .b64 	%rd<32>;


	ld.param.s8 	%rs1, [_Z17inplace_fft_outerP6float2iiib_param_4];
	ld.param.u64 	%rd16, [_Z17inplace_fft_outerP6float2iiib_param_0];
	ld.param.u32 	%r24, [_Z17inplace_fft_outerP6float2iiib_param_2];
	ld.param.u32 	%r25, [_Z17inplace_fft_outerP6float2iiib_param_3];
	ld.param.u32 	%r23, [_Z17inplace_fft_outerP6float2iiib_param_1];
	cvta.to.global.u64 	%rd1, %rd16;
	mov.u32 	%r26, %ctaid.x;
	mov.u32 	%r27, %tid.x;
	mad.lo.s32 	%r28, %r26, %r25, %r27;
	mul.lo.s32 	%r1, %r28, %r23;
	shr.u32 	%r29, %r23, 31;
	add.s32 	%r30, %r23, %r29;
	shr.s32 	%r2, %r30, 1;
	setp.lt.s32 	%p1, %r23, 2;
	@%p1 bra 	$L__BB8_17;

	ld.global.nc.f32 	%f2, [M_PI];
	add.ftz.f32 	%f1, %f2, %f2;
	setp.eq.s16 	%p2, %rs1, 0;
	selp.f64 	%fd2, 0dBFF0000000000000, 0d3FF0000000000000, %p2;
	cvt.rn.f64.s32 	%fd3, %r23;
	mul.f64 	%fd1, %fd2, %fd3;
	and.b32  	%r49, %r2, 3;
	add.s32 	%r32, %r2, -1;
	setp.lt.u32 	%p3, %r32, 3;
	mov.u32 	%r46, 0;
	@%p3 bra 	$L__BB8_12;

	sub.s32 	%r44, %r49, %r2;
	cvt.s64.s32 	%rd17, %r1;
	cvt.s64.s32 	%rd18, %r2;
	add.s64 	%rd19, %rd18, %rd17;
	mul.wide.s32 	%rd20, %r1, 8;
	add.s64 	%rd21, %rd1, %rd20;
	add.s64 	%rd28, %rd21, 16;
	add.s32 	%r43, %r2, %r1;
	add.s32 	%r42, %r43, 3;
	shl.b64 	%rd22, %rd19, 3;
	add.s64 	%rd29, %rd1, %rd22;
	mov.u32 	%r46, 0;

$L__BB8_3:
	add.s64 	%rd7, %rd28, -16;
	setp.ge.s32 	%p4, %r43, %r24;
	@%p4 bra 	$L__BB8_5;

	cvt.rn.f32.s32 	%f3, %r46;
	mul.ftz.f32 	%f4, %f1, %f3;
	cvt.ftz.f64.f32 	%fd4, %f4;
	div.rn.f64 	%fd5, %fd4, %fd1;
	cvt.rn.ftz.f32.f64 	%f5, %fd5;
	cos.approx.ftz.f32 	%f6, %f5;
	sin.approx.ftz.f32 	%f7, %f5;
	ld.global.v2.f32 	{%f8, %f9}, [%rd7];
	ld.global.v2.f32 	{%f12, %f13}, [%rd29];
	mul.ftz.f32 	%f16, %f6, %f12;
	mul.ftz.f32 	%f17, %f7, %f13;
	sub.ftz.f32 	%f18, %f16, %f17;
	mul.ftz.f32 	%f19, %f7, %f12;
	fma.rn.ftz.f32 	%f20, %f6, %f13, %f19;
	add.ftz.f32 	%f21, %f9, %f20;
	add.ftz.f32 	%f22, %f8, %f18;
	st.global.v2.f32 	[%rd7], {%f22, %f21};
	sub.ftz.f32 	%f23, %f9, %f20;
	sub.ftz.f32 	%f24, %f8, %f18;
	st.global.v2.f32 	[%rd29], {%f24, %f23};

$L__BB8_5:
	add.s32 	%r34, %r43, 1;
	setp.ge.s32 	%p5, %r34, %r24;
	@%p5 bra 	$L__BB8_7;

	add.s32 	%r35, %r46, 1;
	cvt.rn.f32.s32 	%f25, %r35;
	mul.ftz.f32 	%f26, %f1, %f25;
	cvt.ftz.f64.f32 	%fd6, %f26;
	div.rn.f64 	%fd7, %fd6, %fd1;
	cvt.rn.ftz.f32.f64 	%f27, %fd7;
	cos.approx.ftz.f32 	%f28, %f27;
	sin.approx.ftz.f32 	%f29, %f27;
	ld.global.v2.f32 	{%f30, %f31}, [%rd7+8];
	ld.global.v2.f32 	{%f34, %f35}, [%rd29+8];
	mul.ftz.f32 	%f38, %f28, %f34;
	mul.ftz.f32 	%f39, %f29, %f35;
	sub.ftz.f32 	%f40, %f38, %f39;
	mul.ftz.f32 	%f41, %f29, %f34;
	fma.rn.ftz.f32 	%f42, %f28, %f35, %f41;
	add.ftz.f32 	%f43, %f31, %f42;
	add.ftz.f32 	%f44, %f30, %f40;
	st.global.v2.f32 	[%rd7+8], {%f44, %f43};
	sub.ftz.f32 	%f45, %f31, %f42;
	sub.ftz.f32 	%f46, %f30, %f40;
	st.global.v2.f32 	[%rd29+8], {%f46, %f45};

$L__BB8_7:
	add.s32 	%r36, %r42, -1;
	setp.ge.s32 	%p6, %r36, %r24;
	@%p6 bra 	$L__BB8_9;

	add.s32 	%r37, %r46, 2;
	cvt.rn.f32.s32 	%f47, %r37;
	mul.ftz.f32 	%f48, %f1, %f47;
	cvt.ftz.f64.f32 	%fd8, %f48;
	div.rn.f64 	%fd9, %fd8, %fd1;
	cvt.rn.ftz.f32.f64 	%f49, %fd9;
	cos.approx.ftz.f32 	%f50, %f49;
	sin.approx.ftz.f32 	%f51, %f49;
	ld.global.v2.f32 	{%f52, %f53}, [%rd7+16];
	ld.global.v2.f32 	{%f56, %f57}, [%rd29+16];
	mul.ftz.f32 	%f60, %f50, %f56;
	mul.ftz.f32 	%f61, %f51, %f57;
	sub.ftz.f32 	%f62, %f60, %f61;
	mul.ftz.f32 	%f63, %f51, %f56;
	fma.rn.ftz.f32 	%f64, %f50, %f57, %f63;
	add.ftz.f32 	%f65, %f53, %f64;
	add.ftz.f32 	%f66, %f52, %f62;
	st.global.v2.f32 	[%rd7+16], {%f66, %f65};
	sub.ftz.f32 	%f67, %f53, %f64;
	sub.ftz.f32 	%f68, %f52, %f62;
	st.global.v2.f32 	[%rd29+16], {%f68, %f67};

$L__BB8_9:
	add.s32 	%r38, %r43, 3;
	setp.ge.s32 	%p7, %r38, %r24;
	@%p7 bra 	$L__BB8_11;

	add.s32 	%r39, %r46, 3;
	cvt.rn.f32.s32 	%f69, %r39;
	mul.ftz.f32 	%f70, %f1, %f69;
	cvt.ftz.f64.f32 	%fd10, %f70;
	div.rn.f64 	%fd11, %fd10, %fd1;
	cvt.rn.ftz.f32.f64 	%f71, %fd11;
	cos.approx.ftz.f32 	%f72, %f71;
	sin.approx.ftz.f32 	%f73, %f71;
	ld.global.v2.f32 	{%f74, %f75}, [%rd7+24];
	ld.global.v2.f32 	{%f78, %f79}, [%rd29+24];
	mul.ftz.f32 	%f82, %f72, %f78;
	mul.ftz.f32 	%f83, %f73, %f79;
	sub.ftz.f32 	%f84, %f82, %f83;
	mul.ftz.f32 	%f85, %f73, %f78;
	fma.rn.ftz.f32 	%f86, %f72, %f79, %f85;
	add.ftz.f32 	%f87, %f75, %f86;
	add.ftz.f32 	%f88, %f74, %f84;
	st.global.v2.f32 	[%rd7+24], {%f88, %f87};
	sub.ftz.f32 	%f89, %f75, %f86;
	sub.ftz.f32 	%f90, %f74, %f84;
	st.global.v2.f32 	[%rd29+24], {%f90, %f89};

$L__BB8_11:
	add.s64 	%rd29, %rd29, 32;
	add.s32 	%r46, %r46, 4;
	add.s32 	%r43, %r43, 4;
	add.s64 	%rd28, %rd28, 32;
	add.s32 	%r42, %r42, 4;
	add.s32 	%r44, %r44, 4;
	setp.ne.s32 	%p8, %r44, 0;
	@%p8 bra 	$L__BB8_3;

$L__BB8_12:
	setp.eq.s32 	%p9, %r49, 0;
	@%p9 bra 	$L__BB8_17;

	cvt.s64.s32 	%rd23, %r2;
	add.s32 	%r40, %r46, %r1;
	cvt.s64.s32 	%rd24, %r40;
	add.s64 	%rd25, %rd23, %rd24;
	shl.b64 	%rd26, %rd25, 3;
	add.s64 	%rd31, %rd1, %rd26;
	mul.wide.s32 	%rd27, %r40, 8;
	add.s64 	%rd30, %rd1, %rd27;
	add.s32 	%r41, %r46, %r2;
	add.s32 	%r47, %r41, %r1;

$L__BB8_14:
	.pragma "nounroll";
	setp.ge.s32 	%p10, %r47, %r24;
	@%p10 bra 	$L__BB8_16;

	cvt.rn.f32.s32 	%f91, %r46;
	mul.ftz.f32 	%f92, %f1, %f91;
	cvt.ftz.f64.f32 	%fd12, %f92;
	div.rn.f64 	%fd13, %fd12, %fd1;
	cvt.rn.ftz.f32.f64 	%f93, %fd13;
	cos.approx.ftz.f32 	%f94, %f93;
	sin.approx.ftz.f32 	%f95, %f93;
	ld.global.v2.f32 	{%f96, %f97}, [%rd30];
	ld.global.v2.f32 	{%f100, %f101}, [%rd31];
	mul.ftz.f32 	%f104, %f94, %f100;
	mul.ftz.f32 	%f105, %f95, %f101;
	sub.ftz.f32 	%f106, %f104, %f105;
	mul.ftz.f32 	%f107, %f95, %f100;
	fma.rn.ftz.f32 	%f108, %f94, %f101, %f107;
	add.ftz.f32 	%f109, %f97, %f108;
	add.ftz.f32 	%f110, %f96, %f106;
	st.global.v2.f32 	[%rd30], {%f110, %f109};
	sub.ftz.f32 	%f111, %f97, %f108;
	sub.ftz.f32 	%f112, %f96, %f106;
	st.global.v2.f32 	[%rd31], {%f112, %f111};

$L__BB8_16:
	add.s32 	%r46, %r46, 1;
	add.s64 	%rd31, %rd31, 8;
	add.s64 	%rd30, %rd30, 8;
	add.s32 	%r47, %r47, 1;
	add.s32 	%r49, %r49, -1;
	setp.ne.s32 	%p11, %r49, 0;
	@%p11 bra 	$L__BB8_14;

$L__BB8_17:
	ret;

}
	// .globl	_ZN4dim3C1Ejjj
.visible .func _ZN4dim3C1Ejjj(
	.param .b64 _ZN4dim3C1Ejjj_param_0,
	.param .b32 _ZN4dim3C1Ejjj_param_1,
	.param .b32 _ZN4dim3C1Ejjj_param_2,
	.param .b32 _ZN4dim3C1Ejjj_param_3
)
{
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<2>;


	ld.param.u64 	%rd1, [_ZN4dim3C1Ejjj_param_0];
	ld.param.u32 	%r1, [_ZN4dim3C1Ejjj_param_1];
	ld.param.u32 	%r2, [_ZN4dim3C1Ejjj_param_2];
	ld.param.u32 	%r3, [_ZN4dim3C1Ejjj_param_3];
	st.u32 	[%rd1], %r1;
	st.u32 	[%rd1+4], %r2;
	st.u32 	[%rd1+8], %r3;
	ret;

}
	// .globl	_ZN4dim3C2Ejjj
.visible .func _ZN4dim3C2Ejjj(
	.param .b64 _ZN4dim3C2Ejjj_param_0,
	.param .b32 _ZN4dim3C2Ejjj_param_1,
	.param .b32 _ZN4dim3C2Ejjj_param_2,
	.param .b32 _ZN4dim3C2Ejjj_param_3
)
{
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<2>;


	ld.param.u64 	%rd1, [_ZN4dim3C2Ejjj_param_0];
	ld.param.u32 	%r1, [_ZN4dim3C2Ejjj_param_1];
	ld.param.u32 	%r2, [_ZN4dim3C2Ejjj_param_2];
	ld.param.u32 	%r3, [_ZN4dim3C2Ejjj_param_3];
	st.u32 	[%rd1], %r1;
	st.u32 	[%rd1+4], %r2;
	st.u32 	[%rd1+8], %r3;
	ret;

}

