//
// Generated by LLVM NVPTX Back-End
//

.version 7.0
.target sm_80
.address_size 64

	// .globl	_ZN81_$LT$nonphysical_ptx$$shared$$primitive$$F32$u20$as$u20$core$$ops$$arith$$Rem$GT$3rem17h47656ded776ad7efE
.visible .func _ZN4core5slice5index22slice_index_order_fail17h464a34c2ded0572eE
(
.param .b64 _ZN4core5slice5index22slice_index_order_fail17h464a34c2ded0572eE_param_0,
	.param .b64 _ZN4core5slice5index22slice_index_order_fail17h464a34c2ded0572eE_param_1,
	.param .b64 _ZN4core5slice5index22slice_index_order_fail17h464a34c2ded0572eE_param_2
)
.noreturn{
	trap;
	exit;
}
.visible .func _ZN4core9panicking5panic17hc7c8a74e6511bb99E
(
.param .b64 _ZN4core9panicking5panic17hc7c8a74e6511bb99E_param_0,
	.param .b64 _ZN4core9panicking5panic17hc7c8a74e6511bb99E_param_1,
	.param .b64 _ZN4core9panicking5panic17hc7c8a74e6511bb99E_param_2
)
.noreturn{
	trap;
	exit;
}
.visible .func _ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E
(
.param .b64 _ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E_param_0
)
.noreturn{
	trap;
	exit;
}
.visible .func _ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E
(
.param .b64 _ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E_param_0,
	.param .b64 _ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E_param_1,
	.param .b64 _ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E_param_2
)
.noreturn{
	trap;
	exit;
}
.visible .func _ZN4core9panicking11panic_const23panic_const_rem_by_zero17h18083b86e5b8e9ccE
(
.param .b64 _ZN4core9panicking11panic_const23panic_const_rem_by_zero17h18083b86e5b8e9ccE_param_0
)
.noreturn{
	trap;
	exit;
}
.visible .func _ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E
(
.param .b64 _ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E_param_0
)
.noreturn{
	trap;
	exit;
}
.global .align 1 .b8 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_0[78] = {47, 114, 117, 115, 116, 99, 47, 56, 101, 56, 54, 99, 57, 53, 54, 55, 49, 53, 52, 100, 99, 53, 97, 57, 97, 100, 97, 49, 53, 97, 98, 49, 57, 54, 100, 50, 51, 101, 97, 101, 50, 98, 100, 55, 100, 56, 57, 47, 108, 105, 98, 114, 97, 114, 121, 47, 99, 111, 114, 101, 47, 115, 114, 99, 47, 115, 108, 105, 99, 101, 47, 105, 116, 101, 114, 46, 114, 115};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_1[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_0), 78, 128849020424};
.global .align 1 .b8 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_2[35] = {97, 115, 115, 101, 114, 116, 105, 111, 110, 32, 102, 97, 105, 108, 101, 100, 58, 32, 109, 105, 100, 32, 60, 61, 32, 115, 101, 108, 102, 46, 108, 101, 110, 40, 41};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_3[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_0), 78, 180388628163};
.global .align 1 .b8 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4[19] = {110, 111, 116, 32, 121, 101, 116, 32, 105, 109, 112, 108, 101, 109, 101, 110, 116, 101, 100};
.global .align 1 .b8 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_5[35] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 115, 104, 97, 114, 101, 100, 92, 102, 108, 111, 97, 116, 46, 114, 115};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_6[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_5), 35, 38654705867};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_7[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_5), 35, 38654705872};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_8[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_5), 35, 38654705877};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_9[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_5), 35, 38654705940};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_10[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_5), 35, 38654705945};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_11[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_5), 35, 38654705950};
.global .align 1 .b8 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_12[39] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 115, 104, 97, 114, 101, 100, 92, 112, 114, 105, 109, 105, 116, 105, 118, 101, 46, 114, 115};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_13[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_12), 39, 38654706006};
.global .align 1 .b8 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_14[38] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 115, 104, 97, 114, 101, 100, 92, 117, 110, 115, 105, 103, 110, 101, 100, 46, 114, 115};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_15[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_14), 38, 60129542290};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_16[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_14), 38, 38654705833};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_17[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_14), 38, 60129542519};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_18[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_14), 38, 38654706062};
.global .align 1 .b8 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_19[40] = {97, 115, 115, 101, 114, 116, 105, 111, 110, 32, 102, 97, 105, 108, 101, 100, 58, 32, 105, 110, 100, 101, 120, 32, 60, 32, 115, 101, 108, 102, 46, 112, 116, 114, 46, 108, 101, 110, 40, 41};
.global .align 1 .b8 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_20[41] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 99, 117, 100, 97, 92, 103, 108, 111, 98, 97, 108, 92, 97, 116, 111, 109, 105, 99, 46, 114, 115};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_21[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_20), 41, 38654705859};
.global .align 1 .b8 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22[47] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 115, 104, 97, 114, 101, 100, 92, 118, 101, 99, 116, 111, 114, 92, 112, 116, 120, 95, 118, 101, 99, 116, 111, 114, 46, 114, 115};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_23[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 21474836543};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_24[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 21474836590};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_25[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 21474836639};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_26[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 21474836686};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_27[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 21474836692};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_28[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 21474836698};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_29[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 73014444256};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_30[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 73014444267};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_31[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 73014444277};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_32[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 73014444288};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_33[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 73014444300};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_34[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 73014444313};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_35[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 73014444326};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_36[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 73014444557};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_37[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 73014444566};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_38[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 73014444574};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_39[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 73014444583};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_40[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 73014444592};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_41[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 73014444601};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_42[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 73014444610};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_43[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 21474837288};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_44[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_22), 47, 21474837293};
.global .align 1 .b8 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_45[54] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 103, 114, 97, 112, 104, 92, 109, 101, 114, 103, 101, 95, 115, 111, 114, 116, 92, 112, 116, 120, 95, 109, 101, 114, 103, 101, 95, 115, 111, 114, 116, 46, 114, 115};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_46[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_45), 54, 107374182510};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_47[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_45), 54, 85899346032};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_48[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_45), 54, 270582939765};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_49[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_45), 54, 287762808950};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_50[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_45), 54, 227633266807};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_51[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_45), 54, 73014444159};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_52[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_45), 54, 73014444160};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_53[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_45), 54, 73014444161};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_54[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_45), 54, 42949673149};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_55[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_45), 54, 42949673185};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_56[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_45), 54, 55834575129};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_57[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_45), 54, 21474836857};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_58[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_45), 54, 68719477103};
.global .align 1 .b8 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_59[54] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 103, 114, 97, 112, 104, 92, 104, 97, 115, 104, 95, 116, 97, 98, 108, 101, 92, 112, 116, 120, 95, 104, 97, 115, 104, 95, 116, 97, 98, 108, 101, 46, 114, 115};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_60[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_59), 54, 73014444068};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_61[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_59), 54, 68719476791};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_62[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_59), 54, 111669149752};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_63[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_59), 54, 68719476813};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_64[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_59), 54, 73014444110};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_65[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_20), 41, 38654706152};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_66[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_20), 41, 38654706042};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_67[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_59), 54, 55834574995};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_68[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_59), 54, 55834574996};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_69[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_20), 41, 38654705779};
.global .align 1 .b8 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_70[27] = {97, 115, 115, 101, 114, 116, 105, 111, 110, 32, 102, 97, 105, 108, 101, 100, 58, 32, 105, 110, 100, 101, 120, 32, 60, 32, 78};
.global .align 1 .b8 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_71[34] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 99, 117, 100, 97, 92, 115, 104, 97, 114, 101, 100, 46, 114, 115};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_72[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_71), 34, 38654705726};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_73[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_71), 34, 38654705742};
.global .align 1 .b8 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_74[27] = {97, 115, 115, 101, 114, 116, 105, 111, 110, 32, 102, 97, 105, 108, 101, 100, 58, 32, 115, 116, 101, 112, 32, 33, 61, 32, 48};
.global .align 1 .b8 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_75[89] = {47, 114, 117, 115, 116, 99, 47, 56, 101, 56, 54, 99, 57, 53, 54, 55, 49, 53, 52, 100, 99, 53, 97, 57, 97, 100, 97, 49, 53, 97, 98, 49, 57, 54, 100, 50, 51, 101, 97, 101, 50, 98, 100, 55, 100, 56, 57, 47, 108, 105, 98, 114, 97, 114, 121, 47, 99, 111, 114, 101, 47, 115, 114, 99, 47, 105, 116, 101, 114, 47, 97, 100, 97, 112, 116, 101, 114, 115, 47, 115, 116, 101, 112, 95, 98, 121, 46, 114, 115};
.global .align 8 .u64 anon_$_9f9aa6c1f4ee146dd317953f26141824_$_76[3] = {generic(anon_$_9f9aa6c1f4ee146dd317953f26141824_$_75), 89, 38654705699};

.visible .func  (.param .b32 func_retval0) _ZN81_$LT$nonphysical_ptx$$shared$$primitive$$F32$u20$as$u20$core$$ops$$arith$$Rem$GT$3rem17h47656ded776ad7efE(
	.param .b32 _ZN81_$LT$nonphysical_ptx$$shared$$primitive$$F32$u20$as$u20$core$$ops$$arith$$Rem$GT$3rem17h47656ded776ad7efE_param_0,
	.param .b32 _ZN81_$LT$nonphysical_ptx$$shared$$primitive$$F32$u20$as$u20$core$$ops$$arith$$Rem$GT$3rem17h47656ded776ad7efE_param_1
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_13;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 0, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 0

}
	// .globl	_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E
.visible .func  (.param .b32 func_retval0) _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E(
	.param .b32 _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E_param_0,
	.param .b32 _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<3>;

	ld.param.u32 	%r2, [_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E_param_1];
	setp.eq.s32 	%p1, %r2, 0;
	@%p1 bra 	$L__BB1_2;
	ld.param.u32 	%r1, [_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E_param_0];
	div.u32 	%r3, %r1, %r2;
	st.param.b32 	[func_retval0+0], %r3;
	ret;
$L__BB1_2:
	mov.u64 	%rd1, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_15;
	cvta.global.u64 	%rd2, %rd1;
	{ // callseq 1, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 1

}
	// .globl	_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E
.visible .func _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E(
	.param .b64 _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E_param_0,
	.param .b32 _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<4>;

	ld.param.u32 	%r1, [_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E_param_1];
	setp.eq.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB2_2;
	ld.param.u64 	%rd1, [_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E_param_0];
	ld.u32 	%r2, [%rd1];
	div.u32 	%r3, %r2, %r1;
	st.u32 	[%rd1], %r3;
	ret;
$L__BB2_2:
	mov.u64 	%rd2, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_16;
	cvta.global.u64 	%rd3, %rd2;
	{ // callseq 2, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd3;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 2

}
	// .globl	_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE
.visible .func  (.param .b64 func_retval0) _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE(
	.param .b64 _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE_param_0,
	.param .b64 _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd5, [_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE_param_1];
	setp.eq.s64 	%p1, %rd5, 0;
	@%p1 bra 	$L__BB3_5;
	ld.param.u64 	%rd4, [_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE_param_0];
	or.b64  	%rd6, %rd4, %rd5;
	and.b64  	%rd7, %rd6, -4294967296;
	setp.ne.s64 	%p2, %rd7, 0;
	@%p2 bra 	$L__BB3_3;
	bra.uni 	$L__BB3_2;
$L__BB3_3:
	div.u64 	%rd10, %rd4, %rd5;
	bra.uni 	$L__BB3_4;
$L__BB3_2:
	cvt.u32.u64 	%r1, %rd5;
	cvt.u32.u64 	%r2, %rd4;
	div.u32 	%r3, %r2, %r1;
	cvt.u64.u32 	%rd10, %r3;
$L__BB3_4:
	st.param.b64 	[func_retval0+0], %rd10;
	ret;
$L__BB3_5:
	mov.u64 	%rd8, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_17;
	cvta.global.u64 	%rd9, %rd8;
	{ // callseq 3, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd9;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 3

}
	// .globl	_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE
.visible .func _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE(
	.param .b64 _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE_param_0,
	.param .b64 _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<12>;

	ld.param.u64 	%rd6, [_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE_param_1];
	setp.eq.s64 	%p1, %rd6, 0;
	@%p1 bra 	$L__BB4_5;
	ld.param.u64 	%rd5, [_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE_param_0];
	ld.u64 	%rd1, [%rd5];
	or.b64  	%rd7, %rd1, %rd6;
	and.b64  	%rd8, %rd7, -4294967296;
	setp.ne.s64 	%p2, %rd8, 0;
	@%p2 bra 	$L__BB4_3;
	bra.uni 	$L__BB4_2;
$L__BB4_3:
	div.u64 	%rd11, %rd1, %rd6;
	bra.uni 	$L__BB4_4;
$L__BB4_2:
	cvt.u32.u64 	%r1, %rd6;
	cvt.u32.u64 	%r2, %rd1;
	div.u32 	%r3, %r2, %r1;
	cvt.u64.u32 	%rd11, %r3;
$L__BB4_4:
	st.u64 	[%rd5], %rd11;
	ret;
$L__BB4_5:
	mov.u64 	%rd9, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_18;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 4, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd10;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 4

}
	// .globl	vector_sum_f32
.visible .entry vector_sum_f32(
	.param .u64 vector_sum_f32_param_0
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<107>;
	.reg .f32 	%f<16>;
	.reg .b64 	%rd<54>;

	ld.param.u64 	%rd23, [vector_sum_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd23;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	mov.u32 	%r6, %tid.x;
	cvt.u64.u32 	%rd4, %r6;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	shl.b64 	%rd24, %rd3, 2;
	add.s64 	%rd5, %rd2, %rd24;
	mul.wide.u32 	%rd25, %r7, %r1;
	add.s64 	%rd6, %rd25, %rd4;
	// begin inline asm
	.shared .align 4 .b8 nonphysical[128];
    mov.u32 %r5, nonphysical;
	// end inline asm
	setp.eq.s64 	%p1, %rd6, 0;
	@%p1 bra 	$L__BB5_1;
	setp.gt.u64 	%p2, %rd3, %rd6;
	shl.b64 	%rd26, %rd6, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd52, %rd28, %rd5, %p2;
	selp.b64 	%rd53, %rd27, 0, %p2;
	bra.uni 	$L__BB5_6;
$L__BB5_1:
	setp.eq.s64 	%p3, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p3;
	add.s64 	%rd52, %rd2, %rd29;
	selp.b64 	%rd53, 0, %rd2, %p3;
$L__BB5_6:
	setp.eq.s64 	%p4, %rd53, 0;
	mov.f32 	%f13, 0f00000000;
	@%p4 bra 	$L__BB5_8;
	mov.u32 	%r8, %nctaid.x;
	mul.wide.u32 	%rd7, %r1, %r8;
	add.s64 	%rd8, %rd7, -1;
	ld.u32 	%r11, [%rd53];
	mov.b32 	%r10, 0;
	// begin inline asm
	add.rn.ftz.f32 %r9, %r10, %r11;
	// end inline asm
	mov.b32 	%f13, %r9;
	sub.s64 	%rd30, %rd5, %rd52;
	shr.u64 	%rd31, %rd30, 2;
	setp.gt.u64 	%p5, %rd31, %rd8;
	@%p5 bra 	$L__BB5_2;
	bra.uni 	$L__BB5_8;
$L__BB5_2:
	shl.b64 	%rd32, %rd7, 2;
	add.s64 	%rd33, %rd52, %rd32;
	add.s64 	%rd34, %rd33, -4;
	ld.u32 	%r14, [%rd33+-4];
	// begin inline asm
	add.rn.ftz.f32 %r106, %r9, %r14;
	// end inline asm
	mov.b32 	%f13, %r106;
	sub.s64 	%rd35, %rd5, %rd34;
	add.s64 	%rd36, %rd35, -4;
	shr.u64 	%rd37, %rd36, 2;
	setp.le.u64 	%p6, %rd37, %rd8;
	@%p6 bra 	$L__BB5_8;
	shl.b64 	%rd40, %rd7, 3;
	sub.s64 	%rd41, %rd5, %rd40;
	sub.s64 	%rd51, %rd41, %rd52;
	add.s64 	%rd42, %rd40, %rd52;
	add.s64 	%rd50, %rd42, -4;
$L__BB5_4:
	ld.u32 	%r17, [%rd50];
	// begin inline asm
	add.rn.ftz.f32 %r15, %r106, %r17;
	// end inline asm
	mov.b32 	%f13, %r15;
	shr.u64 	%rd43, %rd51, 2;
	setp.gt.u64 	%p7, %rd43, %rd8;
	sub.s64 	%rd51, %rd51, %rd32;
	add.s64 	%rd50, %rd50, %rd32;
	mov.u32 	%r106, %r15;
	@%p7 bra 	$L__BB5_4;
$L__BB5_8:
	cvt.u32.u64 	%r53, %rd4;
	mov.b32 	%r21, %f13;
	mov.b32 	%r19, 16;
	mov.b32 	%r67, 31;
	// begin inline asm
	shfl.sync.bfly.b32 %r18,%r21, %r19, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r28, %r21, %r18;
	// end inline asm
	mov.b32 	%r26, 8;
	// begin inline asm
	shfl.sync.bfly.b32 %r25,%r28, %r26, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r35, %r28, %r25;
	// end inline asm
	mov.b32 	%r33, 4;
	// begin inline asm
	shfl.sync.bfly.b32 %r32,%r35, %r33, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r42, %r35, %r32;
	// end inline asm
	mov.b32 	%r40, 2;
	// begin inline asm
	shfl.sync.bfly.b32 %r39,%r42, %r40, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r49, %r42, %r39;
	// end inline asm
	mov.b32 	%r47, 1;
	// begin inline asm
	shfl.sync.bfly.b32 %r46,%r49, %r47, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r104, %r49, %r46;
	// end inline asm
	and.b64  	%rd22, %rd4, 31;
	setp.ne.s64 	%p8, %rd22, 0;
	@%p8 bra 	$L__BB5_10;
	mov.b32 	%f6, %r104;
	shr.u32 	%r54, %r53, 3;
	and.b32  	%r55, %r54, 124;
	add.s32 	%r56, %r5, %r55;
	// begin inline asm
	st.shared.f32 [%r56], %r104;
	// end inline asm
$L__BB5_10:
	bar.sync 	0;
	shr.u32 	%r59, %r1, 5;
	setp.ge.u32 	%p9, %r53, %r59;
	mov.f32 	%f14, 0f00000000;
	@%p9 bra 	$L__BB5_12;
	cvt.u32.u64 	%r60, %rd22;
	shl.b32 	%r61, %r60, 2;
	add.s32 	%r63, %r5, %r61;
	// begin inline asm
	ld.shared.f32 %r62, [%r63];
	// end inline asm
	mov.b32 	%f14, %r62;
$L__BB5_12:
	setp.gt.u32 	%p10, %r53, 31;
	@%p10 bra 	$L__BB5_14;
	setp.eq.s64 	%p11, %rd22, 0;
	mov.b32 	%r68, %f14;
	// begin inline asm
	shfl.sync.bfly.b32 %r65,%r68, %r19, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r75, %r68, %r65;
	// end inline asm
	// begin inline asm
	shfl.sync.bfly.b32 %r72,%r75, %r26, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r82, %r75, %r72;
	// end inline asm
	// begin inline asm
	shfl.sync.bfly.b32 %r79,%r82, %r33, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r89, %r82, %r79;
	// end inline asm
	// begin inline asm
	shfl.sync.bfly.b32 %r86,%r89, %r40, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r96, %r89, %r86;
	// end inline asm
	// begin inline asm
	shfl.sync.bfly.b32 %r93,%r96, %r47, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r97, %r96, %r93;
	// end inline asm
	@%p11 bra 	$L__BB5_15;
	bra.uni 	$L__BB5_14;
$L__BB5_15:
	ld.global.nc.u64 	%rd44, [%rd1+24];
	setp.ne.s64 	%p12, %rd44, 0;
	@%p12 bra 	$L__BB5_17;
	bra.uni 	$L__BB5_16;
$L__BB5_17:
	mov.b32 	%f9, %r97;
	ld.global.nc.u64 	%rd45, [%rd1+16];
	// begin inline asm
	red.global.add.f32 [%rd45], %r97;
	// end inline asm
$L__BB5_14:
	ret;
$L__BB5_16:
	mov.u64 	%rd46, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_19;
	cvta.global.u64 	%rd47, %rd46;
	mov.u64 	%rd48, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_21;
	cvta.global.u64 	%rd49, %rd48;
	{ // callseq 5, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd47;
	.param .b64 param1;
	st.param.b64 	[param1+0], 40;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd49;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 5

}
	// .globl	vector_product_f32
.visible .entry vector_product_f32(
	.param .u64 vector_product_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_23;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 6, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 6

}
	// .globl	vector_greater_f32
.visible .entry vector_greater_f32(
	.param .u64 vector_greater_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_24;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 7, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 7

}
	// .globl	vector_lesser_f32
.visible .entry vector_lesser_f32(
	.param .u64 vector_lesser_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_25;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 8, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 8

}
	// .globl	vector_mean_f32
.visible .entry vector_mean_f32(
	.param .u64 vector_mean_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_26;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 9, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 9

}
	// .globl	vector_variance_f32
.visible .entry vector_variance_f32(
	.param .u64 vector_variance_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_27;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 10, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 10

}
	// .globl	vector_deviation_f32
.visible .entry vector_deviation_f32(
	.param .u64 vector_deviation_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_28;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 11, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 11

}
	// .globl	vector_add_f32
.visible .entry vector_add_f32(
	.param .u64 vector_add_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_add_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB12_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB12_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB12_4;
	bra.uni 	$L__BB12_3;
$L__BB12_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB12_5;
$L__BB12_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB12_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB12_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB12_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB12_9;
	bra.uni 	$L__BB12_8;
$L__BB12_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB12_10;
$L__BB12_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB12_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB12_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB12_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB12_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB12_15;
$L__BB12_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB12_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB12_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB12_18;
$L__BB12_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB12_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB12_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	add.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB12_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB12_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB12_28;
	bra.uni 	$L__BB12_22;
$L__BB12_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB12_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB12_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	add.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB12_31:
	st.f32 	[%rd53], %f17;
$L__BB12_32:
	ret;
$L__BB12_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB12_23;
$L__BB12_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB12_23;
	bra.uni 	$L__BB12_28;
$L__BB12_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB12_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	add.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB12_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB12_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	add.rn.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB12_27;
$L__BB12_33:
	mov.u64 	%rd99, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_29;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 12, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 12

}
	// .globl	vector_sub_f32
.visible .entry vector_sub_f32(
	.param .u64 vector_sub_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_sub_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB13_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB13_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB13_4;
	bra.uni 	$L__BB13_3;
$L__BB13_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB13_5;
$L__BB13_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB13_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB13_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB13_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB13_9;
	bra.uni 	$L__BB13_8;
$L__BB13_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB13_10;
$L__BB13_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB13_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB13_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB13_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB13_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB13_15;
$L__BB13_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB13_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB13_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB13_18;
$L__BB13_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB13_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB13_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	sub.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB13_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB13_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB13_28;
	bra.uni 	$L__BB13_22;
$L__BB13_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB13_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB13_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	sub.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB13_31:
	st.f32 	[%rd53], %f17;
$L__BB13_32:
	ret;
$L__BB13_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB13_23;
$L__BB13_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB13_23;
	bra.uni 	$L__BB13_28;
$L__BB13_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB13_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	sub.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB13_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB13_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	sub.rn.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB13_27;
$L__BB13_33:
	mov.u64 	%rd99, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_30;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 13, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 13

}
	// .globl	vector_mul_f32
.visible .entry vector_mul_f32(
	.param .u64 vector_mul_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_mul_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB14_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB14_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB14_4;
	bra.uni 	$L__BB14_3;
$L__BB14_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB14_5;
$L__BB14_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB14_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB14_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB14_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB14_9;
	bra.uni 	$L__BB14_8;
$L__BB14_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB14_10;
$L__BB14_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB14_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB14_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB14_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB14_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB14_15;
$L__BB14_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB14_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB14_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB14_18;
$L__BB14_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB14_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB14_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB14_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB14_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB14_28;
	bra.uni 	$L__BB14_22;
$L__BB14_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB14_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB14_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB14_31:
	st.f32 	[%rd53], %f17;
$L__BB14_32:
	ret;
$L__BB14_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB14_23;
$L__BB14_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB14_23;
	bra.uni 	$L__BB14_28;
$L__BB14_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB14_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB14_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB14_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB14_27;
$L__BB14_33:
	mov.u64 	%rd99, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_31;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 14, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 14

}
	// .globl	vector_div_f32
.visible .entry vector_div_f32(
	.param .u64 vector_div_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_div_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB15_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB15_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB15_4;
	bra.uni 	$L__BB15_3;
$L__BB15_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB15_5;
$L__BB15_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB15_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB15_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB15_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB15_9;
	bra.uni 	$L__BB15_8;
$L__BB15_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB15_10;
$L__BB15_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB15_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB15_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB15_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB15_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB15_15;
$L__BB15_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB15_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB15_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB15_18;
$L__BB15_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB15_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB15_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB15_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB15_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB15_28;
	bra.uni 	$L__BB15_22;
$L__BB15_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB15_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB15_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB15_31:
	st.f32 	[%rd53], %f17;
$L__BB15_32:
	ret;
$L__BB15_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB15_23;
$L__BB15_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB15_23;
	bra.uni 	$L__BB15_28;
$L__BB15_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB15_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB15_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB15_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB15_27;
$L__BB15_33:
	mov.u64 	%rd99, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_32;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 15, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 15

}
	// .globl	vector_scale_f32
.visible .entry vector_scale_f32(
	.param .u64 vector_scale_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_scale_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB16_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB16_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB16_4;
	bra.uni 	$L__BB16_3;
$L__BB16_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB16_5;
$L__BB16_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB16_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB16_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB16_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB16_9;
	bra.uni 	$L__BB16_8;
$L__BB16_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB16_10;
$L__BB16_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB16_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB16_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB16_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB16_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB16_15;
$L__BB16_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB16_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB16_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB16_18;
$L__BB16_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB16_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB16_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB16_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB16_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB16_28;
	bra.uni 	$L__BB16_22;
$L__BB16_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB16_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB16_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB16_31:
	st.f32 	[%rd53], %f17;
$L__BB16_32:
	ret;
$L__BB16_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB16_23;
$L__BB16_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB16_23;
	bra.uni 	$L__BB16_28;
$L__BB16_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB16_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB16_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB16_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB16_27;
$L__BB16_33:
	mov.u64 	%rd99, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_33;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 16, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 16

}
	// .globl	vector_descale_f32
.visible .entry vector_descale_f32(
	.param .u64 vector_descale_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_descale_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB17_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB17_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB17_4;
	bra.uni 	$L__BB17_3;
$L__BB17_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB17_5;
$L__BB17_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB17_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB17_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB17_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB17_9;
	bra.uni 	$L__BB17_8;
$L__BB17_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB17_10;
$L__BB17_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB17_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB17_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB17_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB17_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB17_15;
$L__BB17_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB17_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB17_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB17_18;
$L__BB17_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB17_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB17_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB17_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB17_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB17_28;
	bra.uni 	$L__BB17_22;
$L__BB17_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB17_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB17_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB17_31:
	st.f32 	[%rd53], %f17;
$L__BB17_32:
	ret;
$L__BB17_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB17_23;
$L__BB17_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB17_23;
	bra.uni 	$L__BB17_28;
$L__BB17_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB17_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB17_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB17_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB17_27;
$L__BB17_33:
	mov.u64 	%rd99, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_34;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 17, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 17

}
	// .globl	vector_powf_f32
.visible .entry vector_powf_f32(
	.param .u64 vector_powf_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_powf_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB18_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB18_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB18_4;
	bra.uni 	$L__BB18_3;
$L__BB18_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB18_5;
$L__BB18_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB18_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB18_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB18_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB18_9;
	bra.uni 	$L__BB18_8;
$L__BB18_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB18_10;
$L__BB18_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB18_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB18_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB18_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB18_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB18_15;
$L__BB18_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB18_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB18_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB18_18;
$L__BB18_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB18_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB18_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	lg2.approx.ftz.f32 %r11, %r12;
    mul.rn.ftz.f32 %r11, %r11, %r13;
    ex2.approx.ftz.f32 %r11, %r11;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB18_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB18_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB18_28;
	bra.uni 	$L__BB18_22;
$L__BB18_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB18_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB18_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	lg2.approx.ftz.f32 %r20, %r21;
    mul.rn.ftz.f32 %r20, %r20, %r22;
    ex2.approx.ftz.f32 %r20, %r20;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB18_31:
	st.f32 	[%rd53], %f17;
$L__BB18_32:
	ret;
$L__BB18_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB18_23;
$L__BB18_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB18_23;
	bra.uni 	$L__BB18_28;
$L__BB18_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB18_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	lg2.approx.ftz.f32 %r14, %r15;
    mul.rn.ftz.f32 %r14, %r14, %r16;
    ex2.approx.ftz.f32 %r14, %r14;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB18_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB18_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	lg2.approx.ftz.f32 %r17, %r18;
    mul.rn.ftz.f32 %r17, %r17, %r16;
    ex2.approx.ftz.f32 %r17, %r17;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB18_27;
$L__BB18_33:
	mov.u64 	%rd99, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_35;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 18, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 18

}
	// .globl	vector_ln_f32
.visible .entry vector_ln_f32(
	.param .u64 vector_ln_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_ln_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB19_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB19_3;
	bra.uni 	$L__BB19_2;
$L__BB19_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB19_4;
$L__BB19_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB19_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB19_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB19_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB19_8;
	bra.uni 	$L__BB19_7;
$L__BB19_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB19_9;
$L__BB19_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB19_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB19_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB19_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB19_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB19_14;
$L__BB19_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB19_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB19_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB19_17;
$L__BB19_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB19_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB19_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	lg2.approx.ftz.f32 %r11, %r12;
    mul.rn.ftz.f32 %r11, %r11, 0f3F317218;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB19_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB19_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB19_27;
	bra.uni 	$L__BB19_21;
$L__BB19_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB19_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB19_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	lg2.approx.ftz.f32 %r17, %r18;
    mul.rn.ftz.f32 %r17, %r17, 0f3F317218;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB19_30:
	st.f32 	[%rd52], %f16;
$L__BB19_31:
	ret;
$L__BB19_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB19_22;
$L__BB19_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB19_22;
	bra.uni 	$L__BB19_27;
$L__BB19_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB19_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	lg2.approx.ftz.f32 %r13, %r14;
    mul.rn.ftz.f32 %r13, %r13, 0f3F317218;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB19_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB19_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	lg2.approx.ftz.f32 %r15, %r16;
    mul.rn.ftz.f32 %r15, %r15, 0f3F317218;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB19_26;

}
	// .globl	vector_log2_f32
.visible .entry vector_log2_f32(
	.param .u64 vector_log2_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_log2_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB20_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB20_3;
	bra.uni 	$L__BB20_2;
$L__BB20_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB20_4;
$L__BB20_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB20_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB20_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB20_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB20_8;
	bra.uni 	$L__BB20_7;
$L__BB20_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB20_9;
$L__BB20_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB20_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB20_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB20_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB20_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB20_14;
$L__BB20_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB20_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB20_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB20_17;
$L__BB20_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB20_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB20_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	lg2.approx.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB20_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB20_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB20_27;
	bra.uni 	$L__BB20_21;
$L__BB20_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB20_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB20_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	lg2.approx.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB20_30:
	st.f32 	[%rd52], %f16;
$L__BB20_31:
	ret;
$L__BB20_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB20_22;
$L__BB20_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB20_22;
	bra.uni 	$L__BB20_27;
$L__BB20_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB20_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	lg2.approx.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB20_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB20_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	lg2.approx.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB20_26;

}
	// .globl	vector_exp_f32
.visible .entry vector_exp_f32(
	.param .u64 vector_exp_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_exp_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB21_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB21_3;
	bra.uni 	$L__BB21_2;
$L__BB21_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB21_4;
$L__BB21_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB21_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB21_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB21_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB21_8;
	bra.uni 	$L__BB21_7;
$L__BB21_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB21_9;
$L__BB21_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB21_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB21_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB21_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB21_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB21_14;
$L__BB21_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB21_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB21_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB21_17;
$L__BB21_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB21_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB21_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r11, %r11;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB21_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB21_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB21_27;
	bra.uni 	$L__BB21_21;
$L__BB21_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB21_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB21_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r17, %r17;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB21_30:
	st.f32 	[%rd52], %f16;
$L__BB21_31:
	ret;
$L__BB21_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB21_22;
$L__BB21_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB21_22;
	bra.uni 	$L__BB21_27;
$L__BB21_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB21_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	mul.rn.ftz.f32 %r13, %r14, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r13, %r13;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB21_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB21_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	mul.rn.ftz.f32 %r15, %r16, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r15, %r15;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB21_26;

}
	// .globl	vector_exp2_f32
.visible .entry vector_exp2_f32(
	.param .u64 vector_exp2_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_exp2_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB22_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB22_3;
	bra.uni 	$L__BB22_2;
$L__BB22_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB22_4;
$L__BB22_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB22_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB22_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB22_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB22_8;
	bra.uni 	$L__BB22_7;
$L__BB22_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB22_9;
$L__BB22_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB22_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB22_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB22_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB22_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB22_14;
$L__BB22_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB22_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB22_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB22_17;
$L__BB22_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB22_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB22_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	ex2.approx.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB22_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB22_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB22_27;
	bra.uni 	$L__BB22_21;
$L__BB22_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB22_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB22_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	ex2.approx.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB22_30:
	st.f32 	[%rd52], %f16;
$L__BB22_31:
	ret;
$L__BB22_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB22_22;
$L__BB22_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB22_22;
	bra.uni 	$L__BB22_27;
$L__BB22_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB22_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	ex2.approx.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB22_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB22_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	ex2.approx.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB22_26;

}
	// .globl	vector_recip_f32
.visible .entry vector_recip_f32(
	.param .u64 vector_recip_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_recip_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB23_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB23_3;
	bra.uni 	$L__BB23_2;
$L__BB23_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB23_4;
$L__BB23_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB23_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB23_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB23_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB23_8;
	bra.uni 	$L__BB23_7;
$L__BB23_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB23_9;
$L__BB23_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB23_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB23_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB23_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB23_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB23_14;
$L__BB23_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB23_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB23_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB23_17;
$L__BB23_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB23_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB23_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	rcp.approx.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB23_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB23_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB23_27;
	bra.uni 	$L__BB23_21;
$L__BB23_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB23_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB23_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	rcp.approx.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB23_30:
	st.f32 	[%rd52], %f16;
$L__BB23_31:
	ret;
$L__BB23_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB23_22;
$L__BB23_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB23_22;
	bra.uni 	$L__BB23_27;
$L__BB23_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB23_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	rcp.approx.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB23_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB23_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	rcp.approx.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB23_26;

}
	// .globl	vector_sin_f32
.visible .entry vector_sin_f32(
	.param .u64 vector_sin_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_sin_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB24_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB24_3;
	bra.uni 	$L__BB24_2;
$L__BB24_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB24_4;
$L__BB24_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB24_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB24_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB24_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB24_8;
	bra.uni 	$L__BB24_7;
$L__BB24_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB24_9;
$L__BB24_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB24_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB24_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB24_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB24_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB24_14;
$L__BB24_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB24_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB24_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB24_17;
$L__BB24_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB24_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB24_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	sin.approx.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB24_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB24_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB24_27;
	bra.uni 	$L__BB24_21;
$L__BB24_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB24_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB24_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	sin.approx.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB24_30:
	st.f32 	[%rd52], %f16;
$L__BB24_31:
	ret;
$L__BB24_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB24_22;
$L__BB24_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB24_22;
	bra.uni 	$L__BB24_27;
$L__BB24_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB24_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	sin.approx.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB24_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB24_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	sin.approx.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB24_26;

}
	// .globl	vector_cos_f32
.visible .entry vector_cos_f32(
	.param .u64 vector_cos_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_cos_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB25_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB25_3;
	bra.uni 	$L__BB25_2;
$L__BB25_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB25_4;
$L__BB25_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB25_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB25_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB25_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB25_8;
	bra.uni 	$L__BB25_7;
$L__BB25_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB25_9;
$L__BB25_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB25_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB25_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB25_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB25_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB25_14;
$L__BB25_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB25_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB25_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB25_17;
$L__BB25_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB25_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB25_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	cos.approx.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB25_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB25_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB25_27;
	bra.uni 	$L__BB25_21;
$L__BB25_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB25_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB25_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	cos.approx.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB25_30:
	st.f32 	[%rd52], %f16;
$L__BB25_31:
	ret;
$L__BB25_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB25_22;
$L__BB25_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB25_22;
	bra.uni 	$L__BB25_27;
$L__BB25_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB25_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	cos.approx.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB25_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB25_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	cos.approx.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB25_26;

}
	// .globl	vector_tan_f32
.visible .entry vector_tan_f32(
	.param .u64 vector_tan_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_tan_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB26_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB26_3;
	bra.uni 	$L__BB26_2;
$L__BB26_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB26_4;
$L__BB26_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB26_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB26_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB26_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB26_8;
	bra.uni 	$L__BB26_7;
$L__BB26_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB26_9;
$L__BB26_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB26_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB26_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB26_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB26_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB26_14;
$L__BB26_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB26_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB26_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB26_17;
$L__BB26_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB26_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB26_19;
	ld.u32 	%r13, [%rd106];
	// begin inline asm
	sin.approx.ftz.f32 %r12, %r13;
    cos.approx.ftz.f32 %r11, %r13;
    div.approx.ftz.f32 %r12, %r12, %r11;
	// end inline asm
	mov.b32 	%f13, %r12;
$L__BB26_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB26_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB26_27;
	bra.uni 	$L__BB26_21;
$L__BB26_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB26_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB26_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r22, [%rd53+-4];
	// begin inline asm
	sin.approx.ftz.f32 %r21, %r22;
    cos.approx.ftz.f32 %r20, %r22;
    div.approx.ftz.f32 %r21, %r21, %r20;
	// end inline asm
	mov.b32 	%f16, %r21;
$L__BB26_30:
	st.f32 	[%rd52], %f16;
$L__BB26_31:
	ret;
$L__BB26_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB26_22;
$L__BB26_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB26_22;
	bra.uni 	$L__BB26_27;
$L__BB26_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB26_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r16, [%rd43];
	// begin inline asm
	sin.approx.ftz.f32 %r15, %r16;
    cos.approx.ftz.f32 %r14, %r16;
    div.approx.ftz.f32 %r15, %r15, %r14;
	// end inline asm
	mov.b32 	%f14, %r15;
$L__BB26_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB26_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r19, [%rd47];
	// begin inline asm
	sin.approx.ftz.f32 %r18, %r19;
    cos.approx.ftz.f32 %r17, %r19;
    div.approx.ftz.f32 %r18, %r18, %r17;
	// end inline asm
	mov.b32 	%f15, %r18;
	bra.uni 	$L__BB26_26;

}
	// .globl	vector_asin_f32
.visible .entry vector_asin_f32(
	.param .u64 vector_asin_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_asin_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB27_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB27_3;
	bra.uni 	$L__BB27_2;
$L__BB27_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB27_4;
$L__BB27_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB27_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB27_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB27_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB27_8;
	bra.uni 	$L__BB27_7;
$L__BB27_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB27_9;
$L__BB27_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB27_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB27_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB27_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB27_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB27_16;
	bra.uni 	$L__BB27_14;
$L__BB27_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB27_16;
	bra.uni 	$L__BB27_14;
$L__BB27_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB27_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB27_17;
	bra.uni 	$L__BB27_16;
$L__BB27_17:
	ret;
$L__BB27_16:
	mov.u64 	%rd45, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_6;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 19, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 19

}
	// .globl	vector_acos_f32
.visible .entry vector_acos_f32(
	.param .u64 vector_acos_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_acos_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB28_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB28_3;
	bra.uni 	$L__BB28_2;
$L__BB28_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB28_4;
$L__BB28_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB28_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB28_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB28_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB28_8;
	bra.uni 	$L__BB28_7;
$L__BB28_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB28_9;
$L__BB28_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB28_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB28_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB28_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB28_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB28_16;
	bra.uni 	$L__BB28_14;
$L__BB28_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB28_16;
	bra.uni 	$L__BB28_14;
$L__BB28_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB28_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB28_17;
	bra.uni 	$L__BB28_16;
$L__BB28_17:
	ret;
$L__BB28_16:
	mov.u64 	%rd45, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_7;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 20, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 20

}
	// .globl	vector_atan_f32
.visible .entry vector_atan_f32(
	.param .u64 vector_atan_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_atan_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB29_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB29_3;
	bra.uni 	$L__BB29_2;
$L__BB29_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB29_4;
$L__BB29_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB29_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB29_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB29_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB29_8;
	bra.uni 	$L__BB29_7;
$L__BB29_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB29_9;
$L__BB29_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB29_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB29_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB29_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB29_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB29_16;
	bra.uni 	$L__BB29_14;
$L__BB29_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB29_16;
	bra.uni 	$L__BB29_14;
$L__BB29_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB29_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB29_17;
	bra.uni 	$L__BB29_16;
$L__BB29_17:
	ret;
$L__BB29_16:
	mov.u64 	%rd45, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_8;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 21, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 21

}
	// .globl	vector_sinh_f32
.visible .entry vector_sinh_f32(
	.param .u64 vector_sinh_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_sinh_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB30_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB30_3;
	bra.uni 	$L__BB30_2;
$L__BB30_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB30_4;
$L__BB30_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB30_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB30_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB30_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB30_8;
	bra.uni 	$L__BB30_7;
$L__BB30_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB30_9;
$L__BB30_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB30_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB30_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB30_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB30_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB30_14;
$L__BB30_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB30_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB30_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB30_17;
$L__BB30_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB30_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB30_19;
	ld.u32 	%r13, [%rd106];
	// begin inline asm
	mul.rn.ftz.f32 %r12, %r13, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r12, %r12;
    neg.ftz.f32 %r11, %r13;
    mul.rn.ftz.f32 %r11, %r11, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r11, %r11;
    sub.rn.ftz.f32 %r12, %r12, %r11;
    div.approx.ftz.f32 %r12, %r12, 0f40000000;
	// end inline asm
	mov.b32 	%f13, %r12;
$L__BB30_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB30_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB30_27;
	bra.uni 	$L__BB30_21;
$L__BB30_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB30_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB30_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r22, [%rd53+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r21, %r22, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r21, %r21;
    neg.ftz.f32 %r20, %r22;
    mul.rn.ftz.f32 %r20, %r20, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r20, %r20;
    sub.rn.ftz.f32 %r21, %r21, %r20;
    div.approx.ftz.f32 %r21, %r21, 0f40000000;
	// end inline asm
	mov.b32 	%f16, %r21;
$L__BB30_30:
	st.f32 	[%rd52], %f16;
$L__BB30_31:
	ret;
$L__BB30_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB30_22;
$L__BB30_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB30_22;
	bra.uni 	$L__BB30_27;
$L__BB30_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB30_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r16, [%rd43];
	// begin inline asm
	mul.rn.ftz.f32 %r15, %r16, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r15, %r15;
    neg.ftz.f32 %r14, %r16;
    mul.rn.ftz.f32 %r14, %r14, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r14, %r14;
    sub.rn.ftz.f32 %r15, %r15, %r14;
    div.approx.ftz.f32 %r15, %r15, 0f40000000;
	// end inline asm
	mov.b32 	%f14, %r15;
$L__BB30_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB30_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r19, [%rd47];
	// begin inline asm
	mul.rn.ftz.f32 %r18, %r19, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r18, %r18;
    neg.ftz.f32 %r17, %r19;
    mul.rn.ftz.f32 %r17, %r17, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r17, %r17;
    sub.rn.ftz.f32 %r18, %r18, %r17;
    div.approx.ftz.f32 %r18, %r18, 0f40000000;
	// end inline asm
	mov.b32 	%f15, %r18;
	bra.uni 	$L__BB30_26;

}
	// .globl	vector_cosh_f32
.visible .entry vector_cosh_f32(
	.param .u64 vector_cosh_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_cosh_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB31_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB31_3;
	bra.uni 	$L__BB31_2;
$L__BB31_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB31_4;
$L__BB31_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB31_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB31_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB31_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB31_8;
	bra.uni 	$L__BB31_7;
$L__BB31_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB31_9;
$L__BB31_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB31_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB31_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB31_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB31_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB31_14;
$L__BB31_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB31_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB31_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB31_17;
$L__BB31_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB31_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB31_19;
	ld.u32 	%r13, [%rd106];
	// begin inline asm
	mul.rn.ftz.f32 %r12, %r13, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r12, %r12;
    neg.ftz.f32 %r11, %r13;
    mul.rn.ftz.f32 %r11, %r11, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r11, %r11;
    add.rn.ftz.f32 %r12, %r12, %r11;
    div.approx.ftz.f32 %r12, %r12, 0f40000000;
	// end inline asm
	mov.b32 	%f13, %r12;
$L__BB31_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB31_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB31_27;
	bra.uni 	$L__BB31_21;
$L__BB31_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB31_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB31_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r22, [%rd53+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r21, %r22, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r21, %r21;
    neg.ftz.f32 %r20, %r22;
    mul.rn.ftz.f32 %r20, %r20, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r20, %r20;
    add.rn.ftz.f32 %r21, %r21, %r20;
    div.approx.ftz.f32 %r21, %r21, 0f40000000;
	// end inline asm
	mov.b32 	%f16, %r21;
$L__BB31_30:
	st.f32 	[%rd52], %f16;
$L__BB31_31:
	ret;
$L__BB31_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB31_22;
$L__BB31_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB31_22;
	bra.uni 	$L__BB31_27;
$L__BB31_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB31_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r16, [%rd43];
	// begin inline asm
	mul.rn.ftz.f32 %r15, %r16, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r15, %r15;
    neg.ftz.f32 %r14, %r16;
    mul.rn.ftz.f32 %r14, %r14, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r14, %r14;
    add.rn.ftz.f32 %r15, %r15, %r14;
    div.approx.ftz.f32 %r15, %r15, 0f40000000;
	// end inline asm
	mov.b32 	%f14, %r15;
$L__BB31_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB31_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r19, [%rd47];
	// begin inline asm
	mul.rn.ftz.f32 %r18, %r19, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r18, %r18;
    neg.ftz.f32 %r17, %r19;
    mul.rn.ftz.f32 %r17, %r17, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r17, %r17;
    add.rn.ftz.f32 %r18, %r18, %r17;
    div.approx.ftz.f32 %r18, %r18, 0f40000000;
	// end inline asm
	mov.b32 	%f15, %r18;
	bra.uni 	$L__BB31_26;

}
	// .globl	vector_tanh_f32
.visible .entry vector_tanh_f32(
	.param .u64 vector_tanh_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_tanh_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB32_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB32_3;
	bra.uni 	$L__BB32_2;
$L__BB32_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB32_4;
$L__BB32_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB32_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB32_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB32_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB32_8;
	bra.uni 	$L__BB32_7;
$L__BB32_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB32_9;
$L__BB32_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB32_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB32_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB32_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB32_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB32_14;
$L__BB32_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB32_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB32_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB32_17;
$L__BB32_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB32_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB32_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	tanh.approx.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB32_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB32_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB32_27;
	bra.uni 	$L__BB32_21;
$L__BB32_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB32_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB32_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	tanh.approx.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB32_30:
	st.f32 	[%rd52], %f16;
$L__BB32_31:
	ret;
$L__BB32_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB32_22;
$L__BB32_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB32_22;
	bra.uni 	$L__BB32_27;
$L__BB32_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB32_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	tanh.approx.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB32_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB32_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	tanh.approx.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB32_26;

}
	// .globl	vector_asinh_f32
.visible .entry vector_asinh_f32(
	.param .u64 vector_asinh_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_asinh_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB33_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB33_3;
	bra.uni 	$L__BB33_2;
$L__BB33_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB33_4;
$L__BB33_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB33_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB33_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB33_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB33_8;
	bra.uni 	$L__BB33_7;
$L__BB33_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB33_9;
$L__BB33_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB33_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB33_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB33_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB33_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB33_16;
	bra.uni 	$L__BB33_14;
$L__BB33_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB33_16;
	bra.uni 	$L__BB33_14;
$L__BB33_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB33_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB33_17;
	bra.uni 	$L__BB33_16;
$L__BB33_17:
	ret;
$L__BB33_16:
	mov.u64 	%rd45, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_9;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 22, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 22

}
	// .globl	vector_acosh_f32
.visible .entry vector_acosh_f32(
	.param .u64 vector_acosh_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_acosh_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB34_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB34_3;
	bra.uni 	$L__BB34_2;
$L__BB34_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB34_4;
$L__BB34_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB34_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB34_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB34_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB34_8;
	bra.uni 	$L__BB34_7;
$L__BB34_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB34_9;
$L__BB34_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB34_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB34_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB34_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB34_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB34_16;
	bra.uni 	$L__BB34_14;
$L__BB34_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB34_16;
	bra.uni 	$L__BB34_14;
$L__BB34_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB34_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB34_17;
	bra.uni 	$L__BB34_16;
$L__BB34_17:
	ret;
$L__BB34_16:
	mov.u64 	%rd45, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_10;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 23, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 23

}
	// .globl	vector_atanh_f32
.visible .entry vector_atanh_f32(
	.param .u64 vector_atanh_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_atanh_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB35_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB35_3;
	bra.uni 	$L__BB35_2;
$L__BB35_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB35_4;
$L__BB35_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB35_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB35_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB35_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB35_8;
	bra.uni 	$L__BB35_7;
$L__BB35_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB35_9;
$L__BB35_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB35_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB35_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB35_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB35_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB35_16;
	bra.uni 	$L__BB35_14;
$L__BB35_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB35_16;
	bra.uni 	$L__BB35_14;
$L__BB35_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB35_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB35_17;
	bra.uni 	$L__BB35_16;
$L__BB35_17:
	ret;
$L__BB35_16:
	mov.u64 	%rd45, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_11;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 24, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 24

}
	// .globl	vector_l1_norm_f32
.visible .entry vector_l1_norm_f32(
	.param .u64 vector_l1_norm_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_l1_norm_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB36_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB36_3;
	bra.uni 	$L__BB36_2;
$L__BB36_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB36_4;
$L__BB36_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB36_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB36_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB36_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB36_8;
	bra.uni 	$L__BB36_7;
$L__BB36_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB36_9;
$L__BB36_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB36_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB36_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB36_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB36_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB36_14;
$L__BB36_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB36_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB36_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB36_17;
$L__BB36_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB36_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB36_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	abs.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB36_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB36_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB36_27;
	bra.uni 	$L__BB36_21;
$L__BB36_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB36_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB36_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	abs.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB36_30:
	st.f32 	[%rd52], %f16;
$L__BB36_31:
	ret;
$L__BB36_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB36_22;
$L__BB36_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB36_22;
	bra.uni 	$L__BB36_27;
$L__BB36_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB36_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	abs.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB36_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB36_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	abs.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB36_26;

}
	// .globl	vector_l2_norm_f32
.visible .entry vector_l2_norm_f32(
	.param .u64 vector_l2_norm_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_l2_norm_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB37_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB37_3;
	bra.uni 	$L__BB37_2;
$L__BB37_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB37_4;
$L__BB37_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB37_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB37_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB37_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB37_8;
	bra.uni 	$L__BB37_7;
$L__BB37_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB37_9;
$L__BB37_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB37_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB37_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB37_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB37_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB37_14;
$L__BB37_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB37_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB37_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB37_17;
$L__BB37_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB37_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB37_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB37_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB37_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB37_27;
	bra.uni 	$L__BB37_21;
$L__BB37_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB37_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB37_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB37_30:
	st.f32 	[%rd52], %f16;
$L__BB37_31:
	ret;
$L__BB37_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB37_22;
$L__BB37_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB37_22;
	bra.uni 	$L__BB37_27;
$L__BB37_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB37_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	mul.rn.ftz.f32 %r13, %r14, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB37_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB37_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	mul.rn.ftz.f32 %r15, %r16, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB37_26;

}
	// .globl	vector_add_ref_f32
.visible .entry vector_add_ref_f32(
	.param .u64 vector_add_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_add_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB38_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB38_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB38_4;
$L__BB38_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB38_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB38_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	add.rn.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB38_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB38_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	add.rn.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB38_7;
$L__BB38_8:
	ret;
$L__BB38_9:
	mov.u64 	%rd37, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_36;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 25, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 25

}
	// .globl	vector_sub_ref_f32
.visible .entry vector_sub_ref_f32(
	.param .u64 vector_sub_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_sub_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB39_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB39_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB39_4;
$L__BB39_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB39_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB39_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	sub.rn.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB39_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB39_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	sub.rn.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB39_7;
$L__BB39_8:
	ret;
$L__BB39_9:
	mov.u64 	%rd37, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_37;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 26, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 26

}
	// .globl	vector_mul_ref_f32
.visible .entry vector_mul_ref_f32(
	.param .u64 vector_mul_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_mul_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB40_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB40_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB40_4;
$L__BB40_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB40_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB40_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB40_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB40_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	mul.rn.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB40_7;
$L__BB40_8:
	ret;
$L__BB40_9:
	mov.u64 	%rd37, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_38;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 27, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 27

}
	// .globl	vector_div_ref_f32
.visible .entry vector_div_ref_f32(
	.param .u64 vector_div_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_div_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB41_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB41_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB41_4;
$L__BB41_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB41_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB41_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB41_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB41_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	div.approx.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB41_7;
$L__BB41_8:
	ret;
$L__BB41_9:
	mov.u64 	%rd37, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_39;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 28, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 28

}
	// .globl	vector_scale_ref_f32
.visible .entry vector_scale_ref_f32(
	.param .u64 vector_scale_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_scale_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB42_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB42_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB42_4;
$L__BB42_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB42_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB42_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB42_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB42_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	mul.rn.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB42_7;
$L__BB42_8:
	ret;
$L__BB42_9:
	mov.u64 	%rd37, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_40;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 29, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 29

}
	// .globl	vector_descale_ref_f32
.visible .entry vector_descale_ref_f32(
	.param .u64 vector_descale_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_descale_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB43_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB43_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB43_4;
$L__BB43_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB43_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB43_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB43_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB43_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	div.approx.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB43_7;
$L__BB43_8:
	ret;
$L__BB43_9:
	mov.u64 	%rd37, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_41;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 30, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 30

}
	// .globl	vector_powf_ref_f32
.visible .entry vector_powf_ref_f32(
	.param .u64 vector_powf_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_powf_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB44_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB44_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB44_4;
$L__BB44_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB44_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB44_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	lg2.approx.ftz.f32 %r5, %r6;
    mul.rn.ftz.f32 %r5, %r5, %r7;
    ex2.approx.ftz.f32 %r5, %r5;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB44_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB44_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	lg2.approx.ftz.f32 %r8, %r9;
    mul.rn.ftz.f32 %r8, %r8, %r7;
    ex2.approx.ftz.f32 %r8, %r8;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB44_7;
$L__BB44_8:
	ret;
$L__BB44_9:
	mov.u64 	%rd37, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_42;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 31, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 31

}
	// .globl	vector_ln_ref_f32
.visible .entry vector_ln_ref_f32(
	.param .u64 vector_ln_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_ln_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB45_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB45_3;
$L__BB45_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB45_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB45_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	lg2.approx.ftz.f32 %r5, %r6;
    mul.rn.ftz.f32 %r5, %r5, 0f3F317218;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB45_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB45_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	lg2.approx.ftz.f32 %r7, %r8;
    mul.rn.ftz.f32 %r7, %r7, 0f3F317218;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB45_6;
$L__BB45_7:
	ret;

}
	// .globl	vector_log2_ref_f32
.visible .entry vector_log2_ref_f32(
	.param .u64 vector_log2_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_log2_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB46_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB46_3;
$L__BB46_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB46_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB46_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	lg2.approx.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB46_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB46_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	lg2.approx.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB46_6;
$L__BB46_7:
	ret;

}
	// .globl	vector_exp_ref_f32
.visible .entry vector_exp_ref_f32(
	.param .u64 vector_exp_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_exp_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB47_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB47_3;
$L__BB47_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB47_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB47_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	mul.rn.ftz.f32 %r5, %r6, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r5, %r5;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB47_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB47_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	mul.rn.ftz.f32 %r7, %r8, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r7, %r7;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB47_6;
$L__BB47_7:
	ret;

}
	// .globl	vector_exp2_ref_f32
.visible .entry vector_exp2_ref_f32(
	.param .u64 vector_exp2_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_exp2_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB48_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB48_3;
$L__BB48_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB48_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB48_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	ex2.approx.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB48_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB48_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	ex2.approx.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB48_6;
$L__BB48_7:
	ret;

}
	// .globl	vector_recip_ref_f32
.visible .entry vector_recip_ref_f32(
	.param .u64 vector_recip_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_recip_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB49_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB49_3;
$L__BB49_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB49_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB49_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	rcp.approx.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB49_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB49_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	rcp.approx.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB49_6;
$L__BB49_7:
	ret;

}
	// .globl	vector_sin_ref_f32
.visible .entry vector_sin_ref_f32(
	.param .u64 vector_sin_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_sin_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB50_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB50_3;
$L__BB50_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB50_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB50_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	sin.approx.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB50_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB50_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	sin.approx.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB50_6;
$L__BB50_7:
	ret;

}
	// .globl	vector_cos_ref_f32
.visible .entry vector_cos_ref_f32(
	.param .u64 vector_cos_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_cos_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB51_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB51_3;
$L__BB51_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB51_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB51_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	cos.approx.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB51_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB51_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	cos.approx.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB51_6;
$L__BB51_7:
	ret;

}
	// .globl	vector_tan_ref_f32
.visible .entry vector_tan_ref_f32(
	.param .u64 vector_tan_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_tan_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB52_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB52_3;
$L__BB52_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB52_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB52_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r7, [%rd38];
	// begin inline asm
	sin.approx.ftz.f32 %r6, %r7;
    cos.approx.ftz.f32 %r5, %r7;
    div.approx.ftz.f32 %r6, %r6, %r5;
	// end inline asm
	st.u32 	[%rd38], %r6;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB52_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB52_6:
	ld.u32 	%r10, [%rd35];
	// begin inline asm
	sin.approx.ftz.f32 %r9, %r10;
    cos.approx.ftz.f32 %r8, %r10;
    div.approx.ftz.f32 %r9, %r9, %r8;
	// end inline asm
	st.u32 	[%rd35], %r9;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB52_6;
$L__BB52_7:
	ret;

}
	// .globl	vector_asin_ref_f32
.visible .entry vector_asin_ref_f32(
	.param .u64 vector_asin_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_asin_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB53_3;
	bra.uni 	$L__BB53_1;
$L__BB53_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB53_2;
	bra.uni 	$L__BB53_4;
$L__BB53_2:
	ret;
$L__BB53_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB53_2;
$L__BB53_4:
	mov.u64 	%rd7, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_6;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 32, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 32

}
	// .globl	vector_acos_ref_f32
.visible .entry vector_acos_ref_f32(
	.param .u64 vector_acos_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_acos_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB54_3;
	bra.uni 	$L__BB54_1;
$L__BB54_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB54_2;
	bra.uni 	$L__BB54_4;
$L__BB54_2:
	ret;
$L__BB54_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB54_2;
$L__BB54_4:
	mov.u64 	%rd7, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_7;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 33, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 33

}
	// .globl	vector_atan_ref_f32
.visible .entry vector_atan_ref_f32(
	.param .u64 vector_atan_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_atan_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB55_3;
	bra.uni 	$L__BB55_1;
$L__BB55_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB55_2;
	bra.uni 	$L__BB55_4;
$L__BB55_2:
	ret;
$L__BB55_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB55_2;
$L__BB55_4:
	mov.u64 	%rd7, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_8;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 34, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 34

}
	// .globl	vector_sinh_ref_f32
.visible .entry vector_sinh_ref_f32(
	.param .u64 vector_sinh_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_sinh_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB56_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB56_3;
$L__BB56_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB56_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB56_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r7, [%rd38];
	// begin inline asm
	mul.rn.ftz.f32 %r6, %r7, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r6, %r6;
    neg.ftz.f32 %r5, %r7;
    mul.rn.ftz.f32 %r5, %r5, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r5, %r5;
    sub.rn.ftz.f32 %r6, %r6, %r5;
    div.approx.ftz.f32 %r6, %r6, 0f40000000;
	// end inline asm
	st.u32 	[%rd38], %r6;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB56_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB56_6:
	ld.u32 	%r10, [%rd35];
	// begin inline asm
	mul.rn.ftz.f32 %r9, %r10, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r9, %r9;
    neg.ftz.f32 %r8, %r10;
    mul.rn.ftz.f32 %r8, %r8, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r8, %r8;
    sub.rn.ftz.f32 %r9, %r9, %r8;
    div.approx.ftz.f32 %r9, %r9, 0f40000000;
	// end inline asm
	st.u32 	[%rd35], %r9;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB56_6;
$L__BB56_7:
	ret;

}
	// .globl	vector_cosh_ref_f32
.visible .entry vector_cosh_ref_f32(
	.param .u64 vector_cosh_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_cosh_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB57_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB57_3;
$L__BB57_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB57_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB57_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r7, [%rd38];
	// begin inline asm
	mul.rn.ftz.f32 %r6, %r7, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r6, %r6;
    neg.ftz.f32 %r5, %r7;
    mul.rn.ftz.f32 %r5, %r5, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r5, %r5;
    add.rn.ftz.f32 %r6, %r6, %r5;
    div.approx.ftz.f32 %r6, %r6, 0f40000000;
	// end inline asm
	st.u32 	[%rd38], %r6;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB57_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB57_6:
	ld.u32 	%r10, [%rd35];
	// begin inline asm
	mul.rn.ftz.f32 %r9, %r10, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r9, %r9;
    neg.ftz.f32 %r8, %r10;
    mul.rn.ftz.f32 %r8, %r8, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r8, %r8;
    add.rn.ftz.f32 %r9, %r9, %r8;
    div.approx.ftz.f32 %r9, %r9, 0f40000000;
	// end inline asm
	st.u32 	[%rd35], %r9;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB57_6;
$L__BB57_7:
	ret;

}
	// .globl	vector_tanh_ref_f32
.visible .entry vector_tanh_ref_f32(
	.param .u64 vector_tanh_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_tanh_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB58_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB58_3;
$L__BB58_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB58_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB58_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	tanh.approx.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB58_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB58_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	tanh.approx.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB58_6;
$L__BB58_7:
	ret;

}
	// .globl	vector_asinh_ref_f32
.visible .entry vector_asinh_ref_f32(
	.param .u64 vector_asinh_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_asinh_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB59_3;
	bra.uni 	$L__BB59_1;
$L__BB59_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB59_2;
	bra.uni 	$L__BB59_4;
$L__BB59_2:
	ret;
$L__BB59_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB59_2;
$L__BB59_4:
	mov.u64 	%rd7, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_9;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 35, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 35

}
	// .globl	vector_acosh_ref_f32
.visible .entry vector_acosh_ref_f32(
	.param .u64 vector_acosh_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_acosh_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB60_3;
	bra.uni 	$L__BB60_1;
$L__BB60_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB60_2;
	bra.uni 	$L__BB60_4;
$L__BB60_2:
	ret;
$L__BB60_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB60_2;
$L__BB60_4:
	mov.u64 	%rd7, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_10;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 36, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 36

}
	// .globl	vector_atanh_ref_f32
.visible .entry vector_atanh_ref_f32(
	.param .u64 vector_atanh_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_atanh_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB61_3;
	bra.uni 	$L__BB61_1;
$L__BB61_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB61_2;
	bra.uni 	$L__BB61_4;
$L__BB61_2:
	ret;
$L__BB61_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB61_2;
$L__BB61_4:
	mov.u64 	%rd7, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_11;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 37, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 37

}
	// .globl	vector_add_vec_f32
.visible .entry vector_add_vec_f32(
	.param .u64 vector_add_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_add_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB62_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB62_3;
	bra.uni 	$L__BB62_2;
$L__BB62_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB62_4;
$L__BB62_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB62_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB62_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB62_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB62_8;
	bra.uni 	$L__BB62_7;
$L__BB62_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB62_9;
$L__BB62_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB62_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB62_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB62_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB62_13;
	bra.uni 	$L__BB62_12;
$L__BB62_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB62_14;
$L__BB62_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB62_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB62_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB62_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB62_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB62_19;
$L__BB62_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB62_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB62_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB62_22;
$L__BB62_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB62_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB62_27;
	@%p11 bra 	$L__BB62_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB62_26;
$L__BB62_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB62_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB62_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB62_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	add.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB62_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB62_38;
	bra.uni 	$L__BB62_30;
$L__BB62_38:
	ret;
$L__BB62_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB62_31;
$L__BB62_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB62_31;
	bra.uni 	$L__BB62_38;
$L__BB62_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB62_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB62_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB62_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB62_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB62_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	add.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB62_37;

}
	// .globl	vector_sub_vec_f32
.visible .entry vector_sub_vec_f32(
	.param .u64 vector_sub_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_sub_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB63_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB63_3;
	bra.uni 	$L__BB63_2;
$L__BB63_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB63_4;
$L__BB63_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB63_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB63_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB63_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB63_8;
	bra.uni 	$L__BB63_7;
$L__BB63_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB63_9;
$L__BB63_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB63_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB63_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB63_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB63_13;
	bra.uni 	$L__BB63_12;
$L__BB63_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB63_14;
$L__BB63_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB63_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB63_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB63_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB63_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB63_19;
$L__BB63_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB63_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB63_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB63_22;
$L__BB63_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB63_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB63_27;
	@%p11 bra 	$L__BB63_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB63_26;
$L__BB63_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB63_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB63_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB63_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	sub.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB63_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB63_38;
	bra.uni 	$L__BB63_30;
$L__BB63_38:
	ret;
$L__BB63_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB63_31;
$L__BB63_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB63_31;
	bra.uni 	$L__BB63_38;
$L__BB63_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB63_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB63_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB63_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB63_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB63_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	sub.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB63_37;

}
	// .globl	vector_mul_vec_f32
.visible .entry vector_mul_vec_f32(
	.param .u64 vector_mul_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_mul_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB64_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB64_3;
	bra.uni 	$L__BB64_2;
$L__BB64_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB64_4;
$L__BB64_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB64_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB64_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB64_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB64_8;
	bra.uni 	$L__BB64_7;
$L__BB64_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB64_9;
$L__BB64_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB64_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB64_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB64_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB64_13;
	bra.uni 	$L__BB64_12;
$L__BB64_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB64_14;
$L__BB64_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB64_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB64_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB64_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB64_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB64_19;
$L__BB64_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB64_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB64_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB64_22;
$L__BB64_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB64_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB64_27;
	@%p11 bra 	$L__BB64_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB64_26;
$L__BB64_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB64_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB64_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB64_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB64_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB64_38;
	bra.uni 	$L__BB64_30;
$L__BB64_38:
	ret;
$L__BB64_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB64_31;
$L__BB64_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB64_31;
	bra.uni 	$L__BB64_38;
$L__BB64_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB64_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB64_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB64_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB64_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB64_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB64_37;

}
	// .globl	vector_div_vec_f32
.visible .entry vector_div_vec_f32(
	.param .u64 vector_div_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_div_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB65_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB65_3;
	bra.uni 	$L__BB65_2;
$L__BB65_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB65_4;
$L__BB65_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB65_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB65_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB65_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB65_8;
	bra.uni 	$L__BB65_7;
$L__BB65_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB65_9;
$L__BB65_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB65_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB65_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB65_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB65_13;
	bra.uni 	$L__BB65_12;
$L__BB65_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB65_14;
$L__BB65_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB65_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB65_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB65_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB65_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB65_19;
$L__BB65_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB65_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB65_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB65_22;
$L__BB65_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB65_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB65_27;
	@%p11 bra 	$L__BB65_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB65_26;
$L__BB65_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB65_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB65_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB65_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB65_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB65_38;
	bra.uni 	$L__BB65_30;
$L__BB65_38:
	ret;
$L__BB65_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB65_31;
$L__BB65_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB65_31;
	bra.uni 	$L__BB65_38;
$L__BB65_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB65_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB65_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB65_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB65_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB65_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB65_37;

}
	// .globl	vector_powf_vec_f32
.visible .entry vector_powf_vec_f32(
	.param .u64 vector_powf_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_powf_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB66_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB66_3;
	bra.uni 	$L__BB66_2;
$L__BB66_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB66_4;
$L__BB66_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB66_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB66_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB66_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB66_8;
	bra.uni 	$L__BB66_7;
$L__BB66_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB66_9;
$L__BB66_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB66_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB66_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB66_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB66_13;
	bra.uni 	$L__BB66_12;
$L__BB66_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB66_14;
$L__BB66_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB66_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB66_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB66_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB66_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB66_19;
$L__BB66_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB66_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB66_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB66_22;
$L__BB66_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB66_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB66_27;
	@%p11 bra 	$L__BB66_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB66_26;
$L__BB66_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB66_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB66_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB66_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	lg2.approx.ftz.f32 %r14, %r15;
    mul.rn.ftz.f32 %r14, %r14, %r16;
    ex2.approx.ftz.f32 %r14, %r14;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB66_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB66_38;
	bra.uni 	$L__BB66_30;
$L__BB66_38:
	ret;
$L__BB66_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB66_31;
$L__BB66_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB66_31;
	bra.uni 	$L__BB66_38;
$L__BB66_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB66_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB66_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB66_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB66_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB66_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	lg2.approx.ftz.f32 %r17, %r18;
    mul.rn.ftz.f32 %r17, %r17, %r19;
    ex2.approx.ftz.f32 %r17, %r17;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB66_37;

}
	// .globl	vector_greater_vec_f32
.visible .entry vector_greater_vec_f32(
	.param .u64 vector_greater_vec_f32_param_0
)
{
	.reg .pred 	%p<36>;
	.reg .b32 	%r<14>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_greater_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB67_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB67_3;
	bra.uni 	$L__BB67_2;
$L__BB67_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB67_4;
$L__BB67_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB67_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB67_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB67_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB67_8;
	bra.uni 	$L__BB67_7;
$L__BB67_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB67_9;
$L__BB67_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB67_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB67_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB67_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB67_13;
	bra.uni 	$L__BB67_12;
$L__BB67_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB67_14;
$L__BB67_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB67_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB67_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB67_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB67_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB67_19;
$L__BB67_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB67_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB67_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB67_22;
$L__BB67_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB67_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p35, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p35 bra 	$L__BB67_27;
	@%p11 bra 	$L__BB67_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB67_26;
$L__BB67_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB67_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB67_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB67_29;
	ld.f32 	%f6, [%rd146];
	ld.f32 	%f7, [%rd145];
	setp.gt.f32 	%p22, %f6, %f7;
	selp.f32 	%f11, %f6, %f7, %p22;
$L__BB67_29:
	st.f32 	[%rd138], %f11;
	setp.eq.s64 	%p23, %rd31, 1;
	@%p23 bra 	$L__BB67_38;
	bra.uni 	$L__BB67_30;
$L__BB67_38:
	ret;
$L__BB67_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p28, 0;
	bra.uni 	$L__BB67_31;
$L__BB67_37:
	selp.b64 	%rd148, %rd119, %rd12, %p24;
	selp.b64 	%rd62, %rd120, 0, %p24;
	selp.b64 	%rd149, %rd123, %rd3, %p26;
	st.f32 	[%rd62], %f12;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p33, %rd147, 0;
	@%p33 bra 	$L__BB67_31;
	bra.uni 	$L__BB67_38;
$L__BB67_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p25, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p25 bra 	$L__BB67_35;
	selp.b64 	%rd152, 0, %rd6, %p35;
	setp.eq.s64 	%p27, %rd154, 0;
	@%p27 bra 	$L__BB67_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB67_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p29, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p29;
	selp.b64 	%rd155, %rd128, 0, %p29;
	setp.eq.s64 	%p30, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p30;
	mov.u64 	%rd154, 0;
	mov.pred 	%p35, %p28;
$L__BB67_35:
	setp.gt.u64 	%p24, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p26, %rd122, %rd6;
	setp.eq.s64 	%p31, %rd156, 0;
	@%p31 bra 	$L__BB67_37;
	ld.f32 	%f9, [%rd156];
	ld.f32 	%f10, [%rd155];
	setp.gt.f32 	%p32, %f9, %f10;
	selp.f32 	%f12, %f9, %f10, %p32;
	bra.uni 	$L__BB67_37;

}
	// .globl	vector_lesser_vec_f32
.visible .entry vector_lesser_vec_f32(
	.param .u64 vector_lesser_vec_f32_param_0
)
{
	.reg .pred 	%p<36>;
	.reg .b32 	%r<14>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_lesser_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB68_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB68_3;
	bra.uni 	$L__BB68_2;
$L__BB68_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB68_4;
$L__BB68_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB68_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB68_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB68_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB68_8;
	bra.uni 	$L__BB68_7;
$L__BB68_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB68_9;
$L__BB68_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB68_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB68_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB68_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB68_13;
	bra.uni 	$L__BB68_12;
$L__BB68_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB68_14;
$L__BB68_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB68_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB68_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB68_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB68_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB68_19;
$L__BB68_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB68_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB68_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB68_22;
$L__BB68_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB68_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p35, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p35 bra 	$L__BB68_27;
	@%p11 bra 	$L__BB68_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB68_26;
$L__BB68_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB68_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB68_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB68_29;
	ld.f32 	%f6, [%rd146];
	ld.f32 	%f7, [%rd145];
	setp.lt.f32 	%p22, %f6, %f7;
	selp.f32 	%f11, %f6, %f7, %p22;
$L__BB68_29:
	st.f32 	[%rd138], %f11;
	setp.eq.s64 	%p23, %rd31, 1;
	@%p23 bra 	$L__BB68_38;
	bra.uni 	$L__BB68_30;
$L__BB68_38:
	ret;
$L__BB68_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p28, 0;
	bra.uni 	$L__BB68_31;
$L__BB68_37:
	selp.b64 	%rd148, %rd119, %rd12, %p24;
	selp.b64 	%rd62, %rd120, 0, %p24;
	selp.b64 	%rd149, %rd123, %rd3, %p26;
	st.f32 	[%rd62], %f12;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p33, %rd147, 0;
	@%p33 bra 	$L__BB68_31;
	bra.uni 	$L__BB68_38;
$L__BB68_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p25, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p25 bra 	$L__BB68_35;
	selp.b64 	%rd152, 0, %rd6, %p35;
	setp.eq.s64 	%p27, %rd154, 0;
	@%p27 bra 	$L__BB68_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB68_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p29, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p29;
	selp.b64 	%rd155, %rd128, 0, %p29;
	setp.eq.s64 	%p30, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p30;
	mov.u64 	%rd154, 0;
	mov.pred 	%p35, %p28;
$L__BB68_35:
	setp.gt.u64 	%p24, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p26, %rd122, %rd6;
	setp.eq.s64 	%p31, %rd156, 0;
	@%p31 bra 	$L__BB68_37;
	ld.f32 	%f9, [%rd156];
	ld.f32 	%f10, [%rd155];
	setp.lt.f32 	%p32, %f9, %f10;
	selp.f32 	%f12, %f9, %f10, %p32;
	bra.uni 	$L__BB68_37;

}
	// .globl	vector_dot_f32
.visible .entry vector_dot_f32()
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_43;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 38, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 38

}
	// .globl	vector_quote_f32
.visible .entry vector_quote_f32()
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_4;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_44;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 39, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 39

}
	// .globl	vector_add_vec_ref_f32
.visible .entry vector_add_vec_ref_f32(
	.param .u64 vector_add_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_add_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB71_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB71_3;
	bra.uni 	$L__BB71_2;
$L__BB71_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB71_4;
$L__BB71_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB71_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB71_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB71_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB71_8;
	bra.uni 	$L__BB71_7;
$L__BB71_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB71_9;
$L__BB71_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB71_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB71_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB71_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB71_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB71_14;
$L__BB71_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB71_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB71_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB71_17;
$L__BB71_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB71_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	add.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB71_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB71_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB71_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	add.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	add.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB71_20;
$L__BB71_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB71_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	add.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB71_23:
	ret;

}
	// .globl	vector_sub_vec_ref_f32
.visible .entry vector_sub_vec_ref_f32(
	.param .u64 vector_sub_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_sub_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB72_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB72_3;
	bra.uni 	$L__BB72_2;
$L__BB72_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB72_4;
$L__BB72_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB72_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB72_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB72_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB72_8;
	bra.uni 	$L__BB72_7;
$L__BB72_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB72_9;
$L__BB72_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB72_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB72_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB72_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB72_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB72_14;
$L__BB72_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB72_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB72_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB72_17;
$L__BB72_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB72_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	sub.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB72_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB72_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB72_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	sub.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	sub.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB72_20;
$L__BB72_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB72_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	sub.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB72_23:
	ret;

}
	// .globl	vector_mul_vec_ref_f32
.visible .entry vector_mul_vec_ref_f32(
	.param .u64 vector_mul_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_mul_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB73_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB73_3;
	bra.uni 	$L__BB73_2;
$L__BB73_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB73_4;
$L__BB73_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB73_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB73_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB73_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB73_8;
	bra.uni 	$L__BB73_7;
$L__BB73_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB73_9;
$L__BB73_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB73_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB73_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB73_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB73_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB73_14;
$L__BB73_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB73_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB73_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB73_17;
$L__BB73_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB73_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB73_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB73_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB73_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB73_20;
$L__BB73_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB73_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	mul.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB73_23:
	ret;

}
	// .globl	vector_div_vec_ref_f32
.visible .entry vector_div_vec_ref_f32(
	.param .u64 vector_div_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_div_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB74_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB74_3;
	bra.uni 	$L__BB74_2;
$L__BB74_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB74_4;
$L__BB74_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB74_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB74_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB74_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB74_8;
	bra.uni 	$L__BB74_7;
$L__BB74_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB74_9;
$L__BB74_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB74_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB74_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB74_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB74_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB74_14;
$L__BB74_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB74_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB74_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB74_17;
$L__BB74_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB74_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	div.approx.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB74_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB74_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB74_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB74_20;
$L__BB74_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB74_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	div.approx.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB74_23:
	ret;

}
	// .globl	vector_powf_vec_ref_f32
.visible .entry vector_powf_vec_ref_f32(
	.param .u64 vector_powf_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_powf_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB75_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB75_3;
	bra.uni 	$L__BB75_2;
$L__BB75_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB75_4;
$L__BB75_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB75_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB75_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB75_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB75_8;
	bra.uni 	$L__BB75_7;
$L__BB75_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB75_9;
$L__BB75_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB75_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB75_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB75_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB75_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB75_14;
$L__BB75_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB75_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB75_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB75_17;
$L__BB75_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB75_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	lg2.approx.ftz.f32 %r11, %r12;
    mul.rn.ftz.f32 %r11, %r11, %r13;
    ex2.approx.ftz.f32 %r11, %r11;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB75_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB75_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB75_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	lg2.approx.ftz.f32 %r14, %r15;
    mul.rn.ftz.f32 %r14, %r14, %r16;
    ex2.approx.ftz.f32 %r14, %r14;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	lg2.approx.ftz.f32 %r17, %r18;
    mul.rn.ftz.f32 %r17, %r17, %r19;
    ex2.approx.ftz.f32 %r17, %r17;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB75_20;
$L__BB75_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB75_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	lg2.approx.ftz.f32 %r20, %r21;
    mul.rn.ftz.f32 %r20, %r20, %r22;
    ex2.approx.ftz.f32 %r20, %r20;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB75_23:
	ret;

}
	// .globl	vector_greater_vec_ref_f32
.visible .entry vector_greater_vec_ref_f32(
	.param .u64 vector_greater_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_greater_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB76_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB76_3;
	bra.uni 	$L__BB76_2;
$L__BB76_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB76_4;
$L__BB76_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB76_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB76_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB76_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB76_8;
	bra.uni 	$L__BB76_7;
$L__BB76_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB76_9;
$L__BB76_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB76_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB76_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB76_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB76_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB76_14;
$L__BB76_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB76_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB76_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB76_17;
$L__BB76_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB76_17:
	ld.f32 	%f1, [%rd103];
	ld.f32 	%f2, [%rd101];
	setp.gt.f32 	%p12, %f2, %f1;
	selp.f32 	%f3, %f2, %f1, %p12;
	st.f32 	[%rd101], %f3;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB76_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p14 bra 	$L__BB76_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB76_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p15, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p15;
	selp.b64 	%rd76, %rd74, 0, %p15;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p16, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd79, %rd9, %p16;
	selp.b64 	%rd82, %rd80, 0, %p16;
	ld.f32 	%f4, [%rd82];
	ld.f32 	%f5, [%rd76];
	setp.gt.f32 	%p17, %f5, %f4;
	selp.f32 	%f6, %f5, %f4, %p17;
	st.f32 	[%rd76], %f6;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd81, %rd95;
	ld.f32 	%f7, [%rd107+-4];
	ld.f32 	%f8, [%rd108+-4];
	setp.gt.f32 	%p18, %f8, %f7;
	selp.f32 	%f9, %f8, %f7, %p18;
	st.f32 	[%rd108+-4], %f9;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p19, %rd104, 0;
	@%p19 bra 	$L__BB76_20;
$L__BB76_21:
	setp.eq.s64 	%p20, %rd36, 0;
	@%p20 bra 	$L__BB76_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p21, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p21;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p22, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p22;
	ld.f32 	%f10, [%rd93];
	ld.f32 	%f11, [%rd88];
	setp.gt.f32 	%p23, %f11, %f10;
	selp.f32 	%f12, %f11, %f10, %p23;
	st.f32 	[%rd88], %f12;
$L__BB76_23:
	ret;

}
	// .globl	vector_lesser_vec_ref_f32
.visible .entry vector_lesser_vec_ref_f32(
	.param .u64 vector_lesser_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_lesser_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB77_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB77_3;
	bra.uni 	$L__BB77_2;
$L__BB77_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB77_4;
$L__BB77_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB77_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB77_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB77_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB77_8;
	bra.uni 	$L__BB77_7;
$L__BB77_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB77_9;
$L__BB77_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB77_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB77_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB77_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB77_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB77_14;
$L__BB77_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB77_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB77_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB77_17;
$L__BB77_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB77_17:
	ld.f32 	%f1, [%rd103];
	ld.f32 	%f2, [%rd101];
	setp.lt.f32 	%p12, %f2, %f1;
	selp.f32 	%f3, %f2, %f1, %p12;
	st.f32 	[%rd101], %f3;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB77_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p14 bra 	$L__BB77_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB77_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p15, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p15;
	selp.b64 	%rd76, %rd74, 0, %p15;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p16, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd79, %rd9, %p16;
	selp.b64 	%rd82, %rd80, 0, %p16;
	ld.f32 	%f4, [%rd82];
	ld.f32 	%f5, [%rd76];
	setp.lt.f32 	%p17, %f5, %f4;
	selp.f32 	%f6, %f5, %f4, %p17;
	st.f32 	[%rd76], %f6;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd81, %rd95;
	ld.f32 	%f7, [%rd107+-4];
	ld.f32 	%f8, [%rd108+-4];
	setp.lt.f32 	%p18, %f8, %f7;
	selp.f32 	%f9, %f8, %f7, %p18;
	st.f32 	[%rd108+-4], %f9;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p19, %rd104, 0;
	@%p19 bra 	$L__BB77_20;
$L__BB77_21:
	setp.eq.s64 	%p20, %rd36, 0;
	@%p20 bra 	$L__BB77_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p21, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p21;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p22, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p22;
	ld.f32 	%f10, [%rd93];
	ld.f32 	%f11, [%rd88];
	setp.lt.f32 	%p23, %f11, %f10;
	selp.f32 	%f12, %f11, %f10, %p23;
	st.f32 	[%rd88], %f12;
$L__BB77_23:
	ret;

}
	// .globl	merge_path
.visible .entry merge_path(
	.param .u64 merge_path_param_0
)
{
	.reg .pred 	%p<48>;
	.reg .b32 	%r<77>;
	.reg .f32 	%f<11>;
	.reg .b64 	%rd<176>;

	ld.param.u64 	%rd67, [merge_path_param_0];
	cvta.to.global.u64 	%rd1, %rd67;
	// begin inline asm
	.shared .align 4 .b8 a[4096];
    mov.u32 %r4, a;
	// end inline asm
	// begin inline asm
	.shared .align 4 .b8 b[4096];
    mov.u32 %r5, b;
	// end inline asm
	ld.global.nc.u64 	%rd68, [%rd1+40];
	setp.eq.s64 	%p1, %rd68, 0;
	@%p1 bra 	$L__BB78_65;
	ld.global.nc.u64 	%rd69, [%rd1+32];
	ld.u32 	%r9, [%rd69];
	setp.eq.s32 	%p2, %r9, 0;
	@%p2 bra 	$L__BB78_66;
	mov.u32 	%r6, %tid.x;
	cvt.u64.u32 	%rd2, %r6;
	mov.u32 	%r7, %ntid.x;
	cvt.u64.u32 	%rd3, %r7;
	mov.u32 	%r8, %ctaid.x;
	mul.wide.u32 	%rd6, %r9, 2;
	setp.lt.u64 	%p3, %rd2, %rd6;
	mov.u64 	%rd22, 0;
	cvt.u32.u64 	%r74, %rd6;
	cvt.u32.u64 	%r75, %rd2;
	mov.u64 	%rd8, %rd22;
	@%p3 bra 	$L__BB78_4;
	div.u32 	%r12, %r75, %r74;
	cvt.u64.u32 	%rd8, %r12;
$L__BB78_4:
	cvt.u64.u32 	%rd4, %r8;
	cvt.u64.u32 	%rd5, %r9;
	setp.lt.u64 	%p4, %rd3, %rd6;
	mov.u64 	%rd162, %rd22;
	@%p4 bra 	$L__BB78_6;
	cvt.u32.u64 	%r14, %rd3;
	div.u32 	%r15, %r14, %r74;
	cvt.u64.u32 	%rd162, %r15;
$L__BB78_6:
	mul.lo.s64 	%rd74, %rd162, %rd4;
	add.s64 	%rd13, %rd74, %rd8;
	ld.global.nc.u64 	%rd14, [%rd1+8];
	ld.global.nc.u64 	%rd15, [%rd1];
	shl.b64 	%rd16, %rd13, 1;
	mul.hi.u64 	%rd75, %rd16, %rd5;
	setp.ne.s64 	%p5, %rd75, 0;
	mul.lo.s64 	%rd17, %rd16, %rd5;
	setp.ge.u64 	%p6, %rd17, %rd14;
	or.pred  	%p7, %p5, %p6;
	@%p7 bra 	$L__BB78_10;
	add.s64 	%rd76, %rd17, %rd5;
	setp.lt.u64 	%p8, %rd76, %rd17;
	min.u64 	%rd77, %rd14, %rd76;
	selp.b64 	%rd18, %rd14, %rd77, %p8;
	setp.ge.u64 	%p9, %rd18, %rd17;
	@%p9 bra 	$L__BB78_9;
	bra.uni 	$L__BB78_8;
$L__BB78_9:
	sub.s64 	%rd21, %rd18, %rd17;
	shl.b64 	%rd78, %rd17, 2;
	add.s64 	%rd22, %rd15, %rd78;
$L__BB78_10:
	setp.eq.s64 	%p10, %rd22, 0;
	@%p10 bra 	$L__BB78_11;
	or.b64  	%rd83, %rd16, 1;
	mul.hi.u64 	%rd84, %rd83, %rd5;
	setp.ne.s64 	%p11, %rd84, 0;
	mul.lo.s64 	%rd23, %rd83, %rd5;
	setp.ge.u64 	%p12, %rd23, %rd14;
	or.pred  	%p13, %p11, %p12;
	mov.u64 	%rd28, 0;
	@%p13 bra 	$L__BB78_16;
	add.s64 	%rd85, %rd23, %rd5;
	setp.lt.u64 	%p14, %rd85, %rd23;
	min.u64 	%rd86, %rd14, %rd85;
	selp.b64 	%rd24, %rd14, %rd86, %p14;
	setp.ge.u64 	%p15, %rd24, %rd23;
	@%p15 bra 	$L__BB78_15;
	bra.uni 	$L__BB78_14;
$L__BB78_15:
	sub.s64 	%rd27, %rd24, %rd23;
	shl.b64 	%rd87, %rd23, 2;
	add.s64 	%rd28, %rd15, %rd87;
$L__BB78_16:
	setp.ne.s64 	%p16, %rd28, 0;
	@%p16 bra 	$L__BB78_18;
	bra.uni 	$L__BB78_17;
$L__BB78_18:
	ld.global.nc.u64 	%rd29, [%rd1+24];
	ld.global.nc.u64 	%rd30, [%rd1+16];
	mul.hi.u64 	%rd92, %rd13, %rd6;
	setp.ne.s64 	%p17, %rd92, 0;
	mul.lo.s64 	%rd31, %rd13, %rd6;
	setp.ge.u64 	%p18, %rd31, %rd29;
	or.pred  	%p19, %p17, %p18;
	mov.u64 	%rd36, 0;
	@%p19 bra 	$L__BB78_22;
	add.s64 	%rd93, %rd31, %rd6;
	setp.lt.u64 	%p20, %rd93, %rd31;
	min.u64 	%rd94, %rd29, %rd93;
	selp.b64 	%rd32, %rd29, %rd94, %p20;
	setp.ge.u64 	%p21, %rd32, %rd31;
	@%p21 bra 	$L__BB78_21;
	bra.uni 	$L__BB78_20;
$L__BB78_21:
	shl.b64 	%rd95, %rd31, 2;
	add.s64 	%rd36, %rd30, %rd95;
	sub.s64 	%rd35, %rd32, %rd31;
$L__BB78_22:
	setp.ne.s64 	%p22, %rd36, 0;
	@%p22 bra 	$L__BB78_24;
	bra.uni 	$L__BB78_23;
$L__BB78_24:
	setp.eq.s64 	%p23, %rd21, 0;
	@%p23 bra 	$L__BB78_67;
	setp.lt.u64 	%p24, %rd2, %rd21;
	mov.u64 	%rd38, %rd2;
	@%p24 bra 	$L__BB78_27;
	cvt.u32.u64 	%r16, %rd21;
	rem.u32 	%r18, %r75, %r16;
	cvt.u64.u32 	%rd38, %r18;
$L__BB78_27:
	setp.eq.s64 	%p25, %rd27, 0;
	@%p25 bra 	$L__BB78_68;
	setp.lt.u64 	%p26, %rd2, %rd27;
	mov.u64 	%rd170, %rd2;
	@%p26 bra 	$L__BB78_30;
	cvt.u32.u64 	%r19, %rd27;
	rem.u32 	%r21, %r75, %r19;
	cvt.u64.u32 	%rd170, %r21;
$L__BB78_30:
	setp.eq.s64 	%p27, %rd35, 0;
	@%p27 bra 	$L__BB78_69;
	mul.lo.s64 	%rd9, %rd8, %rd6;
	sub.s64 	%rd10, %rd2, %rd9;
	shl.b64 	%rd100, %rd38, 2;
	add.s64 	%rd101, %rd22, %rd100;
	shl.b32 	%r27, %r75, 2;
	add.s32 	%r22, %r27, %r4;
	ld.u32 	%r23, [%rd101];
	// begin inline asm
	st.shared.f32 [%r22], %r23;
	// end inline asm
	shl.b64 	%rd102, %rd170, 2;
	add.s64 	%rd103, %rd28, %rd102;
	add.s32 	%r24, %r27, %r5;
	ld.u32 	%r25, [%rd103];
	// begin inline asm
	st.shared.f32 [%r24], %r25;
	// end inline asm
	bar.sync 	0;
	min.u64 	%rd44, %rd10, %rd21;
	max.u64 	%rd104, %rd10, %rd21;
	sub.s64 	%rd45, %rd104, %rd21;
	add.s64 	%rd43, %rd9, -1;
	cvt.u32.u64 	%r71, %rd45;
$L__BB78_32:
	cvt.u32.u64 	%r28, %rd44;
	sub.s32 	%r29, %r28, %r71;
	abs.s32 	%r30, %r29;
	shr.u32 	%r31, %r30, 31;
	add.s32 	%r32, %r30, %r31;
	shr.s32 	%r33, %r32, 1;
	cvt.s64.s32 	%rd46, %r33;
	add.s64 	%rd47, %rd45, %rd46;
	sub.s64 	%rd48, %rd44, %rd46;
	setp.le.u64 	%p28, %rd47, %rd27;
	@%p28 bra 	$L__BB78_34;
	bra.uni 	$L__BB78_33;
$L__BB78_34:
	setp.ne.s64 	%p29, %rd48, %rd21;
	setp.ne.s64 	%p30, %rd47, 0;
	and.pred  	%p31, %p29, %p30;
	@%p31 bra 	$L__BB78_40;
	bra.uni 	$L__BB78_35;
$L__BB78_40:
	add.s64 	%rd53, %rd48, %rd9;
	setp.lt.u64 	%p32, %rd53, 1024;
	@%p32 bra 	$L__BB78_42;
	bra.uni 	$L__BB78_41;
$L__BB78_42:
	cvt.u32.u64 	%r36, %rd53;
	shl.b32 	%r37, %r36, 2;
	add.s32 	%r35, %r37, %r4;
	// begin inline asm
	ld.shared.f32 %r34, [%r35];
	// end inline asm
	add.s64 	%rd54, %rd43, %rd47;
	setp.lt.u64 	%p33, %rd54, 1024;
	@%p33 bra 	$L__BB78_44;
	bra.uni 	$L__BB78_43;
$L__BB78_44:
	mov.b32 	%f1, %r34;
	cvt.u32.u64 	%r40, %rd54;
	shl.b32 	%r41, %r40, 2;
	add.s32 	%r39, %r41, %r5;
	// begin inline asm
	ld.shared.f32 %r38, [%r39];
	// end inline asm
	mov.b32 	%f7, %r38;
	setp.gt.f32 	%p34, %f1, %f7;
	@%p34 bra 	$L__BB78_35;
	bra.uni 	$L__BB78_33;
$L__BB78_35:
	setp.eq.s64 	%p35, %rd47, %rd27;
	@%p35 bra 	$L__BB78_45;
	setp.eq.s64 	%p36, %rd44, %rd46;
	@%p36 bra 	$L__BB78_37;
	add.s64 	%rd56, %rd43, %rd48;
	setp.lt.u64 	%p37, %rd56, 1024;
	@%p37 bra 	$L__BB78_49;
	bra.uni 	$L__BB78_48;
$L__BB78_49:
	cvt.u32.u64 	%r44, %rd56;
	shl.b32 	%r45, %r44, 2;
	add.s32 	%r43, %r45, %r4;
	// begin inline asm
	ld.shared.f32 %r42, [%r43];
	// end inline asm
	add.s64 	%rd57, %rd47, %rd9;
	setp.lt.u64 	%p38, %rd57, 1024;
	@%p38 bra 	$L__BB78_51;
	bra.uni 	$L__BB78_50;
$L__BB78_51:
	mov.b32 	%f2, %r42;
	cvt.u32.u64 	%r48, %rd57;
	shl.b32 	%r49, %r48, 2;
	add.s32 	%r47, %r49, %r5;
	// begin inline asm
	ld.shared.f32 %r46, [%r47];
	// end inline asm
	mov.b32 	%f8, %r46;
	setp.le.f32 	%p39, %f2, %f8;
	@%p39 bra 	$L__BB78_37;
	add.s64 	%rd45, %rd47, 1;
	add.s64 	%rd44, %rd48, -1;
	bra.uni 	$L__BB78_32;
$L__BB78_33:
	add.s64 	%rd49, %rd48, 1;
	cvt.u32.u64 	%r71, %rd49;
	bra.uni 	$L__BB78_32;
$L__BB78_37:
	setp.lt.u64 	%p40, %rd48, %rd21;
	@%p40 bra 	$L__BB78_54;
	bra.uni 	$L__BB78_38;
$L__BB78_54:
	add.s64 	%rd174, %rd48, %rd9;
	setp.lt.u64 	%p41, %rd174, 1024;
	@%p41 bra 	$L__BB78_56;
	bra.uni 	$L__BB78_55;
$L__BB78_56:
	cvt.u32.u64 	%r52, %rd174;
	shl.b32 	%r53, %r52, 2;
	add.s32 	%r51, %r53, %r4;
	// begin inline asm
	ld.shared.f32 %r50, [%r51];
	// end inline asm
	add.s64 	%rd63, %rd47, %rd9;
	setp.lt.u64 	%p42, %rd63, 1024;
	@%p42 bra 	$L__BB78_58;
	bra.uni 	$L__BB78_57;
$L__BB78_58:
	mov.b32 	%f4, %r50;
	cvt.u32.u64 	%r56, %rd63;
	shl.b32 	%r57, %r56, 2;
	add.s32 	%r55, %r57, %r5;
	// begin inline asm
	ld.shared.f32 %r54, [%r55];
	// end inline asm
	mov.b32 	%f9, %r54;
	setp.gtu.f32 	%p43, %f4, %f9;
	@%p43 bra 	$L__BB78_38;
	bra.uni 	$L__BB78_59;
$L__BB78_45:
	setp.ge.u64 	%p44, %rd48, %rd21;
	mov.u64 	%rd47, %rd27;
	@%p44 bra 	$L__BB78_38;
	add.s64 	%rd174, %rd48, %rd9;
$L__BB78_59:
	setp.lt.u64 	%p46, %rd174, 1024;
	@%p46 bra 	$L__BB78_61;
	bra.uni 	$L__BB78_60;
$L__BB78_61:
	cvt.u32.u64 	%r64, %rd174;
	shl.b32 	%r65, %r64, 2;
	add.s32 	%r63, %r65, %r4;
	// begin inline asm
	ld.shared.f32 %r62, [%r63];
	// end inline asm
	mov.b32 	%f10, %r62;
	bra.uni 	$L__BB78_62;
$L__BB78_38:
	add.s64 	%rd61, %rd47, %rd9;
	setp.lt.u64 	%p45, %rd61, 1024;
	@%p45 bra 	$L__BB78_53;
	bra.uni 	$L__BB78_39;
$L__BB78_53:
	cvt.u32.u64 	%r60, %rd61;
	shl.b32 	%r61, %r60, 2;
	add.s32 	%r59, %r61, %r5;
	// begin inline asm
	ld.shared.f32 %r58, [%r59];
	// end inline asm
	mov.b32 	%f10, %r58;
$L__BB78_62:
	setp.lt.u64 	%p47, %rd2, %rd35;
	@%p47 bra 	$L__BB78_64;
	cvt.u32.u64 	%r66, %rd35;
	rem.u32 	%r68, %r75, %r66;
	cvt.u64.u32 	%rd2, %r68;
$L__BB78_64:
	shl.b64 	%rd137, %rd2, 2;
	add.s64 	%rd138, %rd36, %rd137;
	st.f32 	[%rd138], %f10;
	ret;
$L__BB78_48:
	mov.u64 	%rd113, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_70;
	cvta.global.u64 	%rd114, %rd113;
	mov.u64 	%rd115, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_72;
	cvta.global.u64 	%rd116, %rd115;
	{ // callseq 45, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd114;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd116;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 45
$L__BB78_50:
	mov.u64 	%rd117, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_70;
	cvta.global.u64 	%rd118, %rd117;
	mov.u64 	%rd119, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_72;
	cvta.global.u64 	%rd120, %rd119;
	{ // callseq 46, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd118;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd120;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 46
$L__BB78_41:
	mov.u64 	%rd105, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_70;
	cvta.global.u64 	%rd106, %rd105;
	mov.u64 	%rd107, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_72;
	cvta.global.u64 	%rd108, %rd107;
	{ // callseq 43, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd106;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd108;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 43
$L__BB78_43:
	mov.u64 	%rd109, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_70;
	cvta.global.u64 	%rd110, %rd109;
	mov.u64 	%rd111, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_72;
	cvta.global.u64 	%rd112, %rd111;
	{ // callseq 44, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd110;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd112;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 44
$L__BB78_65:
	mov.u64 	%rd153, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_46;
	cvta.global.u64 	%rd154, %rd153;
	{ // callseq 58, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd154;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 58
$L__BB78_66:
	mov.u64 	%rd151, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_47;
	cvta.global.u64 	%rd152, %rd151;
	{ // callseq 57, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd152;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_rem_by_zero17h18083b86e5b8e9ccE, 
	(
	param0
	);
	} // callseq 57
$L__BB78_11:
	mov.u64 	%rd149, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_48;
	cvta.global.u64 	%rd150, %rd149;
	{ // callseq 56, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd150;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 56
$L__BB78_17:
	mov.u64 	%rd147, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_49;
	cvta.global.u64 	%rd148, %rd147;
	{ // callseq 55, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd148;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 55
$L__BB78_23:
	mov.u64 	%rd145, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_50;
	cvta.global.u64 	%rd146, %rd145;
	{ // callseq 54, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd146;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 54
$L__BB78_67:
	mov.u64 	%rd143, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_51;
	cvta.global.u64 	%rd144, %rd143;
	{ // callseq 53, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd144;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_rem_by_zero17h18083b86e5b8e9ccE, 
	(
	param0
	);
	} // callseq 53
$L__BB78_68:
	mov.u64 	%rd141, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_52;
	cvta.global.u64 	%rd142, %rd141;
	{ // callseq 52, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_rem_by_zero17h18083b86e5b8e9ccE, 
	(
	param0
	);
	} // callseq 52
$L__BB78_69:
	mov.u64 	%rd139, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_53;
	cvta.global.u64 	%rd140, %rd139;
	{ // callseq 51, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_rem_by_zero17h18083b86e5b8e9ccE, 
	(
	param0
	);
	} // callseq 51
$L__BB78_39:
	mov.u64 	%rd129, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_70;
	cvta.global.u64 	%rd130, %rd129;
	mov.u64 	%rd131, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_72;
	cvta.global.u64 	%rd132, %rd131;
	{ // callseq 49, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd130;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd132;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 49
$L__BB78_8:
	mov.u64 	%rd79, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_1;
	cvta.global.u64 	%rd80, %rd79;
	{ // callseq 40, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd17;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd18;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd80;
	call.uni 
	_ZN4core5slice5index22slice_index_order_fail17h464a34c2ded0572eE, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 40
$L__BB78_14:
	mov.u64 	%rd88, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_1;
	cvta.global.u64 	%rd89, %rd88;
	{ // callseq 41, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd23;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd24;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd89;
	call.uni 
	_ZN4core5slice5index22slice_index_order_fail17h464a34c2ded0572eE, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 41
$L__BB78_20:
	mov.u64 	%rd96, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_2;
	cvta.global.u64 	%rd97, %rd96;
	mov.u64 	%rd98, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_3;
	cvta.global.u64 	%rd99, %rd98;
	{ // callseq 42, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd97;
	.param .b64 param1;
	st.param.b64 	[param1+0], 35;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd99;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 42
$L__BB78_60:
	mov.u64 	%rd133, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_70;
	cvta.global.u64 	%rd134, %rd133;
	mov.u64 	%rd135, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_72;
	cvta.global.u64 	%rd136, %rd135;
	{ // callseq 50, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd134;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd136;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 50
$L__BB78_55:
	mov.u64 	%rd121, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_70;
	cvta.global.u64 	%rd122, %rd121;
	mov.u64 	%rd123, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_72;
	cvta.global.u64 	%rd124, %rd123;
	{ // callseq 47, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd122;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd124;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 47
$L__BB78_57:
	mov.u64 	%rd125, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_70;
	cvta.global.u64 	%rd126, %rd125;
	mov.u64 	%rd127, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_72;
	cvta.global.u64 	%rd128, %rd127;
	{ // callseq 48, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd126;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd128;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 48

}
	// .globl	insert_skip
.visible .entry insert_skip(
	.param .u64 insert_skip_param_0
)
{
	.local .align 4 .b8 	__local_depot79[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<26>;
	.reg .b32 	%r<8>;
	.reg .f32 	%f<23>;
	.reg .b64 	%rd<136>;

	mov.u64 	%SPL, __local_depot79;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd61, [insert_skip_param_0];
	cvta.to.global.u64 	%rd1, %rd61;
	ld.global.nc.u64 	%rd4, [%rd1];
	ld.global.nc.u64 	%rd5, [%rd1+8];
	shr.u64 	%rd65, %rd5, 2;
	and.b64  	%rd66, %rd5, 3;
	setp.ne.s64 	%p1, %rd66, 0;
	selp.u64 	%rd67, 1, 0, %p1;
	add.s64 	%rd6, %rd65, %rd67;
	setp.ne.s64 	%p2, %rd6, 0;
	@%p2 bra 	$L__BB79_2;
	bra.uni 	$L__BB79_1;
$L__BB79_2:
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd63, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd64, %r3, %r2;
	add.s64 	%rd3, %rd64, %rd63;
	ld.global.nc.u64 	%rd71, [%rd1+24];
	shl.b64 	%rd15, %rd3, 2;
	setp.ge.u64 	%p3, %rd15, %rd71;
	mov.u64 	%rd116, 0;
	mov.u64 	%rd117, %rd116;
	@%p3 bra 	$L__BB79_4;
	add.s64 	%rd72, %rd15, 4;
	min.u64 	%rd73, %rd71, %rd72;
	sub.s64 	%rd74, %rd73, %rd15;
	and.b64  	%rd116, %rd74, 4611686018427387903;
	ld.global.nc.u64 	%rd75, [%rd1+16];
	shl.b64 	%rd76, %rd15, 2;
	add.s64 	%rd117, %rd75, %rd76;
$L__BB79_4:
	setp.ne.s64 	%p4, %rd117, 0;
	@%p4 bra 	$L__BB79_6;
	bra.uni 	$L__BB79_5;
$L__BB79_6:
	add.u64 	%rd62, %SP, 0;
	add.u64 	%rd135, %SPL, 0;
	mov.b32 	%r4, 0;
	st.local.u32 	[%rd135], %r4;
	st.local.u32 	[%rd135+4], %r4;
	st.local.u32 	[%rd135+8], %r4;
	st.local.u32 	[%rd135+12], %r4;
	setp.le.u64 	%p5, %rd5, %rd3;
	mov.u64 	%rd119, 0;
	@%p5 bra 	$L__BB79_11;
	max.u64 	%rd78, %rd5, %rd3;
	not.b64 	%rd79, %rd3;
	add.s64 	%rd20, %rd79, %rd78;
	or.b64  	%rd80, %rd20, %rd6;
	and.b64  	%rd81, %rd80, -4294967296;
	setp.ne.s64 	%p6, %rd81, 0;
	@%p6 bra 	$L__BB79_9;
	bra.uni 	$L__BB79_8;
$L__BB79_9:
	div.u64 	%rd118, %rd20, %rd6;
	bra.uni 	$L__BB79_10;
$L__BB79_8:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd20;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd118, %r7;
$L__BB79_10:
	add.s64 	%rd119, %rd118, 1;
$L__BB79_11:
	add.s64 	%rd7, %rd135, 4;
	setp.eq.s64 	%p7, %rd119, 0;
	mov.f32 	%f21, 0f00000000;
	mov.f32 	%f22, %f21;
	@%p7 bra 	$L__BB79_23;
	shl.b64 	%rd70, %rd5, 2;
	add.s64 	%rd13, %rd4, %rd70;
	setp.eq.s64 	%p8, %rd3, 0;
	@%p8 bra 	$L__BB79_14;
	setp.gt.u64 	%p9, %rd5, %rd3;
	add.s64 	%rd83, %rd4, %rd15;
	add.s64 	%rd84, %rd83, 4;
	selp.b64 	%rd125, %rd84, %rd13, %p9;
	selp.b64 	%rd121, %rd83, 0, %p9;
	bra.uni 	$L__BB79_15;
$L__BB79_14:
	setp.eq.s64 	%p10, %rd5, 0;
	selp.b64 	%rd85, 0, 4, %p10;
	add.s64 	%rd125, %rd4, %rd85;
	selp.b64 	%rd121, 0, %rd4, %p10;
$L__BB79_15:
	ld.f32 	%f22, [%rd121];
	st.local.f32 	[%rd135], %f22;
	setp.eq.s64 	%p11, %rd119, 1;
	@%p11 bra 	$L__BB79_23;
	add.s64 	%rd14, %rd6, -1;
	min.u64 	%rd26, %rd119, 4;
	add.s64 	%rd33, %rd26, -1;
	add.s64 	%rd87, %rd26, -2;
	and.b64  	%rd127, %rd33, 3;
	setp.lt.u64 	%p12, %rd87, 3;
	mov.u64 	%rd129, 4;
	shl.b64 	%rd115, %rd6, 2;
	@%p12 bra 	$L__BB79_20;
	and.b64  	%rd122, %rd33, -4;
	mov.u64 	%rd123, 0;
$L__BB79_18:
	add.s64 	%rd89, %rd135, %rd123;
	sub.s64 	%rd90, %rd13, %rd125;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p13, %rd91, %rd14;
	add.s64 	%rd93, %rd125, %rd115;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd95, %rd94, 0, %p13;
	selp.b64 	%rd96, %rd93, %rd13, %p13;
	ld.f32 	%f15, [%rd95];
	st.local.f32 	[%rd89+4], %f15;
	add.s64 	%rd97, %rd96, %rd115;
	ld.f32 	%f16, [%rd97+-4];
	st.local.f32 	[%rd89+8], %f16;
	add.s64 	%rd98, %rd97, %rd115;
	ld.f32 	%f17, [%rd98+-4];
	st.local.f32 	[%rd89+12], %f17;
	add.s64 	%rd125, %rd98, %rd115;
	ld.f32 	%f18, [%rd125+-4];
	st.local.f32 	[%rd89+16], %f18;
	add.s64 	%rd123, %rd123, 16;
	add.s64 	%rd122, %rd122, -4;
	setp.ne.s64 	%p14, %rd122, 0;
	@%p14 bra 	$L__BB79_18;
	add.s64 	%rd129, %rd123, 4;
$L__BB79_20:
	setp.eq.s64 	%p15, %rd127, 0;
	@%p15 bra 	$L__BB79_22;
$L__BB79_21:
	.pragma "nounroll";
	add.s64 	%rd99, %rd135, %rd129;
	sub.s64 	%rd100, %rd13, %rd125;
	shr.u64 	%rd101, %rd100, 2;
	setp.gt.u64 	%p16, %rd101, %rd14;
	add.s64 	%rd103, %rd125, %rd115;
	add.s64 	%rd104, %rd103, -4;
	selp.b64 	%rd105, %rd104, 0, %p16;
	selp.b64 	%rd125, %rd103, %rd13, %p16;
	add.s64 	%rd129, %rd129, 4;
	ld.f32 	%f19, [%rd105];
	st.local.f32 	[%rd99], %f19;
	add.s64 	%rd127, %rd127, -1;
	setp.ne.s64 	%p17, %rd127, 0;
	@%p17 bra 	$L__BB79_21;
$L__BB79_22:
	ld.local.f32 	%f22, [%rd135];
	ld.local.f32 	%f21, [%rd135+4];
$L__BB79_23:
	add.s64 	%rd9, %rd135, 8;
	cvta.local.u64 	%rd8, %rd7;
	setp.lt.f32 	%p18, %f22, %f21;
	mov.u64 	%rd130, %rd8;
	@%p18 bra 	$L__BB79_25;
	st.local.f32 	[%rd7], %f22;
	add.u64 	%rd130, %SP, 0;
$L__BB79_25:
	add.s64 	%rd11, %rd135, 12;
	cvta.local.u64 	%rd10, %rd9;
	st.f32 	[%rd130], %f21;
	ld.local.f32 	%f6, [%rd9];
	ld.local.f32 	%f7, [%rd7];
	setp.lt.f32 	%p19, %f7, %f6;
	mov.u64 	%rd131, %rd10;
	@%p19 bra 	$L__BB79_28;
	st.local.f32 	[%rd135+8], %f7;
	ld.local.f32 	%f8, [%rd135];
	setp.lt.f32 	%p20, %f8, %f6;
	mov.u64 	%rd131, %rd8;
	@%p20 bra 	$L__BB79_28;
	st.local.f32 	[%rd7], %f8;
	add.u64 	%rd131, %SP, 0;
$L__BB79_28:
	cvta.local.u64 	%rd132, %rd11;
	st.f32 	[%rd131], %f6;
	ld.local.f32 	%f9, [%rd11];
	ld.local.f32 	%f10, [%rd9];
	setp.lt.f32 	%p21, %f10, %f9;
	@%p21 bra 	$L__BB79_32;
	st.local.f32 	[%rd11], %f10;
	ld.local.f32 	%f11, [%rd7];
	setp.lt.f32 	%p22, %f11, %f9;
	mov.u64 	%rd132, %rd10;
	@%p22 bra 	$L__BB79_32;
	st.local.f32 	[%rd135+8], %f11;
	ld.local.f32 	%f12, [%rd135];
	setp.lt.f32 	%p23, %f12, %f9;
	mov.u64 	%rd132, %rd8;
	@%p23 bra 	$L__BB79_32;
	st.local.f32 	[%rd7], %f12;
	mov.u64 	%rd132, %rd62;
$L__BB79_32:
	st.f32 	[%rd132], %f9;
	setp.eq.s64 	%p24, %rd116, 0;
	@%p24 bra 	$L__BB79_35;
	min.u64 	%rd133, %rd116, 4;
$L__BB79_34:
	ld.local.f32 	%f20, [%rd135];
	st.f32 	[%rd117], %f20;
	add.s64 	%rd135, %rd135, 4;
	add.s64 	%rd117, %rd117, 4;
	add.s64 	%rd133, %rd133, -1;
	setp.ne.s64 	%p25, %rd133, 0;
	@%p25 bra 	$L__BB79_34;
$L__BB79_35:
	ret;
$L__BB79_1:
	mov.u64 	%rd111, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_74;
	cvta.global.u64 	%rd112, %rd111;
	mov.u64 	%rd113, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_76;
	cvta.global.u64 	%rd114, %rd113;
	{ // callseq 60, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd112;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd114;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 60
$L__BB79_5:
	mov.u64 	%rd109, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_54;
	cvta.global.u64 	%rd110, %rd109;
	{ // callseq 59, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd110;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 59

}
	// .globl	insert_skip_stride
.visible .entry insert_skip_stride(
	.param .u64 insert_skip_stride_param_0
)
{
	.local .align 4 .b8 	__local_depot80[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<26>;
	.reg .b32 	%r<8>;
	.reg .f32 	%f<23>;
	.reg .b64 	%rd<138>;

	mov.u64 	%SPL, __local_depot80;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd62, [insert_skip_stride_param_0];
	cvta.to.global.u64 	%rd1, %rd62;
	ld.global.nc.u64 	%rd5, [%rd1];
	ld.global.nc.u64 	%rd6, [%rd1+8];
	shr.u64 	%rd65, %rd6, 2;
	and.b64  	%rd66, %rd6, 3;
	setp.ne.s64 	%p1, %rd66, 0;
	selp.u64 	%rd67, 1, 0, %p1;
	add.s64 	%rd7, %rd65, %rd67;
	setp.ne.s64 	%p2, %rd7, 0;
	@%p2 bra 	$L__BB80_2;
	bra.uni 	$L__BB80_1;
$L__BB80_2:
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd3, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd64, %r3, %r2;
	add.s64 	%rd4, %rd64, %rd3;
	shr.u64 	%rd71, %rd4, 3;
	add.s64 	%rd72, %rd71, %rd3;
	ld.global.nc.u64 	%rd73, [%rd1+24];
	shl.b64 	%rd16, %rd72, 2;
	setp.ge.u64 	%p3, %rd16, %rd73;
	mov.u64 	%rd118, 0;
	mov.u64 	%rd119, %rd118;
	@%p3 bra 	$L__BB80_4;
	add.s64 	%rd74, %rd16, 4;
	min.u64 	%rd75, %rd73, %rd74;
	sub.s64 	%rd76, %rd75, %rd16;
	and.b64  	%rd118, %rd76, 4611686018427387903;
	ld.global.nc.u64 	%rd77, [%rd1+16];
	shl.b64 	%rd78, %rd16, 2;
	add.s64 	%rd119, %rd77, %rd78;
$L__BB80_4:
	setp.ne.s64 	%p4, %rd119, 0;
	@%p4 bra 	$L__BB80_6;
	bra.uni 	$L__BB80_5;
$L__BB80_6:
	add.u64 	%rd63, %SP, 0;
	add.u64 	%rd137, %SPL, 0;
	mov.b32 	%r4, 0;
	st.local.u32 	[%rd137], %r4;
	st.local.u32 	[%rd137+4], %r4;
	st.local.u32 	[%rd137+8], %r4;
	st.local.u32 	[%rd137+12], %r4;
	setp.le.u64 	%p5, %rd6, %rd4;
	mov.u64 	%rd121, 0;
	@%p5 bra 	$L__BB80_11;
	max.u64 	%rd80, %rd6, %rd4;
	not.b64 	%rd81, %rd4;
	add.s64 	%rd21, %rd81, %rd80;
	or.b64  	%rd82, %rd21, %rd7;
	and.b64  	%rd83, %rd82, -4294967296;
	setp.ne.s64 	%p6, %rd83, 0;
	@%p6 bra 	$L__BB80_9;
	bra.uni 	$L__BB80_8;
$L__BB80_9:
	div.u64 	%rd120, %rd21, %rd7;
	bra.uni 	$L__BB80_10;
$L__BB80_8:
	cvt.u32.u64 	%r5, %rd7;
	cvt.u32.u64 	%r6, %rd21;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd120, %r7;
$L__BB80_10:
	add.s64 	%rd121, %rd120, 1;
$L__BB80_11:
	add.s64 	%rd8, %rd137, 4;
	setp.eq.s64 	%p7, %rd121, 0;
	mov.f32 	%f21, 0f00000000;
	mov.f32 	%f22, %f21;
	@%p7 bra 	$L__BB80_23;
	shl.b64 	%rd70, %rd6, 2;
	add.s64 	%rd14, %rd5, %rd70;
	setp.eq.s64 	%p8, %rd4, 0;
	@%p8 bra 	$L__BB80_14;
	setp.gt.u64 	%p9, %rd6, %rd4;
	shl.b64 	%rd84, %rd4, 2;
	add.s64 	%rd85, %rd5, %rd84;
	add.s64 	%rd86, %rd85, 4;
	selp.b64 	%rd127, %rd86, %rd14, %p9;
	selp.b64 	%rd123, %rd85, 0, %p9;
	bra.uni 	$L__BB80_15;
$L__BB80_14:
	setp.eq.s64 	%p10, %rd6, 0;
	selp.b64 	%rd87, 0, 4, %p10;
	add.s64 	%rd127, %rd5, %rd87;
	selp.b64 	%rd123, 0, %rd5, %p10;
$L__BB80_15:
	ld.f32 	%f22, [%rd123];
	st.local.f32 	[%rd137], %f22;
	setp.eq.s64 	%p11, %rd121, 1;
	@%p11 bra 	$L__BB80_23;
	add.s64 	%rd15, %rd7, -1;
	min.u64 	%rd27, %rd121, 4;
	add.s64 	%rd34, %rd27, -1;
	add.s64 	%rd89, %rd27, -2;
	and.b64  	%rd129, %rd34, 3;
	setp.lt.u64 	%p12, %rd89, 3;
	mov.u64 	%rd131, 4;
	shl.b64 	%rd117, %rd7, 2;
	@%p12 bra 	$L__BB80_20;
	and.b64  	%rd124, %rd34, -4;
	mov.u64 	%rd125, 0;
$L__BB80_18:
	add.s64 	%rd91, %rd137, %rd125;
	sub.s64 	%rd92, %rd14, %rd127;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p13, %rd93, %rd15;
	add.s64 	%rd95, %rd127, %rd117;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd97, %rd96, 0, %p13;
	selp.b64 	%rd98, %rd95, %rd14, %p13;
	ld.f32 	%f15, [%rd97];
	st.local.f32 	[%rd91+4], %f15;
	add.s64 	%rd99, %rd98, %rd117;
	ld.f32 	%f16, [%rd99+-4];
	st.local.f32 	[%rd91+8], %f16;
	add.s64 	%rd100, %rd99, %rd117;
	ld.f32 	%f17, [%rd100+-4];
	st.local.f32 	[%rd91+12], %f17;
	add.s64 	%rd127, %rd100, %rd117;
	ld.f32 	%f18, [%rd127+-4];
	st.local.f32 	[%rd91+16], %f18;
	add.s64 	%rd125, %rd125, 16;
	add.s64 	%rd124, %rd124, -4;
	setp.ne.s64 	%p14, %rd124, 0;
	@%p14 bra 	$L__BB80_18;
	add.s64 	%rd131, %rd125, 4;
$L__BB80_20:
	setp.eq.s64 	%p15, %rd129, 0;
	@%p15 bra 	$L__BB80_22;
$L__BB80_21:
	.pragma "nounroll";
	add.s64 	%rd101, %rd137, %rd131;
	sub.s64 	%rd102, %rd14, %rd127;
	shr.u64 	%rd103, %rd102, 2;
	setp.gt.u64 	%p16, %rd103, %rd15;
	add.s64 	%rd105, %rd127, %rd117;
	add.s64 	%rd106, %rd105, -4;
	selp.b64 	%rd107, %rd106, 0, %p16;
	selp.b64 	%rd127, %rd105, %rd14, %p16;
	add.s64 	%rd131, %rd131, 4;
	ld.f32 	%f19, [%rd107];
	st.local.f32 	[%rd101], %f19;
	add.s64 	%rd129, %rd129, -1;
	setp.ne.s64 	%p17, %rd129, 0;
	@%p17 bra 	$L__BB80_21;
$L__BB80_22:
	ld.local.f32 	%f22, [%rd137];
	ld.local.f32 	%f21, [%rd137+4];
$L__BB80_23:
	add.s64 	%rd10, %rd137, 8;
	cvta.local.u64 	%rd9, %rd8;
	setp.lt.f32 	%p18, %f22, %f21;
	mov.u64 	%rd132, %rd9;
	@%p18 bra 	$L__BB80_25;
	st.local.f32 	[%rd8], %f22;
	add.u64 	%rd132, %SP, 0;
$L__BB80_25:
	add.s64 	%rd12, %rd137, 12;
	cvta.local.u64 	%rd11, %rd10;
	st.f32 	[%rd132], %f21;
	ld.local.f32 	%f6, [%rd10];
	ld.local.f32 	%f7, [%rd8];
	setp.lt.f32 	%p19, %f7, %f6;
	mov.u64 	%rd133, %rd11;
	@%p19 bra 	$L__BB80_28;
	st.local.f32 	[%rd137+8], %f7;
	ld.local.f32 	%f8, [%rd137];
	setp.lt.f32 	%p20, %f8, %f6;
	mov.u64 	%rd133, %rd9;
	@%p20 bra 	$L__BB80_28;
	st.local.f32 	[%rd8], %f8;
	add.u64 	%rd133, %SP, 0;
$L__BB80_28:
	cvta.local.u64 	%rd134, %rd12;
	st.f32 	[%rd133], %f6;
	ld.local.f32 	%f9, [%rd12];
	ld.local.f32 	%f10, [%rd10];
	setp.lt.f32 	%p21, %f10, %f9;
	@%p21 bra 	$L__BB80_32;
	st.local.f32 	[%rd12], %f10;
	ld.local.f32 	%f11, [%rd8];
	setp.lt.f32 	%p22, %f11, %f9;
	mov.u64 	%rd134, %rd11;
	@%p22 bra 	$L__BB80_32;
	st.local.f32 	[%rd137+8], %f11;
	ld.local.f32 	%f12, [%rd137];
	setp.lt.f32 	%p23, %f12, %f9;
	mov.u64 	%rd134, %rd9;
	@%p23 bra 	$L__BB80_32;
	st.local.f32 	[%rd8], %f12;
	mov.u64 	%rd134, %rd63;
$L__BB80_32:
	st.f32 	[%rd134], %f9;
	setp.eq.s64 	%p24, %rd118, 0;
	@%p24 bra 	$L__BB80_35;
	min.u64 	%rd135, %rd118, 4;
$L__BB80_34:
	ld.local.f32 	%f20, [%rd137];
	st.f32 	[%rd119], %f20;
	add.s64 	%rd137, %rd137, 4;
	add.s64 	%rd119, %rd119, 4;
	add.s64 	%rd135, %rd135, -1;
	setp.ne.s64 	%p25, %rd135, 0;
	@%p25 bra 	$L__BB80_34;
$L__BB80_35:
	ret;
$L__BB80_1:
	mov.u64 	%rd113, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_74;
	cvta.global.u64 	%rd114, %rd113;
	mov.u64 	%rd115, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_76;
	cvta.global.u64 	%rd116, %rd115;
	{ // callseq 62, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd114;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd116;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 62
$L__BB80_5:
	mov.u64 	%rd111, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_55;
	cvta.global.u64 	%rd112, %rd111;
	{ // callseq 61, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd112;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 61

}
	// .globl	matrix_sort
.visible .entry matrix_sort(
	.param .u64 matrix_sort_param_0
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<6>;
	.reg .f32 	%f<7>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [matrix_sort_param_0];
	cvta.to.global.u64 	%rd1, %rd19;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd36, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	ld.global.nc.u64 	%rd5, [%rd1+8];
	mul.wide.u32 	%rd20, %r3, %r2;
	add.s64 	%rd6, %rd20, %rd36;
	setp.ge.u64 	%p1, %rd6, %rd5;
	@%p1 bra 	$L__BB81_10;
	cvt.u64.u32 	%rd37, %r3;
	cvt.u32.u64 	%r4, %rd37;
	cvt.u32.u64 	%r5, %rd36;
	ld.global.nc.u64 	%rd7, [%rd1];
	shl.b64 	%rd23, %rd6, 2;
	setp.eq.s32 	%p2, %r5, 0;
	setp.gt.u32 	%p3, %r4, 6;
	mov.f32 	%f6, 0f00000000;
	or.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB81_6;
	cvt.u64.u32 	%rd3, %r2;
	add.s64 	%rd24, %rd7, %rd23;
	ld.f32 	%f1, [%rd24];
	mov.u64 	%rd38, 0;
$L__BB81_3:
	add.s64 	%rd12, %rd36, -1;
	mad.lo.s64 	%rd14, %rd3, %rd37, %rd3;
	add.s64 	%rd15, %rd12, %rd14;
	setp.lt.u64 	%p5, %rd15, %rd5;
	@%p5 bra 	$L__BB81_4;
	bra.uni 	$L__BB81_9;
$L__BB81_4:
	add.s64 	%rd13, %rd37, 1;
	add.s64 	%rd28, %rd36, %rd14;
	shl.b64 	%rd29, %rd28, 2;
	add.s64 	%rd30, %rd7, %rd29;
	ld.f32 	%f5, [%rd30+-4];
	setp.geu.f32 	%p6, %f1, %f5;
	selp.b64 	%rd37, %rd13, %rd37, %p6;
	selp.b64 	%rd36, %rd36, %rd12, %p6;
	add.s64 	%rd38, %rd38, 1;
	setp.ne.s64 	%p7, %rd36, 0;
	setp.lt.u64 	%p8, %rd37, 7;
	and.pred  	%p9, %p7, %p8;
	@%p9 bra 	$L__BB81_3;
	cvt.rn.f32.u64 	%f6, %rd38;
$L__BB81_6:
	ld.global.nc.u64 	%rd8, [%rd1+24];
	setp.lt.u64 	%p10, %rd6, %rd8;
	@%p10 bra 	$L__BB81_7;
	bra.uni 	$L__BB81_8;
$L__BB81_7:
	ld.global.nc.u64 	%rd33, [%rd1+16];
	add.s64 	%rd35, %rd33, %rd23;
	st.f32 	[%rd35], %f6;
	ret;
$L__BB81_9:
	mov.u64 	%rd26, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_58;
	cvta.global.u64 	%rd27, %rd26;
	{ // callseq 64, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd15;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd5;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd27;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 64
$L__BB81_10:
	mov.u64 	%rd21, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_56;
	cvta.global.u64 	%rd22, %rd21;
	{ // callseq 63, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd6;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd5;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd22;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 63
$L__BB81_8:
	mov.u64 	%rd31, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_57;
	cvta.global.u64 	%rd32, %rd31;
	{ // callseq 65, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd6;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd8;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd32;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 65

}
	// .globl	insert_hash_table_f32
.visible .entry insert_hash_table_f32(
	.param .u64 insert_hash_table_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<31>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<85>;

	ld.param.u64 	%rd37, [insert_hash_table_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd37;
	ld.global.nc.u64 	%rd83, [%rd1];
	ld.global.nc.u64 	%rd38, [%rd1+8];
	mov.u32 	%r5, %tid.x;
	cvt.u64.u32 	%rd39, %r5;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mul.wide.u32 	%rd41, %r7, %r6;
	add.s64 	%rd80, %rd41, %rd39;
	mov.u32 	%r8, %nctaid.x;
	mul.wide.u32 	%rd5, %r6, %r8;
	ld.global.nc.u64 	%rd84, [%rd1+16];
	ld.global.nc.u64 	%rd8, [%rd1+24];
	setp.le.u64 	%p2, %rd38, %rd80;
	not.b64 	%rd44, %rd80;
	mov.u64 	%rd79, 0;
	mov.u64 	%rd77, %rd79;
	@%p2 bra 	$L__BB82_5;
	max.u64 	%rd43, %rd38, %rd80;
	add.s64 	%rd10, %rd44, %rd43;
	or.b64  	%rd45, %rd10, %rd5;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.ne.s64 	%p3, %rd46, 0;
	@%p3 bra 	$L__BB82_3;
	bra.uni 	$L__BB82_2;
$L__BB82_3:
	div.u64 	%rd76, %rd10, %rd5;
	bra.uni 	$L__BB82_4;
$L__BB82_2:
	cvt.u32.u64 	%r9, %rd5;
	cvt.u32.u64 	%r10, %rd10;
	div.u32 	%r11, %r10, %r9;
	cvt.u64.u32 	%rd76, %r11;
$L__BB82_4:
	add.s64 	%rd77, %rd76, 1;
$L__BB82_5:
	setp.le.u64 	%p4, %rd8, %rd80;
	@%p4 bra 	$L__BB82_10;
	max.u64 	%rd48, %rd8, %rd80;
	add.s64 	%rd16, %rd44, %rd48;
	or.b64  	%rd50, %rd16, %rd5;
	and.b64  	%rd51, %rd50, -4294967296;
	setp.ne.s64 	%p5, %rd51, 0;
	@%p5 bra 	$L__BB82_8;
	bra.uni 	$L__BB82_7;
$L__BB82_8:
	div.u64 	%rd78, %rd16, %rd5;
	bra.uni 	$L__BB82_9;
$L__BB82_7:
	cvt.u32.u64 	%r12, %rd5;
	cvt.u32.u64 	%r13, %rd16;
	div.u32 	%r14, %r13, %r12;
	cvt.u64.u32 	%rd78, %r14;
$L__BB82_9:
	add.s64 	%rd79, %rd78, 1;
$L__BB82_10:
	min.u64 	%rd22, %rd77, %rd79;
	setp.eq.s64 	%p6, %rd22, 0;
	@%p6 bra 	$L__BB82_21;
	shl.b64 	%rd40, %rd38, 2;
	shl.b64 	%rd42, %rd8, 2;
	add.s64 	%rd3, %rd83, %rd40;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd9, %rd84, %rd42;
	ld.global.nc.u64 	%rd23, [%rd1+40];
	ld.global.nc.u64 	%rd24, [%rd1+32];
	ld.global.nc.u64 	%rd25, [%rd1+56];
	ld.global.nc.u64 	%rd26, [%rd1+48];
	mov.u64 	%rd81, 0;
	mov.b32 	%r26, 2139095039;
	mov.u64 	%rd82, %rd81;
	bra.uni 	$L__BB82_14;
$L__BB82_13:
	setp.eq.s64 	%p13, %rd82, %rd22;
	mov.u64 	%rd80, 0;
	mov.u64 	%rd81, %rd6;
	@%p13 bra 	$L__BB82_21;
$L__BB82_14:
	add.s64 	%rd82, %rd82, 1;
	add.s64 	%rd53, %rd81, %rd80;
	sub.s64 	%rd54, %rd3, %rd83;
	shr.u64 	%rd55, %rd54, 2;
	setp.gt.u64 	%p7, %rd55, %rd53;
	shl.b64 	%rd56, %rd53, 2;
	add.s64 	%rd57, %rd83, %rd56;
	add.s64 	%rd58, %rd57, 4;
	selp.b64 	%rd83, %rd58, %rd3, %p7;
	selp.b64 	%rd59, %rd57, 0, %p7;
	sub.s64 	%rd60, %rd9, %rd84;
	shr.u64 	%rd61, %rd60, 2;
	setp.gt.u64 	%p8, %rd61, %rd53;
	add.s64 	%rd62, %rd84, %rd56;
	add.s64 	%rd63, %rd62, 4;
	selp.b64 	%rd84, %rd63, %rd9, %p8;
	selp.b64 	%rd64, %rd62, 0, %p8;
	ld.f32 	%f1, [%rd59];
	ld.f32 	%f2, [%rd64];
	mov.b32 	%r15, %f1;
	shr.u32 	%r16, %r15, 16;
	xor.b32  	%r17, %r16, %r15;
	mul.lo.s32 	%r18, %r17, -2048144789;
	shr.u32 	%r19, %r18, 13;
	xor.b32  	%r20, %r19, %r18;
	mul.lo.s32 	%r21, %r20, -1028477387;
	shr.u32 	%r22, %r21, 16;
	xor.b32  	%r23, %r22, %r21;
	and.b32  	%r30, %r23, 1023;
	bra.uni 	$L__BB82_15;
$L__BB82_12:
	add.s32 	%r27, %r30, 1;
	and.b32  	%r30, %r27, 134217727;
	@%p1 bra 	$L__BB82_15;
	bra.uni 	$L__BB82_13;
$L__BB82_15:
	cvt.u64.u32 	%rd35, %r30;
	setp.gt.u64 	%p9, %rd23, %rd35;
	@%p9 bra 	$L__BB82_17;
	bra.uni 	$L__BB82_16;
$L__BB82_17:
	shl.b64 	%rd70, %rd35, 2;
	add.s64 	%rd69, %rd70, %rd24;
	// begin inline asm
	atom.global.cas.b32 %r24,[%rd69], %r26, %r15;
	// end inline asm
	mov.b32 	%f3, %r24;
	setp.neu.f32 	%p10, %f3, 0f7F7FFFFF;
	setp.neu.f32 	%p11, %f3, %f1;
	and.pred  	%p1, %p10, %p11;
	@%p1 bra 	$L__BB82_12;
	setp.gt.u64 	%p12, %rd25, %rd35;
	@%p12 bra 	$L__BB82_19;
	bra.uni 	$L__BB82_20;
$L__BB82_19:
	add.s64 	%rd74, %rd26, %rd70;
	st.f32 	[%rd74], %f2;
	@%p1 bra 	$L__BB82_15;
	bra.uni 	$L__BB82_13;
$L__BB82_21:
	ret;
$L__BB82_16:
	mov.u64 	%rd65, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_19;
	cvta.global.u64 	%rd66, %rd65;
	mov.u64 	%rd67, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_69;
	cvta.global.u64 	%rd68, %rd67;
	{ // callseq 66, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd66;
	.param .b64 param1;
	st.param.b64 	[param1+0], 40;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd68;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 66
$L__BB82_20:
	mov.u64 	%rd71, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_60;
	cvta.global.u64 	%rd72, %rd71;
	{ // callseq 67, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd35;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd25;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd72;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 67

}
	// .globl	lookup_hash_table_f32
.visible .entry lookup_hash_table_f32(
	.param .u64 lookup_hash_table_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<30>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<83>;

	ld.param.u64 	%rd38, [lookup_hash_table_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd38;
	ld.global.nc.u64 	%rd78, [%rd1];
	ld.global.nc.u64 	%rd39, [%rd1+8];
	mov.u32 	%r5, %tid.x;
	cvt.u64.u32 	%rd40, %r5;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mul.wide.u32 	%rd42, %r7, %r6;
	add.s64 	%rd79, %rd42, %rd40;
	mov.u32 	%r8, %nctaid.x;
	mul.wide.u32 	%rd5, %r6, %r8;
	ld.global.nc.u64 	%rd81, [%rd1+16];
	ld.global.nc.u64 	%rd8, [%rd1+24];
	setp.le.u64 	%p2, %rd39, %rd79;
	not.b64 	%rd45, %rd79;
	mov.u64 	%rd77, 0;
	mov.u64 	%rd75, %rd77;
	@%p2 bra 	$L__BB83_5;
	max.u64 	%rd44, %rd39, %rd79;
	add.s64 	%rd10, %rd45, %rd44;
	or.b64  	%rd46, %rd10, %rd5;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.ne.s64 	%p3, %rd47, 0;
	@%p3 bra 	$L__BB83_3;
	bra.uni 	$L__BB83_2;
$L__BB83_3:
	div.u64 	%rd74, %rd10, %rd5;
	bra.uni 	$L__BB83_4;
$L__BB83_2:
	cvt.u32.u64 	%r9, %rd5;
	cvt.u32.u64 	%r10, %rd10;
	div.u32 	%r11, %r10, %r9;
	cvt.u64.u32 	%rd74, %r11;
$L__BB83_4:
	add.s64 	%rd75, %rd74, 1;
$L__BB83_5:
	setp.le.u64 	%p4, %rd8, %rd79;
	@%p4 bra 	$L__BB83_10;
	max.u64 	%rd49, %rd8, %rd79;
	add.s64 	%rd16, %rd45, %rd49;
	or.b64  	%rd51, %rd16, %rd5;
	and.b64  	%rd52, %rd51, -4294967296;
	setp.ne.s64 	%p5, %rd52, 0;
	@%p5 bra 	$L__BB83_8;
	bra.uni 	$L__BB83_7;
$L__BB83_8:
	div.u64 	%rd76, %rd16, %rd5;
	bra.uni 	$L__BB83_9;
$L__BB83_7:
	cvt.u32.u64 	%r12, %rd5;
	cvt.u32.u64 	%r13, %rd16;
	div.u32 	%r14, %r13, %r12;
	cvt.u64.u32 	%rd76, %r14;
$L__BB83_9:
	add.s64 	%rd77, %rd76, 1;
$L__BB83_10:
	min.u64 	%rd22, %rd75, %rd77;
	setp.eq.s64 	%p6, %rd22, 0;
	@%p6 bra 	$L__BB83_22;
	shl.b64 	%rd41, %rd39, 2;
	shl.b64 	%rd43, %rd8, 2;
	add.s64 	%rd3, %rd78, %rd41;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd9, %rd81, %rd43;
	ld.global.nc.u64 	%rd23, [%rd1+40];
	ld.global.nc.u64 	%rd24, [%rd1+32];
	ld.global.nc.u64 	%rd25, [%rd1+56];
	ld.global.nc.u64 	%rd26, [%rd1+48];
	mov.u64 	%rd80, 0;
	mov.pred 	%p15, 0;
	mov.pred 	%p12, -1;
	mov.u64 	%rd82, %rd80;
	bra.uni 	$L__BB83_14;
$L__BB83_13:
	setp.eq.s64 	%p16, %rd82, %rd22;
	mov.u64 	%rd79, 0;
	mov.u64 	%rd80, %rd6;
	@%p16 bra 	$L__BB83_22;
$L__BB83_14:
	add.s64 	%rd82, %rd82, 1;
	add.s64 	%rd54, %rd80, %rd79;
	sub.s64 	%rd55, %rd3, %rd78;
	shr.u64 	%rd56, %rd55, 2;
	setp.gt.u64 	%p7, %rd56, %rd54;
	shl.b64 	%rd57, %rd54, 2;
	add.s64 	%rd58, %rd78, %rd57;
	add.s64 	%rd59, %rd58, 4;
	selp.b64 	%rd78, %rd59, %rd3, %p7;
	selp.b64 	%rd60, %rd58, 0, %p7;
	sub.s64 	%rd61, %rd9, %rd81;
	shr.u64 	%rd62, %rd61, 2;
	setp.gt.u64 	%p8, %rd62, %rd54;
	add.s64 	%rd63, %rd81, %rd57;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd81, %rd64, %rd9, %p8;
	selp.b64 	%rd35, %rd63, 0, %p8;
	ld.f32 	%f1, [%rd60];
	mov.b32 	%r15, %f1;
	shr.u32 	%r16, %r15, 16;
	xor.b32  	%r17, %r16, %r15;
	mul.lo.s32 	%r18, %r17, -2048144789;
	shr.u32 	%r19, %r18, 13;
	xor.b32  	%r20, %r19, %r18;
	mul.lo.s32 	%r21, %r20, -1028477387;
	shr.u32 	%r22, %r21, 16;
	xor.b32  	%r23, %r22, %r21;
	and.b32  	%r29, %r23, 1023;
	bra.uni 	$L__BB83_15;
$L__BB83_20:
	add.s32 	%r24, %r29, 1;
	and.b32  	%r29, %r24, 134217727;
	@%p12 bra 	$L__BB83_15;
	bra.uni 	$L__BB83_13;
$L__BB83_15:
	cvt.u64.u32 	%rd36, %r29;
	setp.le.u64 	%p9, %rd23, %rd36;
	@%p9 bra 	$L__BB83_23;
	shl.b64 	%rd67, %rd36, 2;
	add.s64 	%rd68, %rd24, %rd67;
	ld.f32 	%f2, [%rd68];
	setp.eq.f32 	%p10, %f2, %f1;
	@%p10 bra 	$L__BB83_17;
	bra.uni 	$L__BB83_19;
$L__BB83_17:
	setp.gt.u64 	%p14, %rd25, %rd36;
	@%p14 bra 	$L__BB83_18;
	bra.uni 	$L__BB83_21;
$L__BB83_18:
	add.s64 	%rd72, %rd26, %rd67;
	ld.f32 	%f3, [%rd72];
	st.f32 	[%rd35], %f3;
	@%p15 bra 	$L__BB83_15;
	bra.uni 	$L__BB83_13;
$L__BB83_19:
	setp.eq.f32 	%p11, %f2, 0f7F7FFFFF;
	@%p11 bra 	$L__BB83_12;
	bra.uni 	$L__BB83_20;
$L__BB83_12:
	mov.b32 	%r25, 0;
	st.u32 	[%rd35], %r25;
	@%p15 bra 	$L__BB83_15;
	bra.uni 	$L__BB83_13;
$L__BB83_22:
	ret;
$L__BB83_23:
	mov.u64 	%rd65, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_61;
	cvta.global.u64 	%rd66, %rd65;
	{ // callseq 68, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd36;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd23;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd66;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 68
$L__BB83_21:
	mov.u64 	%rd69, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_62;
	cvta.global.u64 	%rd70, %rd69;
	{ // callseq 69, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd36;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd25;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd70;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 69

}
	// .globl	delete_hash_table_f32
.visible .entry delete_hash_table_f32(
	.param .u64 delete_hash_table_f32_param_0
)
{
	.reg .pred 	%p<26>;
	.reg .b32 	%r<39>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<61>;

	ld.param.u64 	%rd26, [delete_hash_table_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd26;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	mov.u32 	%r9, %tid.x;
	cvt.u64.u32 	%rd27, %r9;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	shl.b64 	%rd28, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd28;
	mul.wide.u32 	%rd29, %r11, %r10;
	add.s64 	%rd5, %rd29, %rd27;
	setp.eq.s64 	%p3, %rd5, 0;
	@%p3 bra 	$L__BB84_1;
	setp.gt.u64 	%p4, %rd3, %rd5;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd31, %rd2, %rd30;
	add.s64 	%rd32, %rd31, 4;
	selp.b64 	%rd19, %rd32, %rd4, %p4;
	selp.b64 	%rd60, %rd31, 0, %p4;
	bra.uni 	$L__BB84_15;
$L__BB84_1:
	setp.eq.s64 	%p5, %rd3, 0;
	selp.b64 	%rd33, 0, 4, %p5;
	add.s64 	%rd19, %rd2, %rd33;
	selp.b64 	%rd60, 0, %rd2, %p5;
$L__BB84_15:
	setp.eq.s64 	%p6, %rd60, 0;
	@%p6 bra 	$L__BB84_25;
	mov.u32 	%r12, %nctaid.x;
	mul.wide.u32 	%rd6, %r10, %r12;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f3, [%rd60];
	mov.b32 	%r13, %f3;
	shr.u32 	%r14, %r13, 16;
	xor.b32  	%r15, %r14, %r13;
	mul.lo.s32 	%r16, %r15, -2048144789;
	shr.u32 	%r17, %r16, 13;
	xor.b32  	%r18, %r17, %r16;
	mul.lo.s32 	%r19, %r18, -1028477387;
	shr.u32 	%r20, %r19, 16;
	xor.b32  	%r21, %r20, %r19;
	and.b32  	%r38, %r21, 1023;
	ld.global.nc.u64 	%rd21, [%rd1+40];
	ld.global.nc.u64 	%rd22, [%rd1+32];
	ld.global.nc.u64 	%rd23, [%rd1+56];
	ld.global.nc.u64 	%rd24, [%rd1+48];
	mov.b32 	%r23, 2139095039;
	mov.pred 	%p13, 0;
	mov.pred 	%p11, -1;
	bra.uni 	$L__BB84_17;
$L__BB84_21:
	setp.eq.f32 	%p10, %f4, 0f00000000;
	mov.pred 	%p25, %p13;
	@%p10 bra 	$L__BB84_23;
	bra.uni 	$L__BB84_22;
$L__BB84_23:
	@%p25 bra 	$L__BB84_17;
	bra.uni 	$L__BB84_2;
$L__BB84_17:
	cvt.u64.u32 	%rd25, %r38;
	setp.le.u64 	%p7, %rd21, %rd25;
	@%p7 bra 	$L__BB84_27;
	shl.b64 	%rd36, %rd25, 2;
	add.s64 	%rd37, %rd22, %rd36;
	ld.f32 	%f4, [%rd37];
	setp.eq.f32 	%p8, %f4, %f3;
	@%p8 bra 	$L__BB84_19;
	bra.uni 	$L__BB84_21;
$L__BB84_19:
	setp.gt.u64 	%p12, %rd23, %rd25;
	@%p12 bra 	$L__BB84_20;
	bra.uni 	$L__BB84_24;
$L__BB84_20:
	add.s64 	%rd41, %rd24, %rd36;
	st.u32 	[%rd41], %r23;
	mov.pred 	%p25, %p13;
	bra.uni 	$L__BB84_23;
$L__BB84_22:
	add.s32 	%r22, %r38, 1;
	and.b32  	%r38, %r22, 134217727;
	mov.pred 	%p25, %p11;
	bra.uni 	$L__BB84_23;
$L__BB84_2:
	sub.s64 	%rd42, %rd4, %rd19;
	shr.u64 	%rd43, %rd42, 2;
	setp.le.u64 	%p14, %rd43, %rd7;
	@%p14 bra 	$L__BB84_25;
	shl.b64 	%rd44, %rd6, 2;
	add.s64 	%rd13, %rd19, %rd44;
	add.s64 	%rd57, %rd13, -4;
	mov.pred 	%p21, 0;
	mov.pred 	%p19, -1;
	bra.uni 	$L__BB84_5;
$L__BB84_4:
	sub.s64 	%rd53, %rd4, %rd13;
	shr.u64 	%rd54, %rd53, 2;
	setp.le.u64 	%p22, %rd54, %rd7;
	setp.gt.u64 	%p23, %rd54, %rd7;
	add.s64 	%rd56, %rd13, %rd44;
	add.s64 	%rd57, %rd56, -4;
	selp.b64 	%rd13, %rd56, %rd4, %p23;
	@%p22 bra 	$L__BB84_25;
$L__BB84_5:
	ld.f32 	%f1, [%rd57];
	mov.b32 	%r24, %f1;
	shr.u32 	%r25, %r24, 16;
	xor.b32  	%r26, %r25, %r24;
	mul.lo.s32 	%r27, %r26, -2048144789;
	shr.u32 	%r28, %r27, 13;
	xor.b32  	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, -1028477387;
	shr.u32 	%r31, %r30, 16;
	xor.b32  	%r32, %r31, %r30;
	and.b32  	%r36, %r32, 1023;
	bra.uni 	$L__BB84_6;
$L__BB84_10:
	setp.eq.f32 	%p18, %f2, 0f00000000;
	mov.pred 	%p24, %p21;
	@%p18 bra 	$L__BB84_12;
	bra.uni 	$L__BB84_11;
$L__BB84_12:
	@%p24 bra 	$L__BB84_6;
	bra.uni 	$L__BB84_4;
$L__BB84_6:
	cvt.u64.u32 	%rd14, %r36;
	setp.le.u64 	%p15, %rd21, %rd14;
	@%p15 bra 	$L__BB84_26;
	shl.b64 	%rd47, %rd14, 2;
	add.s64 	%rd48, %rd22, %rd47;
	ld.f32 	%f2, [%rd48];
	setp.eq.f32 	%p16, %f2, %f1;
	@%p16 bra 	$L__BB84_8;
	bra.uni 	$L__BB84_10;
$L__BB84_8:
	setp.gt.u64 	%p20, %rd23, %rd14;
	@%p20 bra 	$L__BB84_9;
	bra.uni 	$L__BB84_13;
$L__BB84_9:
	add.s64 	%rd52, %rd24, %rd47;
	st.u32 	[%rd52], %r23;
	mov.pred 	%p24, %p21;
	bra.uni 	$L__BB84_12;
$L__BB84_11:
	add.s32 	%r33, %r36, 1;
	and.b32  	%r36, %r33, 134217727;
	mov.pred 	%p24, %p19;
	bra.uni 	$L__BB84_12;
$L__BB84_25:
	ret;
$L__BB84_26:
	mov.u64 	%rd45, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_63;
	cvta.global.u64 	%rd46, %rd45;
	{ // callseq 72, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd14;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd21;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd46;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 72
$L__BB84_13:
	mov.u64 	%rd49, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_64;
	cvta.global.u64 	%rd50, %rd49;
	{ // callseq 73, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd14;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd23;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd50;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 73
$L__BB84_27:
	mov.u64 	%rd34, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_63;
	cvta.global.u64 	%rd35, %rd34;
	{ // callseq 70, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd25;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd21;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd35;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 70
$L__BB84_24:
	mov.u64 	%rd38, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_64;
	cvta.global.u64 	%rd39, %rd38;
	{ // callseq 71, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd25;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd23;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd39;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 71

}
	// .globl	count_hash_table_f32
.visible .entry count_hash_table_f32(
	.param .u64 count_hash_table_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<73>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<47>;

	ld.param.u64 	%rd22, [count_hash_table_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd22;
	// begin inline asm
	.shared .align 4 .b8 nonphysical[128];
    mov.u32 %r14, nonphysical;
	// end inline asm
	mov.u32 	%r2, %tid.x;
	cvt.u64.u32 	%rd23, %r2;
	mov.u32 	%r3, %ntid.x;
	ld.global.nc.u64 	%rd3, [%rd1+32];
	ld.global.nc.u64 	%rd4, [%rd1+40];
	mov.u32 	%r15, %ctaid.x;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd5, %rd3, %rd24;
	mul.wide.u32 	%rd25, %r15, %r3;
	add.s64 	%rd6, %rd25, %rd23;
	setp.eq.s64 	%p1, %rd6, 0;
	@%p1 bra 	$L__BB85_1;
	setp.gt.u64 	%p2, %rd4, %rd6;
	shl.b64 	%rd26, %rd6, 2;
	add.s64 	%rd27, %rd3, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd45, %rd28, %rd5, %p2;
	selp.b64 	%rd46, %rd27, 0, %p2;
	bra.uni 	$L__BB85_3;
$L__BB85_1:
	setp.eq.s64 	%p3, %rd4, 0;
	selp.b64 	%rd29, 0, 4, %p3;
	add.s64 	%rd45, %rd3, %rd29;
	selp.b64 	%rd46, 0, %rd3, %p3;
$L__BB85_3:
	and.b64  	%rd2, %rd23, 31;
	setp.eq.s64 	%p4, %rd46, 0;
	mov.b32 	%r71, 0;
	@%p4 bra 	$L__BB85_7;
	mov.u32 	%r16, %nctaid.x;
	mul.wide.u32 	%rd7, %r3, %r16;
	add.s64 	%rd8, %rd7, -1;
	ld.f32 	%f1, [%rd46];
	setp.neu.f32 	%p5, %f1, 0f7F7FFFFF;
	selp.u32 	%r71, 1, 0, %p5;
	sub.s64 	%rd30, %rd5, %rd45;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd8;
	@%p6 bra 	$L__BB85_7;
	shl.b64 	%rd19, %rd7, 2;
	sub.s64 	%rd34, %rd5, %rd19;
	sub.s64 	%rd44, %rd34, %rd45;
	add.s64 	%rd35, %rd19, %rd45;
	add.s64 	%rd43, %rd35, -4;
$L__BB85_6:
	ld.f32 	%f2, [%rd43];
	setp.neu.f32 	%p7, %f2, 0f7F7FFFFF;
	selp.u32 	%r18, 1, 0, %p7;
	add.s32 	%r71, %r71, %r18;
	shr.u64 	%rd36, %rd44, 2;
	setp.gt.u64 	%p8, %rd36, %rd8;
	sub.s64 	%rd44, %rd44, %rd19;
	add.s64 	%rd43, %rd43, %rd19;
	@%p8 bra 	$L__BB85_6;
$L__BB85_7:
	mov.b32 	%r20, 16;
	mov.b32 	%r51, 31;
	// begin inline asm
	shfl.sync.bfly.b32 %r19,%r71, %r20, %r51, 4294967295;
	// end inline asm
	add.s32 	%r26, %r19, %r71;
	mov.b32 	%r24, 8;
	// begin inline asm
	shfl.sync.bfly.b32 %r23,%r26, %r24, %r51, 4294967295;
	// end inline asm
	add.s32 	%r30, %r23, %r26;
	mov.b32 	%r28, 4;
	// begin inline asm
	shfl.sync.bfly.b32 %r27,%r30, %r28, %r51, 4294967295;
	// end inline asm
	add.s32 	%r34, %r27, %r30;
	mov.b32 	%r32, 2;
	// begin inline asm
	shfl.sync.bfly.b32 %r31,%r34, %r32, %r51, 4294967295;
	// end inline asm
	add.s32 	%r38, %r31, %r34;
	mov.b32 	%r36, 1;
	// begin inline asm
	shfl.sync.bfly.b32 %r35,%r38, %r36, %r51, 4294967295;
	// end inline asm
	setp.ne.s64 	%p9, %rd2, 0;
	@%p9 bra 	$L__BB85_9;
	add.s32 	%r42, %r35, %r38;
	shr.u32 	%r39, %r2, 3;
	and.b32  	%r40, %r39, 124;
	add.s32 	%r41, %r14, %r40;
	// begin inline asm
	st.shared.u32 [%r41], %r42;
	// end inline asm
$L__BB85_9:
	bar.sync 	0;
	shr.u32 	%r44, %r3, 5;
	setp.ge.u32 	%p10, %r2, %r44;
	mov.b32 	%r72, 0;
	@%p10 bra 	$L__BB85_11;
	cvt.u32.u64 	%r45, %rd2;
	shl.b32 	%r46, %r45, 2;
	add.s32 	%r48, %r14, %r46;
	// begin inline asm
	ld.shared.u32 %r72, [%r48];
	// end inline asm
$L__BB85_11:
	setp.gt.u32 	%p11, %r2, 31;
	@%p11 bra 	$L__BB85_13;
	setp.eq.s64 	%p12, %rd2, 0;
	// begin inline asm
	shfl.sync.bfly.b32 %r49,%r72, %r20, %r51, 4294967295;
	// end inline asm
	add.s32 	%r56, %r49, %r72;
	// begin inline asm
	shfl.sync.bfly.b32 %r53,%r56, %r24, %r51, 4294967295;
	// end inline asm
	add.s32 	%r60, %r53, %r56;
	// begin inline asm
	shfl.sync.bfly.b32 %r57,%r60, %r28, %r51, 4294967295;
	// end inline asm
	add.s32 	%r64, %r57, %r60;
	// begin inline asm
	shfl.sync.bfly.b32 %r61,%r64, %r32, %r51, 4294967295;
	// end inline asm
	add.s32 	%r68, %r61, %r64;
	// begin inline asm
	shfl.sync.bfly.b32 %r65,%r68, %r36, %r51, 4294967295;
	// end inline asm
	@%p12 bra 	$L__BB85_14;
	bra.uni 	$L__BB85_13;
$L__BB85_14:
	ld.global.nc.u64 	%rd37, [%rd1+72];
	setp.ne.s64 	%p13, %rd37, 0;
	@%p13 bra 	$L__BB85_16;
	bra.uni 	$L__BB85_15;
$L__BB85_16:
	add.s32 	%r69, %r65, %r68;
	ld.global.nc.u64 	%rd38, [%rd1+64];
	// begin inline asm
	red.global.add.u32 [%rd38], %r69;
	// end inline asm
$L__BB85_13:
	ret;
$L__BB85_15:
	mov.u64 	%rd39, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_19;
	cvta.global.u64 	%rd40, %rd39;
	mov.u64 	%rd41, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_65;
	cvta.global.u64 	%rd42, %rd41;
	{ // callseq 74, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd40;
	.param .b64 param1;
	st.param.b64 	[param1+0], 40;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd42;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 74

}
	// .globl	iterate_hash_table_f32
.visible .entry iterate_hash_table_f32(
	.param .u64 iterate_hash_table_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<16>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<128>;

	ld.param.u64 	%rd58, [iterate_hash_table_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd58;
	ld.global.nc.u64 	%rd2, [%rd1+32];
	ld.global.nc.u64 	%rd3, [%rd1+40];
	mov.u32 	%r2, %tid.x;
	cvt.u64.u32 	%rd59, %r2;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mul.wide.u32 	%rd61, %r4, %r3;
	add.s64 	%rd5, %rd61, %rd59;
	mov.u32 	%r5, %nctaid.x;
	mul.wide.u32 	%rd6, %r3, %r5;
	ld.global.nc.u64 	%rd8, [%rd1+48];
	ld.global.nc.u64 	%rd9, [%rd1+56];
	setp.le.u64 	%p1, %rd3, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd116, 0;
	mov.u64 	%rd114, %rd116;
	@%p1 bra 	$L__BB86_5;
	max.u64 	%rd63, %rd3, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p2, %rd66, 0;
	@%p2 bra 	$L__BB86_3;
	bra.uni 	$L__BB86_2;
$L__BB86_3:
	div.u64 	%rd113, %rd11, %rd6;
	bra.uni 	$L__BB86_4;
$L__BB86_2:
	cvt.u32.u64 	%r6, %rd6;
	cvt.u32.u64 	%r7, %rd11;
	div.u32 	%r8, %r7, %r6;
	cvt.u64.u32 	%rd113, %r8;
$L__BB86_4:
	add.s64 	%rd114, %rd113, 1;
$L__BB86_5:
	setp.le.u64 	%p3, %rd9, %rd5;
	@%p3 bra 	$L__BB86_10;
	max.u64 	%rd68, %rd9, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p4, %rd71, 0;
	@%p4 bra 	$L__BB86_8;
	bra.uni 	$L__BB86_7;
$L__BB86_8:
	div.u64 	%rd115, %rd17, %rd6;
	bra.uni 	$L__BB86_9;
$L__BB86_7:
	cvt.u32.u64 	%r9, %rd6;
	cvt.u32.u64 	%r10, %rd17;
	div.u32 	%r11, %r10, %r9;
	cvt.u64.u32 	%rd115, %r11;
$L__BB86_9:
	add.s64 	%rd116, %rd115, 1;
$L__BB86_10:
	min.u64 	%rd23, %rd114, %rd116;
	setp.eq.s64 	%p5, %rd23, 0;
	@%p5 bra 	$L__BB86_31;
	shl.b64 	%rd60, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd4, %rd2, %rd60;
	setp.eq.s64 	%p6, %rd5, 0;
	shl.b64 	%rd112, %rd5, 2;
	@%p6 bra 	$L__BB86_13;
	setp.gt.u64 	%p7, %rd3, %rd5;
	add.s64 	%rd73, %rd2, %rd112;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd122, %rd74, %rd4, %p7;
	selp.b64 	%rd118, %rd73, 0, %p7;
	bra.uni 	$L__BB86_14;
$L__BB86_13:
	setp.eq.s64 	%p8, %rd3, 0;
	selp.b64 	%rd75, 0, 4, %p8;
	add.s64 	%rd122, %rd2, %rd75;
	selp.b64 	%rd118, 0, %rd2, %p8;
$L__BB86_14:
	add.s64 	%rd10, %rd8, %rd62;
	@%p6 bra 	$L__BB86_16;
	setp.gt.u64 	%p10, %rd9, %rd5;
	add.s64 	%rd77, %rd8, %rd112;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd123, %rd78, %rd10, %p10;
	selp.b64 	%rd120, %rd77, 0, %p10;
	bra.uni 	$L__BB86_17;
$L__BB86_16:
	setp.eq.s64 	%p11, %rd9, 0;
	selp.b64 	%rd79, 0, 4, %p11;
	add.s64 	%rd123, %rd8, %rd79;
	selp.b64 	%rd120, 0, %rd8, %p11;
$L__BB86_17:
	ld.f32 	%f1, [%rd118];
	setp.eq.f32 	%p12, %f1, 0f7F7FFFFF;
	@%p12 bra 	$L__BB86_22;
	ld.global.nc.u64 	%rd80, [%rd1+72];
	setp.eq.s64 	%p13, %rd80, 0;
	@%p13 bra 	$L__BB86_26;
	ld.f32 	%f2, [%rd120];
	ld.global.nc.u64 	%rd40, [%rd1+8];
	cvt.u32.u64 	%r13, %rd40;
	ld.global.nc.u64 	%rd81, [%rd1+64];
	// begin inline asm
	atom.global.inc.u32 %r12,[%rd81], %r13;
	// end inline asm
	cvt.u64.u32 	%rd127, %r12;
	setp.le.u64 	%p14, %rd40, %rd127;
	@%p14 bra 	$L__BB86_32;
	ld.global.nc.u64 	%rd82, [%rd1];
	shl.b64 	%rd83, %rd127, 2;
	add.s64 	%rd84, %rd82, %rd83;
	st.f32 	[%rd84], %f1;
	ld.global.nc.u64 	%rd43, [%rd1+24];
	setp.le.u64 	%p15, %rd43, %rd127;
	@%p15 bra 	$L__BB86_33;
	ld.global.nc.u64 	%rd85, [%rd1+16];
	add.s64 	%rd87, %rd85, %rd83;
	st.f32 	[%rd87], %f2;
$L__BB86_22:
	setp.eq.s64 	%p16, %rd23, 1;
	@%p16 bra 	$L__BB86_31;
	bra.uni 	$L__BB86_23;
$L__BB86_31:
	ret;
$L__BB86_23:
	add.s64 	%rd7, %rd6, -1;
	ld.global.nc.u64 	%rd39, [%rd1+72];
	ld.global.nc.u64 	%rd40, [%rd1+8];
	cvt.u32.u64 	%r15, %rd40;
	ld.global.nc.u64 	%rd99, [%rd1+64];
	ld.global.nc.u64 	%rd42, [%rd1];
	ld.global.nc.u64 	%rd43, [%rd1+24];
	ld.global.nc.u64 	%rd44, [%rd1+16];
	add.s64 	%rd46, %rd23, -1;
	setp.ne.s64 	%p20, %rd39, 0;
	bra.uni 	$L__BB86_24;
$L__BB86_30:
	selp.b64 	%rd122, %rd91, %rd4, %p17;
	selp.b64 	%rd123, %rd96, %rd10, %p18;
	add.s64 	%rd46, %rd46, -1;
	setp.ne.s64 	%p23, %rd46, 0;
	@%p23 bra 	$L__BB86_24;
	bra.uni 	$L__BB86_31;
$L__BB86_24:
	sub.s64 	%rd88, %rd4, %rd122;
	shr.u64 	%rd89, %rd88, 2;
	setp.gt.u64 	%p17, %rd89, %rd7;
	shl.b64 	%rd90, %rd6, 2;
	add.s64 	%rd91, %rd122, %rd90;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p17;
	sub.s64 	%rd94, %rd10, %rd123;
	shr.u64 	%rd95, %rd94, 2;
	setp.gt.u64 	%p18, %rd95, %rd7;
	add.s64 	%rd96, %rd123, %rd90;
	ld.f32 	%f3, [%rd93];
	setp.eq.f32 	%p19, %f3, 0f7F7FFFFF;
	@%p19 bra 	$L__BB86_30;
	@%p20 bra 	$L__BB86_27;
	bra.uni 	$L__BB86_26;
$L__BB86_27:
	add.s64 	%rd97, %rd96, -4;
	selp.b64 	%rd98, %rd97, 0, %p18;
	ld.f32 	%f4, [%rd98];
	// begin inline asm
	atom.global.inc.u32 %r14,[%rd99], %r15;
	// end inline asm
	cvt.u64.u32 	%rd127, %r14;
	setp.le.u64 	%p21, %rd40, %rd127;
	@%p21 bra 	$L__BB86_32;
	shl.b64 	%rd102, %rd127, 2;
	add.s64 	%rd103, %rd42, %rd102;
	st.f32 	[%rd103], %f3;
	setp.gt.u64 	%p22, %rd43, %rd127;
	@%p22 bra 	$L__BB86_29;
	bra.uni 	$L__BB86_33;
$L__BB86_29:
	add.s64 	%rd107, %rd44, %rd102;
	st.f32 	[%rd107], %f4;
	bra.uni 	$L__BB86_30;
$L__BB86_33:
	mov.u64 	%rd104, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_68;
	cvta.global.u64 	%rd105, %rd104;
	{ // callseq 76, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd127;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd43;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd105;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 76
$L__BB86_32:
	mov.u64 	%rd100, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_67;
	cvta.global.u64 	%rd101, %rd100;
	{ // callseq 75, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd127;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd40;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd101;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 75
$L__BB86_26:
	mov.u64 	%rd108, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_19;
	cvta.global.u64 	%rd109, %rd108;
	mov.u64 	%rd110, anon_$_9f9aa6c1f4ee146dd317953f26141824_$_66;
	cvta.global.u64 	%rd111, %rd110;
	{ // callseq 77, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd109;
	.param .b64 param1;
	st.param.b64 	[param1+0], 40;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd111;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 77

}