//
// Generated by LLVM NVPTX Back-End
//

.version 7.0
.target sm_80
.address_size 64

	// .globl	fft_forward_128_kernel
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E
(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_3
)
;
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E
(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_3
)
;
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E
(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_3
)
;
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E
(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_3
)
;
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E
(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_3
)
;
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE
(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_3
)
;
.visible .func _ZN4core9panicking5panic17hc7c8a74e6511bb99E
(
.param .b64 _ZN4core9panicking5panic17hc7c8a74e6511bb99E_param_0,
	.param .b64 _ZN4core9panicking5panic17hc7c8a74e6511bb99E_param_1,
	.param .b64 _ZN4core9panicking5panic17hc7c8a74e6511bb99E_param_2
)
.noreturn{
	trap;
	exit;
}
.visible .func _ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E
(
.param .b64 _ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E_param_0
)
.noreturn{
	trap;
	exit;
}
.visible .func _ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E
(
.param .b64 _ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E_param_0
)
.noreturn{
	trap;
	exit;
}
.visible .func _ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E
(
.param .b64 _ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E_param_0,
	.param .b64 _ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E_param_1,
	.param .b64 _ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E_param_2
)
.noreturn{
	trap;
	exit;
}
.global .align 1 .b8 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_0[89] = {47, 114, 117, 115, 116, 99, 47, 56, 101, 56, 54, 99, 57, 53, 54, 55, 49, 53, 52, 100, 99, 53, 97, 57, 97, 100, 97, 49, 53, 97, 98, 49, 57, 54, 100, 50, 51, 101, 97, 101, 50, 98, 100, 55, 100, 56, 57, 47, 108, 105, 98, 114, 97, 114, 121, 47, 99, 111, 114, 101, 47, 115, 114, 99, 47, 105, 116, 101, 114, 47, 97, 100, 97, 112, 116, 101, 114, 115, 47, 115, 116, 101, 112, 95, 98, 121, 46, 114, 115};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_0), 89, 107374182681};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_0), 89, 120259084570};
.global .align 1 .b8 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_3[49] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 115, 105, 103, 110, 97, 108, 92, 102, 111, 117, 114, 105, 101, 114, 92, 112, 116, 120, 95, 102, 111, 117, 114, 105, 101, 114, 46, 114, 115};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_4[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_3), 49, 244813136093};
.global .align 1 .b8 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_5[27] = {97, 115, 115, 101, 114, 116, 105, 111, 110, 32, 102, 97, 105, 108, 101, 100, 58, 32, 115, 116, 101, 112, 32, 33, 61, 32, 48};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_6[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_0), 89, 38654705699};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_7[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_3), 49, 244813136131};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_8[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_3), 49, 244813136169};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_9[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_3), 49, 244813136206};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_10[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_3), 49, 266287972780};
.global .align 1 .b8 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_11[49] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 115, 105, 103, 110, 97, 108, 92, 119, 97, 118, 101, 108, 101, 116, 92, 112, 116, 120, 95, 119, 97, 118, 101, 108, 101, 116, 46, 114, 115};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_12[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_11), 49, 261993005168};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_13[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_11), 49, 300647710845};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_14[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_11), 49, 261993005220};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_15[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_11), 49, 300647710897};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_16[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_11), 49, 210453397727};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_17[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_11), 49, 210453397746};
.global .align 1 .b8 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18[19] = {110, 111, 116, 32, 121, 101, 116, 32, 105, 109, 112, 108, 101, 109, 101, 110, 116, 101, 100};
.global .align 1 .b8 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_19[35] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 115, 104, 97, 114, 101, 100, 92, 102, 108, 111, 97, 116, 46, 114, 115};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_20[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_19), 35, 38654705867};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_21[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_19), 35, 38654705872};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_22[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_19), 35, 38654705877};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_23[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_19), 35, 38654705940};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_24[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_19), 35, 38654705945};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_25[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_19), 35, 38654705950};
.global .align 1 .b8 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_26[39] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 115, 104, 97, 114, 101, 100, 92, 112, 114, 105, 109, 105, 116, 105, 118, 101, 46, 114, 115};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_27[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_26), 39, 38654706006};
.global .align 1 .b8 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_28[38] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 115, 104, 97, 114, 101, 100, 92, 117, 110, 115, 105, 103, 110, 101, 100, 46, 114, 115};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_29[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_28), 38, 60129542290};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_30[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_28), 38, 38654705833};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_31[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_28), 38, 60129542519};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_32[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_28), 38, 38654706062};
.global .align 1 .b8 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_33[40] = {97, 115, 115, 101, 114, 116, 105, 111, 110, 32, 102, 97, 105, 108, 101, 100, 58, 32, 105, 110, 100, 101, 120, 32, 60, 32, 115, 101, 108, 102, 46, 112, 116, 114, 46, 108, 101, 110, 40, 41};
.global .align 1 .b8 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_34[41] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 99, 117, 100, 97, 92, 103, 108, 111, 98, 97, 108, 92, 97, 116, 111, 109, 105, 99, 46, 114, 115};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_35[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_34), 41, 38654705859};
.global .align 1 .b8 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36[47] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 115, 104, 97, 114, 101, 100, 92, 118, 101, 99, 116, 111, 114, 92, 112, 116, 120, 95, 118, 101, 99, 116, 111, 114, 46, 114, 115};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_37[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 21474836543};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_38[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 21474836590};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_39[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 21474836639};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_40[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 21474836686};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_41[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 21474836692};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_42[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 21474836698};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_43[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 73014444256};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_44[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 73014444267};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_45[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 73014444277};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_46[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 73014444288};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_47[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 73014444311};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_48[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 73014444324};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_49[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 73014444337};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_50[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 73014444568};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_51[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 73014444577};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_52[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 73014444585};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_53[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 73014444594};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_54[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 73014444611};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_55[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 73014444620};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_56[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 73014444629};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_57[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 21474837333};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_58[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_36), 47, 21474837338};
.global .align 1 .b8 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_59[54] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 103, 114, 97, 112, 104, 92, 104, 97, 115, 104, 95, 116, 97, 98, 108, 101, 92, 112, 116, 120, 95, 104, 97, 115, 104, 95, 116, 97, 98, 108, 101, 46, 114, 115};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_60[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_59), 54, 73014444068};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_61[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_59), 54, 68719476791};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_62[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_59), 54, 111669149752};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_63[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_59), 54, 68719476813};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_64[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_59), 54, 73014444110};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_65[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_34), 41, 38654706152};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_66[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_34), 41, 38654706042};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_67[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_59), 54, 55834574995};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_68[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_59), 54, 55834574996};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_69[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_34), 41, 38654705779};
.global .align 1 .b8 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70[27] = {97, 115, 115, 101, 114, 116, 105, 111, 110, 32, 102, 97, 105, 108, 101, 100, 58, 32, 105, 110, 100, 101, 120, 32, 60, 32, 78};
.global .align 1 .b8 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_71[34] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 99, 117, 100, 97, 92, 115, 104, 97, 114, 101, 100, 46, 114, 115};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_71), 34, 38654705781};
.global .align 8 .u64 anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73[3] = {generic(anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_71), 34, 38654705800};

.visible .entry fft_forward_128_kernel(
	.param .u64 fft_forward_128_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot0[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<18>;
	.reg .b32 	%r<50>;
	.reg .b64 	%rd<170>;

	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd67, [fft_forward_128_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd67;
	add.u64 	%rd69, %SP, 16;
	add.u64 	%rd70, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[1024];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd70], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd71, [%rd1+8];
	and.b64  	%rd72, %rd71, -128;
	mul.wide.u32 	%rd73, %r5, 128;
	setp.lt.u64 	%p3, %rd73, %rd72;
	sub.s64 	%rd75, %rd71, %rd73;
	setp.gt.u64 	%p4, %rd75, 127;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB0_2;
	bra.uni 	$L__BB0_1;
$L__BB0_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd77, %rd3, 128;
	setp.gt.u32 	%p6, %r6, 127;
	not.b64 	%rd78, %rd3;
	add.s64 	%rd9, %rd78, %rd77;
	mov.u64 	%rd156, 0;
	and.b64  	%rd152, %rd9, -4294967296;
	mov.u64 	%rd154, %rd156;
	@%p6 bra 	$L__BB0_7;
	setp.ne.s64 	%p7, %rd152, 0;
	@%p7 bra 	$L__BB0_5;
	bra.uni 	$L__BB0_4;
$L__BB0_5:
	div.u64 	%rd153, %rd9, %rd4;
	bra.uni 	$L__BB0_6;
$L__BB0_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd9;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd153, %r9;
$L__BB0_6:
	add.s64 	%rd154, %rd153, 1;
$L__BB0_7:
	shl.b64 	%rd74, %rd73, 3;
	@%p6 bra 	$L__BB0_12;
	setp.ne.s64 	%p9, %rd152, 0;
	@%p9 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_9;
$L__BB0_10:
	div.u64 	%rd155, %rd9, %rd4;
	bra.uni 	$L__BB0_11;
$L__BB0_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd9;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd155, %r13;
$L__BB0_11:
	add.s64 	%rd156, %rd155, 1;
$L__BB0_12:
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd7, %rd6, %rd74;
	add.s64 	%rd8, %rd4, -1;
	min.u64 	%rd20, %rd154, %rd156;
	setp.eq.s64 	%p10, %rd20, 0;
	shl.b64 	%rd149, %rd3, 3;
	@%p10 bra 	$L__BB0_18;
	add.s64 	%rd83, %rd7, %rd149;
	shl.b32 	%r19, %r6, 3;
	add.s32 	%r14, %r2, %r19;
	add.s32 	%r15, %r14, 4;
	ld.u32 	%r16, [%rd83];
	ld.u32 	%r17, [%rd83+4];
	// begin inline asm
	st.shared.f32 [%r14], %r16;
st.shared.f32 [%r15], %r17;
	// end inline asm
	setp.eq.s64 	%p11, %rd20, 1;
	@%p11 bra 	$L__BB0_18;
	add.s64 	%rd160, %rd3, 1;
	add.s64 	%rd159, %rd20, -1;
	shl.b64 	%rd84, %rd5, 10;
	shl.b64 	%rd23, %rd4, 3;
	add.s64 	%rd85, %rd84, %rd23;
	add.s64 	%rd87, %rd85, %rd149;
	add.s64 	%rd158, %rd6, %rd87;
	mov.u64 	%rd88, 1016;
	sub.s64 	%rd157, %rd88, %rd149;
$L__BB0_15:
	add.s64 	%rd31, %rd160, %rd8;
	setp.lt.u64 	%p13, %rd31, 128;
	@%p13 bra 	$L__BB0_17;
	bra.uni 	$L__BB0_16;
$L__BB0_17:
	shr.u64 	%rd89, %rd157, 3;
	setp.gt.u64 	%p12, %rd89, %rd8;
	selp.b64 	%rd30, %rd158, 0, %p12;
	setp.lt.u64 	%p1, %rd31, %rd160;
	add.s64 	%rd94, %rd160, %rd4;
	selp.b64 	%rd160, 128, %rd94, %p1;
	cvt.u32.u64 	%r24, %rd31;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r20, %r25, %r2;
	add.s32 	%r21, %r20, 4;
	ld.u32 	%r22, [%rd30];
	ld.u32 	%r23, [%rd30+4];
	// begin inline asm
	st.shared.f32 [%r20], %r22;
st.shared.f32 [%r21], %r23;
	// end inline asm
	add.s64 	%rd159, %rd159, -1;
	add.s64 	%rd158, %rd158, %rd23;
	sub.s64 	%rd157, %rd157, %rd23;
	setp.ne.s64 	%p14, %rd159, 0;
	@%p14 bra 	$L__BB0_15;
$L__BB0_18:
	add.u64 	%rd68, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd96, [%rd1+16];
	ld.global.nc.u64 	%rd97, [%rd1+24];
	st.local.u64 	[%rd2], %rd96;
	st.local.u64 	[%rd2+8], %rd97;
	bar.sync 	0;
	{ // callseq 1, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd68;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd69;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 1
	bar.sync 	0;
	mov.u64 	%rd164, 0;
	mov.u64 	%rd162, %rd164;
	@%p6 bra 	$L__BB0_23;
	setp.ne.s64 	%p16, %rd152, 0;
	@%p16 bra 	$L__BB0_21;
	bra.uni 	$L__BB0_20;
$L__BB0_21:
	div.u64 	%rd161, %rd9, %rd4;
	bra.uni 	$L__BB0_22;
$L__BB0_20:
	cvt.u32.u64 	%r27, %rd4;
	cvt.u32.u64 	%r28, %rd9;
	div.u32 	%r29, %r28, %r27;
	cvt.u64.u32 	%rd161, %r29;
$L__BB0_22:
	add.s64 	%rd162, %rd161, 1;
$L__BB0_23:
	@%p6 bra 	$L__BB0_28;
	setp.ne.s64 	%p18, %rd152, 0;
	@%p18 bra 	$L__BB0_26;
	bra.uni 	$L__BB0_25;
$L__BB0_26:
	div.u64 	%rd163, %rd9, %rd4;
	bra.uni 	$L__BB0_27;
$L__BB0_25:
	cvt.u32.u64 	%r31, %rd4;
	cvt.u32.u64 	%r32, %rd9;
	div.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd163, %r33;
$L__BB0_27:
	add.s64 	%rd164, %rd163, 1;
$L__BB0_28:
	min.u64 	%rd46, %rd162, %rd164;
	setp.eq.s64 	%p19, %rd46, 0;
	@%p19 bra 	$L__BB0_36;
	cvt.u16.u64 	%rs1, %rd3;
	or.b16  	%rs2, %rs1, -128;
	and.b16  	%rs3, %rs2, 240;
	and.b16  	%rs4, %rs2, 15;
	shl.b16 	%rs5, %rs4, 4;
	shr.u16 	%rs6, %rs3, 4;
	or.b16  	%rs7, %rs6, %rs5;
	and.b16  	%rs8, %rs7, 51;
	shl.b16 	%rs9, %rs8, 2;
	shr.u16 	%rs10, %rs7, 2;
	and.b16  	%rs11, %rs10, 51;
	or.b16  	%rs12, %rs11, %rs9;
	and.b16  	%rs13, %rs12, 85;
	add.s64 	%rd104, %rd7, %rd149;
	shl.b16 	%rs14, %rs12, 1;
	and.b16  	%rs15, %rs14, 340;
	shl.b16 	%rs16, %rs13, 3;
	or.b16  	%rs17, %rs16, %rs15;
	cvt.u32.u16 	%r38, %rs17;
	add.s32 	%r39, %r38, 1020;
	and.b32  	%r40, %r39, 1016;
	add.s32 	%r36, %r2, %r40;
	add.s32 	%r37, %r36, 4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd104], %r34;
	st.u32 	[%rd104+4], %r35;
	setp.eq.s64 	%p20, %rd46, 1;
	@%p20 bra 	$L__BB0_36;
	add.s64 	%rd168, %rd3, 1;
	shl.b64 	%rd105, %rd5, 10;
	shl.b64 	%rd48, %rd4, 3;
	add.s64 	%rd106, %rd105, %rd48;
	add.s64 	%rd108, %rd106, %rd149;
	add.s64 	%rd167, %rd6, %rd108;
	mov.u64 	%rd109, 1016;
	sub.s64 	%rd166, %rd109, %rd149;
	add.s64 	%rd165, %rd46, -1;
$L__BB0_31:
	add.s64 	%rd57, %rd168, %rd8;
	add.s64 	%rd112, %rd57, 128;
	shr.u64 	%rd113, %rd112, 1;
	and.b64  	%rd114, %rd113, 1431655765;
	shl.b64 	%rd115, %rd112, 1;
	and.b64  	%rd116, %rd115, 2863311530;
	or.b64  	%rd117, %rd114, %rd116;
	shr.u64 	%rd118, %rd117, 2;
	and.b64  	%rd119, %rd118, 858993459;
	shl.b64 	%rd120, %rd117, 2;
	and.b64  	%rd121, %rd120, 3435973836;
	or.b64  	%rd122, %rd119, %rd121;
	shr.u64 	%rd123, %rd122, 4;
	and.b64  	%rd124, %rd123, 252645135;
	shl.b64 	%rd125, %rd122, 4;
	and.b64  	%rd126, %rd125, 4042322160;
	or.b64  	%rd127, %rd124, %rd126;
	shr.u64 	%rd128, %rd127, 8;
	and.b64  	%rd129, %rd128, 16711935;
	shl.b64 	%rd130, %rd127, 8;
	and.b64  	%rd131, %rd130, 4278255360;
	or.b64  	%rd132, %rd129, %rd131;
	shr.u64 	%rd133, %rd132, 16;
	shl.b64 	%rd134, %rd132, 16;
	or.b64  	%rd58, %rd133, %rd134;
	setp.eq.s64 	%p22, %rd58, 0;
	mov.u64 	%rd169, 64;
	@%p22 bra 	$L__BB0_33;
	add.s64 	%rd135, %rd58, -1;
	not.b64 	%rd136, %rd58;
	and.b64  	%rd137, %rd136, %rd135;
	popc.b64 	%r41, %rd137;
	cvt.u64.u32 	%rd169, %r41;
$L__BB0_33:
	setp.lt.u64 	%p23, %rd169, 31;
	and.b64  	%rd138, %rd169, 63;
	selp.b64 	%rd139, %rd138, 31, %p23;
	cvt.u32.u64 	%r42, %rd139;
	shr.u64 	%rd140, %rd58, %r42;
	add.s64 	%rd62, %rd140, -1;
	setp.lt.u64 	%p24, %rd62, 256;
	@%p24 bra 	$L__BB0_35;
	bra.uni 	$L__BB0_34;
$L__BB0_35:
	shr.u64 	%rd111, %rd166, 3;
	setp.gt.u64 	%p21, %rd111, %rd8;
	selp.b64 	%rd56, %rd167, 0, %p21;
	setp.lt.u64 	%p2, %rd57, %rd168;
	setp.gt.u64 	%p25, %rd57, 127;
	add.s64 	%rd145, %rd57, 1;
	selp.b64 	%rd146, 128, %rd145, %p25;
	selp.b64 	%rd168, 128, %rd146, %p2;
	cvt.u32.u64 	%r47, %rd62;
	shl.b32 	%r48, %r47, 2;
	and.b32  	%r49, %r48, 1016;
	add.s32 	%r45, %r49, %r2;
	add.s32 	%r46, %r45, 4;
	// begin inline asm
	ld.shared.f32 %r43, [%r45];
ld.shared.f32 %r44, [%r46];
	// end inline asm
	st.u32 	[%rd56], %r43;
	st.u32 	[%rd56+4], %r44;
	add.s64 	%rd167, %rd167, %rd48;
	sub.s64 	%rd166, %rd166, %rd48;
	add.s64 	%rd165, %rd165, -1;
	setp.ne.s64 	%p26, %rd165, 0;
	@%p26 bra 	$L__BB0_31;
$L__BB0_36:
	ret;
$L__BB0_16:
	mov.u64 	%rd90, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd91, %rd90;
	mov.u64 	%rd92, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd93, %rd92;
	{ // callseq 0, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd93;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 0
$L__BB0_34:
	mov.u64 	%rd141, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd142, %rd141;
	mov.u64 	%rd143, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd144, %rd143;
	{ // callseq 2, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd144;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 2
$L__BB0_1:
	mov.u64 	%rd147, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_4;
	cvta.global.u64 	%rd148, %rd147;
	{ // callseq 3, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd148;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 3

}
	// .globl	fft_forward_256_kernel
.visible .entry fft_forward_256_kernel(
	.param .u64 fft_forward_256_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot1[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<48>;
	.reg .b64 	%rd<170>;

	mov.u64 	%SPL, __local_depot1;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd67, [fft_forward_256_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd67;
	add.u64 	%rd69, %SP, 16;
	add.u64 	%rd70, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[2048];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd70], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd71, [%rd1+8];
	and.b64  	%rd72, %rd71, -256;
	mul.wide.u32 	%rd73, %r5, 256;
	setp.lt.u64 	%p3, %rd73, %rd72;
	sub.s64 	%rd75, %rd71, %rd73;
	setp.gt.u64 	%p4, %rd75, 255;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB1_2;
	bra.uni 	$L__BB1_1;
$L__BB1_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd77, %rd3, 256;
	setp.gt.u32 	%p6, %r6, 255;
	not.b64 	%rd78, %rd3;
	add.s64 	%rd9, %rd78, %rd77;
	mov.u64 	%rd156, 0;
	and.b64  	%rd152, %rd9, -4294967296;
	mov.u64 	%rd154, %rd156;
	@%p6 bra 	$L__BB1_7;
	setp.ne.s64 	%p7, %rd152, 0;
	@%p7 bra 	$L__BB1_5;
	bra.uni 	$L__BB1_4;
$L__BB1_5:
	div.u64 	%rd153, %rd9, %rd4;
	bra.uni 	$L__BB1_6;
$L__BB1_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd9;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd153, %r9;
$L__BB1_6:
	add.s64 	%rd154, %rd153, 1;
$L__BB1_7:
	shl.b64 	%rd74, %rd73, 3;
	@%p6 bra 	$L__BB1_12;
	setp.ne.s64 	%p9, %rd152, 0;
	@%p9 bra 	$L__BB1_10;
	bra.uni 	$L__BB1_9;
$L__BB1_10:
	div.u64 	%rd155, %rd9, %rd4;
	bra.uni 	$L__BB1_11;
$L__BB1_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd9;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd155, %r13;
$L__BB1_11:
	add.s64 	%rd156, %rd155, 1;
$L__BB1_12:
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd7, %rd6, %rd74;
	add.s64 	%rd8, %rd4, -1;
	min.u64 	%rd20, %rd154, %rd156;
	setp.eq.s64 	%p10, %rd20, 0;
	shl.b64 	%rd149, %rd3, 3;
	@%p10 bra 	$L__BB1_18;
	add.s64 	%rd83, %rd7, %rd149;
	shl.b32 	%r19, %r6, 3;
	add.s32 	%r14, %r2, %r19;
	add.s32 	%r15, %r14, 4;
	ld.u32 	%r16, [%rd83];
	ld.u32 	%r17, [%rd83+4];
	// begin inline asm
	st.shared.f32 [%r14], %r16;
st.shared.f32 [%r15], %r17;
	// end inline asm
	setp.eq.s64 	%p11, %rd20, 1;
	@%p11 bra 	$L__BB1_18;
	add.s64 	%rd160, %rd3, 1;
	add.s64 	%rd159, %rd20, -1;
	shl.b64 	%rd84, %rd5, 11;
	shl.b64 	%rd23, %rd4, 3;
	add.s64 	%rd85, %rd84, %rd23;
	add.s64 	%rd87, %rd85, %rd149;
	add.s64 	%rd158, %rd6, %rd87;
	mov.u64 	%rd88, 2040;
	sub.s64 	%rd157, %rd88, %rd149;
$L__BB1_15:
	add.s64 	%rd31, %rd160, %rd8;
	setp.lt.u64 	%p13, %rd31, 256;
	@%p13 bra 	$L__BB1_17;
	bra.uni 	$L__BB1_16;
$L__BB1_17:
	shr.u64 	%rd89, %rd157, 3;
	setp.gt.u64 	%p12, %rd89, %rd8;
	selp.b64 	%rd30, %rd158, 0, %p12;
	setp.lt.u64 	%p1, %rd31, %rd160;
	add.s64 	%rd94, %rd160, %rd4;
	selp.b64 	%rd160, 256, %rd94, %p1;
	cvt.u32.u64 	%r24, %rd31;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r20, %r25, %r2;
	add.s32 	%r21, %r20, 4;
	ld.u32 	%r22, [%rd30];
	ld.u32 	%r23, [%rd30+4];
	// begin inline asm
	st.shared.f32 [%r20], %r22;
st.shared.f32 [%r21], %r23;
	// end inline asm
	add.s64 	%rd159, %rd159, -1;
	add.s64 	%rd158, %rd158, %rd23;
	sub.s64 	%rd157, %rd157, %rd23;
	setp.ne.s64 	%p14, %rd159, 0;
	@%p14 bra 	$L__BB1_15;
$L__BB1_18:
	add.u64 	%rd68, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd96, [%rd1+16];
	ld.global.nc.u64 	%rd97, [%rd1+24];
	st.local.u64 	[%rd2], %rd96;
	st.local.u64 	[%rd2+8], %rd97;
	bar.sync 	0;
	{ // callseq 5, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd68;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd69;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 5
	bar.sync 	0;
	mov.u64 	%rd164, 0;
	mov.u64 	%rd162, %rd164;
	@%p6 bra 	$L__BB1_23;
	setp.ne.s64 	%p16, %rd152, 0;
	@%p16 bra 	$L__BB1_21;
	bra.uni 	$L__BB1_20;
$L__BB1_21:
	div.u64 	%rd161, %rd9, %rd4;
	bra.uni 	$L__BB1_22;
$L__BB1_20:
	cvt.u32.u64 	%r27, %rd4;
	cvt.u32.u64 	%r28, %rd9;
	div.u32 	%r29, %r28, %r27;
	cvt.u64.u32 	%rd161, %r29;
$L__BB1_22:
	add.s64 	%rd162, %rd161, 1;
$L__BB1_23:
	@%p6 bra 	$L__BB1_28;
	setp.ne.s64 	%p18, %rd152, 0;
	@%p18 bra 	$L__BB1_26;
	bra.uni 	$L__BB1_25;
$L__BB1_26:
	div.u64 	%rd163, %rd9, %rd4;
	bra.uni 	$L__BB1_27;
$L__BB1_25:
	cvt.u32.u64 	%r31, %rd4;
	cvt.u32.u64 	%r32, %rd9;
	div.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd163, %r33;
$L__BB1_27:
	add.s64 	%rd164, %rd163, 1;
$L__BB1_28:
	min.u64 	%rd46, %rd162, %rd164;
	setp.eq.s64 	%p19, %rd46, 0;
	@%p19 bra 	$L__BB1_36;
	cvt.u16.u64 	%rs1, %rd3;
	and.b16  	%rs2, %rs1, 240;
	and.b16  	%rs3, %rs1, 15;
	shl.b16 	%rs4, %rs3, 4;
	shr.u16 	%rs5, %rs2, 4;
	or.b16  	%rs6, %rs5, %rs4;
	and.b16  	%rs7, %rs6, 51;
	shl.b16 	%rs8, %rs7, 2;
	shr.u16 	%rs9, %rs6, 2;
	and.b16  	%rs10, %rs9, 51;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 85;
	shl.b16 	%rs13, %rs12, 1;
	shr.u16 	%rs14, %rs11, 1;
	and.b16  	%rs15, %rs14, 85;
	or.b16  	%rs16, %rs15, %rs13;
	add.s64 	%rd104, %rd7, %rd149;
	mul.wide.u16 	%r38, %rs16, 8;
	add.s32 	%r36, %r2, %r38;
	add.s32 	%r37, %r36, 4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd104], %r34;
	st.u32 	[%rd104+4], %r35;
	setp.eq.s64 	%p20, %rd46, 1;
	@%p20 bra 	$L__BB1_36;
	add.s64 	%rd168, %rd3, 1;
	shl.b64 	%rd105, %rd5, 11;
	shl.b64 	%rd48, %rd4, 3;
	add.s64 	%rd106, %rd105, %rd48;
	add.s64 	%rd108, %rd106, %rd149;
	add.s64 	%rd167, %rd6, %rd108;
	mov.u64 	%rd109, 2040;
	sub.s64 	%rd166, %rd109, %rd149;
	add.s64 	%rd165, %rd46, -1;
$L__BB1_31:
	add.s64 	%rd57, %rd168, %rd8;
	add.s64 	%rd112, %rd57, 256;
	shr.u64 	%rd113, %rd112, 1;
	and.b64  	%rd114, %rd113, 1431655765;
	shl.b64 	%rd115, %rd112, 1;
	and.b64  	%rd116, %rd115, 2863311530;
	or.b64  	%rd117, %rd114, %rd116;
	shr.u64 	%rd118, %rd117, 2;
	and.b64  	%rd119, %rd118, 858993459;
	shl.b64 	%rd120, %rd117, 2;
	and.b64  	%rd121, %rd120, 3435973836;
	or.b64  	%rd122, %rd119, %rd121;
	shr.u64 	%rd123, %rd122, 4;
	and.b64  	%rd124, %rd123, 252645135;
	shl.b64 	%rd125, %rd122, 4;
	and.b64  	%rd126, %rd125, 4042322160;
	or.b64  	%rd127, %rd124, %rd126;
	shr.u64 	%rd128, %rd127, 8;
	and.b64  	%rd129, %rd128, 16711935;
	shl.b64 	%rd130, %rd127, 8;
	and.b64  	%rd131, %rd130, 4278255360;
	or.b64  	%rd132, %rd129, %rd131;
	shr.u64 	%rd133, %rd132, 16;
	shl.b64 	%rd134, %rd132, 16;
	or.b64  	%rd58, %rd133, %rd134;
	setp.eq.s64 	%p22, %rd58, 0;
	mov.u64 	%rd169, 64;
	@%p22 bra 	$L__BB1_33;
	add.s64 	%rd135, %rd58, -1;
	not.b64 	%rd136, %rd58;
	and.b64  	%rd137, %rd136, %rd135;
	popc.b64 	%r39, %rd137;
	cvt.u64.u32 	%rd169, %r39;
$L__BB1_33:
	setp.lt.u64 	%p23, %rd169, 31;
	and.b64  	%rd138, %rd169, 63;
	selp.b64 	%rd139, %rd138, 31, %p23;
	cvt.u32.u64 	%r40, %rd139;
	shr.u64 	%rd140, %rd58, %r40;
	add.s64 	%rd62, %rd140, -1;
	setp.lt.u64 	%p24, %rd62, 512;
	@%p24 bra 	$L__BB1_35;
	bra.uni 	$L__BB1_34;
$L__BB1_35:
	shr.u64 	%rd111, %rd166, 3;
	setp.gt.u64 	%p21, %rd111, %rd8;
	selp.b64 	%rd56, %rd167, 0, %p21;
	setp.lt.u64 	%p2, %rd57, %rd168;
	setp.gt.u64 	%p25, %rd57, 255;
	add.s64 	%rd145, %rd57, 1;
	selp.b64 	%rd146, 256, %rd145, %p25;
	selp.b64 	%rd168, 256, %rd146, %p2;
	cvt.u32.u64 	%r45, %rd62;
	shl.b32 	%r46, %r45, 2;
	and.b32  	%r47, %r46, 2040;
	add.s32 	%r43, %r47, %r2;
	add.s32 	%r44, %r43, 4;
	// begin inline asm
	ld.shared.f32 %r41, [%r43];
ld.shared.f32 %r42, [%r44];
	// end inline asm
	st.u32 	[%rd56], %r41;
	st.u32 	[%rd56+4], %r42;
	add.s64 	%rd167, %rd167, %rd48;
	sub.s64 	%rd166, %rd166, %rd48;
	add.s64 	%rd165, %rd165, -1;
	setp.ne.s64 	%p26, %rd165, 0;
	@%p26 bra 	$L__BB1_31;
$L__BB1_36:
	ret;
$L__BB1_16:
	mov.u64 	%rd90, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd91, %rd90;
	mov.u64 	%rd92, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd93, %rd92;
	{ // callseq 4, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd93;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 4
$L__BB1_34:
	mov.u64 	%rd141, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd142, %rd141;
	mov.u64 	%rd143, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd144, %rd143;
	{ // callseq 6, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd144;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 6
$L__BB1_1:
	mov.u64 	%rd147, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_4;
	cvta.global.u64 	%rd148, %rd147;
	{ // callseq 7, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd148;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 7

}
	// .globl	fft_forward_512_kernel
.visible .entry fft_forward_512_kernel(
	.param .u64 fft_forward_512_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot2[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<170>;

	mov.u64 	%SPL, __local_depot2;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd67, [fft_forward_512_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd67;
	add.u64 	%rd69, %SP, 16;
	add.u64 	%rd70, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[4096];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd70], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd71, [%rd1+8];
	and.b64  	%rd72, %rd71, -512;
	mul.wide.u32 	%rd73, %r5, 512;
	setp.lt.u64 	%p3, %rd73, %rd72;
	sub.s64 	%rd75, %rd71, %rd73;
	setp.gt.u64 	%p4, %rd75, 511;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB2_2;
	bra.uni 	$L__BB2_1;
$L__BB2_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd77, %rd3, 512;
	setp.gt.u32 	%p6, %r6, 511;
	not.b64 	%rd78, %rd3;
	add.s64 	%rd9, %rd78, %rd77;
	mov.u64 	%rd156, 0;
	and.b64  	%rd152, %rd9, -4294967296;
	mov.u64 	%rd154, %rd156;
	@%p6 bra 	$L__BB2_7;
	setp.ne.s64 	%p7, %rd152, 0;
	@%p7 bra 	$L__BB2_5;
	bra.uni 	$L__BB2_4;
$L__BB2_5:
	div.u64 	%rd153, %rd9, %rd4;
	bra.uni 	$L__BB2_6;
$L__BB2_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd9;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd153, %r9;
$L__BB2_6:
	add.s64 	%rd154, %rd153, 1;
$L__BB2_7:
	shl.b64 	%rd74, %rd73, 3;
	@%p6 bra 	$L__BB2_12;
	setp.ne.s64 	%p9, %rd152, 0;
	@%p9 bra 	$L__BB2_10;
	bra.uni 	$L__BB2_9;
$L__BB2_10:
	div.u64 	%rd155, %rd9, %rd4;
	bra.uni 	$L__BB2_11;
$L__BB2_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd9;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd155, %r13;
$L__BB2_11:
	add.s64 	%rd156, %rd155, 1;
$L__BB2_12:
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd7, %rd6, %rd74;
	add.s64 	%rd8, %rd4, -1;
	min.u64 	%rd20, %rd154, %rd156;
	setp.eq.s64 	%p10, %rd20, 0;
	shl.b64 	%rd149, %rd3, 3;
	@%p10 bra 	$L__BB2_18;
	add.s64 	%rd83, %rd7, %rd149;
	shl.b32 	%r19, %r6, 3;
	add.s32 	%r14, %r2, %r19;
	add.s32 	%r15, %r14, 4;
	ld.u32 	%r16, [%rd83];
	ld.u32 	%r17, [%rd83+4];
	// begin inline asm
	st.shared.f32 [%r14], %r16;
st.shared.f32 [%r15], %r17;
	// end inline asm
	setp.eq.s64 	%p11, %rd20, 1;
	@%p11 bra 	$L__BB2_18;
	add.s64 	%rd160, %rd3, 1;
	add.s64 	%rd159, %rd20, -1;
	shl.b64 	%rd84, %rd5, 12;
	shl.b64 	%rd23, %rd4, 3;
	add.s64 	%rd85, %rd84, %rd23;
	add.s64 	%rd87, %rd85, %rd149;
	add.s64 	%rd158, %rd6, %rd87;
	mov.u64 	%rd88, 4088;
	sub.s64 	%rd157, %rd88, %rd149;
$L__BB2_15:
	add.s64 	%rd31, %rd160, %rd8;
	setp.lt.u64 	%p13, %rd31, 512;
	@%p13 bra 	$L__BB2_17;
	bra.uni 	$L__BB2_16;
$L__BB2_17:
	shr.u64 	%rd89, %rd157, 3;
	setp.gt.u64 	%p12, %rd89, %rd8;
	selp.b64 	%rd30, %rd158, 0, %p12;
	setp.lt.u64 	%p1, %rd31, %rd160;
	add.s64 	%rd94, %rd160, %rd4;
	selp.b64 	%rd160, 512, %rd94, %p1;
	cvt.u32.u64 	%r24, %rd31;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r20, %r25, %r2;
	add.s32 	%r21, %r20, 4;
	ld.u32 	%r22, [%rd30];
	ld.u32 	%r23, [%rd30+4];
	// begin inline asm
	st.shared.f32 [%r20], %r22;
st.shared.f32 [%r21], %r23;
	// end inline asm
	add.s64 	%rd159, %rd159, -1;
	add.s64 	%rd158, %rd158, %rd23;
	sub.s64 	%rd157, %rd157, %rd23;
	setp.ne.s64 	%p14, %rd159, 0;
	@%p14 bra 	$L__BB2_15;
$L__BB2_18:
	add.u64 	%rd68, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd96, [%rd1+16];
	ld.global.nc.u64 	%rd97, [%rd1+24];
	st.local.u64 	[%rd2], %rd96;
	st.local.u64 	[%rd2+8], %rd97;
	bar.sync 	0;
	{ // callseq 9, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd68;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd69;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 9
	bar.sync 	0;
	mov.u64 	%rd164, 0;
	mov.u64 	%rd162, %rd164;
	@%p6 bra 	$L__BB2_23;
	setp.ne.s64 	%p16, %rd152, 0;
	@%p16 bra 	$L__BB2_21;
	bra.uni 	$L__BB2_20;
$L__BB2_21:
	div.u64 	%rd161, %rd9, %rd4;
	bra.uni 	$L__BB2_22;
$L__BB2_20:
	cvt.u32.u64 	%r27, %rd4;
	cvt.u32.u64 	%r28, %rd9;
	div.u32 	%r29, %r28, %r27;
	cvt.u64.u32 	%rd161, %r29;
$L__BB2_22:
	add.s64 	%rd162, %rd161, 1;
$L__BB2_23:
	@%p6 bra 	$L__BB2_28;
	setp.ne.s64 	%p18, %rd152, 0;
	@%p18 bra 	$L__BB2_26;
	bra.uni 	$L__BB2_25;
$L__BB2_26:
	div.u64 	%rd163, %rd9, %rd4;
	bra.uni 	$L__BB2_27;
$L__BB2_25:
	cvt.u32.u64 	%r31, %rd4;
	cvt.u32.u64 	%r32, %rd9;
	div.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd163, %r33;
$L__BB2_27:
	add.s64 	%rd164, %rd163, 1;
$L__BB2_28:
	min.u64 	%rd46, %rd162, %rd164;
	setp.eq.s64 	%p19, %rd46, 0;
	@%p19 bra 	$L__BB2_36;
	cvt.u16.u64 	%rs1, %rd3;
	and.b16  	%rs2, %rs1, 240;
	and.b16  	%rs3, %rs1, 15;
	shl.b16 	%rs4, %rs3, 4;
	shr.u16 	%rs5, %rs2, 4;
	or.b16  	%rs6, %rs5, %rs4;
	and.b16  	%rs7, %rs6, 51;
	shl.b16 	%rs8, %rs7, 2;
	shr.u16 	%rs9, %rs6, 2;
	and.b16  	%rs10, %rs9, 51;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 85;
	add.s64 	%rd104, %rd7, %rd149;
	shl.b16 	%rs13, %rs11, 3;
	and.b16  	%rs14, %rs13, 1360;
	shl.b16 	%rs15, %rs12, 5;
	or.b16  	%rs16, %rs15, %rs14;
	cvt.u32.u16 	%r39, %rs16;
	shr.u32 	%r40, %r6, 5;
	and.b32  	%r41, %r40, 8;
	or.b32  	%r42, %r41, %r39;
	and.b32  	%r43, %r42, 4088;
	add.s32 	%r36, %r2, %r43;
	add.s32 	%r37, %r36, 4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd104], %r34;
	st.u32 	[%rd104+4], %r35;
	setp.eq.s64 	%p20, %rd46, 1;
	@%p20 bra 	$L__BB2_36;
	add.s64 	%rd168, %rd3, 1;
	shl.b64 	%rd105, %rd5, 12;
	shl.b64 	%rd48, %rd4, 3;
	add.s64 	%rd106, %rd105, %rd48;
	add.s64 	%rd108, %rd106, %rd149;
	add.s64 	%rd167, %rd6, %rd108;
	mov.u64 	%rd109, 4088;
	sub.s64 	%rd166, %rd109, %rd149;
	add.s64 	%rd165, %rd46, -1;
$L__BB2_31:
	add.s64 	%rd57, %rd168, %rd8;
	add.s64 	%rd112, %rd57, 512;
	shr.u64 	%rd113, %rd112, 1;
	and.b64  	%rd114, %rd113, 1431655765;
	shl.b64 	%rd115, %rd112, 1;
	and.b64  	%rd116, %rd115, 2863311530;
	or.b64  	%rd117, %rd114, %rd116;
	shr.u64 	%rd118, %rd117, 2;
	and.b64  	%rd119, %rd118, 858993459;
	shl.b64 	%rd120, %rd117, 2;
	and.b64  	%rd121, %rd120, 3435973836;
	or.b64  	%rd122, %rd119, %rd121;
	shr.u64 	%rd123, %rd122, 4;
	and.b64  	%rd124, %rd123, 252645135;
	shl.b64 	%rd125, %rd122, 4;
	and.b64  	%rd126, %rd125, 4042322160;
	or.b64  	%rd127, %rd124, %rd126;
	shr.u64 	%rd128, %rd127, 8;
	and.b64  	%rd129, %rd128, 16711935;
	shl.b64 	%rd130, %rd127, 8;
	and.b64  	%rd131, %rd130, 4278255360;
	or.b64  	%rd132, %rd129, %rd131;
	shr.u64 	%rd133, %rd132, 16;
	shl.b64 	%rd134, %rd132, 16;
	or.b64  	%rd58, %rd133, %rd134;
	setp.eq.s64 	%p22, %rd58, 0;
	mov.u64 	%rd169, 64;
	@%p22 bra 	$L__BB2_33;
	add.s64 	%rd135, %rd58, -1;
	not.b64 	%rd136, %rd58;
	and.b64  	%rd137, %rd136, %rd135;
	popc.b64 	%r44, %rd137;
	cvt.u64.u32 	%rd169, %r44;
$L__BB2_33:
	setp.lt.u64 	%p23, %rd169, 31;
	and.b64  	%rd138, %rd169, 63;
	selp.b64 	%rd139, %rd138, 31, %p23;
	cvt.u32.u64 	%r45, %rd139;
	shr.u64 	%rd140, %rd58, %r45;
	add.s64 	%rd62, %rd140, -1;
	setp.lt.u64 	%p24, %rd62, 1024;
	@%p24 bra 	$L__BB2_35;
	bra.uni 	$L__BB2_34;
$L__BB2_35:
	shr.u64 	%rd111, %rd166, 3;
	setp.gt.u64 	%p21, %rd111, %rd8;
	selp.b64 	%rd56, %rd167, 0, %p21;
	setp.lt.u64 	%p2, %rd57, %rd168;
	setp.gt.u64 	%p25, %rd57, 511;
	add.s64 	%rd145, %rd57, 1;
	selp.b64 	%rd146, 512, %rd145, %p25;
	selp.b64 	%rd168, 512, %rd146, %p2;
	cvt.u32.u64 	%r50, %rd62;
	shl.b32 	%r51, %r50, 2;
	and.b32  	%r52, %r51, 4088;
	add.s32 	%r48, %r52, %r2;
	add.s32 	%r49, %r48, 4;
	// begin inline asm
	ld.shared.f32 %r46, [%r48];
ld.shared.f32 %r47, [%r49];
	// end inline asm
	st.u32 	[%rd56], %r46;
	st.u32 	[%rd56+4], %r47;
	add.s64 	%rd167, %rd167, %rd48;
	sub.s64 	%rd166, %rd166, %rd48;
	add.s64 	%rd165, %rd165, -1;
	setp.ne.s64 	%p26, %rd165, 0;
	@%p26 bra 	$L__BB2_31;
$L__BB2_36:
	ret;
$L__BB2_16:
	mov.u64 	%rd90, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd91, %rd90;
	mov.u64 	%rd92, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd93, %rd92;
	{ // callseq 8, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd93;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 8
$L__BB2_34:
	mov.u64 	%rd141, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd142, %rd141;
	mov.u64 	%rd143, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd144, %rd143;
	{ // callseq 10, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd144;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 10
$L__BB2_1:
	mov.u64 	%rd147, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_4;
	cvta.global.u64 	%rd148, %rd147;
	{ // callseq 11, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd148;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 11

}
	// .globl	fft_forward_1024_kernel
.visible .entry fft_forward_1024_kernel(
	.param .u64 fft_forward_1024_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot3[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<20>;
	.reg .b32 	%r<49>;
	.reg .b64 	%rd<122>;

	mov.u64 	%SPL, __local_depot3;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd45, [fft_forward_1024_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd45;
	add.u64 	%rd47, %SP, 16;
	add.u64 	%rd48, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[8192];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd48], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd49, [%rd1+8];
	and.b64  	%rd50, %rd49, -1024;
	mul.wide.u32 	%rd6, %r5, 1024;
	setp.lt.u64 	%p3, %rd6, %rd50;
	sub.s64 	%rd51, %rd49, %rd6;
	setp.gt.u64 	%p4, %rd51, 1023;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB3_2;
	bra.uni 	$L__BB3_1;
$L__BB3_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r10, %rd3;
	ld.global.nc.u64 	%rd7, [%rd1];
	shl.b64 	%rd52, %rd6, 3;
	add.s64 	%rd53, %rd7, %rd52;
	add.s64 	%rd8, %rd4, -1;
	cvt.u16.u64 	%rs2, %rd3;
	xor.b16  	%rs1, %rs2, 1023;
	cvt.u16.u64 	%rs3, %rd4;
	shl.b64 	%rd54, %rd3, 3;
	add.s64 	%rd9, %rd53, %rd54;
	shl.b32 	%r11, %r10, 3;
	add.s32 	%r6, %r2, %r11;
	add.s32 	%r7, %r6, 4;
	ld.u32 	%r8, [%rd9];
	ld.u32 	%r9, [%rd9+4];
	// begin inline asm
	st.shared.f32 [%r6], %r8;
st.shared.f32 [%r7], %r9;
	// end inline asm
	setp.lt.u16 	%p6, %rs1, %rs3;
	cvt.u32.u64 	%r48, %rd4;
	@%p6 bra 	$L__BB3_7;
	add.s64 	%rd116, %rd3, 1;
	cvt.u32.u16 	%r12, %rs1;
	div.u32 	%r14, %r12, %r48;
	cvt.u64.u32 	%rd115, %r14;
	shl.b64 	%rd55, %rd5, 13;
	shl.b64 	%rd12, %rd4, 3;
	add.s64 	%rd56, %rd55, %rd12;
	add.s64 	%rd58, %rd56, %rd54;
	add.s64 	%rd114, %rd7, %rd58;
	xor.b64  	%rd113, %rd54, 8184;
$L__BB3_4:
	add.s64 	%rd20, %rd116, %rd8;
	setp.lt.u64 	%p8, %rd20, 1024;
	@%p8 bra 	$L__BB3_6;
	bra.uni 	$L__BB3_5;
$L__BB3_6:
	shr.u64 	%rd59, %rd113, 3;
	setp.gt.u64 	%p7, %rd59, %rd8;
	selp.b64 	%rd19, %rd114, 0, %p7;
	setp.lt.u64 	%p1, %rd20, %rd116;
	add.s64 	%rd64, %rd116, %rd4;
	selp.b64 	%rd116, 1024, %rd64, %p1;
	cvt.u32.u64 	%r19, %rd20;
	shl.b32 	%r20, %r19, 3;
	add.s32 	%r15, %r20, %r2;
	add.s32 	%r16, %r15, 4;
	ld.u32 	%r17, [%rd19];
	ld.u32 	%r18, [%rd19+4];
	// begin inline asm
	st.shared.f32 [%r15], %r17;
st.shared.f32 [%r16], %r18;
	// end inline asm
	add.s64 	%rd115, %rd115, -1;
	add.s64 	%rd114, %rd114, %rd12;
	sub.s64 	%rd113, %rd113, %rd12;
	setp.ne.s64 	%p9, %rd115, 0;
	@%p9 bra 	$L__BB3_4;
$L__BB3_7:
	add.u64 	%rd46, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd65, [%rd1+16];
	ld.global.nc.u64 	%rd66, [%rd1+24];
	st.local.u64 	[%rd2], %rd65;
	st.local.u64 	[%rd2+8], %rd66;
	bar.sync 	0;
	{ // callseq 13, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd47;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 13
	bar.sync 	0;
	xor.b64  	%rd69, %rd3, 1023;
	and.b16  	%rs5, %rs2, 240;
	and.b16  	%rs6, %rs2, 15;
	shl.b16 	%rs7, %rs6, 4;
	shr.u16 	%rs8, %rs5, 4;
	or.b16  	%rs9, %rs8, %rs7;
	and.b16  	%rs10, %rs9, 51;
	shl.b16 	%rs11, %rs10, 2;
	shr.u16 	%rs12, %rs9, 2;
	and.b16  	%rs13, %rs12, 51;
	or.b16  	%rs14, %rs13, %rs11;
	and.b16  	%rs15, %rs14, 85;
	shr.u32 	%r29, %r10, 8;
	shr.u32 	%r30, %r10, 6;
	and.b32  	%r31, %r30, 4;
	or.b32  	%r32, %r29, %r31;
	shl.b16 	%rs16, %rs14, 4;
	and.b16  	%rs17, %rs16, 2720;
	shl.b16 	%rs18, %rs15, 6;
	or.b16  	%rs19, %rs18, %rs17;
	cvt.u32.u16 	%r33, %rs19;
	shl.b32 	%r34, %r32, 2;
	or.b32  	%r35, %r34, %r33;
	or.b32  	%r36, %r35, 4;
	add.s32 	%r37, %r36, 8188;
	and.b32  	%r38, %r37, 8184;
	add.s32 	%r23, %r2, %r38;
	add.s32 	%r24, %r23, 4;
	// begin inline asm
	ld.shared.f32 %r21, [%r23];
ld.shared.f32 %r22, [%r24];
	// end inline asm
	st.u32 	[%rd9], %r21;
	st.u32 	[%rd9+4], %r22;
	setp.lt.u64 	%p10, %rd69, %rd4;
	@%p10 bra 	$L__BB3_14;
	cvt.u32.u64 	%r26, %rd69;
	div.u32 	%r28, %r26, %r48;
	cvt.u64.u32 	%rd117, %r28;
	add.s64 	%rd120, %rd3, 1;
	shl.b64 	%rd70, %rd5, 13;
	shl.b64 	%rd27, %rd4, 3;
	add.s64 	%rd71, %rd70, %rd27;
	add.s64 	%rd73, %rd71, %rd54;
	add.s64 	%rd119, %rd7, %rd73;
	xor.b64  	%rd118, %rd54, 8184;
$L__BB3_9:
	add.s64 	%rd35, %rd120, %rd8;
	add.s64 	%rd76, %rd35, 1024;
	shr.u64 	%rd77, %rd76, 1;
	and.b64  	%rd78, %rd77, 1431655765;
	shl.b64 	%rd79, %rd76, 1;
	and.b64  	%rd80, %rd79, 2863311530;
	or.b64  	%rd81, %rd78, %rd80;
	shr.u64 	%rd82, %rd81, 2;
	and.b64  	%rd83, %rd82, 858993459;
	shl.b64 	%rd84, %rd81, 2;
	and.b64  	%rd85, %rd84, 3435973836;
	or.b64  	%rd86, %rd83, %rd85;
	shr.u64 	%rd87, %rd86, 4;
	and.b64  	%rd88, %rd87, 252645135;
	shl.b64 	%rd89, %rd86, 4;
	and.b64  	%rd90, %rd89, 4042322160;
	or.b64  	%rd91, %rd88, %rd90;
	shr.u64 	%rd92, %rd91, 8;
	and.b64  	%rd93, %rd92, 16711935;
	shl.b64 	%rd94, %rd91, 8;
	and.b64  	%rd95, %rd94, 4278255360;
	or.b64  	%rd96, %rd93, %rd95;
	shr.u64 	%rd97, %rd96, 16;
	shl.b64 	%rd98, %rd96, 16;
	or.b64  	%rd36, %rd97, %rd98;
	setp.eq.s64 	%p12, %rd36, 0;
	mov.u64 	%rd121, 64;
	@%p12 bra 	$L__BB3_11;
	add.s64 	%rd99, %rd36, -1;
	not.b64 	%rd100, %rd36;
	and.b64  	%rd101, %rd100, %rd99;
	popc.b64 	%r39, %rd101;
	cvt.u64.u32 	%rd121, %r39;
$L__BB3_11:
	setp.lt.u64 	%p13, %rd121, 31;
	and.b64  	%rd102, %rd121, 63;
	selp.b64 	%rd103, %rd102, 31, %p13;
	cvt.u32.u64 	%r40, %rd103;
	shr.u64 	%rd104, %rd36, %r40;
	add.s64 	%rd40, %rd104, -1;
	setp.lt.u64 	%p14, %rd40, 2048;
	@%p14 bra 	$L__BB3_13;
	bra.uni 	$L__BB3_12;
$L__BB3_13:
	shr.u64 	%rd75, %rd118, 3;
	setp.gt.u64 	%p11, %rd75, %rd8;
	selp.b64 	%rd34, %rd119, 0, %p11;
	setp.lt.u64 	%p2, %rd35, %rd120;
	setp.gt.u64 	%p15, %rd35, 1023;
	add.s64 	%rd109, %rd35, 1;
	selp.b64 	%rd110, 1024, %rd109, %p15;
	selp.b64 	%rd120, 1024, %rd110, %p2;
	cvt.u32.u64 	%r45, %rd40;
	shl.b32 	%r46, %r45, 2;
	and.b32  	%r47, %r46, 8184;
	add.s32 	%r43, %r47, %r2;
	add.s32 	%r44, %r43, 4;
	// begin inline asm
	ld.shared.f32 %r41, [%r43];
ld.shared.f32 %r42, [%r44];
	// end inline asm
	st.u32 	[%rd34], %r41;
	st.u32 	[%rd34+4], %r42;
	add.s64 	%rd119, %rd119, %rd27;
	sub.s64 	%rd118, %rd118, %rd27;
	add.s64 	%rd117, %rd117, -1;
	setp.ne.s64 	%p16, %rd117, 0;
	@%p16 bra 	$L__BB3_9;
$L__BB3_14:
	ret;
$L__BB3_5:
	mov.u64 	%rd60, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd61, %rd60;
	mov.u64 	%rd62, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd63, %rd62;
	{ // callseq 12, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd61;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd63;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 12
$L__BB3_12:
	mov.u64 	%rd105, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd106, %rd105;
	mov.u64 	%rd107, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd108, %rd107;
	{ // callseq 14, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd106;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd108;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 14
$L__BB3_1:
	mov.u64 	%rd111, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_4;
	cvta.global.u64 	%rd112, %rd111;
	{ // callseq 15, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd112;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 15

}
	// .globl	fft_forward_2048_kernel
.visible .entry fft_forward_2048_kernel(
	.param .u64 fft_forward_2048_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot4[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<24>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<112>;

	mov.u64 	%SPL, __local_depot4;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd40, [fft_forward_2048_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd40;
	add.u64 	%rd42, %SP, 16;
	add.u64 	%rd43, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[16384];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd43], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd44, [%rd1+8];
	and.b64  	%rd45, %rd44, -2048;
	mul.wide.u32 	%rd6, %r5, 2048;
	setp.lt.u64 	%p3, %rd6, %rd45;
	sub.s64 	%rd46, %rd44, %rd6;
	setp.gt.u64 	%p4, %rd46, 2047;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB4_2;
	bra.uni 	$L__BB4_1;
$L__BB4_2:
	add.u64 	%rd41, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r10, %rd3;
	ld.global.nc.u64 	%rd47, [%rd1];
	shl.b64 	%rd48, %rd6, 3;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd7, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 2047;
	shl.b64 	%rd50, %rd3, 3;
	add.s64 	%rd8, %rd49, %rd50;
	shl.b32 	%r11, %r10, 3;
	add.s32 	%r6, %r2, %r11;
	add.s32 	%r7, %r6, 4;
	ld.u32 	%r8, [%rd8];
	ld.u32 	%r9, [%rd8+4];
	// begin inline asm
	st.shared.f32 [%r6], %r8;
st.shared.f32 [%r7], %r9;
	// end inline asm
	add.s64 	%rd110, %rd3, 1;
	cvt.u32.u16 	%r12, %rs2;
	cvt.u32.u64 	%r13, %rd4;
	div.u32 	%r14, %r12, %r13;
	cvt.u64.u32 	%rd105, %r14;
	shl.b64 	%rd51, %rd5, 14;
	shl.b64 	%rd11, %rd4, 3;
	add.s64 	%rd52, %rd51, %rd11;
	add.s64 	%rd53, %rd52, %rd50;
	add.s64 	%rd109, %rd47, %rd53;
	xor.b64  	%rd108, %rd50, 16376;
	mov.u64 	%rd103, %rd108;
	mov.u64 	%rd104, %rd109;
	mov.u64 	%rd106, %rd110;
$L__BB4_3:
	add.s64 	%rd19, %rd106, %rd7;
	setp.lt.u64 	%p7, %rd19, 2048;
	@%p7 bra 	$L__BB4_5;
	bra.uni 	$L__BB4_4;
$L__BB4_5:
	shr.u64 	%rd54, %rd103, 3;
	setp.gt.u64 	%p6, %rd54, %rd7;
	selp.b64 	%rd18, %rd104, 0, %p6;
	setp.lt.u64 	%p1, %rd19, %rd106;
	add.s64 	%rd59, %rd106, %rd4;
	selp.b64 	%rd106, 2048, %rd59, %p1;
	cvt.u32.u64 	%r19, %rd19;
	shl.b32 	%r20, %r19, 3;
	add.s32 	%r15, %r20, %r2;
	add.s32 	%r16, %r15, 4;
	ld.u32 	%r17, [%rd18];
	ld.u32 	%r18, [%rd18+4];
	// begin inline asm
	st.shared.f32 [%r15], %r17;
st.shared.f32 [%r16], %r18;
	// end inline asm
	add.s64 	%rd105, %rd105, -1;
	add.s64 	%rd104, %rd104, %rd11;
	sub.s64 	%rd103, %rd103, %rd11;
	setp.ne.s64 	%p8, %rd105, 0;
	@%p8 bra 	$L__BB4_3;
	ld.global.nc.u64 	%rd60, [%rd1+16];
	ld.global.nc.u64 	%rd61, [%rd1+24];
	st.local.u64 	[%rd2], %rd60;
	st.local.u64 	[%rd2+8], %rd61;
	bar.sync 	0;
	{ // callseq 17, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd41;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd42;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 17
	bar.sync 	0;
	xor.b32  	%r26, %r10, 2047;
	div.u32 	%r28, %r26, %r13;
	cvt.u64.u32 	%rd107, %r28;
	or.b16  	%rs3, %rs1, 2048;
	shl.b16 	%rs4, %rs1, 12;
	shr.u16 	%rs5, %rs3, 4;
	and.b16  	%rs6, %rs5, 176;
	or.b16  	%rs7, %rs4, %rs6;
	shl.b16 	%rs8, %rs1, 4;
	and.b16  	%rs9, %rs8, 3840;
	or.b16  	%rs10, %rs9, %rs7;
	and.b16  	%rs11, %rs10, 13107;
	shl.b16 	%rs12, %rs11, 2;
	shr.u16 	%rs13, %rs10, 2;
	and.b16  	%rs14, %rs13, 13107;
	or.b16  	%rs15, %rs14, %rs12;
	and.b16  	%rs16, %rs15, 21845;
	shl.b16 	%rs17, %rs16, 1;
	shr.u16 	%rs18, %rs15, 1;
	and.b16  	%rs19, %rs18, 21845;
	or.b16  	%rs20, %rs19, %rs17;
	shr.u16 	%rs21, %rs20, 2;
	add.s16 	%rs22, %rs21, 16380;
	and.b16  	%rs23, %rs22, 16368;
	cvt.u32.u16 	%r29, %rs23;
	add.s32 	%r23, %r2, %r29;
	add.s32 	%r24, %r23, 4;
	// begin inline asm
	ld.shared.f32 %r21, [%r23];
ld.shared.f32 %r22, [%r24];
	// end inline asm
	st.u32 	[%rd8], %r21;
	st.u32 	[%rd8+4], %r22;
$L__BB4_7:
	add.s64 	%rd30, %rd110, %rd7;
	add.s64 	%rd66, %rd30, 2048;
	shr.u64 	%rd67, %rd66, 1;
	and.b64  	%rd68, %rd67, 1431655765;
	shl.b64 	%rd69, %rd66, 1;
	and.b64  	%rd70, %rd69, 2863311530;
	or.b64  	%rd71, %rd68, %rd70;
	shr.u64 	%rd72, %rd71, 2;
	and.b64  	%rd73, %rd72, 858993459;
	shl.b64 	%rd74, %rd71, 2;
	and.b64  	%rd75, %rd74, 3435973836;
	or.b64  	%rd76, %rd73, %rd75;
	shr.u64 	%rd77, %rd76, 4;
	and.b64  	%rd78, %rd77, 252645135;
	shl.b64 	%rd79, %rd76, 4;
	and.b64  	%rd80, %rd79, 4042322160;
	or.b64  	%rd81, %rd78, %rd80;
	shr.u64 	%rd82, %rd81, 8;
	and.b64  	%rd83, %rd82, 16711935;
	shl.b64 	%rd84, %rd81, 8;
	and.b64  	%rd85, %rd84, 4278255360;
	or.b64  	%rd86, %rd83, %rd85;
	shr.u64 	%rd87, %rd86, 16;
	shl.b64 	%rd88, %rd86, 16;
	or.b64  	%rd31, %rd87, %rd88;
	setp.eq.s64 	%p10, %rd31, 0;
	mov.u64 	%rd111, 64;
	@%p10 bra 	$L__BB4_9;
	add.s64 	%rd89, %rd31, -1;
	not.b64 	%rd90, %rd31;
	and.b64  	%rd91, %rd90, %rd89;
	popc.b64 	%r30, %rd91;
	cvt.u64.u32 	%rd111, %r30;
$L__BB4_9:
	setp.lt.u64 	%p11, %rd111, 31;
	and.b64  	%rd92, %rd111, 63;
	selp.b64 	%rd93, %rd92, 31, %p11;
	cvt.u32.u64 	%r31, %rd93;
	shr.u64 	%rd94, %rd31, %r31;
	add.s64 	%rd35, %rd94, -1;
	setp.lt.u64 	%p12, %rd35, 4096;
	@%p12 bra 	$L__BB4_11;
	bra.uni 	$L__BB4_10;
$L__BB4_11:
	shr.u64 	%rd65, %rd108, 3;
	setp.gt.u64 	%p9, %rd65, %rd7;
	selp.b64 	%rd29, %rd109, 0, %p9;
	setp.lt.u64 	%p2, %rd30, %rd110;
	setp.gt.u64 	%p13, %rd30, 2047;
	add.s64 	%rd99, %rd30, 1;
	selp.b64 	%rd100, 2048, %rd99, %p13;
	selp.b64 	%rd110, 2048, %rd100, %p2;
	cvt.u32.u64 	%r36, %rd35;
	shl.b32 	%r37, %r36, 2;
	and.b32  	%r38, %r37, 16376;
	add.s32 	%r34, %r38, %r2;
	add.s32 	%r35, %r34, 4;
	// begin inline asm
	ld.shared.f32 %r32, [%r34];
ld.shared.f32 %r33, [%r35];
	// end inline asm
	st.u32 	[%rd29], %r32;
	st.u32 	[%rd29+4], %r33;
	add.s64 	%rd109, %rd109, %rd11;
	sub.s64 	%rd108, %rd108, %rd11;
	add.s64 	%rd107, %rd107, -1;
	setp.ne.s64 	%p14, %rd107, 0;
	@%p14 bra 	$L__BB4_7;
	ret;
$L__BB4_4:
	mov.u64 	%rd55, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd56, %rd55;
	mov.u64 	%rd57, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd58, %rd57;
	{ // callseq 16, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd56;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd58;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 16
$L__BB4_10:
	mov.u64 	%rd95, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd96, %rd95;
	mov.u64 	%rd97, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd98, %rd97;
	{ // callseq 18, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd96;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd98;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 18
$L__BB4_1:
	mov.u64 	%rd101, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_4;
	cvta.global.u64 	%rd102, %rd101;
	{ // callseq 19, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd102;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 19

}
	// .globl	fft_forward_4096_kernel
.visible .entry fft_forward_4096_kernel(
	.param .u64 fft_forward_4096_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot5[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<46>;
	.reg .b64 	%rd<136>;

	mov.u64 	%SPL, __local_depot5;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd41, [fft_forward_4096_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd41;
	add.u64 	%rd43, %SP, 16;
	add.u64 	%rd44, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[32768];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd44], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd45, [%rd1+8];
	and.b64  	%rd46, %rd45, -4096;
	mul.wide.u32 	%rd6, %r5, 4096;
	setp.lt.u64 	%p3, %rd6, %rd46;
	sub.s64 	%rd47, %rd45, %rd6;
	setp.gt.u64 	%p4, %rd47, 4095;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB5_2;
	bra.uni 	$L__BB5_1;
$L__BB5_2:
	add.u64 	%rd42, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r10, %rd3;
	ld.global.nc.u64 	%rd48, [%rd1];
	shl.b64 	%rd49, %rd6, 3;
	add.s64 	%rd50, %rd48, %rd49;
	add.s64 	%rd7, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 4095;
	shl.b64 	%rd51, %rd3, 3;
	add.s64 	%rd8, %rd50, %rd51;
	add.s64 	%rd134, %rd3, 1;
	shl.b32 	%r11, %r10, 3;
	add.s32 	%r6, %r2, %r11;
	add.s32 	%r7, %r6, 4;
	ld.u32 	%r8, [%rd8];
	ld.u32 	%r9, [%rd8+4];
	// begin inline asm
	st.shared.f32 [%r6], %r8;
st.shared.f32 [%r7], %r9;
	// end inline asm
	cvt.u32.u16 	%r12, %rs2;
	cvt.u32.u64 	%r13, %rd4;
	div.u32 	%r14, %r12, %r13;
	cvt.u64.u32 	%rd129, %r14;
	shl.b64 	%rd52, %rd5, 15;
	shl.b64 	%rd11, %rd4, 3;
	add.s64 	%rd53, %rd52, %rd11;
	add.s64 	%rd54, %rd53, %rd51;
	add.s64 	%rd133, %rd48, %rd54;
	xor.b64  	%rd132, %rd51, 32760;
	mov.u64 	%rd127, %rd132;
	mov.u64 	%rd128, %rd133;
	mov.u64 	%rd130, %rd134;
$L__BB5_3:
	add.s64 	%rd19, %rd130, %rd7;
	setp.lt.u64 	%p7, %rd19, 4096;
	@%p7 bra 	$L__BB5_5;
	bra.uni 	$L__BB5_4;
$L__BB5_5:
	shr.u64 	%rd55, %rd127, 3;
	setp.gt.u64 	%p6, %rd55, %rd7;
	selp.b64 	%rd18, %rd128, 0, %p6;
	setp.lt.u64 	%p1, %rd19, %rd130;
	add.s64 	%rd60, %rd130, %rd4;
	selp.b64 	%rd130, 4096, %rd60, %p1;
	cvt.u32.u64 	%r19, %rd19;
	shl.b32 	%r20, %r19, 3;
	add.s32 	%r15, %r20, %r2;
	add.s32 	%r16, %r15, 4;
	ld.u32 	%r17, [%rd18];
	ld.u32 	%r18, [%rd18+4];
	// begin inline asm
	st.shared.f32 [%r15], %r17;
st.shared.f32 [%r16], %r18;
	// end inline asm
	add.s64 	%rd129, %rd129, -1;
	add.s64 	%rd128, %rd128, %rd11;
	sub.s64 	%rd127, %rd127, %rd11;
	setp.ne.s64 	%p8, %rd129, 0;
	@%p8 bra 	$L__BB5_3;
	ld.global.nc.u64 	%rd61, [%rd1+16];
	ld.global.nc.u64 	%rd62, [%rd1+24];
	st.local.u64 	[%rd2], %rd61;
	st.local.u64 	[%rd2+8], %rd62;
	bar.sync 	0;
	{ // callseq 21, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd42;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd43;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 21
	bar.sync 	0;
	shr.u64 	%rd65, %rd3, 1;
	and.b64  	%rd66, %rd65, 341;
	shl.b64 	%rd67, %rd3, 1;
	and.b64  	%rd68, %rd67, 682;
	or.b64  	%rd69, %rd68, %rd66;
	shr.u64 	%rd70, %rd69, 2;
	and.b64  	%rd71, %rd70, 51;
	shl.b64 	%rd72, %rd69, 2;
	and.b64  	%rd73, %rd72, 3276;
	or.b64  	%rd74, %rd73, %rd71;
	or.b64  	%rd75, %rd74, 32768;
	shr.u64 	%rd76, %rd75, 4;
	and.b64  	%rd77, %rd76, 2063;
	shl.b64 	%rd78, %rd74, 4;
	and.b64  	%rd79, %rd78, 49392;
	or.b64  	%rd80, %rd77, %rd79;
	{ .reg .b32 tmp; mov.b64 {%r25, tmp}, %rd80; }
	prmt.b32 	%r26, %r25, 0, 291;
	{ .reg .b32 tmp; mov.b64 {tmp, %r27}, %rd80; }
	prmt.b32 	%r28, %r27, 0, 291;
	mov.b64 	%rd81, {%r28, %r26};
	shr.u64 	%rd82, %rd81, 32;
	or.b64  	%rd83, %rd82, 2147483648;
	add.s64 	%rd84, %rd83, -1;
	not.b64 	%rd85, %rd83;
	and.b64  	%rd86, %rd85, %rd84;
	popc.b64 	%r29, %rd86;
	shr.u64 	%rd87, %rd82, %r29;
	add.s64 	%rd25, %rd87, -1;
	setp.gt.u64 	%p9, %rd25, 8191;
	@%p9 bra 	$L__BB5_11;
	xor.b32  	%r22, %r10, 4095;
	div.u32 	%r24, %r22, %r13;
	cvt.u64.u32 	%rd131, %r24;
	cvt.u32.u64 	%r34, %rd25;
	shl.b32 	%r35, %r34, 2;
	and.b32  	%r36, %r35, 32760;
	add.s32 	%r32, %r2, %r36;
	add.s32 	%r33, %r32, 4;
	// begin inline asm
	ld.shared.f32 %r30, [%r32];
ld.shared.f32 %r31, [%r33];
	// end inline asm
	st.u32 	[%rd8], %r30;
	st.u32 	[%rd8+4], %r31;
$L__BB5_8:
	add.s64 	%rd31, %rd134, %rd7;
	add.s64 	%rd90, %rd31, 4096;
	shr.u64 	%rd91, %rd90, 1;
	and.b64  	%rd92, %rd91, 1431655765;
	shl.b64 	%rd93, %rd90, 1;
	and.b64  	%rd94, %rd93, 2863311530;
	or.b64  	%rd95, %rd92, %rd94;
	shr.u64 	%rd96, %rd95, 2;
	and.b64  	%rd97, %rd96, 858993459;
	shl.b64 	%rd98, %rd95, 2;
	and.b64  	%rd99, %rd98, 3435973836;
	or.b64  	%rd100, %rd97, %rd99;
	shr.u64 	%rd101, %rd100, 4;
	and.b64  	%rd102, %rd101, 252645135;
	shl.b64 	%rd103, %rd100, 4;
	and.b64  	%rd104, %rd103, 4042322160;
	or.b64  	%rd105, %rd102, %rd104;
	shr.u64 	%rd106, %rd105, 8;
	and.b64  	%rd107, %rd106, 16711935;
	shl.b64 	%rd108, %rd105, 8;
	and.b64  	%rd109, %rd108, 4278255360;
	or.b64  	%rd110, %rd107, %rd109;
	shr.u64 	%rd111, %rd110, 16;
	shl.b64 	%rd112, %rd110, 16;
	or.b64  	%rd32, %rd111, %rd112;
	setp.eq.s64 	%p11, %rd32, 0;
	mov.u64 	%rd135, 64;
	@%p11 bra 	$L__BB5_10;
	add.s64 	%rd113, %rd32, -1;
	not.b64 	%rd114, %rd32;
	and.b64  	%rd115, %rd114, %rd113;
	popc.b64 	%r37, %rd115;
	cvt.u64.u32 	%rd135, %r37;
$L__BB5_10:
	setp.lt.u64 	%p12, %rd135, 31;
	and.b64  	%rd116, %rd135, 63;
	selp.b64 	%rd117, %rd116, 31, %p12;
	cvt.u32.u64 	%r38, %rd117;
	shr.u64 	%rd118, %rd32, %r38;
	add.s64 	%rd36, %rd118, -1;
	setp.lt.u64 	%p13, %rd36, 8192;
	@%p13 bra 	$L__BB5_12;
	bra.uni 	$L__BB5_11;
$L__BB5_12:
	shr.u64 	%rd89, %rd132, 3;
	setp.gt.u64 	%p10, %rd89, %rd7;
	selp.b64 	%rd30, %rd133, 0, %p10;
	setp.lt.u64 	%p2, %rd31, %rd134;
	setp.gt.u64 	%p14, %rd31, 4095;
	add.s64 	%rd123, %rd31, 1;
	selp.b64 	%rd124, 4096, %rd123, %p14;
	selp.b64 	%rd134, 4096, %rd124, %p2;
	cvt.u32.u64 	%r43, %rd36;
	shl.b32 	%r44, %r43, 2;
	and.b32  	%r45, %r44, 32760;
	add.s32 	%r41, %r45, %r2;
	add.s32 	%r42, %r41, 4;
	// begin inline asm
	ld.shared.f32 %r39, [%r41];
ld.shared.f32 %r40, [%r42];
	// end inline asm
	st.u32 	[%rd30], %r39;
	st.u32 	[%rd30+4], %r40;
	add.s64 	%rd133, %rd133, %rd11;
	sub.s64 	%rd132, %rd132, %rd11;
	add.s64 	%rd131, %rd131, -1;
	setp.ne.s64 	%p15, %rd131, 0;
	@%p15 bra 	$L__BB5_8;
	ret;
$L__BB5_11:
	mov.u64 	%rd119, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd120, %rd119;
	mov.u64 	%rd121, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd122, %rd121;
	{ // callseq 22, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd120;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd122;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 22
$L__BB5_4:
	mov.u64 	%rd56, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd57, %rd56;
	mov.u64 	%rd58, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd59, %rd58;
	{ // callseq 20, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd57;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd59;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 20
$L__BB5_1:
	mov.u64 	%rd125, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_4;
	cvta.global.u64 	%rd126, %rd125;
	{ // callseq 23, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd126;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 23

}
	// .globl	fft_backward_128_kernel
.visible .entry fft_backward_128_kernel(
	.param .u64 fft_backward_128_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot6[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<18>;
	.reg .b32 	%r<74>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<170>;

	mov.u64 	%SPL, __local_depot6;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd66, [fft_backward_128_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd66;
	add.u64 	%rd68, %SP, 16;
	add.u64 	%rd69, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[1024];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd69], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd70, [%rd1+8];
	and.b64  	%rd71, %rd70, -128;
	mul.wide.u32 	%rd72, %r5, 128;
	setp.lt.u64 	%p3, %rd72, %rd71;
	sub.s64 	%rd74, %rd70, %rd72;
	setp.gt.u64 	%p4, %rd74, 127;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB6_2;
	bra.uni 	$L__BB6_1;
$L__BB6_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd76, %rd3, 128;
	setp.gt.u32 	%p6, %r6, 127;
	not.b64 	%rd77, %rd3;
	add.s64 	%rd9, %rd77, %rd76;
	mov.u64 	%rd156, 0;
	and.b64  	%rd152, %rd9, -4294967296;
	mov.u64 	%rd154, %rd156;
	@%p6 bra 	$L__BB6_7;
	setp.ne.s64 	%p7, %rd152, 0;
	@%p7 bra 	$L__BB6_5;
	bra.uni 	$L__BB6_4;
$L__BB6_5:
	div.u64 	%rd153, %rd9, %rd4;
	bra.uni 	$L__BB6_6;
$L__BB6_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd9;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd153, %r9;
$L__BB6_6:
	add.s64 	%rd154, %rd153, 1;
$L__BB6_7:
	shl.b64 	%rd73, %rd72, 3;
	@%p6 bra 	$L__BB6_12;
	setp.ne.s64 	%p9, %rd152, 0;
	@%p9 bra 	$L__BB6_10;
	bra.uni 	$L__BB6_9;
$L__BB6_10:
	div.u64 	%rd155, %rd9, %rd4;
	bra.uni 	$L__BB6_11;
$L__BB6_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd9;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd155, %r13;
$L__BB6_11:
	add.s64 	%rd156, %rd155, 1;
$L__BB6_12:
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd7, %rd6, %rd73;
	add.s64 	%rd8, %rd4, -1;
	min.u64 	%rd20, %rd154, %rd156;
	setp.eq.s64 	%p10, %rd20, 0;
	shl.b64 	%rd149, %rd3, 3;
	@%p10 bra 	$L__BB6_18;
	add.s64 	%rd82, %rd7, %rd149;
	ld.u32 	%r15, [%rd82+4];
	ld.u32 	%r18, [%rd82];
	// begin inline asm
	neg.ftz.f32 %r14, %r15;
	// end inline asm
	shl.b32 	%r21, %r6, 3;
	add.s32 	%r16, %r2, %r21;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	st.shared.f32 [%r16], %r18;
st.shared.f32 [%r17], %r14;
	// end inline asm
	setp.eq.s64 	%p11, %rd20, 1;
	@%p11 bra 	$L__BB6_18;
	add.s64 	%rd160, %rd3, 1;
	add.s64 	%rd159, %rd20, -1;
	shl.b64 	%rd83, %rd5, 10;
	shl.b64 	%rd23, %rd4, 3;
	add.s64 	%rd84, %rd83, %rd23;
	add.s64 	%rd86, %rd84, %rd149;
	add.s64 	%rd158, %rd6, %rd86;
	mov.u64 	%rd87, 1016;
	sub.s64 	%rd157, %rd87, %rd149;
$L__BB6_15:
	shr.u64 	%rd88, %rd157, 3;
	setp.gt.u64 	%p12, %rd88, %rd8;
	selp.b64 	%rd89, %rd158, 0, %p12;
	add.s64 	%rd30, %rd160, %rd8;
	ld.f32 	%f1, [%rd89];
	ld.u32 	%r23, [%rd89+4];
	// begin inline asm
	neg.ftz.f32 %r22, %r23;
	// end inline asm
	setp.lt.u64 	%p13, %rd30, 128;
	@%p13 bra 	$L__BB6_17;
	bra.uni 	$L__BB6_16;
$L__BB6_17:
	setp.lt.u64 	%p1, %rd30, %rd160;
	mov.b32 	%f2, %r22;
	add.s64 	%rd94, %rd160, %rd4;
	selp.b64 	%rd160, 128, %rd94, %p1;
	cvt.u32.u64 	%r28, %rd30;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r24, %r29, %r2;
	add.s32 	%r25, %r24, 4;
	mov.b32 	%r26, %f1;
	// begin inline asm
	st.shared.f32 [%r24], %r26;
st.shared.f32 [%r25], %r22;
	// end inline asm
	add.s64 	%rd159, %rd159, -1;
	add.s64 	%rd158, %rd158, %rd23;
	sub.s64 	%rd157, %rd157, %rd23;
	setp.ne.s64 	%p14, %rd159, 0;
	@%p14 bra 	$L__BB6_15;
$L__BB6_18:
	add.u64 	%rd67, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd96, [%rd1+16];
	ld.global.nc.u64 	%rd97, [%rd1+24];
	st.local.u64 	[%rd2], %rd96;
	st.local.u64 	[%rd2+8], %rd97;
	bar.sync 	0;
	{ // callseq 25, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd67;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd68;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 25
	mov.b32 	%r31, 1124073472;
	// begin inline asm
	rcp.approx.ftz.f32 %r73, %r31;
	// end inline asm
	bar.sync 	0;
	mov.u64 	%rd164, 0;
	mov.u64 	%rd162, %rd164;
	@%p6 bra 	$L__BB6_23;
	setp.ne.s64 	%p16, %rd152, 0;
	@%p16 bra 	$L__BB6_21;
	bra.uni 	$L__BB6_20;
$L__BB6_21:
	div.u64 	%rd161, %rd9, %rd4;
	bra.uni 	$L__BB6_22;
$L__BB6_20:
	cvt.u32.u64 	%r33, %rd4;
	cvt.u32.u64 	%r34, %rd9;
	div.u32 	%r35, %r34, %r33;
	cvt.u64.u32 	%rd161, %r35;
$L__BB6_22:
	add.s64 	%rd162, %rd161, 1;
$L__BB6_23:
	@%p6 bra 	$L__BB6_28;
	setp.ne.s64 	%p18, %rd152, 0;
	@%p18 bra 	$L__BB6_26;
	bra.uni 	$L__BB6_25;
$L__BB6_26:
	div.u64 	%rd163, %rd9, %rd4;
	bra.uni 	$L__BB6_27;
$L__BB6_25:
	cvt.u32.u64 	%r37, %rd4;
	cvt.u32.u64 	%r38, %rd9;
	div.u32 	%r39, %r38, %r37;
	cvt.u64.u32 	%rd163, %r39;
$L__BB6_27:
	add.s64 	%rd164, %rd163, 1;
$L__BB6_28:
	min.u64 	%rd45, %rd162, %rd164;
	setp.eq.s64 	%p19, %rd45, 0;
	@%p19 bra 	$L__BB6_36;
	mov.b32 	%f3, %r73;
	cvt.u16.u64 	%rs1, %rd3;
	or.b16  	%rs2, %rs1, -128;
	and.b16  	%rs3, %rs2, 240;
	and.b16  	%rs4, %rs2, 15;
	shl.b16 	%rs5, %rs4, 4;
	shr.u16 	%rs6, %rs3, 4;
	or.b16  	%rs7, %rs6, %rs5;
	and.b16  	%rs8, %rs7, 51;
	shl.b16 	%rs9, %rs8, 2;
	shr.u16 	%rs10, %rs7, 2;
	and.b16  	%rs11, %rs10, 51;
	or.b16  	%rs12, %rs11, %rs9;
	and.b16  	%rs13, %rs12, 85;
	add.s64 	%rd104, %rd7, %rd149;
	shl.b16 	%rs14, %rs12, 1;
	and.b16  	%rs15, %rs14, 340;
	shl.b16 	%rs16, %rs13, 3;
	or.b16  	%rs17, %rs16, %rs15;
	cvt.u32.u16 	%r52, %rs17;
	add.s32 	%r53, %r52, 1020;
	and.b32  	%r54, %r53, 1016;
	add.s32 	%r42, %r2, %r54;
	add.s32 	%r43, %r42, 4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r44, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r46, %r40, %r73;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r44, %r73;
	// end inline asm
	st.u32 	[%rd104], %r46;
	st.u32 	[%rd104+4], %r49;
	setp.eq.s64 	%p20, %rd45, 1;
	@%p20 bra 	$L__BB6_36;
	add.s64 	%rd168, %rd3, 1;
	shl.b64 	%rd105, %rd5, 10;
	shl.b64 	%rd47, %rd4, 3;
	add.s64 	%rd106, %rd105, %rd47;
	add.s64 	%rd108, %rd106, %rd149;
	add.s64 	%rd167, %rd6, %rd108;
	mov.u64 	%rd109, 1016;
	sub.s64 	%rd166, %rd109, %rd149;
	add.s64 	%rd165, %rd45, -1;
$L__BB6_31:
	add.s64 	%rd56, %rd168, %rd8;
	add.s64 	%rd112, %rd56, 128;
	shr.u64 	%rd113, %rd112, 1;
	and.b64  	%rd114, %rd113, 1431655765;
	shl.b64 	%rd115, %rd112, 1;
	and.b64  	%rd116, %rd115, 2863311530;
	or.b64  	%rd117, %rd114, %rd116;
	shr.u64 	%rd118, %rd117, 2;
	and.b64  	%rd119, %rd118, 858993459;
	shl.b64 	%rd120, %rd117, 2;
	and.b64  	%rd121, %rd120, 3435973836;
	or.b64  	%rd122, %rd119, %rd121;
	shr.u64 	%rd123, %rd122, 4;
	and.b64  	%rd124, %rd123, 252645135;
	shl.b64 	%rd125, %rd122, 4;
	and.b64  	%rd126, %rd125, 4042322160;
	or.b64  	%rd127, %rd124, %rd126;
	shr.u64 	%rd128, %rd127, 8;
	and.b64  	%rd129, %rd128, 16711935;
	shl.b64 	%rd130, %rd127, 8;
	and.b64  	%rd131, %rd130, 4278255360;
	or.b64  	%rd132, %rd129, %rd131;
	shr.u64 	%rd133, %rd132, 16;
	shl.b64 	%rd134, %rd132, 16;
	or.b64  	%rd57, %rd133, %rd134;
	setp.eq.s64 	%p22, %rd57, 0;
	mov.u64 	%rd169, 64;
	@%p22 bra 	$L__BB6_33;
	add.s64 	%rd135, %rd57, -1;
	not.b64 	%rd136, %rd57;
	and.b64  	%rd137, %rd136, %rd135;
	popc.b64 	%r55, %rd137;
	cvt.u64.u32 	%rd169, %r55;
$L__BB6_33:
	setp.lt.u64 	%p23, %rd169, 31;
	and.b64  	%rd138, %rd169, 63;
	selp.b64 	%rd139, %rd138, 31, %p23;
	cvt.u32.u64 	%r56, %rd139;
	shr.u64 	%rd140, %rd57, %r56;
	add.s64 	%rd61, %rd140, -1;
	setp.lt.u64 	%p24, %rd61, 256;
	@%p24 bra 	$L__BB6_35;
	bra.uni 	$L__BB6_34;
$L__BB6_35:
	shr.u64 	%rd111, %rd166, 3;
	setp.gt.u64 	%p21, %rd111, %rd8;
	selp.b64 	%rd55, %rd167, 0, %p21;
	setp.lt.u64 	%p2, %rd56, %rd168;
	setp.gt.u64 	%p25, %rd56, 127;
	add.s64 	%rd145, %rd56, 1;
	selp.b64 	%rd146, 128, %rd145, %p25;
	selp.b64 	%rd168, 128, %rd146, %p2;
	cvt.u32.u64 	%r69, %rd61;
	shl.b32 	%r70, %r69, 2;
	and.b32  	%r71, %r70, 1016;
	add.s32 	%r59, %r71, %r2;
	add.s32 	%r60, %r59, 4;
	// begin inline asm
	ld.shared.f32 %r57, [%r59];
ld.shared.f32 %r58, [%r60];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r61, %r58;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r63, %r57, %r73;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r66, %r61, %r73;
	// end inline asm
	st.u32 	[%rd55], %r63;
	st.u32 	[%rd55+4], %r66;
	add.s64 	%rd167, %rd167, %rd47;
	sub.s64 	%rd166, %rd166, %rd47;
	add.s64 	%rd165, %rd165, -1;
	setp.ne.s64 	%p26, %rd165, 0;
	@%p26 bra 	$L__BB6_31;
$L__BB6_36:
	ret;
$L__BB6_16:
	mov.u64 	%rd90, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd91, %rd90;
	mov.u64 	%rd92, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd93, %rd92;
	{ // callseq 24, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd93;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 24
$L__BB6_34:
	mov.u64 	%rd141, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd142, %rd141;
	mov.u64 	%rd143, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd144, %rd143;
	{ // callseq 26, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd144;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 26
$L__BB6_1:
	mov.u64 	%rd147, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_8;
	cvta.global.u64 	%rd148, %rd147;
	{ // callseq 27, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd148;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 27

}
	// .globl	fft_backward_256_kernel
.visible .entry fft_backward_256_kernel(
	.param .u64 fft_backward_256_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot7[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<72>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<170>;

	mov.u64 	%SPL, __local_depot7;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd66, [fft_backward_256_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd66;
	add.u64 	%rd68, %SP, 16;
	add.u64 	%rd69, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[2048];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd69], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd70, [%rd1+8];
	and.b64  	%rd71, %rd70, -256;
	mul.wide.u32 	%rd72, %r5, 256;
	setp.lt.u64 	%p3, %rd72, %rd71;
	sub.s64 	%rd74, %rd70, %rd72;
	setp.gt.u64 	%p4, %rd74, 255;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB7_2;
	bra.uni 	$L__BB7_1;
$L__BB7_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd76, %rd3, 256;
	setp.gt.u32 	%p6, %r6, 255;
	not.b64 	%rd77, %rd3;
	add.s64 	%rd9, %rd77, %rd76;
	mov.u64 	%rd156, 0;
	and.b64  	%rd152, %rd9, -4294967296;
	mov.u64 	%rd154, %rd156;
	@%p6 bra 	$L__BB7_7;
	setp.ne.s64 	%p7, %rd152, 0;
	@%p7 bra 	$L__BB7_5;
	bra.uni 	$L__BB7_4;
$L__BB7_5:
	div.u64 	%rd153, %rd9, %rd4;
	bra.uni 	$L__BB7_6;
$L__BB7_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd9;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd153, %r9;
$L__BB7_6:
	add.s64 	%rd154, %rd153, 1;
$L__BB7_7:
	shl.b64 	%rd73, %rd72, 3;
	@%p6 bra 	$L__BB7_12;
	setp.ne.s64 	%p9, %rd152, 0;
	@%p9 bra 	$L__BB7_10;
	bra.uni 	$L__BB7_9;
$L__BB7_10:
	div.u64 	%rd155, %rd9, %rd4;
	bra.uni 	$L__BB7_11;
$L__BB7_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd9;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd155, %r13;
$L__BB7_11:
	add.s64 	%rd156, %rd155, 1;
$L__BB7_12:
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd7, %rd6, %rd73;
	add.s64 	%rd8, %rd4, -1;
	min.u64 	%rd20, %rd154, %rd156;
	setp.eq.s64 	%p10, %rd20, 0;
	shl.b64 	%rd149, %rd3, 3;
	@%p10 bra 	$L__BB7_18;
	add.s64 	%rd82, %rd7, %rd149;
	ld.u32 	%r15, [%rd82+4];
	ld.u32 	%r18, [%rd82];
	// begin inline asm
	neg.ftz.f32 %r14, %r15;
	// end inline asm
	shl.b32 	%r21, %r6, 3;
	add.s32 	%r16, %r2, %r21;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	st.shared.f32 [%r16], %r18;
st.shared.f32 [%r17], %r14;
	// end inline asm
	setp.eq.s64 	%p11, %rd20, 1;
	@%p11 bra 	$L__BB7_18;
	add.s64 	%rd160, %rd3, 1;
	add.s64 	%rd159, %rd20, -1;
	shl.b64 	%rd83, %rd5, 11;
	shl.b64 	%rd23, %rd4, 3;
	add.s64 	%rd84, %rd83, %rd23;
	add.s64 	%rd86, %rd84, %rd149;
	add.s64 	%rd158, %rd6, %rd86;
	mov.u64 	%rd87, 2040;
	sub.s64 	%rd157, %rd87, %rd149;
$L__BB7_15:
	shr.u64 	%rd88, %rd157, 3;
	setp.gt.u64 	%p12, %rd88, %rd8;
	selp.b64 	%rd89, %rd158, 0, %p12;
	add.s64 	%rd30, %rd160, %rd8;
	ld.f32 	%f1, [%rd89];
	ld.u32 	%r23, [%rd89+4];
	// begin inline asm
	neg.ftz.f32 %r22, %r23;
	// end inline asm
	setp.lt.u64 	%p13, %rd30, 256;
	@%p13 bra 	$L__BB7_17;
	bra.uni 	$L__BB7_16;
$L__BB7_17:
	setp.lt.u64 	%p1, %rd30, %rd160;
	mov.b32 	%f2, %r22;
	add.s64 	%rd94, %rd160, %rd4;
	selp.b64 	%rd160, 256, %rd94, %p1;
	cvt.u32.u64 	%r28, %rd30;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r24, %r29, %r2;
	add.s32 	%r25, %r24, 4;
	mov.b32 	%r26, %f1;
	// begin inline asm
	st.shared.f32 [%r24], %r26;
st.shared.f32 [%r25], %r22;
	// end inline asm
	add.s64 	%rd159, %rd159, -1;
	add.s64 	%rd158, %rd158, %rd23;
	sub.s64 	%rd157, %rd157, %rd23;
	setp.ne.s64 	%p14, %rd159, 0;
	@%p14 bra 	$L__BB7_15;
$L__BB7_18:
	add.u64 	%rd67, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd96, [%rd1+16];
	ld.global.nc.u64 	%rd97, [%rd1+24];
	st.local.u64 	[%rd2], %rd96;
	st.local.u64 	[%rd2+8], %rd97;
	bar.sync 	0;
	{ // callseq 29, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd67;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd68;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 29
	mov.b32 	%r31, 1132462080;
	// begin inline asm
	rcp.approx.ftz.f32 %r71, %r31;
	// end inline asm
	bar.sync 	0;
	mov.u64 	%rd164, 0;
	mov.u64 	%rd162, %rd164;
	@%p6 bra 	$L__BB7_23;
	setp.ne.s64 	%p16, %rd152, 0;
	@%p16 bra 	$L__BB7_21;
	bra.uni 	$L__BB7_20;
$L__BB7_21:
	div.u64 	%rd161, %rd9, %rd4;
	bra.uni 	$L__BB7_22;
$L__BB7_20:
	cvt.u32.u64 	%r33, %rd4;
	cvt.u32.u64 	%r34, %rd9;
	div.u32 	%r35, %r34, %r33;
	cvt.u64.u32 	%rd161, %r35;
$L__BB7_22:
	add.s64 	%rd162, %rd161, 1;
$L__BB7_23:
	@%p6 bra 	$L__BB7_28;
	setp.ne.s64 	%p18, %rd152, 0;
	@%p18 bra 	$L__BB7_26;
	bra.uni 	$L__BB7_25;
$L__BB7_26:
	div.u64 	%rd163, %rd9, %rd4;
	bra.uni 	$L__BB7_27;
$L__BB7_25:
	cvt.u32.u64 	%r37, %rd4;
	cvt.u32.u64 	%r38, %rd9;
	div.u32 	%r39, %r38, %r37;
	cvt.u64.u32 	%rd163, %r39;
$L__BB7_27:
	add.s64 	%rd164, %rd163, 1;
$L__BB7_28:
	min.u64 	%rd45, %rd162, %rd164;
	setp.eq.s64 	%p19, %rd45, 0;
	@%p19 bra 	$L__BB7_36;
	mov.b32 	%f3, %r71;
	cvt.u16.u64 	%rs1, %rd3;
	and.b16  	%rs2, %rs1, 240;
	and.b16  	%rs3, %rs1, 15;
	shl.b16 	%rs4, %rs3, 4;
	shr.u16 	%rs5, %rs2, 4;
	or.b16  	%rs6, %rs5, %rs4;
	and.b16  	%rs7, %rs6, 51;
	shl.b16 	%rs8, %rs7, 2;
	shr.u16 	%rs9, %rs6, 2;
	and.b16  	%rs10, %rs9, 51;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 85;
	shl.b16 	%rs13, %rs12, 1;
	shr.u16 	%rs14, %rs11, 1;
	and.b16  	%rs15, %rs14, 85;
	or.b16  	%rs16, %rs15, %rs13;
	add.s64 	%rd104, %rd7, %rd149;
	mul.wide.u16 	%r52, %rs16, 8;
	add.s32 	%r42, %r2, %r52;
	add.s32 	%r43, %r42, 4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r44, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r46, %r40, %r71;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r44, %r71;
	// end inline asm
	st.u32 	[%rd104], %r46;
	st.u32 	[%rd104+4], %r49;
	setp.eq.s64 	%p20, %rd45, 1;
	@%p20 bra 	$L__BB7_36;
	add.s64 	%rd168, %rd3, 1;
	shl.b64 	%rd105, %rd5, 11;
	shl.b64 	%rd47, %rd4, 3;
	add.s64 	%rd106, %rd105, %rd47;
	add.s64 	%rd108, %rd106, %rd149;
	add.s64 	%rd167, %rd6, %rd108;
	mov.u64 	%rd109, 2040;
	sub.s64 	%rd166, %rd109, %rd149;
	add.s64 	%rd165, %rd45, -1;
$L__BB7_31:
	add.s64 	%rd56, %rd168, %rd8;
	add.s64 	%rd112, %rd56, 256;
	shr.u64 	%rd113, %rd112, 1;
	and.b64  	%rd114, %rd113, 1431655765;
	shl.b64 	%rd115, %rd112, 1;
	and.b64  	%rd116, %rd115, 2863311530;
	or.b64  	%rd117, %rd114, %rd116;
	shr.u64 	%rd118, %rd117, 2;
	and.b64  	%rd119, %rd118, 858993459;
	shl.b64 	%rd120, %rd117, 2;
	and.b64  	%rd121, %rd120, 3435973836;
	or.b64  	%rd122, %rd119, %rd121;
	shr.u64 	%rd123, %rd122, 4;
	and.b64  	%rd124, %rd123, 252645135;
	shl.b64 	%rd125, %rd122, 4;
	and.b64  	%rd126, %rd125, 4042322160;
	or.b64  	%rd127, %rd124, %rd126;
	shr.u64 	%rd128, %rd127, 8;
	and.b64  	%rd129, %rd128, 16711935;
	shl.b64 	%rd130, %rd127, 8;
	and.b64  	%rd131, %rd130, 4278255360;
	or.b64  	%rd132, %rd129, %rd131;
	shr.u64 	%rd133, %rd132, 16;
	shl.b64 	%rd134, %rd132, 16;
	or.b64  	%rd57, %rd133, %rd134;
	setp.eq.s64 	%p22, %rd57, 0;
	mov.u64 	%rd169, 64;
	@%p22 bra 	$L__BB7_33;
	add.s64 	%rd135, %rd57, -1;
	not.b64 	%rd136, %rd57;
	and.b64  	%rd137, %rd136, %rd135;
	popc.b64 	%r53, %rd137;
	cvt.u64.u32 	%rd169, %r53;
$L__BB7_33:
	setp.lt.u64 	%p23, %rd169, 31;
	and.b64  	%rd138, %rd169, 63;
	selp.b64 	%rd139, %rd138, 31, %p23;
	cvt.u32.u64 	%r54, %rd139;
	shr.u64 	%rd140, %rd57, %r54;
	add.s64 	%rd61, %rd140, -1;
	setp.lt.u64 	%p24, %rd61, 512;
	@%p24 bra 	$L__BB7_35;
	bra.uni 	$L__BB7_34;
$L__BB7_35:
	shr.u64 	%rd111, %rd166, 3;
	setp.gt.u64 	%p21, %rd111, %rd8;
	selp.b64 	%rd55, %rd167, 0, %p21;
	setp.lt.u64 	%p2, %rd56, %rd168;
	setp.gt.u64 	%p25, %rd56, 255;
	add.s64 	%rd145, %rd56, 1;
	selp.b64 	%rd146, 256, %rd145, %p25;
	selp.b64 	%rd168, 256, %rd146, %p2;
	cvt.u32.u64 	%r67, %rd61;
	shl.b32 	%r68, %r67, 2;
	and.b32  	%r69, %r68, 2040;
	add.s32 	%r57, %r69, %r2;
	add.s32 	%r58, %r57, 4;
	// begin inline asm
	ld.shared.f32 %r55, [%r57];
ld.shared.f32 %r56, [%r58];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r59, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r61, %r55, %r71;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r64, %r59, %r71;
	// end inline asm
	st.u32 	[%rd55], %r61;
	st.u32 	[%rd55+4], %r64;
	add.s64 	%rd167, %rd167, %rd47;
	sub.s64 	%rd166, %rd166, %rd47;
	add.s64 	%rd165, %rd165, -1;
	setp.ne.s64 	%p26, %rd165, 0;
	@%p26 bra 	$L__BB7_31;
$L__BB7_36:
	ret;
$L__BB7_16:
	mov.u64 	%rd90, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd91, %rd90;
	mov.u64 	%rd92, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd93, %rd92;
	{ // callseq 28, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd93;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 28
$L__BB7_34:
	mov.u64 	%rd141, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd142, %rd141;
	mov.u64 	%rd143, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd144, %rd143;
	{ // callseq 30, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd144;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 30
$L__BB7_1:
	mov.u64 	%rd147, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_8;
	cvta.global.u64 	%rd148, %rd147;
	{ // callseq 31, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd148;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 31

}
	// .globl	fft_backward_512_kernel
.visible .entry fft_backward_512_kernel(
	.param .u64 fft_backward_512_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot8[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<77>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<170>;

	mov.u64 	%SPL, __local_depot8;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd66, [fft_backward_512_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd66;
	add.u64 	%rd68, %SP, 16;
	add.u64 	%rd69, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[4096];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd69], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd70, [%rd1+8];
	and.b64  	%rd71, %rd70, -512;
	mul.wide.u32 	%rd72, %r5, 512;
	setp.lt.u64 	%p3, %rd72, %rd71;
	sub.s64 	%rd74, %rd70, %rd72;
	setp.gt.u64 	%p4, %rd74, 511;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB8_2;
	bra.uni 	$L__BB8_1;
$L__BB8_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd76, %rd3, 512;
	setp.gt.u32 	%p6, %r6, 511;
	not.b64 	%rd77, %rd3;
	add.s64 	%rd9, %rd77, %rd76;
	mov.u64 	%rd156, 0;
	and.b64  	%rd152, %rd9, -4294967296;
	mov.u64 	%rd154, %rd156;
	@%p6 bra 	$L__BB8_7;
	setp.ne.s64 	%p7, %rd152, 0;
	@%p7 bra 	$L__BB8_5;
	bra.uni 	$L__BB8_4;
$L__BB8_5:
	div.u64 	%rd153, %rd9, %rd4;
	bra.uni 	$L__BB8_6;
$L__BB8_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd9;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd153, %r9;
$L__BB8_6:
	add.s64 	%rd154, %rd153, 1;
$L__BB8_7:
	shl.b64 	%rd73, %rd72, 3;
	@%p6 bra 	$L__BB8_12;
	setp.ne.s64 	%p9, %rd152, 0;
	@%p9 bra 	$L__BB8_10;
	bra.uni 	$L__BB8_9;
$L__BB8_10:
	div.u64 	%rd155, %rd9, %rd4;
	bra.uni 	$L__BB8_11;
$L__BB8_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd9;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd155, %r13;
$L__BB8_11:
	add.s64 	%rd156, %rd155, 1;
$L__BB8_12:
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd7, %rd6, %rd73;
	add.s64 	%rd8, %rd4, -1;
	min.u64 	%rd20, %rd154, %rd156;
	setp.eq.s64 	%p10, %rd20, 0;
	shl.b64 	%rd149, %rd3, 3;
	@%p10 bra 	$L__BB8_18;
	add.s64 	%rd82, %rd7, %rd149;
	ld.u32 	%r15, [%rd82+4];
	ld.u32 	%r18, [%rd82];
	// begin inline asm
	neg.ftz.f32 %r14, %r15;
	// end inline asm
	shl.b32 	%r21, %r6, 3;
	add.s32 	%r16, %r2, %r21;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	st.shared.f32 [%r16], %r18;
st.shared.f32 [%r17], %r14;
	// end inline asm
	setp.eq.s64 	%p11, %rd20, 1;
	@%p11 bra 	$L__BB8_18;
	add.s64 	%rd160, %rd3, 1;
	add.s64 	%rd159, %rd20, -1;
	shl.b64 	%rd83, %rd5, 12;
	shl.b64 	%rd23, %rd4, 3;
	add.s64 	%rd84, %rd83, %rd23;
	add.s64 	%rd86, %rd84, %rd149;
	add.s64 	%rd158, %rd6, %rd86;
	mov.u64 	%rd87, 4088;
	sub.s64 	%rd157, %rd87, %rd149;
$L__BB8_15:
	shr.u64 	%rd88, %rd157, 3;
	setp.gt.u64 	%p12, %rd88, %rd8;
	selp.b64 	%rd89, %rd158, 0, %p12;
	add.s64 	%rd30, %rd160, %rd8;
	ld.f32 	%f1, [%rd89];
	ld.u32 	%r23, [%rd89+4];
	// begin inline asm
	neg.ftz.f32 %r22, %r23;
	// end inline asm
	setp.lt.u64 	%p13, %rd30, 512;
	@%p13 bra 	$L__BB8_17;
	bra.uni 	$L__BB8_16;
$L__BB8_17:
	setp.lt.u64 	%p1, %rd30, %rd160;
	mov.b32 	%f2, %r22;
	add.s64 	%rd94, %rd160, %rd4;
	selp.b64 	%rd160, 512, %rd94, %p1;
	cvt.u32.u64 	%r28, %rd30;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r24, %r29, %r2;
	add.s32 	%r25, %r24, 4;
	mov.b32 	%r26, %f1;
	// begin inline asm
	st.shared.f32 [%r24], %r26;
st.shared.f32 [%r25], %r22;
	// end inline asm
	add.s64 	%rd159, %rd159, -1;
	add.s64 	%rd158, %rd158, %rd23;
	sub.s64 	%rd157, %rd157, %rd23;
	setp.ne.s64 	%p14, %rd159, 0;
	@%p14 bra 	$L__BB8_15;
$L__BB8_18:
	add.u64 	%rd67, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd96, [%rd1+16];
	ld.global.nc.u64 	%rd97, [%rd1+24];
	st.local.u64 	[%rd2], %rd96;
	st.local.u64 	[%rd2+8], %rd97;
	bar.sync 	0;
	{ // callseq 33, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd67;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd68;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 33
	mov.b32 	%r31, 1140850688;
	// begin inline asm
	rcp.approx.ftz.f32 %r76, %r31;
	// end inline asm
	bar.sync 	0;
	mov.u64 	%rd164, 0;
	mov.u64 	%rd162, %rd164;
	@%p6 bra 	$L__BB8_23;
	setp.ne.s64 	%p16, %rd152, 0;
	@%p16 bra 	$L__BB8_21;
	bra.uni 	$L__BB8_20;
$L__BB8_21:
	div.u64 	%rd161, %rd9, %rd4;
	bra.uni 	$L__BB8_22;
$L__BB8_20:
	cvt.u32.u64 	%r33, %rd4;
	cvt.u32.u64 	%r34, %rd9;
	div.u32 	%r35, %r34, %r33;
	cvt.u64.u32 	%rd161, %r35;
$L__BB8_22:
	add.s64 	%rd162, %rd161, 1;
$L__BB8_23:
	@%p6 bra 	$L__BB8_28;
	setp.ne.s64 	%p18, %rd152, 0;
	@%p18 bra 	$L__BB8_26;
	bra.uni 	$L__BB8_25;
$L__BB8_26:
	div.u64 	%rd163, %rd9, %rd4;
	bra.uni 	$L__BB8_27;
$L__BB8_25:
	cvt.u32.u64 	%r37, %rd4;
	cvt.u32.u64 	%r38, %rd9;
	div.u32 	%r39, %r38, %r37;
	cvt.u64.u32 	%rd163, %r39;
$L__BB8_27:
	add.s64 	%rd164, %rd163, 1;
$L__BB8_28:
	min.u64 	%rd45, %rd162, %rd164;
	setp.eq.s64 	%p19, %rd45, 0;
	@%p19 bra 	$L__BB8_36;
	mov.b32 	%f3, %r76;
	cvt.u16.u64 	%rs1, %rd3;
	and.b16  	%rs2, %rs1, 240;
	and.b16  	%rs3, %rs1, 15;
	shl.b16 	%rs4, %rs3, 4;
	shr.u16 	%rs5, %rs2, 4;
	or.b16  	%rs6, %rs5, %rs4;
	and.b16  	%rs7, %rs6, 51;
	shl.b16 	%rs8, %rs7, 2;
	shr.u16 	%rs9, %rs6, 2;
	and.b16  	%rs10, %rs9, 51;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 85;
	add.s64 	%rd104, %rd7, %rd149;
	shl.b16 	%rs13, %rs11, 3;
	and.b16  	%rs14, %rs13, 1360;
	shl.b16 	%rs15, %rs12, 5;
	or.b16  	%rs16, %rs15, %rs14;
	cvt.u32.u16 	%r53, %rs16;
	shr.u32 	%r54, %r6, 5;
	and.b32  	%r55, %r54, 8;
	or.b32  	%r56, %r55, %r53;
	and.b32  	%r57, %r56, 4088;
	add.s32 	%r42, %r2, %r57;
	add.s32 	%r43, %r42, 4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r44, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r46, %r40, %r76;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r44, %r76;
	// end inline asm
	st.u32 	[%rd104], %r46;
	st.u32 	[%rd104+4], %r49;
	setp.eq.s64 	%p20, %rd45, 1;
	@%p20 bra 	$L__BB8_36;
	add.s64 	%rd168, %rd3, 1;
	shl.b64 	%rd105, %rd5, 12;
	shl.b64 	%rd47, %rd4, 3;
	add.s64 	%rd106, %rd105, %rd47;
	add.s64 	%rd108, %rd106, %rd149;
	add.s64 	%rd167, %rd6, %rd108;
	mov.u64 	%rd109, 4088;
	sub.s64 	%rd166, %rd109, %rd149;
	add.s64 	%rd165, %rd45, -1;
$L__BB8_31:
	add.s64 	%rd56, %rd168, %rd8;
	add.s64 	%rd112, %rd56, 512;
	shr.u64 	%rd113, %rd112, 1;
	and.b64  	%rd114, %rd113, 1431655765;
	shl.b64 	%rd115, %rd112, 1;
	and.b64  	%rd116, %rd115, 2863311530;
	or.b64  	%rd117, %rd114, %rd116;
	shr.u64 	%rd118, %rd117, 2;
	and.b64  	%rd119, %rd118, 858993459;
	shl.b64 	%rd120, %rd117, 2;
	and.b64  	%rd121, %rd120, 3435973836;
	or.b64  	%rd122, %rd119, %rd121;
	shr.u64 	%rd123, %rd122, 4;
	and.b64  	%rd124, %rd123, 252645135;
	shl.b64 	%rd125, %rd122, 4;
	and.b64  	%rd126, %rd125, 4042322160;
	or.b64  	%rd127, %rd124, %rd126;
	shr.u64 	%rd128, %rd127, 8;
	and.b64  	%rd129, %rd128, 16711935;
	shl.b64 	%rd130, %rd127, 8;
	and.b64  	%rd131, %rd130, 4278255360;
	or.b64  	%rd132, %rd129, %rd131;
	shr.u64 	%rd133, %rd132, 16;
	shl.b64 	%rd134, %rd132, 16;
	or.b64  	%rd57, %rd133, %rd134;
	setp.eq.s64 	%p22, %rd57, 0;
	mov.u64 	%rd169, 64;
	@%p22 bra 	$L__BB8_33;
	add.s64 	%rd135, %rd57, -1;
	not.b64 	%rd136, %rd57;
	and.b64  	%rd137, %rd136, %rd135;
	popc.b64 	%r58, %rd137;
	cvt.u64.u32 	%rd169, %r58;
$L__BB8_33:
	setp.lt.u64 	%p23, %rd169, 31;
	and.b64  	%rd138, %rd169, 63;
	selp.b64 	%rd139, %rd138, 31, %p23;
	cvt.u32.u64 	%r59, %rd139;
	shr.u64 	%rd140, %rd57, %r59;
	add.s64 	%rd61, %rd140, -1;
	setp.lt.u64 	%p24, %rd61, 1024;
	@%p24 bra 	$L__BB8_35;
	bra.uni 	$L__BB8_34;
$L__BB8_35:
	shr.u64 	%rd111, %rd166, 3;
	setp.gt.u64 	%p21, %rd111, %rd8;
	selp.b64 	%rd55, %rd167, 0, %p21;
	setp.lt.u64 	%p2, %rd56, %rd168;
	setp.gt.u64 	%p25, %rd56, 511;
	add.s64 	%rd145, %rd56, 1;
	selp.b64 	%rd146, 512, %rd145, %p25;
	selp.b64 	%rd168, 512, %rd146, %p2;
	cvt.u32.u64 	%r72, %rd61;
	shl.b32 	%r73, %r72, 2;
	and.b32  	%r74, %r73, 4088;
	add.s32 	%r62, %r74, %r2;
	add.s32 	%r63, %r62, 4;
	// begin inline asm
	ld.shared.f32 %r60, [%r62];
ld.shared.f32 %r61, [%r63];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r64, %r61;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r66, %r60, %r76;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r69, %r64, %r76;
	// end inline asm
	st.u32 	[%rd55], %r66;
	st.u32 	[%rd55+4], %r69;
	add.s64 	%rd167, %rd167, %rd47;
	sub.s64 	%rd166, %rd166, %rd47;
	add.s64 	%rd165, %rd165, -1;
	setp.ne.s64 	%p26, %rd165, 0;
	@%p26 bra 	$L__BB8_31;
$L__BB8_36:
	ret;
$L__BB8_16:
	mov.u64 	%rd90, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd91, %rd90;
	mov.u64 	%rd92, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd93, %rd92;
	{ // callseq 32, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd93;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 32
$L__BB8_34:
	mov.u64 	%rd141, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd142, %rd141;
	mov.u64 	%rd143, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd144, %rd143;
	{ // callseq 34, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd144;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 34
$L__BB8_1:
	mov.u64 	%rd147, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_8;
	cvta.global.u64 	%rd148, %rd147;
	{ // callseq 35, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd148;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 35

}
	// .globl	fft_backward_1024_kernel
.visible .entry fft_backward_1024_kernel(
	.param .u64 fft_backward_1024_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot9[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<20>;
	.reg .b32 	%r<73>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<122>;

	mov.u64 	%SPL, __local_depot9;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd44, [fft_backward_1024_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd44;
	add.u64 	%rd46, %SP, 16;
	add.u64 	%rd47, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[8192];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd47], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd48, [%rd1+8];
	and.b64  	%rd49, %rd48, -1024;
	mul.wide.u32 	%rd6, %r5, 1024;
	setp.lt.u64 	%p3, %rd6, %rd49;
	sub.s64 	%rd50, %rd48, %rd6;
	setp.gt.u64 	%p4, %rd50, 1023;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB9_2;
	bra.uni 	$L__BB9_1;
$L__BB9_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r12, %rd3;
	ld.global.nc.u64 	%rd7, [%rd1];
	shl.b64 	%rd51, %rd6, 3;
	add.s64 	%rd52, %rd7, %rd51;
	add.s64 	%rd8, %rd4, -1;
	cvt.u16.u64 	%rs2, %rd3;
	xor.b16  	%rs1, %rs2, 1023;
	cvt.u16.u64 	%rs3, %rd4;
	shl.b64 	%rd53, %rd3, 3;
	add.s64 	%rd9, %rd52, %rd53;
	shl.b32 	%r13, %r12, 3;
	ld.u32 	%r7, [%rd9+4];
	ld.u32 	%r10, [%rd9];
	// begin inline asm
	neg.ftz.f32 %r6, %r7;
	// end inline asm
	add.s32 	%r8, %r2, %r13;
	add.s32 	%r9, %r8, 4;
	// begin inline asm
	st.shared.f32 [%r8], %r10;
st.shared.f32 [%r9], %r6;
	// end inline asm
	setp.lt.u16 	%p6, %rs1, %rs3;
	cvt.u32.u64 	%r70, %rd4;
	@%p6 bra 	$L__BB9_7;
	add.s64 	%rd116, %rd3, 1;
	cvt.u32.u16 	%r14, %rs1;
	div.u32 	%r16, %r14, %r70;
	cvt.u64.u32 	%rd115, %r16;
	shl.b64 	%rd54, %rd5, 13;
	shl.b64 	%rd12, %rd4, 3;
	add.s64 	%rd55, %rd54, %rd12;
	add.s64 	%rd57, %rd55, %rd53;
	add.s64 	%rd114, %rd7, %rd57;
	xor.b64  	%rd113, %rd53, 8184;
$L__BB9_4:
	shr.u64 	%rd58, %rd113, 3;
	setp.gt.u64 	%p7, %rd58, %rd8;
	selp.b64 	%rd59, %rd114, 0, %p7;
	add.s64 	%rd19, %rd116, %rd8;
	ld.f32 	%f1, [%rd59];
	ld.u32 	%r18, [%rd59+4];
	// begin inline asm
	neg.ftz.f32 %r17, %r18;
	// end inline asm
	setp.lt.u64 	%p8, %rd19, 1024;
	@%p8 bra 	$L__BB9_6;
	bra.uni 	$L__BB9_5;
$L__BB9_6:
	setp.lt.u64 	%p1, %rd19, %rd116;
	mov.b32 	%f2, %r17;
	add.s64 	%rd64, %rd116, %rd4;
	selp.b64 	%rd116, 1024, %rd64, %p1;
	cvt.u32.u64 	%r23, %rd19;
	shl.b32 	%r24, %r23, 3;
	add.s32 	%r19, %r24, %r2;
	add.s32 	%r20, %r19, 4;
	mov.b32 	%r21, %f1;
	// begin inline asm
	st.shared.f32 [%r19], %r21;
st.shared.f32 [%r20], %r17;
	// end inline asm
	add.s64 	%rd115, %rd115, -1;
	add.s64 	%rd114, %rd114, %rd12;
	sub.s64 	%rd113, %rd113, %rd12;
	setp.ne.s64 	%p9, %rd115, 0;
	@%p9 bra 	$L__BB9_4;
$L__BB9_7:
	add.u64 	%rd45, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd65, [%rd1+16];
	ld.global.nc.u64 	%rd66, [%rd1+24];
	st.local.u64 	[%rd2], %rd65;
	st.local.u64 	[%rd2+8], %rd66;
	bar.sync 	0;
	{ // callseq 37, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd45;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd46;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 37
	mov.b32 	%r26, 1149239296;
	// begin inline asm
	rcp.approx.ftz.f32 %r25, %r26;
	// end inline asm
	bar.sync 	0;
	xor.b64  	%rd69, %rd3, 1023;
	and.b16  	%rs5, %rs2, 240;
	and.b16  	%rs6, %rs2, 15;
	shl.b16 	%rs7, %rs6, 4;
	shr.u16 	%rs8, %rs5, 4;
	or.b16  	%rs9, %rs8, %rs7;
	and.b16  	%rs10, %rs9, 51;
	shl.b16 	%rs11, %rs10, 2;
	shr.u16 	%rs12, %rs9, 2;
	and.b16  	%rs13, %rs12, 51;
	or.b16  	%rs14, %rs13, %rs11;
	and.b16  	%rs15, %rs14, 85;
	shr.u32 	%r43, %r12, 8;
	shr.u32 	%r44, %r12, 6;
	and.b32  	%r45, %r44, 4;
	or.b32  	%r46, %r43, %r45;
	shl.b16 	%rs16, %rs14, 4;
	and.b16  	%rs17, %rs16, 2720;
	shl.b16 	%rs18, %rs15, 6;
	or.b16  	%rs19, %rs18, %rs17;
	cvt.u32.u16 	%r47, %rs19;
	shl.b32 	%r48, %r46, 2;
	or.b32  	%r49, %r48, %r47;
	or.b32  	%r50, %r49, 4;
	add.s32 	%r51, %r50, 8188;
	and.b32  	%r52, %r51, 8184;
	add.s32 	%r29, %r2, %r52;
	add.s32 	%r30, %r29, 4;
	// begin inline asm
	ld.shared.f32 %r27, [%r29];
ld.shared.f32 %r28, [%r30];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r31, %r28;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r33, %r27, %r25;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r36, %r31, %r25;
	// end inline asm
	st.u32 	[%rd9], %r33;
	st.u32 	[%rd9+4], %r36;
	setp.lt.u64 	%p10, %rd69, %rd4;
	@%p10 bra 	$L__BB9_14;
	mov.b32 	%f3, %r25;
	cvt.u32.u64 	%r40, %rd69;
	div.u32 	%r42, %r40, %r70;
	cvt.u64.u32 	%rd117, %r42;
	add.s64 	%rd120, %rd3, 1;
	shl.b64 	%rd70, %rd5, 13;
	shl.b64 	%rd26, %rd4, 3;
	add.s64 	%rd71, %rd70, %rd26;
	add.s64 	%rd73, %rd71, %rd53;
	add.s64 	%rd119, %rd7, %rd73;
	xor.b64  	%rd118, %rd53, 8184;
$L__BB9_9:
	add.s64 	%rd34, %rd120, %rd8;
	add.s64 	%rd76, %rd34, 1024;
	shr.u64 	%rd77, %rd76, 1;
	and.b64  	%rd78, %rd77, 1431655765;
	shl.b64 	%rd79, %rd76, 1;
	and.b64  	%rd80, %rd79, 2863311530;
	or.b64  	%rd81, %rd78, %rd80;
	shr.u64 	%rd82, %rd81, 2;
	and.b64  	%rd83, %rd82, 858993459;
	shl.b64 	%rd84, %rd81, 2;
	and.b64  	%rd85, %rd84, 3435973836;
	or.b64  	%rd86, %rd83, %rd85;
	shr.u64 	%rd87, %rd86, 4;
	and.b64  	%rd88, %rd87, 252645135;
	shl.b64 	%rd89, %rd86, 4;
	and.b64  	%rd90, %rd89, 4042322160;
	or.b64  	%rd91, %rd88, %rd90;
	shr.u64 	%rd92, %rd91, 8;
	and.b64  	%rd93, %rd92, 16711935;
	shl.b64 	%rd94, %rd91, 8;
	and.b64  	%rd95, %rd94, 4278255360;
	or.b64  	%rd96, %rd93, %rd95;
	shr.u64 	%rd97, %rd96, 16;
	shl.b64 	%rd98, %rd96, 16;
	or.b64  	%rd35, %rd97, %rd98;
	setp.eq.s64 	%p12, %rd35, 0;
	mov.u64 	%rd121, 64;
	@%p12 bra 	$L__BB9_11;
	add.s64 	%rd99, %rd35, -1;
	not.b64 	%rd100, %rd35;
	and.b64  	%rd101, %rd100, %rd99;
	popc.b64 	%r53, %rd101;
	cvt.u64.u32 	%rd121, %r53;
$L__BB9_11:
	setp.lt.u64 	%p13, %rd121, 31;
	and.b64  	%rd102, %rd121, 63;
	selp.b64 	%rd103, %rd102, 31, %p13;
	cvt.u32.u64 	%r54, %rd103;
	shr.u64 	%rd104, %rd35, %r54;
	add.s64 	%rd39, %rd104, -1;
	setp.lt.u64 	%p14, %rd39, 2048;
	@%p14 bra 	$L__BB9_13;
	bra.uni 	$L__BB9_12;
$L__BB9_13:
	shr.u64 	%rd75, %rd118, 3;
	setp.gt.u64 	%p11, %rd75, %rd8;
	selp.b64 	%rd33, %rd119, 0, %p11;
	setp.lt.u64 	%p2, %rd34, %rd120;
	setp.gt.u64 	%p15, %rd34, 1023;
	add.s64 	%rd109, %rd34, 1;
	selp.b64 	%rd110, 1024, %rd109, %p15;
	selp.b64 	%rd120, 1024, %rd110, %p2;
	cvt.u32.u64 	%r67, %rd39;
	shl.b32 	%r68, %r67, 2;
	and.b32  	%r69, %r68, 8184;
	add.s32 	%r57, %r69, %r2;
	add.s32 	%r58, %r57, 4;
	// begin inline asm
	ld.shared.f32 %r55, [%r57];
ld.shared.f32 %r56, [%r58];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r59, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r61, %r55, %r25;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r64, %r59, %r25;
	// end inline asm
	st.u32 	[%rd33], %r61;
	st.u32 	[%rd33+4], %r64;
	add.s64 	%rd119, %rd119, %rd26;
	sub.s64 	%rd118, %rd118, %rd26;
	add.s64 	%rd117, %rd117, -1;
	setp.ne.s64 	%p16, %rd117, 0;
	@%p16 bra 	$L__BB9_9;
$L__BB9_14:
	ret;
$L__BB9_5:
	mov.u64 	%rd60, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd61, %rd60;
	mov.u64 	%rd62, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd63, %rd62;
	{ // callseq 36, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd61;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd63;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 36
$L__BB9_12:
	mov.u64 	%rd105, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd106, %rd105;
	mov.u64 	%rd107, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd108, %rd107;
	{ // callseq 38, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd106;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd108;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 38
$L__BB9_1:
	mov.u64 	%rd111, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_8;
	cvta.global.u64 	%rd112, %rd111;
	{ // callseq 39, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd112;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 39

}
	// .globl	fft_backward_2048_kernel
.visible .entry fft_backward_2048_kernel(
	.param .u64 fft_backward_2048_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot10[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<24>;
	.reg .b32 	%r<63>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<112>;

	mov.u64 	%SPL, __local_depot10;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd39, [fft_backward_2048_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd39;
	add.u64 	%rd41, %SP, 16;
	add.u64 	%rd42, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[16384];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd42], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd43, [%rd1+8];
	and.b64  	%rd44, %rd43, -2048;
	mul.wide.u32 	%rd6, %r5, 2048;
	setp.lt.u64 	%p3, %rd6, %rd44;
	sub.s64 	%rd45, %rd43, %rd6;
	setp.gt.u64 	%p4, %rd45, 2047;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB10_2;
	bra.uni 	$L__BB10_1;
$L__BB10_2:
	add.u64 	%rd40, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r12, %rd3;
	ld.global.nc.u64 	%rd46, [%rd1];
	shl.b64 	%rd47, %rd6, 3;
	add.s64 	%rd48, %rd46, %rd47;
	add.s64 	%rd7, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 2047;
	shl.b64 	%rd49, %rd3, 3;
	add.s64 	%rd8, %rd48, %rd49;
	shl.b32 	%r13, %r12, 3;
	ld.u32 	%r7, [%rd8+4];
	ld.u32 	%r10, [%rd8];
	// begin inline asm
	neg.ftz.f32 %r11, %r7;
	// end inline asm
	add.s32 	%r8, %r2, %r13;
	add.s32 	%r9, %r8, 4;
	// begin inline asm
	st.shared.f32 [%r8], %r10;
st.shared.f32 [%r9], %r11;
	// end inline asm
	add.s64 	%rd110, %rd3, 1;
	cvt.u32.u16 	%r14, %rs2;
	cvt.u32.u64 	%r15, %rd4;
	div.u32 	%r16, %r14, %r15;
	cvt.u64.u32 	%rd105, %r16;
	shl.b64 	%rd50, %rd5, 14;
	shl.b64 	%rd11, %rd4, 3;
	add.s64 	%rd51, %rd50, %rd11;
	add.s64 	%rd52, %rd51, %rd49;
	add.s64 	%rd109, %rd46, %rd52;
	xor.b64  	%rd108, %rd49, 16376;
	mov.u64 	%rd103, %rd108;
	mov.u64 	%rd104, %rd109;
	mov.u64 	%rd106, %rd110;
$L__BB10_3:
	shr.u64 	%rd53, %rd103, 3;
	setp.gt.u64 	%p6, %rd53, %rd7;
	selp.b64 	%rd54, %rd104, 0, %p6;
	add.s64 	%rd18, %rd106, %rd7;
	ld.f32 	%f1, [%rd54];
	ld.u32 	%r18, [%rd54+4];
	// begin inline asm
	neg.ftz.f32 %r17, %r18;
	// end inline asm
	setp.lt.u64 	%p7, %rd18, 2048;
	@%p7 bra 	$L__BB10_5;
	bra.uni 	$L__BB10_4;
$L__BB10_5:
	setp.lt.u64 	%p1, %rd18, %rd106;
	mov.b32 	%f2, %r17;
	add.s64 	%rd59, %rd106, %rd4;
	selp.b64 	%rd106, 2048, %rd59, %p1;
	cvt.u32.u64 	%r23, %rd18;
	shl.b32 	%r24, %r23, 3;
	add.s32 	%r19, %r24, %r2;
	add.s32 	%r20, %r19, 4;
	mov.b32 	%r21, %f1;
	// begin inline asm
	st.shared.f32 [%r19], %r21;
st.shared.f32 [%r20], %r17;
	// end inline asm
	add.s64 	%rd105, %rd105, -1;
	add.s64 	%rd104, %rd104, %rd11;
	sub.s64 	%rd103, %rd103, %rd11;
	setp.ne.s64 	%p8, %rd105, 0;
	@%p8 bra 	$L__BB10_3;
	ld.global.nc.u64 	%rd60, [%rd1+16];
	ld.global.nc.u64 	%rd61, [%rd1+24];
	st.local.u64 	[%rd2], %rd60;
	st.local.u64 	[%rd2+8], %rd61;
	bar.sync 	0;
	{ // callseq 41, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd40;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd41;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 41
	mov.b32 	%r26, 1157627904;
	// begin inline asm
	rcp.approx.ftz.f32 %r25, %r26;
	// end inline asm
	mov.b32 	%f3, %r25;
	bar.sync 	0;
	xor.b32  	%r40, %r12, 2047;
	div.u32 	%r42, %r40, %r15;
	cvt.u64.u32 	%rd107, %r42;
	or.b16  	%rs3, %rs1, 2048;
	shl.b16 	%rs4, %rs1, 12;
	shr.u16 	%rs5, %rs3, 4;
	and.b16  	%rs6, %rs5, 176;
	or.b16  	%rs7, %rs4, %rs6;
	shl.b16 	%rs8, %rs1, 4;
	and.b16  	%rs9, %rs8, 3840;
	or.b16  	%rs10, %rs9, %rs7;
	and.b16  	%rs11, %rs10, 13107;
	shl.b16 	%rs12, %rs11, 2;
	shr.u16 	%rs13, %rs10, 2;
	and.b16  	%rs14, %rs13, 13107;
	or.b16  	%rs15, %rs14, %rs12;
	and.b16  	%rs16, %rs15, 21845;
	shl.b16 	%rs17, %rs16, 1;
	shr.u16 	%rs18, %rs15, 1;
	and.b16  	%rs19, %rs18, 21845;
	or.b16  	%rs20, %rs19, %rs17;
	shr.u16 	%rs21, %rs20, 2;
	add.s16 	%rs22, %rs21, 16380;
	and.b16  	%rs23, %rs22, 16368;
	cvt.u32.u16 	%r43, %rs23;
	add.s32 	%r29, %r2, %r43;
	add.s32 	%r30, %r29, 4;
	// begin inline asm
	ld.shared.f32 %r34, [%r29];
ld.shared.f32 %r32, [%r30];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r37, %r32;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r33, %r34, %r25;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r36, %r37, %r25;
	// end inline asm
	st.u32 	[%rd8], %r33;
	st.u32 	[%rd8+4], %r36;
$L__BB10_7:
	add.s64 	%rd29, %rd110, %rd7;
	add.s64 	%rd66, %rd29, 2048;
	shr.u64 	%rd67, %rd66, 1;
	and.b64  	%rd68, %rd67, 1431655765;
	shl.b64 	%rd69, %rd66, 1;
	and.b64  	%rd70, %rd69, 2863311530;
	or.b64  	%rd71, %rd68, %rd70;
	shr.u64 	%rd72, %rd71, 2;
	and.b64  	%rd73, %rd72, 858993459;
	shl.b64 	%rd74, %rd71, 2;
	and.b64  	%rd75, %rd74, 3435973836;
	or.b64  	%rd76, %rd73, %rd75;
	shr.u64 	%rd77, %rd76, 4;
	and.b64  	%rd78, %rd77, 252645135;
	shl.b64 	%rd79, %rd76, 4;
	and.b64  	%rd80, %rd79, 4042322160;
	or.b64  	%rd81, %rd78, %rd80;
	shr.u64 	%rd82, %rd81, 8;
	and.b64  	%rd83, %rd82, 16711935;
	shl.b64 	%rd84, %rd81, 8;
	and.b64  	%rd85, %rd84, 4278255360;
	or.b64  	%rd86, %rd83, %rd85;
	shr.u64 	%rd87, %rd86, 16;
	shl.b64 	%rd88, %rd86, 16;
	or.b64  	%rd30, %rd87, %rd88;
	setp.eq.s64 	%p10, %rd30, 0;
	mov.u64 	%rd111, 64;
	@%p10 bra 	$L__BB10_9;
	add.s64 	%rd89, %rd30, -1;
	not.b64 	%rd90, %rd30;
	and.b64  	%rd91, %rd90, %rd89;
	popc.b64 	%r44, %rd91;
	cvt.u64.u32 	%rd111, %r44;
$L__BB10_9:
	setp.lt.u64 	%p11, %rd111, 31;
	and.b64  	%rd92, %rd111, 63;
	selp.b64 	%rd93, %rd92, 31, %p11;
	cvt.u32.u64 	%r45, %rd93;
	shr.u64 	%rd94, %rd30, %r45;
	add.s64 	%rd34, %rd94, -1;
	setp.lt.u64 	%p12, %rd34, 4096;
	@%p12 bra 	$L__BB10_11;
	bra.uni 	$L__BB10_10;
$L__BB10_11:
	shr.u64 	%rd65, %rd108, 3;
	setp.gt.u64 	%p9, %rd65, %rd7;
	selp.b64 	%rd28, %rd109, 0, %p9;
	setp.lt.u64 	%p2, %rd29, %rd110;
	setp.gt.u64 	%p13, %rd29, 2047;
	add.s64 	%rd99, %rd29, 1;
	selp.b64 	%rd100, 2048, %rd99, %p13;
	selp.b64 	%rd110, 2048, %rd100, %p2;
	cvt.u32.u64 	%r58, %rd34;
	shl.b32 	%r59, %r58, 2;
	and.b32  	%r60, %r59, 16376;
	add.s32 	%r48, %r60, %r2;
	add.s32 	%r49, %r48, 4;
	// begin inline asm
	ld.shared.f32 %r46, [%r48];
ld.shared.f32 %r47, [%r49];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r50, %r47;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r52, %r46, %r25;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r55, %r50, %r25;
	// end inline asm
	st.u32 	[%rd28], %r52;
	st.u32 	[%rd28+4], %r55;
	add.s64 	%rd109, %rd109, %rd11;
	sub.s64 	%rd108, %rd108, %rd11;
	add.s64 	%rd107, %rd107, -1;
	setp.ne.s64 	%p14, %rd107, 0;
	@%p14 bra 	$L__BB10_7;
	ret;
$L__BB10_4:
	mov.u64 	%rd55, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd56, %rd55;
	mov.u64 	%rd57, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd58, %rd57;
	{ // callseq 40, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd56;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd58;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 40
$L__BB10_10:
	mov.u64 	%rd95, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd96, %rd95;
	mov.u64 	%rd97, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd98, %rd97;
	{ // callseq 42, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd96;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd98;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 42
$L__BB10_1:
	mov.u64 	%rd101, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_8;
	cvta.global.u64 	%rd102, %rd101;
	{ // callseq 43, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd102;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 43

}
	// .globl	fft_backward_4096_kernel
.visible .entry fft_backward_4096_kernel(
	.param .u64 fft_backward_4096_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot11[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<70>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<136>;

	mov.u64 	%SPL, __local_depot11;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd40, [fft_backward_4096_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd40;
	add.u64 	%rd42, %SP, 16;
	add.u64 	%rd43, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[32768];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd43], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd44, [%rd1+8];
	and.b64  	%rd45, %rd44, -4096;
	mul.wide.u32 	%rd6, %r5, 4096;
	setp.lt.u64 	%p3, %rd6, %rd45;
	sub.s64 	%rd46, %rd44, %rd6;
	setp.gt.u64 	%p4, %rd46, 4095;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB11_2;
	bra.uni 	$L__BB11_1;
$L__BB11_2:
	add.u64 	%rd41, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r12, %rd3;
	ld.global.nc.u64 	%rd47, [%rd1];
	shl.b64 	%rd48, %rd6, 3;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd7, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 4095;
	shl.b64 	%rd50, %rd3, 3;
	add.s64 	%rd8, %rd49, %rd50;
	add.s64 	%rd134, %rd3, 1;
	shl.b32 	%r13, %r12, 3;
	ld.u32 	%r7, [%rd8+4];
	ld.u32 	%r10, [%rd8];
	// begin inline asm
	neg.ftz.f32 %r11, %r7;
	// end inline asm
	add.s32 	%r8, %r2, %r13;
	add.s32 	%r9, %r8, 4;
	// begin inline asm
	st.shared.f32 [%r8], %r10;
st.shared.f32 [%r9], %r11;
	// end inline asm
	cvt.u32.u16 	%r14, %rs2;
	cvt.u32.u64 	%r15, %rd4;
	div.u32 	%r16, %r14, %r15;
	cvt.u64.u32 	%rd129, %r16;
	shl.b64 	%rd51, %rd5, 15;
	shl.b64 	%rd11, %rd4, 3;
	add.s64 	%rd52, %rd51, %rd11;
	add.s64 	%rd53, %rd52, %rd50;
	add.s64 	%rd133, %rd47, %rd53;
	xor.b64  	%rd132, %rd50, 32760;
	mov.u64 	%rd127, %rd132;
	mov.u64 	%rd128, %rd133;
	mov.u64 	%rd130, %rd134;
$L__BB11_3:
	shr.u64 	%rd54, %rd127, 3;
	setp.gt.u64 	%p6, %rd54, %rd7;
	selp.b64 	%rd55, %rd128, 0, %p6;
	add.s64 	%rd18, %rd130, %rd7;
	ld.f32 	%f1, [%rd55];
	ld.u32 	%r18, [%rd55+4];
	// begin inline asm
	neg.ftz.f32 %r17, %r18;
	// end inline asm
	setp.lt.u64 	%p7, %rd18, 4096;
	@%p7 bra 	$L__BB11_5;
	bra.uni 	$L__BB11_4;
$L__BB11_5:
	setp.lt.u64 	%p1, %rd18, %rd130;
	mov.b32 	%f2, %r17;
	add.s64 	%rd60, %rd130, %rd4;
	selp.b64 	%rd130, 4096, %rd60, %p1;
	cvt.u32.u64 	%r23, %rd18;
	shl.b32 	%r24, %r23, 3;
	add.s32 	%r19, %r24, %r2;
	add.s32 	%r20, %r19, 4;
	mov.b32 	%r21, %f1;
	// begin inline asm
	st.shared.f32 [%r19], %r21;
st.shared.f32 [%r20], %r17;
	// end inline asm
	add.s64 	%rd129, %rd129, -1;
	add.s64 	%rd128, %rd128, %rd11;
	sub.s64 	%rd127, %rd127, %rd11;
	setp.ne.s64 	%p8, %rd129, 0;
	@%p8 bra 	$L__BB11_3;
	ld.global.nc.u64 	%rd61, [%rd1+16];
	ld.global.nc.u64 	%rd62, [%rd1+24];
	st.local.u64 	[%rd2], %rd61;
	st.local.u64 	[%rd2+8], %rd62;
	bar.sync 	0;
	{ // callseq 45, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd41;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd42;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 45
	mov.b32 	%r26, 1166016512;
	// begin inline asm
	rcp.approx.ftz.f32 %r69, %r26;
	// end inline asm
	bar.sync 	0;
	shr.u64 	%rd65, %rd3, 1;
	and.b64  	%rd66, %rd65, 341;
	shl.b64 	%rd67, %rd3, 1;
	and.b64  	%rd68, %rd67, 682;
	or.b64  	%rd69, %rd68, %rd66;
	shr.u64 	%rd70, %rd69, 2;
	and.b64  	%rd71, %rd70, 51;
	shl.b64 	%rd72, %rd69, 2;
	and.b64  	%rd73, %rd72, 3276;
	or.b64  	%rd74, %rd73, %rd71;
	or.b64  	%rd75, %rd74, 32768;
	shr.u64 	%rd76, %rd75, 4;
	and.b64  	%rd77, %rd76, 2063;
	shl.b64 	%rd78, %rd74, 4;
	and.b64  	%rd79, %rd78, 49392;
	or.b64  	%rd80, %rd77, %rd79;
	{ .reg .b32 tmp; mov.b64 {%r31, tmp}, %rd80; }
	prmt.b32 	%r32, %r31, 0, 291;
	{ .reg .b32 tmp; mov.b64 {tmp, %r33}, %rd80; }
	prmt.b32 	%r34, %r33, 0, 291;
	mov.b64 	%rd81, {%r34, %r32};
	shr.u64 	%rd82, %rd81, 32;
	or.b64  	%rd83, %rd82, 2147483648;
	add.s64 	%rd84, %rd83, -1;
	not.b64 	%rd85, %rd83;
	and.b64  	%rd86, %rd85, %rd84;
	popc.b64 	%r35, %rd86;
	shr.u64 	%rd87, %rd82, %r35;
	add.s64 	%rd24, %rd87, -1;
	setp.gt.u64 	%p9, %rd24, 8191;
	@%p9 bra 	$L__BB11_11;
	mov.b32 	%f3, %r69;
	xor.b32  	%r28, %r12, 4095;
	div.u32 	%r30, %r28, %r15;
	cvt.u64.u32 	%rd131, %r30;
	cvt.u32.u64 	%r48, %rd24;
	shl.b32 	%r49, %r48, 2;
	and.b32  	%r50, %r49, 32760;
	add.s32 	%r38, %r2, %r50;
	add.s32 	%r39, %r38, 4;
	// begin inline asm
	ld.shared.f32 %r43, [%r38];
ld.shared.f32 %r41, [%r39];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r46, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r42, %r43, %r69;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r46, %r69;
	// end inline asm
	st.u32 	[%rd8], %r42;
	st.u32 	[%rd8+4], %r45;
$L__BB11_8:
	add.s64 	%rd30, %rd134, %rd7;
	add.s64 	%rd90, %rd30, 4096;
	shr.u64 	%rd91, %rd90, 1;
	and.b64  	%rd92, %rd91, 1431655765;
	shl.b64 	%rd93, %rd90, 1;
	and.b64  	%rd94, %rd93, 2863311530;
	or.b64  	%rd95, %rd92, %rd94;
	shr.u64 	%rd96, %rd95, 2;
	and.b64  	%rd97, %rd96, 858993459;
	shl.b64 	%rd98, %rd95, 2;
	and.b64  	%rd99, %rd98, 3435973836;
	or.b64  	%rd100, %rd97, %rd99;
	shr.u64 	%rd101, %rd100, 4;
	and.b64  	%rd102, %rd101, 252645135;
	shl.b64 	%rd103, %rd100, 4;
	and.b64  	%rd104, %rd103, 4042322160;
	or.b64  	%rd105, %rd102, %rd104;
	shr.u64 	%rd106, %rd105, 8;
	and.b64  	%rd107, %rd106, 16711935;
	shl.b64 	%rd108, %rd105, 8;
	and.b64  	%rd109, %rd108, 4278255360;
	or.b64  	%rd110, %rd107, %rd109;
	shr.u64 	%rd111, %rd110, 16;
	shl.b64 	%rd112, %rd110, 16;
	or.b64  	%rd31, %rd111, %rd112;
	setp.eq.s64 	%p11, %rd31, 0;
	mov.u64 	%rd135, 64;
	@%p11 bra 	$L__BB11_10;
	add.s64 	%rd113, %rd31, -1;
	not.b64 	%rd114, %rd31;
	and.b64  	%rd115, %rd114, %rd113;
	popc.b64 	%r51, %rd115;
	cvt.u64.u32 	%rd135, %r51;
$L__BB11_10:
	setp.lt.u64 	%p12, %rd135, 31;
	and.b64  	%rd116, %rd135, 63;
	selp.b64 	%rd117, %rd116, 31, %p12;
	cvt.u32.u64 	%r52, %rd117;
	shr.u64 	%rd118, %rd31, %r52;
	add.s64 	%rd35, %rd118, -1;
	setp.lt.u64 	%p13, %rd35, 8192;
	@%p13 bra 	$L__BB11_12;
	bra.uni 	$L__BB11_11;
$L__BB11_12:
	shr.u64 	%rd89, %rd132, 3;
	setp.gt.u64 	%p10, %rd89, %rd7;
	selp.b64 	%rd29, %rd133, 0, %p10;
	setp.lt.u64 	%p2, %rd30, %rd134;
	setp.gt.u64 	%p14, %rd30, 4095;
	add.s64 	%rd123, %rd30, 1;
	selp.b64 	%rd124, 4096, %rd123, %p14;
	selp.b64 	%rd134, 4096, %rd124, %p2;
	cvt.u32.u64 	%r65, %rd35;
	shl.b32 	%r66, %r65, 2;
	and.b32  	%r67, %r66, 32760;
	add.s32 	%r55, %r67, %r2;
	add.s32 	%r56, %r55, 4;
	// begin inline asm
	ld.shared.f32 %r53, [%r55];
ld.shared.f32 %r54, [%r56];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r57, %r54;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r59, %r53, %r69;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r62, %r57, %r69;
	// end inline asm
	st.u32 	[%rd29], %r59;
	st.u32 	[%rd29+4], %r62;
	add.s64 	%rd133, %rd133, %rd11;
	sub.s64 	%rd132, %rd132, %rd11;
	add.s64 	%rd131, %rd131, -1;
	setp.ne.s64 	%p15, %rd131, 0;
	@%p15 bra 	$L__BB11_8;
	ret;
$L__BB11_11:
	mov.u64 	%rd119, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd120, %rd119;
	mov.u64 	%rd121, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd122, %rd121;
	{ // callseq 46, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd120;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd122;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 46
$L__BB11_4:
	mov.u64 	%rd56, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd57, %rd56;
	mov.u64 	%rd58, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd59, %rd58;
	{ // callseq 44, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd57;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd59;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 44
$L__BB11_1:
	mov.u64 	%rd125, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_8;
	cvta.global.u64 	%rd126, %rd125;
	{ // callseq 47, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd126;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 47

}
	// .globl	fft_forward_shifted_128_kernel
.visible .entry fft_forward_shifted_128_kernel(
	.param .u64 fft_forward_shifted_128_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot12[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<73>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<146>;

	mov.u64 	%SPL, __local_depot12;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd62, [fft_forward_shifted_128_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd62;
	add.u64 	%rd64, %SP, 16;
	add.u64 	%rd65, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[1024];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd65], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd66, [%rd1+8];
	and.b64  	%rd67, %rd66, -128;
	mul.wide.u32 	%rd68, %r5, 128;
	setp.lt.u64 	%p2, %rd68, %rd67;
	sub.s64 	%rd70, %rd66, %rd68;
	setp.gt.u64 	%p3, %rd70, 127;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB12_2;
	bra.uni 	$L__BB12_1;
$L__BB12_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd72, %rd3, 128;
	setp.gt.u32 	%p5, %r6, 127;
	not.b64 	%rd73, %rd3;
	add.s64 	%rd10, %rd73, %rd72;
	mov.u64 	%rd132, 0;
	and.b64  	%rd128, %rd10, -4294967296;
	mov.u64 	%rd130, %rd132;
	@%p5 bra 	$L__BB12_7;
	setp.ne.s64 	%p6, %rd128, 0;
	@%p6 bra 	$L__BB12_5;
	bra.uni 	$L__BB12_4;
$L__BB12_5:
	div.u64 	%rd129, %rd10, %rd4;
	bra.uni 	$L__BB12_6;
$L__BB12_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd10;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd129, %r9;
$L__BB12_6:
	add.s64 	%rd130, %rd129, 1;
$L__BB12_7:
	shl.b64 	%rd69, %rd68, 3;
	@%p5 bra 	$L__BB12_12;
	setp.ne.s64 	%p8, %rd128, 0;
	@%p8 bra 	$L__BB12_10;
	bra.uni 	$L__BB12_9;
$L__BB12_10:
	div.u64 	%rd131, %rd10, %rd4;
	bra.uni 	$L__BB12_11;
$L__BB12_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd10;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd131, %r13;
$L__BB12_11:
	add.s64 	%rd132, %rd131, 1;
$L__BB12_12:
	add.s64 	%rd7, %rd6, %rd69;
	add.s64 	%rd9, %rd4, -1;
	min.u64 	%rd21, %rd130, %rd132;
	setp.eq.s64 	%p9, %rd21, 0;
	shl.b64 	%rd124, %rd3, 3;
	shl.b64 	%rd125, %rd4, 3;
	@%p9 bra 	$L__BB12_18;
	add.s64 	%rd78, %rd7, %rd124;
	shl.b32 	%r19, %r6, 3;
	add.s32 	%r14, %r2, %r19;
	add.s32 	%r15, %r14, 4;
	ld.u32 	%r16, [%rd78];
	ld.u32 	%r17, [%rd78+4];
	// begin inline asm
	st.shared.f32 [%r14], %r16;
st.shared.f32 [%r15], %r17;
	// end inline asm
	setp.eq.s64 	%p10, %rd21, 1;
	@%p10 bra 	$L__BB12_18;
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd136, %rd3, 1;
	add.s64 	%rd135, %rd21, -1;
	shl.b64 	%rd79, %rd5, 10;
	add.s64 	%rd80, %rd79, %rd125;
	add.s64 	%rd82, %rd80, %rd124;
	add.s64 	%rd134, %rd6, %rd82;
	mov.u64 	%rd83, 1016;
	sub.s64 	%rd133, %rd83, %rd124;
$L__BB12_15:
	add.s64 	%rd32, %rd136, %rd9;
	setp.lt.u64 	%p12, %rd32, 128;
	@%p12 bra 	$L__BB12_17;
	bra.uni 	$L__BB12_16;
$L__BB12_17:
	shr.u64 	%rd84, %rd133, 3;
	setp.gt.u64 	%p11, %rd84, %rd9;
	selp.b64 	%rd31, %rd134, 0, %p11;
	setp.lt.u64 	%p1, %rd32, %rd136;
	add.s64 	%rd89, %rd136, %rd4;
	selp.b64 	%rd136, 128, %rd89, %p1;
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r20, %r25, %r2;
	add.s32 	%r21, %r20, 4;
	ld.u32 	%r22, [%rd31];
	ld.u32 	%r23, [%rd31+4];
	// begin inline asm
	st.shared.f32 [%r20], %r22;
st.shared.f32 [%r21], %r23;
	// end inline asm
	add.s64 	%rd135, %rd135, -1;
	add.s64 	%rd134, %rd134, %rd125;
	sub.s64 	%rd133, %rd133, %rd125;
	setp.ne.s64 	%p13, %rd135, 0;
	@%p13 bra 	$L__BB12_15;
$L__BB12_18:
	add.u64 	%rd63, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd91, [%rd1+16];
	ld.global.nc.u64 	%rd92, [%rd1+24];
	st.local.u64 	[%rd2], %rd91;
	st.local.u64 	[%rd2+8], %rd92;
	bar.sync 	0;
	{ // callseq 49, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd63;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd64;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 49
	bar.sync 	0;
	mov.u64 	%rd140, 0;
	mov.u64 	%rd138, %rd140;
	@%p5 bra 	$L__BB12_23;
	setp.ne.s64 	%p15, %rd128, 0;
	@%p15 bra 	$L__BB12_21;
	bra.uni 	$L__BB12_20;
$L__BB12_21:
	div.u64 	%rd137, %rd10, %rd4;
	bra.uni 	$L__BB12_22;
$L__BB12_20:
	cvt.u32.u64 	%r27, %rd4;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	cvt.u64.u32 	%rd137, %r29;
$L__BB12_22:
	add.s64 	%rd138, %rd137, 1;
$L__BB12_23:
	@%p5 bra 	$L__BB12_28;
	setp.ne.s64 	%p17, %rd128, 0;
	@%p17 bra 	$L__BB12_26;
	bra.uni 	$L__BB12_25;
$L__BB12_26:
	div.u64 	%rd139, %rd10, %rd4;
	bra.uni 	$L__BB12_27;
$L__BB12_25:
	cvt.u32.u64 	%r31, %rd4;
	cvt.u32.u64 	%r32, %rd10;
	div.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd139, %r33;
$L__BB12_27:
	add.s64 	%rd140, %rd139, 1;
$L__BB12_28:
	min.u64 	%rd47, %rd138, %rd140;
	setp.eq.s64 	%p18, %rd47, 0;
	@%p18 bra 	$L__BB12_35;
	add.s64 	%rd48, %rd7, %rd124;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, -64;
	and.b16  	%rs3, %rs2, 240;
	and.b16  	%rs4, %rs2, 15;
	shl.b16 	%rs5, %rs4, 4;
	shr.u16 	%rs6, %rs3, 4;
	or.b16  	%rs7, %rs6, %rs5;
	and.b16  	%rs8, %rs7, 51;
	shl.b16 	%rs9, %rs8, 2;
	shr.u16 	%rs10, %rs7, 2;
	and.b16  	%rs11, %rs10, 51;
	or.b16  	%rs12, %rs11, %rs9;
	and.b16  	%rs13, %rs12, 85;
	shl.b16 	%rs14, %rs13, 1;
	shr.u16 	%rs15, %rs12, 1;
	and.b16  	%rs16, %rs15, 85;
	or.b16  	%rs17, %rs16, %rs14;
	mul.wide.u16 	%r38, %rs17, 4;
	add.s32 	%r37, %r2, %r38;
	add.s32 	%r36, %r37, -4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd48], %r34;
	st.u32 	[%rd48+4], %r35;
	setp.eq.s64 	%p19, %rd47, 1;
	@%p19 bra 	$L__BB12_35;
	add.s64 	%rd8, %rd7, 1024;
	add.s64 	%rd145, %rd3, 1;
	add.s64 	%rd144, %rd48, 8;
	add.s64 	%rd51, %rd47, -1;
	and.b64  	%rd52, %rd51, 1;
	setp.eq.s64 	%p20, %rd47, 2;
	@%p20 bra 	$L__BB12_33;
	and.b64  	%rd141, %rd51, -2;
$L__BB12_32:
	sub.s64 	%rd99, %rd8, %rd144;
	shr.u64 	%rd100, %rd99, 3;
	setp.gt.u64 	%p21, %rd100, %rd9;
	add.s64 	%rd103, %rd144, %rd125;
	add.s64 	%rd104, %rd103, -8;
	selp.b64 	%rd105, %rd103, %rd8, %p21;
	selp.b64 	%rd106, %rd104, 0, %p21;
	add.s64 	%rd107, %rd145, %rd9;
	setp.lt.u64 	%p22, %rd107, %rd145;
	setp.gt.u64 	%p23, %rd107, 127;
	add.s64 	%rd108, %rd107, 1;
	selp.b64 	%rd109, 128, %rd108, %p23;
	selp.b64 	%rd110, 128, %rd109, %p22;
	cvt.u16.u64 	%rs18, %rd107;
	and.b16  	%rs19, %rs18, 127;
	xor.b16  	%rs20, %rs19, -64;
	and.b16  	%rs21, %rs20, 240;
	and.b16  	%rs22, %rs20, 15;
	shl.b16 	%rs23, %rs22, 4;
	shr.u16 	%rs24, %rs21, 4;
	or.b16  	%rs25, %rs24, %rs23;
	and.b16  	%rs26, %rs25, 51;
	shl.b16 	%rs27, %rs26, 2;
	shr.u16 	%rs28, %rs25, 2;
	and.b16  	%rs29, %rs28, 51;
	or.b16  	%rs30, %rs29, %rs27;
	and.b16  	%rs31, %rs30, 85;
	shl.b16 	%rs32, %rs31, 1;
	shr.u16 	%rs33, %rs30, 1;
	and.b16  	%rs34, %rs33, 85;
	or.b16  	%rs35, %rs34, %rs32;
	mul.wide.u16 	%r47, %rs35, 4;
	add.s32 	%r42, %r47, %r2;
	add.s32 	%r41, %r42, -4;
	// begin inline asm
	ld.shared.f32 %r39, [%r41];
ld.shared.f32 %r40, [%r42];
	// end inline asm
	st.u32 	[%rd106], %r39;
	st.u32 	[%rd106+4], %r40;
	add.s64 	%rd144, %rd105, %rd125;
	add.s64 	%rd112, %rd110, %rd9;
	setp.lt.u64 	%p24, %rd112, %rd110;
	setp.gt.u64 	%p25, %rd112, 127;
	add.s64 	%rd113, %rd112, 1;
	selp.b64 	%rd114, 128, %rd113, %p25;
	selp.b64 	%rd145, 128, %rd114, %p24;
	cvt.u16.u64 	%rs36, %rd112;
	and.b16  	%rs37, %rs36, 127;
	xor.b16  	%rs38, %rs37, -64;
	and.b16  	%rs39, %rs38, 240;
	and.b16  	%rs40, %rs38, 15;
	shl.b16 	%rs41, %rs40, 4;
	shr.u16 	%rs42, %rs39, 4;
	or.b16  	%rs43, %rs42, %rs41;
	and.b16  	%rs44, %rs43, 51;
	shl.b16 	%rs45, %rs44, 2;
	shr.u16 	%rs46, %rs43, 2;
	and.b16  	%rs47, %rs46, 51;
	or.b16  	%rs48, %rs47, %rs45;
	and.b16  	%rs49, %rs48, 85;
	shl.b16 	%rs50, %rs49, 1;
	shr.u16 	%rs51, %rs48, 1;
	and.b16  	%rs52, %rs51, 85;
	or.b16  	%rs53, %rs52, %rs50;
	mul.wide.u16 	%r48, %rs53, 4;
	add.s32 	%r46, %r48, %r2;
	add.s32 	%r45, %r46, -4;
	// begin inline asm
	ld.shared.f32 %r43, [%r45];
ld.shared.f32 %r44, [%r46];
	// end inline asm
	st.u32 	[%rd144+-8], %r43;
	st.u32 	[%rd144+-4], %r44;
	add.s64 	%rd141, %rd141, -2;
	setp.ne.s64 	%p26, %rd141, 0;
	@%p26 bra 	$L__BB12_32;
$L__BB12_33:
	setp.eq.s64 	%p27, %rd52, 0;
	@%p27 bra 	$L__BB12_35;
	sub.s64 	%rd115, %rd8, %rd144;
	shr.u64 	%rd116, %rd115, 3;
	setp.gt.u64 	%p28, %rd116, %rd9;
	add.s64 	%rd118, %rd144, %rd125;
	add.s64 	%rd119, %rd118, -8;
	selp.b64 	%rd120, %rd119, 0, %p28;
	add.s64 	%rd121, %rd145, %rd9;
	setp.lt.u64 	%p29, %rd121, %rd145;
	cvt.u16.u64 	%rs54, %rd121;
	and.b16  	%rs55, %rs54, 127;
	xor.b16  	%rs56, %rs55, -64;
	selp.b16 	%rs57, -64, %rs56, %p29;
	and.b16  	%rs58, %rs57, 240;
	and.b16  	%rs59, %rs57, 15;
	shl.b16 	%rs60, %rs59, 4;
	shr.u16 	%rs61, %rs58, 4;
	or.b16  	%rs62, %rs61, %rs60;
	and.b16  	%rs63, %rs62, 51;
	shl.b16 	%rs64, %rs63, 2;
	shr.u16 	%rs65, %rs62, 2;
	and.b16  	%rs66, %rs65, 51;
	or.b16  	%rs67, %rs66, %rs64;
	and.b16  	%rs68, %rs67, 85;
	shl.b16 	%rs69, %rs68, 1;
	shr.u16 	%rs70, %rs67, 1;
	and.b16  	%rs71, %rs70, 85;
	or.b16  	%rs72, %rs71, %rs69;
	mul.wide.u16 	%r53, %rs72, 4;
	add.s32 	%r52, %r53, %r2;
	add.s32 	%r51, %r52, -4;
	// begin inline asm
	ld.shared.f32 %r49, [%r51];
ld.shared.f32 %r50, [%r52];
	// end inline asm
	st.u32 	[%rd120], %r49;
	st.u32 	[%rd120+4], %r50;
$L__BB12_35:
	ret;
$L__BB12_16:
	mov.u64 	%rd85, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd86, %rd85;
	mov.u64 	%rd87, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd88, %rd87;
	{ // callseq 48, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd86;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd88;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 48
$L__BB12_1:
	mov.u64 	%rd122, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_7;
	cvta.global.u64 	%rd123, %rd122;
	{ // callseq 50, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd123;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 50

}
	// .globl	fft_forward_shifted_256_kernel
.visible .entry fft_forward_shifted_256_kernel(
	.param .u64 fft_forward_shifted_256_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot13[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<76>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<146>;

	mov.u64 	%SPL, __local_depot13;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd62, [fft_forward_shifted_256_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd62;
	add.u64 	%rd64, %SP, 16;
	add.u64 	%rd65, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[2048];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd65], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd66, [%rd1+8];
	and.b64  	%rd67, %rd66, -256;
	mul.wide.u32 	%rd68, %r5, 256;
	setp.lt.u64 	%p2, %rd68, %rd67;
	sub.s64 	%rd70, %rd66, %rd68;
	setp.gt.u64 	%p3, %rd70, 255;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB13_2;
	bra.uni 	$L__BB13_1;
$L__BB13_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd72, %rd3, 256;
	setp.gt.u32 	%p5, %r6, 255;
	not.b64 	%rd73, %rd3;
	add.s64 	%rd10, %rd73, %rd72;
	mov.u64 	%rd132, 0;
	and.b64  	%rd128, %rd10, -4294967296;
	mov.u64 	%rd130, %rd132;
	@%p5 bra 	$L__BB13_7;
	setp.ne.s64 	%p6, %rd128, 0;
	@%p6 bra 	$L__BB13_5;
	bra.uni 	$L__BB13_4;
$L__BB13_5:
	div.u64 	%rd129, %rd10, %rd4;
	bra.uni 	$L__BB13_6;
$L__BB13_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd10;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd129, %r9;
$L__BB13_6:
	add.s64 	%rd130, %rd129, 1;
$L__BB13_7:
	shl.b64 	%rd69, %rd68, 3;
	@%p5 bra 	$L__BB13_12;
	setp.ne.s64 	%p8, %rd128, 0;
	@%p8 bra 	$L__BB13_10;
	bra.uni 	$L__BB13_9;
$L__BB13_10:
	div.u64 	%rd131, %rd10, %rd4;
	bra.uni 	$L__BB13_11;
$L__BB13_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd10;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd131, %r13;
$L__BB13_11:
	add.s64 	%rd132, %rd131, 1;
$L__BB13_12:
	add.s64 	%rd7, %rd6, %rd69;
	add.s64 	%rd9, %rd4, -1;
	min.u64 	%rd21, %rd130, %rd132;
	setp.eq.s64 	%p9, %rd21, 0;
	shl.b64 	%rd124, %rd3, 3;
	shl.b64 	%rd125, %rd4, 3;
	@%p9 bra 	$L__BB13_18;
	add.s64 	%rd78, %rd7, %rd124;
	shl.b32 	%r19, %r6, 3;
	add.s32 	%r14, %r2, %r19;
	add.s32 	%r15, %r14, 4;
	ld.u32 	%r16, [%rd78];
	ld.u32 	%r17, [%rd78+4];
	// begin inline asm
	st.shared.f32 [%r14], %r16;
st.shared.f32 [%r15], %r17;
	// end inline asm
	setp.eq.s64 	%p10, %rd21, 1;
	@%p10 bra 	$L__BB13_18;
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd136, %rd3, 1;
	add.s64 	%rd135, %rd21, -1;
	shl.b64 	%rd79, %rd5, 11;
	add.s64 	%rd80, %rd79, %rd125;
	add.s64 	%rd82, %rd80, %rd124;
	add.s64 	%rd134, %rd6, %rd82;
	mov.u64 	%rd83, 2040;
	sub.s64 	%rd133, %rd83, %rd124;
$L__BB13_15:
	add.s64 	%rd32, %rd136, %rd9;
	setp.lt.u64 	%p12, %rd32, 256;
	@%p12 bra 	$L__BB13_17;
	bra.uni 	$L__BB13_16;
$L__BB13_17:
	shr.u64 	%rd84, %rd133, 3;
	setp.gt.u64 	%p11, %rd84, %rd9;
	selp.b64 	%rd31, %rd134, 0, %p11;
	setp.lt.u64 	%p1, %rd32, %rd136;
	add.s64 	%rd89, %rd136, %rd4;
	selp.b64 	%rd136, 256, %rd89, %p1;
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r20, %r25, %r2;
	add.s32 	%r21, %r20, 4;
	ld.u32 	%r22, [%rd31];
	ld.u32 	%r23, [%rd31+4];
	// begin inline asm
	st.shared.f32 [%r20], %r22;
st.shared.f32 [%r21], %r23;
	// end inline asm
	add.s64 	%rd135, %rd135, -1;
	add.s64 	%rd134, %rd134, %rd125;
	sub.s64 	%rd133, %rd133, %rd125;
	setp.ne.s64 	%p13, %rd135, 0;
	@%p13 bra 	$L__BB13_15;
$L__BB13_18:
	add.u64 	%rd63, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd91, [%rd1+16];
	ld.global.nc.u64 	%rd92, [%rd1+24];
	st.local.u64 	[%rd2], %rd91;
	st.local.u64 	[%rd2+8], %rd92;
	bar.sync 	0;
	{ // callseq 52, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd63;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd64;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 52
	bar.sync 	0;
	mov.u64 	%rd140, 0;
	mov.u64 	%rd138, %rd140;
	@%p5 bra 	$L__BB13_23;
	setp.ne.s64 	%p15, %rd128, 0;
	@%p15 bra 	$L__BB13_21;
	bra.uni 	$L__BB13_20;
$L__BB13_21:
	div.u64 	%rd137, %rd10, %rd4;
	bra.uni 	$L__BB13_22;
$L__BB13_20:
	cvt.u32.u64 	%r27, %rd4;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	cvt.u64.u32 	%rd137, %r29;
$L__BB13_22:
	add.s64 	%rd138, %rd137, 1;
$L__BB13_23:
	@%p5 bra 	$L__BB13_28;
	setp.ne.s64 	%p17, %rd128, 0;
	@%p17 bra 	$L__BB13_26;
	bra.uni 	$L__BB13_25;
$L__BB13_26:
	div.u64 	%rd139, %rd10, %rd4;
	bra.uni 	$L__BB13_27;
$L__BB13_25:
	cvt.u32.u64 	%r31, %rd4;
	cvt.u32.u64 	%r32, %rd10;
	div.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd139, %r33;
$L__BB13_27:
	add.s64 	%rd140, %rd139, 1;
$L__BB13_28:
	min.u64 	%rd47, %rd138, %rd140;
	setp.eq.s64 	%p18, %rd47, 0;
	@%p18 bra 	$L__BB13_35;
	add.s64 	%rd48, %rd7, %rd124;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 384;
	shl.b16 	%rs3, %rs1, 8;
	shr.u16 	%rs4, %rs2, 8;
	or.b16  	%rs5, %rs3, %rs4;
	shl.b16 	%rs6, %rs5, 4;
	shl.b16 	%rs7, %rs2, 4;
	and.b16  	%rs8, %rs7, 3840;
	or.b16  	%rs9, %rs8, %rs6;
	and.b16  	%rs10, %rs9, 13107;
	shl.b16 	%rs11, %rs10, 2;
	shr.u16 	%rs12, %rs9, 2;
	and.b16  	%rs13, %rs12, 13107;
	or.b16  	%rs14, %rs13, %rs11;
	and.b16  	%rs15, %rs14, 21845;
	shl.b16 	%rs16, %rs15, 1;
	shr.u16 	%rs17, %rs14, 1;
	and.b16  	%rs18, %rs17, 21845;
	or.b16  	%rs19, %rs18, %rs16;
	shr.u16 	%rs20, %rs19, 5;
	cvt.u32.u16 	%r38, %rs20;
	add.s32 	%r37, %r2, %r38;
	add.s32 	%r36, %r37, -4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd48], %r34;
	st.u32 	[%rd48+4], %r35;
	setp.eq.s64 	%p19, %rd47, 1;
	@%p19 bra 	$L__BB13_35;
	add.s64 	%rd8, %rd7, 2048;
	add.s64 	%rd145, %rd3, 1;
	add.s64 	%rd144, %rd48, 8;
	add.s64 	%rd51, %rd47, -1;
	and.b64  	%rd52, %rd51, 1;
	setp.eq.s64 	%p20, %rd47, 2;
	@%p20 bra 	$L__BB13_33;
	and.b64  	%rd141, %rd51, -2;
$L__BB13_32:
	sub.s64 	%rd99, %rd8, %rd144;
	shr.u64 	%rd100, %rd99, 3;
	setp.gt.u64 	%p21, %rd100, %rd9;
	add.s64 	%rd103, %rd144, %rd125;
	add.s64 	%rd104, %rd103, -8;
	selp.b64 	%rd105, %rd103, %rd8, %p21;
	selp.b64 	%rd106, %rd104, 0, %p21;
	add.s64 	%rd107, %rd145, %rd9;
	setp.lt.u64 	%p22, %rd107, %rd145;
	setp.gt.u64 	%p23, %rd107, 255;
	add.s64 	%rd108, %rd107, 1;
	selp.b64 	%rd109, 256, %rd108, %p23;
	selp.b64 	%rd110, 256, %rd109, %p22;
	cvt.u16.u64 	%rs21, %rd107;
	xor.b16  	%rs22, %rs21, 128;
	shl.b16 	%rs23, %rs21, 12;
	shl.b16 	%rs24, %rs22, 4;
	and.b16  	%rs25, %rs24, 3840;
	or.b16  	%rs26, %rs23, %rs25;
	shr.u16 	%rs27, %rs26, 2;
	and.b16  	%rs28, %rs27, 13056;
	or.b16  	%rs29, %rs26, 16;
	and.b16  	%rs30, %rs29, 13072;
	shl.b16 	%rs31, %rs30, 2;
	or.b16  	%rs32, %rs28, %rs31;
	and.b16  	%rs33, %rs32, 21824;
	shl.b16 	%rs34, %rs33, 1;
	shr.u16 	%rs35, %rs32, 1;
	and.b16  	%rs36, %rs35, 21760;
	or.b16  	%rs37, %rs36, %rs34;
	shr.u16 	%rs38, %rs37, 5;
	cvt.u32.u16 	%r47, %rs38;
	add.s32 	%r42, %r2, %r47;
	add.s32 	%r41, %r42, -4;
	// begin inline asm
	ld.shared.f32 %r39, [%r41];
ld.shared.f32 %r40, [%r42];
	// end inline asm
	st.u32 	[%rd106], %r39;
	st.u32 	[%rd106+4], %r40;
	add.s64 	%rd144, %rd105, %rd125;
	add.s64 	%rd111, %rd110, %rd9;
	setp.lt.u64 	%p24, %rd111, %rd110;
	setp.gt.u64 	%p25, %rd111, 255;
	add.s64 	%rd113, %rd111, 1;
	selp.b64 	%rd114, 256, %rd113, %p25;
	selp.b64 	%rd145, 256, %rd114, %p24;
	cvt.u16.u64 	%rs39, %rd111;
	xor.b16  	%rs40, %rs39, 128;
	shl.b16 	%rs41, %rs39, 12;
	shl.b16 	%rs42, %rs40, 4;
	and.b16  	%rs43, %rs42, 3840;
	or.b16  	%rs44, %rs41, %rs43;
	shr.u16 	%rs45, %rs44, 2;
	and.b16  	%rs46, %rs45, 13056;
	or.b16  	%rs47, %rs44, 16;
	and.b16  	%rs48, %rs47, 13072;
	shl.b16 	%rs49, %rs48, 2;
	or.b16  	%rs50, %rs46, %rs49;
	and.b16  	%rs51, %rs50, 21824;
	shl.b16 	%rs52, %rs51, 1;
	shr.u16 	%rs53, %rs50, 1;
	and.b16  	%rs54, %rs53, 21760;
	or.b16  	%rs55, %rs54, %rs52;
	shr.u16 	%rs56, %rs55, 5;
	cvt.u32.u16 	%r48, %rs56;
	add.s32 	%r46, %r2, %r48;
	add.s32 	%r45, %r46, -4;
	// begin inline asm
	ld.shared.f32 %r43, [%r45];
ld.shared.f32 %r44, [%r46];
	// end inline asm
	st.u32 	[%rd144+-8], %r43;
	st.u32 	[%rd144+-4], %r44;
	add.s64 	%rd141, %rd141, -2;
	setp.ne.s64 	%p26, %rd141, 0;
	@%p26 bra 	$L__BB13_32;
$L__BB13_33:
	setp.eq.s64 	%p27, %rd52, 0;
	@%p27 bra 	$L__BB13_35;
	sub.s64 	%rd115, %rd8, %rd144;
	shr.u64 	%rd116, %rd115, 3;
	setp.gt.u64 	%p28, %rd116, %rd9;
	add.s64 	%rd118, %rd144, %rd125;
	add.s64 	%rd119, %rd118, -8;
	selp.b64 	%rd120, %rd119, 0, %p28;
	add.s64 	%rd121, %rd145, %rd9;
	setp.lt.u64 	%p29, %rd121, %rd145;
	cvt.u16.u64 	%rs57, %rd121;
	xor.b16  	%rs58, %rs57, 128;
	selp.b16 	%rs59, 384, %rs58, %p29;
	shl.b16 	%rs60, %rs59, 12;
	shl.b16 	%rs61, %rs59, 4;
	and.b16  	%rs62, %rs61, 3840;
	or.b16  	%rs63, %rs60, %rs62;
	shr.u16 	%rs64, %rs63, 2;
	and.b16  	%rs65, %rs64, 13056;
	or.b16  	%rs66, %rs63, 16;
	and.b16  	%rs67, %rs66, 13072;
	shl.b16 	%rs68, %rs67, 2;
	or.b16  	%rs69, %rs65, %rs68;
	and.b16  	%rs70, %rs69, 21824;
	shl.b16 	%rs71, %rs70, 1;
	shr.u16 	%rs72, %rs69, 1;
	and.b16  	%rs73, %rs72, 21760;
	or.b16  	%rs74, %rs73, %rs71;
	shr.u16 	%rs75, %rs74, 5;
	cvt.u32.u16 	%r53, %rs75;
	add.s32 	%r52, %r2, %r53;
	add.s32 	%r51, %r52, -4;
	// begin inline asm
	ld.shared.f32 %r49, [%r51];
ld.shared.f32 %r50, [%r52];
	// end inline asm
	st.u32 	[%rd120], %r49;
	st.u32 	[%rd120+4], %r50;
$L__BB13_35:
	ret;
$L__BB13_16:
	mov.u64 	%rd85, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd86, %rd85;
	mov.u64 	%rd87, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd88, %rd87;
	{ // callseq 51, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd86;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd88;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 51
$L__BB13_1:
	mov.u64 	%rd122, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_7;
	cvta.global.u64 	%rd123, %rd122;
	{ // callseq 53, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd123;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 53

}
	// .globl	fft_forward_shifted_512_kernel
.visible .entry fft_forward_shifted_512_kernel(
	.param .u64 fft_forward_shifted_512_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot14[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<83>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<146>;

	mov.u64 	%SPL, __local_depot14;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd62, [fft_forward_shifted_512_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd62;
	add.u64 	%rd64, %SP, 16;
	add.u64 	%rd65, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[4096];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd65], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd66, [%rd1+8];
	and.b64  	%rd67, %rd66, -512;
	mul.wide.u32 	%rd68, %r5, 512;
	setp.lt.u64 	%p2, %rd68, %rd67;
	sub.s64 	%rd70, %rd66, %rd68;
	setp.gt.u64 	%p3, %rd70, 511;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB14_2;
	bra.uni 	$L__BB14_1;
$L__BB14_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd72, %rd3, 512;
	setp.gt.u32 	%p5, %r6, 511;
	not.b64 	%rd73, %rd3;
	add.s64 	%rd10, %rd73, %rd72;
	mov.u64 	%rd132, 0;
	and.b64  	%rd128, %rd10, -4294967296;
	mov.u64 	%rd130, %rd132;
	@%p5 bra 	$L__BB14_7;
	setp.ne.s64 	%p6, %rd128, 0;
	@%p6 bra 	$L__BB14_5;
	bra.uni 	$L__BB14_4;
$L__BB14_5:
	div.u64 	%rd129, %rd10, %rd4;
	bra.uni 	$L__BB14_6;
$L__BB14_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd10;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd129, %r9;
$L__BB14_6:
	add.s64 	%rd130, %rd129, 1;
$L__BB14_7:
	shl.b64 	%rd69, %rd68, 3;
	@%p5 bra 	$L__BB14_12;
	setp.ne.s64 	%p8, %rd128, 0;
	@%p8 bra 	$L__BB14_10;
	bra.uni 	$L__BB14_9;
$L__BB14_10:
	div.u64 	%rd131, %rd10, %rd4;
	bra.uni 	$L__BB14_11;
$L__BB14_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd10;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd131, %r13;
$L__BB14_11:
	add.s64 	%rd132, %rd131, 1;
$L__BB14_12:
	add.s64 	%rd7, %rd6, %rd69;
	add.s64 	%rd9, %rd4, -1;
	min.u64 	%rd21, %rd130, %rd132;
	setp.eq.s64 	%p9, %rd21, 0;
	shl.b64 	%rd124, %rd3, 3;
	shl.b64 	%rd125, %rd4, 3;
	@%p9 bra 	$L__BB14_18;
	add.s64 	%rd78, %rd7, %rd124;
	shl.b32 	%r19, %r6, 3;
	add.s32 	%r14, %r2, %r19;
	add.s32 	%r15, %r14, 4;
	ld.u32 	%r16, [%rd78];
	ld.u32 	%r17, [%rd78+4];
	// begin inline asm
	st.shared.f32 [%r14], %r16;
st.shared.f32 [%r15], %r17;
	// end inline asm
	setp.eq.s64 	%p10, %rd21, 1;
	@%p10 bra 	$L__BB14_18;
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd136, %rd3, 1;
	add.s64 	%rd135, %rd21, -1;
	shl.b64 	%rd79, %rd5, 12;
	add.s64 	%rd80, %rd79, %rd125;
	add.s64 	%rd82, %rd80, %rd124;
	add.s64 	%rd134, %rd6, %rd82;
	mov.u64 	%rd83, 4088;
	sub.s64 	%rd133, %rd83, %rd124;
$L__BB14_15:
	add.s64 	%rd32, %rd136, %rd9;
	setp.lt.u64 	%p12, %rd32, 512;
	@%p12 bra 	$L__BB14_17;
	bra.uni 	$L__BB14_16;
$L__BB14_17:
	shr.u64 	%rd84, %rd133, 3;
	setp.gt.u64 	%p11, %rd84, %rd9;
	selp.b64 	%rd31, %rd134, 0, %p11;
	setp.lt.u64 	%p1, %rd32, %rd136;
	add.s64 	%rd89, %rd136, %rd4;
	selp.b64 	%rd136, 512, %rd89, %p1;
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r20, %r25, %r2;
	add.s32 	%r21, %r20, 4;
	ld.u32 	%r22, [%rd31];
	ld.u32 	%r23, [%rd31+4];
	// begin inline asm
	st.shared.f32 [%r20], %r22;
st.shared.f32 [%r21], %r23;
	// end inline asm
	add.s64 	%rd135, %rd135, -1;
	add.s64 	%rd134, %rd134, %rd125;
	sub.s64 	%rd133, %rd133, %rd125;
	setp.ne.s64 	%p13, %rd135, 0;
	@%p13 bra 	$L__BB14_15;
$L__BB14_18:
	add.u64 	%rd63, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd91, [%rd1+16];
	ld.global.nc.u64 	%rd92, [%rd1+24];
	st.local.u64 	[%rd2], %rd91;
	st.local.u64 	[%rd2+8], %rd92;
	bar.sync 	0;
	{ // callseq 55, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd63;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd64;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 55
	bar.sync 	0;
	mov.u64 	%rd140, 0;
	mov.u64 	%rd138, %rd140;
	@%p5 bra 	$L__BB14_23;
	setp.ne.s64 	%p15, %rd128, 0;
	@%p15 bra 	$L__BB14_21;
	bra.uni 	$L__BB14_20;
$L__BB14_21:
	div.u64 	%rd137, %rd10, %rd4;
	bra.uni 	$L__BB14_22;
$L__BB14_20:
	cvt.u32.u64 	%r27, %rd4;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	cvt.u64.u32 	%rd137, %r29;
$L__BB14_22:
	add.s64 	%rd138, %rd137, 1;
$L__BB14_23:
	@%p5 bra 	$L__BB14_28;
	setp.ne.s64 	%p17, %rd128, 0;
	@%p17 bra 	$L__BB14_26;
	bra.uni 	$L__BB14_25;
$L__BB14_26:
	div.u64 	%rd139, %rd10, %rd4;
	bra.uni 	$L__BB14_27;
$L__BB14_25:
	cvt.u32.u64 	%r31, %rd4;
	cvt.u32.u64 	%r32, %rd10;
	div.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd139, %r33;
$L__BB14_27:
	add.s64 	%rd140, %rd139, 1;
$L__BB14_28:
	min.u64 	%rd47, %rd138, %rd140;
	setp.eq.s64 	%p18, %rd47, 0;
	@%p18 bra 	$L__BB14_35;
	add.s64 	%rd48, %rd7, %rd124;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 768;
	shl.b16 	%rs3, %rs1, 8;
	shr.u16 	%rs4, %rs2, 8;
	or.b16  	%rs5, %rs3, %rs4;
	shl.b16 	%rs6, %rs5, 4;
	shl.b16 	%rs7, %rs1, 4;
	and.b16  	%rs8, %rs7, 3840;
	or.b16  	%rs9, %rs8, %rs6;
	and.b16  	%rs10, %rs9, 13107;
	shl.b16 	%rs11, %rs10, 2;
	shr.u16 	%rs12, %rs9, 2;
	and.b16  	%rs13, %rs12, 13107;
	or.b16  	%rs14, %rs13, %rs11;
	and.b16  	%rs15, %rs14, 21845;
	shl.b16 	%rs16, %rs15, 1;
	shr.u16 	%rs17, %rs14, 1;
	and.b16  	%rs18, %rs17, 21845;
	or.b16  	%rs19, %rs18, %rs16;
	shr.u16 	%rs20, %rs19, 4;
	cvt.u32.u16 	%r38, %rs20;
	add.s32 	%r37, %r2, %r38;
	add.s32 	%r36, %r37, -4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd48], %r34;
	st.u32 	[%rd48+4], %r35;
	setp.eq.s64 	%p19, %rd47, 1;
	@%p19 bra 	$L__BB14_35;
	add.s64 	%rd8, %rd7, 4096;
	add.s64 	%rd145, %rd3, 1;
	add.s64 	%rd144, %rd48, 8;
	add.s64 	%rd51, %rd47, -1;
	and.b64  	%rd52, %rd51, 1;
	setp.eq.s64 	%p20, %rd47, 2;
	@%p20 bra 	$L__BB14_33;
	and.b64  	%rd141, %rd51, -2;
$L__BB14_32:
	sub.s64 	%rd99, %rd8, %rd144;
	shr.u64 	%rd100, %rd99, 3;
	setp.gt.u64 	%p21, %rd100, %rd9;
	add.s64 	%rd103, %rd144, %rd125;
	add.s64 	%rd104, %rd103, -8;
	selp.b64 	%rd105, %rd103, %rd8, %p21;
	selp.b64 	%rd106, %rd104, 0, %p21;
	add.s64 	%rd107, %rd145, %rd9;
	setp.lt.u64 	%p22, %rd107, %rd145;
	setp.gt.u64 	%p23, %rd107, 511;
	add.s64 	%rd108, %rd107, 1;
	selp.b64 	%rd109, 512, %rd108, %p23;
	selp.b64 	%rd110, 512, %rd109, %p22;
	cvt.u16.u64 	%rs21, %rd107;
	and.b16  	%rs22, %rs21, 256;
	xor.b16  	%rs23, %rs22, 768;
	shl.b16 	%rs24, %rs21, 12;
	shr.u16 	%rs25, %rs23, 4;
	or.b16  	%rs26, %rs24, %rs25;
	shl.b16 	%rs27, %rs21, 4;
	and.b16  	%rs28, %rs27, 3840;
	or.b16  	%rs29, %rs28, %rs26;
	and.b16  	%rs30, %rs29, 13107;
	shl.b16 	%rs31, %rs30, 2;
	shr.u16 	%rs32, %rs29, 2;
	and.b16  	%rs33, %rs32, 13107;
	or.b16  	%rs34, %rs33, %rs31;
	and.b16  	%rs35, %rs34, 21845;
	shl.b16 	%rs36, %rs35, 1;
	shr.u16 	%rs37, %rs34, 1;
	and.b16  	%rs38, %rs37, 21845;
	or.b16  	%rs39, %rs38, %rs36;
	shr.u16 	%rs40, %rs39, 4;
	cvt.u32.u16 	%r47, %rs40;
	add.s32 	%r42, %r2, %r47;
	add.s32 	%r41, %r42, -4;
	// begin inline asm
	ld.shared.f32 %r39, [%r41];
ld.shared.f32 %r40, [%r42];
	// end inline asm
	st.u32 	[%rd106], %r39;
	st.u32 	[%rd106+4], %r40;
	add.s64 	%rd144, %rd105, %rd125;
	add.s64 	%rd111, %rd110, %rd9;
	setp.lt.u64 	%p24, %rd111, %rd110;
	setp.gt.u64 	%p25, %rd111, 511;
	add.s64 	%rd113, %rd111, 1;
	selp.b64 	%rd114, 512, %rd113, %p25;
	selp.b64 	%rd145, 512, %rd114, %p24;
	cvt.u16.u64 	%rs41, %rd111;
	and.b16  	%rs42, %rs41, 256;
	xor.b16  	%rs43, %rs42, 768;
	shl.b16 	%rs44, %rs41, 12;
	shr.u16 	%rs45, %rs43, 4;
	or.b16  	%rs46, %rs44, %rs45;
	shl.b16 	%rs47, %rs41, 4;
	and.b16  	%rs48, %rs47, 3840;
	or.b16  	%rs49, %rs48, %rs46;
	and.b16  	%rs50, %rs49, 13107;
	shl.b16 	%rs51, %rs50, 2;
	shr.u16 	%rs52, %rs49, 2;
	and.b16  	%rs53, %rs52, 13107;
	or.b16  	%rs54, %rs53, %rs51;
	and.b16  	%rs55, %rs54, 21845;
	shl.b16 	%rs56, %rs55, 1;
	shr.u16 	%rs57, %rs54, 1;
	and.b16  	%rs58, %rs57, 21845;
	or.b16  	%rs59, %rs58, %rs56;
	shr.u16 	%rs60, %rs59, 4;
	cvt.u32.u16 	%r48, %rs60;
	add.s32 	%r46, %r2, %r48;
	add.s32 	%r45, %r46, -4;
	// begin inline asm
	ld.shared.f32 %r43, [%r45];
ld.shared.f32 %r44, [%r46];
	// end inline asm
	st.u32 	[%rd144+-8], %r43;
	st.u32 	[%rd144+-4], %r44;
	add.s64 	%rd141, %rd141, -2;
	setp.ne.s64 	%p26, %rd141, 0;
	@%p26 bra 	$L__BB14_32;
$L__BB14_33:
	setp.eq.s64 	%p27, %rd52, 0;
	@%p27 bra 	$L__BB14_35;
	sub.s64 	%rd115, %rd8, %rd144;
	shr.u64 	%rd116, %rd115, 3;
	setp.gt.u64 	%p28, %rd116, %rd9;
	add.s64 	%rd118, %rd144, %rd125;
	add.s64 	%rd119, %rd118, -8;
	selp.b64 	%rd120, %rd119, 0, %p28;
	add.s64 	%rd121, %rd145, %rd9;
	setp.lt.u64 	%p29, %rd121, %rd145;
	cvt.u16.u64 	%rs61, %rd121;
	and.b16  	%rs62, %rs61, 511;
	xor.b16  	%rs63, %rs62, 768;
	selp.b16 	%rs64, 768, %rs63, %p29;
	shr.u16 	%rs65, %rs64, 8;
	shl.b16 	%rs66, %rs64, 8;
	or.b16  	%rs67, %rs66, %rs65;
	shl.b16 	%rs68, %rs67, 4;
	shl.b16 	%rs69, %rs64, 4;
	and.b16  	%rs70, %rs69, 3840;
	or.b16  	%rs71, %rs70, %rs68;
	and.b16  	%rs72, %rs71, 13107;
	shl.b16 	%rs73, %rs72, 2;
	shr.u16 	%rs74, %rs71, 2;
	and.b16  	%rs75, %rs74, 13107;
	or.b16  	%rs76, %rs75, %rs73;
	and.b16  	%rs77, %rs76, 21845;
	shl.b16 	%rs78, %rs77, 1;
	shr.u16 	%rs79, %rs76, 1;
	and.b16  	%rs80, %rs79, 21845;
	or.b16  	%rs81, %rs80, %rs78;
	shr.u16 	%rs82, %rs81, 4;
	cvt.u32.u16 	%r53, %rs82;
	add.s32 	%r52, %r2, %r53;
	add.s32 	%r51, %r52, -4;
	// begin inline asm
	ld.shared.f32 %r49, [%r51];
ld.shared.f32 %r50, [%r52];
	// end inline asm
	st.u32 	[%rd120], %r49;
	st.u32 	[%rd120+4], %r50;
$L__BB14_35:
	ret;
$L__BB14_16:
	mov.u64 	%rd85, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd86, %rd85;
	mov.u64 	%rd87, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd88, %rd87;
	{ // callseq 54, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd86;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd88;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 54
$L__BB14_1:
	mov.u64 	%rd122, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_7;
	cvta.global.u64 	%rd123, %rd122;
	{ // callseq 56, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd123;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 56

}
	// .globl	fft_forward_shifted_1024_kernel
.visible .entry fft_forward_shifted_1024_kernel(
	.param .u64 fft_forward_shifted_1024_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot15[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<85>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<98>;

	mov.u64 	%SPL, __local_depot15;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd39, [fft_forward_shifted_1024_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd39;
	add.u64 	%rd41, %SP, 16;
	add.u64 	%rd42, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[8192];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd42], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd43, [%rd1+8];
	and.b64  	%rd44, %rd43, -1024;
	mul.wide.u32 	%rd6, %r5, 1024;
	setp.lt.u64 	%p2, %rd6, %rd44;
	sub.s64 	%rd45, %rd43, %rd6;
	setp.gt.u64 	%p3, %rd45, 1023;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB15_2;
	bra.uni 	$L__BB15_1;
$L__BB15_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r10, %rd3;
	ld.global.nc.u64 	%rd7, [%rd1];
	shl.b64 	%rd46, %rd6, 3;
	add.s64 	%rd47, %rd7, %rd46;
	add.s64 	%rd9, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 1023;
	cvt.u16.u64 	%rs3, %rd4;
	shl.b64 	%rd48, %rd3, 3;
	add.s64 	%rd10, %rd47, %rd48;
	shl.b32 	%r11, %r10, 3;
	add.s32 	%r6, %r2, %r11;
	add.s32 	%r7, %r6, 4;
	ld.u32 	%r8, [%rd10];
	ld.u32 	%r9, [%rd10+4];
	// begin inline asm
	st.shared.f32 [%r6], %r8;
st.shared.f32 [%r7], %r9;
	// end inline asm
	setp.lt.u16 	%p5, %rs2, %rs3;
	add.s64 	%rd97, %rd3, 1;
	shl.b64 	%rd88, %rd4, 3;
	@%p5 bra 	$L__BB15_7;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u16 	%r12, %rs2;
	cvt.u32.u64 	%r13, %rd4;
	div.u32 	%r14, %r12, %r13;
	cvt.u64.u32 	%rd91, %r14;
	shl.b64 	%rd49, %rd5, 13;
	add.s64 	%rd50, %rd49, %rd88;
	add.s64 	%rd52, %rd50, %rd48;
	add.s64 	%rd90, %rd7, %rd52;
	xor.b64  	%rd89, %rd48, 8184;
	mov.u64 	%rd92, %rd97;
$L__BB15_4:
	add.s64 	%rd21, %rd92, %rd9;
	setp.lt.u64 	%p7, %rd21, 1024;
	@%p7 bra 	$L__BB15_6;
	bra.uni 	$L__BB15_5;
$L__BB15_6:
	shr.u64 	%rd53, %rd89, 3;
	setp.gt.u64 	%p6, %rd53, %rd9;
	selp.b64 	%rd20, %rd90, 0, %p6;
	setp.lt.u64 	%p1, %rd21, %rd92;
	add.s64 	%rd58, %rd92, %rd4;
	selp.b64 	%rd92, 1024, %rd58, %p1;
	cvt.u32.u64 	%r19, %rd21;
	shl.b32 	%r20, %r19, 3;
	add.s32 	%r15, %r20, %r2;
	add.s32 	%r16, %r15, 4;
	ld.u32 	%r17, [%rd20];
	ld.u32 	%r18, [%rd20+4];
	// begin inline asm
	st.shared.f32 [%r15], %r17;
st.shared.f32 [%r16], %r18;
	// end inline asm
	add.s64 	%rd91, %rd91, -1;
	add.s64 	%rd90, %rd90, %rd88;
	sub.s64 	%rd89, %rd89, %rd88;
	setp.ne.s64 	%p8, %rd91, 0;
	@%p8 bra 	$L__BB15_4;
$L__BB15_7:
	add.u64 	%rd40, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd59, [%rd1+16];
	ld.global.nc.u64 	%rd60, [%rd1+24];
	st.local.u64 	[%rd2], %rd59;
	st.local.u64 	[%rd2+8], %rd60;
	bar.sync 	0;
	{ // callseq 58, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd40;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd41;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 58
	bar.sync 	0;
	xor.b64  	%rd26, %rd3, 1023;
	xor.b16  	%rs4, %rs1, 1536;
	shl.b16 	%rs5, %rs1, 12;
	shr.u16 	%rs6, %rs4, 4;
	and.b16  	%rs7, %rs6, 112;
	or.b16  	%rs8, %rs5, %rs7;
	shl.b16 	%rs9, %rs1, 4;
	and.b16  	%rs10, %rs9, 3840;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 13107;
	shl.b16 	%rs13, %rs12, 2;
	shr.u16 	%rs14, %rs11, 2;
	and.b16  	%rs15, %rs14, 13107;
	or.b16  	%rs16, %rs15, %rs13;
	and.b16  	%rs17, %rs16, 21845;
	shl.b16 	%rs18, %rs17, 1;
	shr.u16 	%rs19, %rs16, 1;
	and.b16  	%rs20, %rs19, 21845;
	or.b16  	%rs21, %rs20, %rs18;
	shr.u16 	%rs22, %rs21, 3;
	cvt.u32.u16 	%r25, %rs22;
	add.s32 	%r24, %r2, %r25;
	add.s32 	%r23, %r24, -4;
	// begin inline asm
	ld.shared.f32 %r21, [%r23];
ld.shared.f32 %r22, [%r24];
	// end inline asm
	st.u32 	[%rd10], %r21;
	st.u32 	[%rd10+4], %r22;
	setp.lt.u64 	%p9, %rd26, %rd4;
	@%p9 bra 	$L__BB15_13;
	add.s64 	%rd8, %rd47, 8192;
	cvt.u32.u64 	%r26, %rd26;
	cvt.u32.u64 	%r27, %rd4;
	div.u32 	%r28, %r26, %r27;
	cvt.u64.u32 	%rd27, %r28;
	add.s64 	%rd96, %rd10, 8;
	and.b64  	%rd29, %rd27, 1;
	setp.eq.s32 	%p10, %r28, 1;
	@%p10 bra 	$L__BB15_11;
	and.b64  	%rd93, %rd27, 1022;
$L__BB15_10:
	sub.s64 	%rd63, %rd8, %rd96;
	shr.u64 	%rd64, %rd63, 3;
	setp.gt.u64 	%p11, %rd64, %rd9;
	add.s64 	%rd67, %rd96, %rd88;
	add.s64 	%rd68, %rd67, -8;
	selp.b64 	%rd69, %rd67, %rd8, %p11;
	selp.b64 	%rd70, %rd68, 0, %p11;
	add.s64 	%rd71, %rd97, %rd9;
	setp.lt.u64 	%p12, %rd71, %rd97;
	setp.gt.u64 	%p13, %rd71, 1023;
	add.s64 	%rd72, %rd71, 1;
	selp.b64 	%rd73, 1024, %rd72, %p13;
	selp.b64 	%rd74, 1024, %rd73, %p12;
	cvt.u16.u64 	%rs23, %rd71;
	and.b16  	%rs24, %rs23, 768;
	xor.b16  	%rs25, %rs24, 1536;
	shl.b16 	%rs26, %rs23, 12;
	shr.u16 	%rs27, %rs25, 4;
	or.b16  	%rs28, %rs26, %rs27;
	shl.b16 	%rs29, %rs23, 4;
	and.b16  	%rs30, %rs29, 3840;
	or.b16  	%rs31, %rs30, %rs28;
	and.b16  	%rs32, %rs31, 13107;
	shl.b16 	%rs33, %rs32, 2;
	shr.u16 	%rs34, %rs31, 2;
	and.b16  	%rs35, %rs34, 13107;
	or.b16  	%rs36, %rs35, %rs33;
	and.b16  	%rs37, %rs36, 21845;
	shl.b16 	%rs38, %rs37, 1;
	shr.u16 	%rs39, %rs36, 1;
	and.b16  	%rs40, %rs39, 21845;
	or.b16  	%rs41, %rs40, %rs38;
	shr.u16 	%rs42, %rs41, 3;
	cvt.u32.u16 	%r37, %rs42;
	add.s32 	%r32, %r2, %r37;
	add.s32 	%r31, %r32, -4;
	// begin inline asm
	ld.shared.f32 %r29, [%r31];
ld.shared.f32 %r30, [%r32];
	// end inline asm
	st.u32 	[%rd70], %r29;
	st.u32 	[%rd70+4], %r30;
	add.s64 	%rd96, %rd69, %rd88;
	add.s64 	%rd75, %rd74, %rd9;
	setp.lt.u64 	%p14, %rd75, %rd74;
	setp.gt.u64 	%p15, %rd75, 1023;
	add.s64 	%rd77, %rd75, 1;
	selp.b64 	%rd78, 1024, %rd77, %p15;
	selp.b64 	%rd97, 1024, %rd78, %p14;
	cvt.u16.u64 	%rs43, %rd75;
	and.b16  	%rs44, %rs43, 768;
	xor.b16  	%rs45, %rs44, 1536;
	shl.b16 	%rs46, %rs43, 12;
	shr.u16 	%rs47, %rs45, 4;
	or.b16  	%rs48, %rs46, %rs47;
	shl.b16 	%rs49, %rs43, 4;
	and.b16  	%rs50, %rs49, 3840;
	or.b16  	%rs51, %rs50, %rs48;
	and.b16  	%rs52, %rs51, 13107;
	shl.b16 	%rs53, %rs52, 2;
	shr.u16 	%rs54, %rs51, 2;
	and.b16  	%rs55, %rs54, 13107;
	or.b16  	%rs56, %rs55, %rs53;
	and.b16  	%rs57, %rs56, 21845;
	shl.b16 	%rs58, %rs57, 1;
	shr.u16 	%rs59, %rs56, 1;
	and.b16  	%rs60, %rs59, 21845;
	or.b16  	%rs61, %rs60, %rs58;
	shr.u16 	%rs62, %rs61, 3;
	cvt.u32.u16 	%r38, %rs62;
	add.s32 	%r36, %r2, %r38;
	add.s32 	%r35, %r36, -4;
	// begin inline asm
	ld.shared.f32 %r33, [%r35];
ld.shared.f32 %r34, [%r36];
	// end inline asm
	st.u32 	[%rd96+-8], %r33;
	st.u32 	[%rd96+-4], %r34;
	add.s64 	%rd93, %rd93, -2;
	setp.ne.s64 	%p16, %rd93, 0;
	@%p16 bra 	$L__BB15_10;
$L__BB15_11:
	setp.eq.s64 	%p17, %rd29, 0;
	@%p17 bra 	$L__BB15_13;
	sub.s64 	%rd79, %rd8, %rd96;
	shr.u64 	%rd80, %rd79, 3;
	setp.gt.u64 	%p18, %rd80, %rd9;
	add.s64 	%rd82, %rd96, %rd88;
	add.s64 	%rd83, %rd82, -8;
	selp.b64 	%rd84, %rd83, 0, %p18;
	add.s64 	%rd85, %rd97, %rd9;
	setp.lt.u64 	%p19, %rd85, %rd97;
	cvt.u16.u64 	%rs63, %rd85;
	and.b16  	%rs64, %rs63, 1023;
	xor.b16  	%rs65, %rs64, 1536;
	selp.b16 	%rs66, 1536, %rs65, %p19;
	shr.u16 	%rs67, %rs66, 8;
	shl.b16 	%rs68, %rs66, 8;
	or.b16  	%rs69, %rs68, %rs67;
	shl.b16 	%rs70, %rs69, 4;
	shl.b16 	%rs71, %rs66, 4;
	and.b16  	%rs72, %rs71, 3840;
	or.b16  	%rs73, %rs72, %rs70;
	and.b16  	%rs74, %rs73, 13107;
	shl.b16 	%rs75, %rs74, 2;
	shr.u16 	%rs76, %rs73, 2;
	and.b16  	%rs77, %rs76, 13107;
	or.b16  	%rs78, %rs77, %rs75;
	and.b16  	%rs79, %rs78, 21845;
	shl.b16 	%rs80, %rs79, 1;
	shr.u16 	%rs81, %rs78, 1;
	and.b16  	%rs82, %rs81, 21845;
	or.b16  	%rs83, %rs82, %rs80;
	shr.u16 	%rs84, %rs83, 3;
	cvt.u32.u16 	%r43, %rs84;
	add.s32 	%r42, %r2, %r43;
	add.s32 	%r41, %r42, -4;
	// begin inline asm
	ld.shared.f32 %r39, [%r41];
ld.shared.f32 %r40, [%r42];
	// end inline asm
	st.u32 	[%rd84], %r39;
	st.u32 	[%rd84+4], %r40;
$L__BB15_13:
	ret;
$L__BB15_5:
	mov.u64 	%rd54, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd55, %rd54;
	mov.u64 	%rd56, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd57, %rd56;
	{ // callseq 57, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd55;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd57;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 57
$L__BB15_1:
	mov.u64 	%rd86, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_7;
	cvta.global.u64 	%rd87, %rd86;
	{ // callseq 59, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd87;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 59

}
	// .globl	fft_forward_shifted_2048_kernel
.visible .entry fft_forward_shifted_2048_kernel(
	.param .u64 fft_forward_shifted_2048_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot16[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<84>;
	.reg .b32 	%r<45>;
	.reg .b64 	%rd<95>;

	mov.u64 	%SPL, __local_depot16;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd37, [fft_forward_shifted_2048_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd37;
	add.u64 	%rd39, %SP, 16;
	add.u64 	%rd40, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[16384];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd40], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd41, [%rd1+8];
	and.b64  	%rd42, %rd41, -2048;
	mul.wide.u32 	%rd6, %r5, 2048;
	setp.lt.u64 	%p2, %rd6, %rd42;
	sub.s64 	%rd43, %rd41, %rd6;
	setp.gt.u64 	%p3, %rd43, 2047;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB16_2;
	bra.uni 	$L__BB16_1;
$L__BB16_2:
	add.u64 	%rd38, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r10, %rd3;
	ld.global.nc.u64 	%rd44, [%rd1];
	shl.b64 	%rd45, %rd6, 3;
	add.s64 	%rd46, %rd44, %rd45;
	add.s64 	%rd7, %rd46, 16384;
	add.s64 	%rd8, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 2047;
	shl.b64 	%rd47, %rd3, 3;
	add.s64 	%rd9, %rd46, %rd47;
	shl.b32 	%r11, %r10, 3;
	add.s32 	%r6, %r2, %r11;
	add.s32 	%r7, %r6, 4;
	ld.u32 	%r8, [%rd9];
	ld.u32 	%r9, [%rd9+4];
	// begin inline asm
	st.shared.f32 [%r6], %r8;
st.shared.f32 [%r7], %r9;
	// end inline asm
	add.s64 	%rd94, %rd3, 1;
	add.s64 	%rd93, %rd9, 8;
	cvt.u32.u16 	%r12, %rs2;
	cvt.u32.u64 	%r13, %rd4;
	div.u32 	%r14, %r12, %r13;
	cvt.u64.u32 	%rd88, %r14;
	shl.b64 	%rd48, %rd5, 14;
	shl.b64 	%rd13, %rd4, 3;
	add.s64 	%rd49, %rd48, %rd13;
	add.s64 	%rd50, %rd49, %rd47;
	add.s64 	%rd87, %rd44, %rd50;
	xor.b64  	%rd86, %rd47, 16376;
	mov.u64 	%rd89, %rd94;
$L__BB16_3:
	add.s64 	%rd21, %rd89, %rd8;
	setp.lt.u64 	%p6, %rd21, 2048;
	@%p6 bra 	$L__BB16_5;
	bra.uni 	$L__BB16_4;
$L__BB16_5:
	shr.u64 	%rd51, %rd86, 3;
	setp.gt.u64 	%p5, %rd51, %rd8;
	selp.b64 	%rd20, %rd87, 0, %p5;
	setp.lt.u64 	%p1, %rd21, %rd89;
	add.s64 	%rd56, %rd89, %rd4;
	selp.b64 	%rd89, 2048, %rd56, %p1;
	cvt.u32.u64 	%r19, %rd21;
	shl.b32 	%r20, %r19, 3;
	add.s32 	%r15, %r20, %r2;
	add.s32 	%r16, %r15, 4;
	ld.u32 	%r17, [%rd20];
	ld.u32 	%r18, [%rd20+4];
	// begin inline asm
	st.shared.f32 [%r15], %r17;
st.shared.f32 [%r16], %r18;
	// end inline asm
	add.s64 	%rd88, %rd88, -1;
	add.s64 	%rd87, %rd87, %rd13;
	sub.s64 	%rd86, %rd86, %rd13;
	setp.ne.s64 	%p7, %rd88, 0;
	@%p7 bra 	$L__BB16_3;
	ld.global.nc.u64 	%rd57, [%rd1+16];
	ld.global.nc.u64 	%rd58, [%rd1+24];
	st.local.u64 	[%rd2], %rd57;
	st.local.u64 	[%rd2+8], %rd58;
	bar.sync 	0;
	{ // callseq 61, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd38;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd39;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 61
	bar.sync 	0;
	xor.b32  	%r26, %r10, 2047;
	div.u32 	%r28, %r26, %r13;
	cvt.u64.u32 	%rd26, %r28;
	or.b16  	%rs3, %rs1, 3072;
	shl.b16 	%rs4, %rs1, 12;
	shr.u16 	%rs5, %rs3, 4;
	and.b16  	%rs6, %rs5, 240;
	or.b16  	%rs7, %rs4, %rs6;
	shl.b16 	%rs8, %rs1, 4;
	and.b16  	%rs9, %rs8, 3840;
	or.b16  	%rs10, %rs9, %rs7;
	and.b16  	%rs11, %rs10, 13107;
	shl.b16 	%rs12, %rs11, 2;
	shr.u16 	%rs13, %rs10, 2;
	and.b16  	%rs14, %rs13, 13107;
	or.b16  	%rs15, %rs14, %rs12;
	and.b16  	%rs16, %rs15, 21845;
	shl.b16 	%rs17, %rs16, 1;
	shr.u16 	%rs18, %rs15, 1;
	and.b16  	%rs19, %rs18, 21845;
	or.b16  	%rs20, %rs19, %rs17;
	shr.u16 	%rs21, %rs20, 2;
	cvt.u32.u16 	%r29, %rs21;
	add.s32 	%r24, %r2, %r29;
	add.s32 	%r23, %r24, -4;
	// begin inline asm
	ld.shared.f32 %r21, [%r23];
ld.shared.f32 %r22, [%r24];
	// end inline asm
	st.u32 	[%rd9], %r21;
	st.u32 	[%rd9+4], %r22;
	and.b64  	%rd27, %rd26, 1;
	setp.eq.s32 	%p8, %r28, 1;
	@%p8 bra 	$L__BB16_9;
	and.b64  	%rd90, %rd26, 2046;
$L__BB16_8:
	sub.s64 	%rd61, %rd7, %rd93;
	shr.u64 	%rd62, %rd61, 3;
	setp.gt.u64 	%p9, %rd62, %rd8;
	add.s64 	%rd65, %rd93, %rd13;
	add.s64 	%rd66, %rd65, -8;
	selp.b64 	%rd67, %rd65, %rd7, %p9;
	selp.b64 	%rd68, %rd66, 0, %p9;
	add.s64 	%rd69, %rd94, %rd8;
	setp.lt.u64 	%p10, %rd69, %rd94;
	setp.gt.u64 	%p11, %rd69, 2047;
	add.s64 	%rd70, %rd69, 1;
	selp.b64 	%rd71, 2048, %rd70, %p11;
	selp.b64 	%rd72, 2048, %rd71, %p10;
	cvt.u16.u64 	%rs22, %rd69;
	and.b16  	%rs23, %rs22, 1792;
	xor.b16  	%rs24, %rs23, 3072;
	shl.b16 	%rs25, %rs22, 12;
	shr.u16 	%rs26, %rs24, 4;
	or.b16  	%rs27, %rs25, %rs26;
	shl.b16 	%rs28, %rs22, 4;
	and.b16  	%rs29, %rs28, 3840;
	or.b16  	%rs30, %rs29, %rs27;
	and.b16  	%rs31, %rs30, 13107;
	shl.b16 	%rs32, %rs31, 2;
	shr.u16 	%rs33, %rs30, 2;
	and.b16  	%rs34, %rs33, 13107;
	or.b16  	%rs35, %rs34, %rs32;
	and.b16  	%rs36, %rs35, 21845;
	shl.b16 	%rs37, %rs36, 1;
	shr.u16 	%rs38, %rs35, 1;
	and.b16  	%rs39, %rs38, 21845;
	or.b16  	%rs40, %rs39, %rs37;
	shr.u16 	%rs41, %rs40, 2;
	cvt.u32.u16 	%r38, %rs41;
	add.s32 	%r33, %r2, %r38;
	add.s32 	%r32, %r33, -4;
	// begin inline asm
	ld.shared.f32 %r30, [%r32];
ld.shared.f32 %r31, [%r33];
	// end inline asm
	st.u32 	[%rd68], %r30;
	st.u32 	[%rd68+4], %r31;
	add.s64 	%rd93, %rd67, %rd13;
	add.s64 	%rd73, %rd72, %rd8;
	setp.lt.u64 	%p12, %rd73, %rd72;
	setp.gt.u64 	%p13, %rd73, 2047;
	add.s64 	%rd75, %rd73, 1;
	selp.b64 	%rd76, 2048, %rd75, %p13;
	selp.b64 	%rd94, 2048, %rd76, %p12;
	cvt.u16.u64 	%rs42, %rd73;
	and.b16  	%rs43, %rs42, 1792;
	xor.b16  	%rs44, %rs43, 3072;
	shl.b16 	%rs45, %rs42, 12;
	shr.u16 	%rs46, %rs44, 4;
	or.b16  	%rs47, %rs45, %rs46;
	shl.b16 	%rs48, %rs42, 4;
	and.b16  	%rs49, %rs48, 3840;
	or.b16  	%rs50, %rs49, %rs47;
	and.b16  	%rs51, %rs50, 13107;
	shl.b16 	%rs52, %rs51, 2;
	shr.u16 	%rs53, %rs50, 2;
	and.b16  	%rs54, %rs53, 13107;
	or.b16  	%rs55, %rs54, %rs52;
	and.b16  	%rs56, %rs55, 21845;
	shl.b16 	%rs57, %rs56, 1;
	shr.u16 	%rs58, %rs55, 1;
	and.b16  	%rs59, %rs58, 21845;
	or.b16  	%rs60, %rs59, %rs57;
	shr.u16 	%rs61, %rs60, 2;
	cvt.u32.u16 	%r39, %rs61;
	add.s32 	%r37, %r2, %r39;
	add.s32 	%r36, %r37, -4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd93+-8], %r34;
	st.u32 	[%rd93+-4], %r35;
	add.s64 	%rd90, %rd90, -2;
	setp.ne.s64 	%p14, %rd90, 0;
	@%p14 bra 	$L__BB16_8;
$L__BB16_9:
	setp.eq.s64 	%p15, %rd27, 0;
	@%p15 bra 	$L__BB16_11;
	sub.s64 	%rd77, %rd7, %rd93;
	shr.u64 	%rd78, %rd77, 3;
	setp.gt.u64 	%p16, %rd78, %rd8;
	add.s64 	%rd80, %rd93, %rd13;
	add.s64 	%rd81, %rd80, -8;
	selp.b64 	%rd82, %rd81, 0, %p16;
	add.s64 	%rd83, %rd94, %rd8;
	setp.lt.u64 	%p17, %rd83, %rd94;
	cvt.u16.u64 	%rs62, %rd83;
	and.b16  	%rs63, %rs62, 2047;
	xor.b16  	%rs64, %rs63, 3072;
	selp.b16 	%rs65, 3072, %rs64, %p17;
	shr.u16 	%rs66, %rs65, 8;
	shl.b16 	%rs67, %rs65, 8;
	or.b16  	%rs68, %rs67, %rs66;
	shl.b16 	%rs69, %rs68, 4;
	shl.b16 	%rs70, %rs65, 4;
	and.b16  	%rs71, %rs70, 3840;
	or.b16  	%rs72, %rs71, %rs69;
	and.b16  	%rs73, %rs72, 13107;
	shl.b16 	%rs74, %rs73, 2;
	shr.u16 	%rs75, %rs72, 2;
	and.b16  	%rs76, %rs75, 13107;
	or.b16  	%rs77, %rs76, %rs74;
	and.b16  	%rs78, %rs77, 21845;
	shl.b16 	%rs79, %rs78, 1;
	shr.u16 	%rs80, %rs77, 1;
	and.b16  	%rs81, %rs80, 21845;
	or.b16  	%rs82, %rs81, %rs79;
	shr.u16 	%rs83, %rs82, 2;
	cvt.u32.u16 	%r44, %rs83;
	add.s32 	%r43, %r2, %r44;
	add.s32 	%r42, %r43, -4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	st.u32 	[%rd82], %r40;
	st.u32 	[%rd82+4], %r41;
$L__BB16_11:
	ret;
$L__BB16_4:
	mov.u64 	%rd52, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd53, %rd52;
	mov.u64 	%rd54, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd55, %rd54;
	{ // callseq 60, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd53;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd55;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 60
$L__BB16_1:
	mov.u64 	%rd84, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_7;
	cvta.global.u64 	%rd85, %rd84;
	{ // callseq 62, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd85;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 62

}
	// .globl	fft_forward_shifted_4096_kernel
.visible .entry fft_forward_shifted_4096_kernel(
	.param .u64 fft_forward_shifted_4096_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot17[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<90>;
	.reg .b32 	%r<45>;
	.reg .b64 	%rd<95>;

	mov.u64 	%SPL, __local_depot17;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd37, [fft_forward_shifted_4096_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd37;
	add.u64 	%rd39, %SP, 16;
	add.u64 	%rd40, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[32768];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd40], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd41, [%rd1+8];
	and.b64  	%rd42, %rd41, -4096;
	mul.wide.u32 	%rd6, %r5, 4096;
	setp.lt.u64 	%p2, %rd6, %rd42;
	sub.s64 	%rd43, %rd41, %rd6;
	setp.gt.u64 	%p3, %rd43, 4095;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB17_2;
	bra.uni 	$L__BB17_1;
$L__BB17_2:
	add.u64 	%rd38, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r10, %rd3;
	ld.global.nc.u64 	%rd44, [%rd1];
	shl.b64 	%rd45, %rd6, 3;
	add.s64 	%rd46, %rd44, %rd45;
	add.s64 	%rd7, %rd46, 32768;
	add.s64 	%rd8, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 4095;
	shl.b64 	%rd47, %rd3, 3;
	add.s64 	%rd9, %rd46, %rd47;
	add.s64 	%rd93, %rd9, 8;
	add.s64 	%rd94, %rd3, 1;
	shl.b32 	%r11, %r10, 3;
	add.s32 	%r6, %r2, %r11;
	add.s32 	%r7, %r6, 4;
	ld.u32 	%r8, [%rd9];
	ld.u32 	%r9, [%rd9+4];
	// begin inline asm
	st.shared.f32 [%r6], %r8;
st.shared.f32 [%r7], %r9;
	// end inline asm
	cvt.u32.u16 	%r12, %rs2;
	cvt.u32.u64 	%r13, %rd4;
	div.u32 	%r14, %r12, %r13;
	cvt.u64.u32 	%rd88, %r14;
	shl.b64 	%rd48, %rd5, 15;
	shl.b64 	%rd13, %rd4, 3;
	add.s64 	%rd49, %rd48, %rd13;
	add.s64 	%rd50, %rd49, %rd47;
	add.s64 	%rd87, %rd44, %rd50;
	xor.b64  	%rd86, %rd47, 32760;
	mov.u64 	%rd89, %rd94;
$L__BB17_3:
	add.s64 	%rd21, %rd89, %rd8;
	setp.lt.u64 	%p6, %rd21, 4096;
	@%p6 bra 	$L__BB17_5;
	bra.uni 	$L__BB17_4;
$L__BB17_5:
	shr.u64 	%rd51, %rd86, 3;
	setp.gt.u64 	%p5, %rd51, %rd8;
	selp.b64 	%rd20, %rd87, 0, %p5;
	setp.lt.u64 	%p1, %rd21, %rd89;
	add.s64 	%rd56, %rd89, %rd4;
	selp.b64 	%rd89, 4096, %rd56, %p1;
	cvt.u32.u64 	%r19, %rd21;
	shl.b32 	%r20, %r19, 3;
	add.s32 	%r15, %r20, %r2;
	add.s32 	%r16, %r15, 4;
	ld.u32 	%r17, [%rd20];
	ld.u32 	%r18, [%rd20+4];
	// begin inline asm
	st.shared.f32 [%r15], %r17;
st.shared.f32 [%r16], %r18;
	// end inline asm
	add.s64 	%rd88, %rd88, -1;
	add.s64 	%rd87, %rd87, %rd13;
	sub.s64 	%rd86, %rd86, %rd13;
	setp.ne.s64 	%p7, %rd88, 0;
	@%p7 bra 	$L__BB17_3;
	ld.global.nc.u64 	%rd57, [%rd1+16];
	ld.global.nc.u64 	%rd58, [%rd1+24];
	st.local.u64 	[%rd2], %rd57;
	st.local.u64 	[%rd2+8], %rd58;
	bar.sync 	0;
	{ // callseq 64, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd38;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd39;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 64
	bar.sync 	0;
	xor.b32  	%r26, %r10, 4095;
	div.u32 	%r28, %r26, %r13;
	cvt.u64.u32 	%rd26, %r28;
	or.b16  	%rs3, %rs1, 6144;
	shl.b16 	%rs4, %rs1, 8;
	shr.u16 	%rs5, %rs3, 8;
	or.b16  	%rs6, %rs4, %rs5;
	and.b16  	%rs7, %rs6, 3851;
	shl.b16 	%rs8, %rs7, 4;
	shr.u16 	%rs9, %rs6, 4;
	and.b16  	%rs10, %rs9, 3841;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 13107;
	shl.b16 	%rs13, %rs12, 2;
	shr.u16 	%rs14, %rs11, 2;
	and.b16  	%rs15, %rs14, 13107;
	or.b16  	%rs16, %rs15, %rs13;
	and.b16  	%rs17, %rs16, 21845;
	shl.b16 	%rs18, %rs17, 1;
	shr.u16 	%rs19, %rs16, 1;
	and.b16  	%rs20, %rs19, 21845;
	or.b16  	%rs21, %rs20, %rs18;
	shr.u16 	%rs22, %rs21, 1;
	cvt.u32.u16 	%r29, %rs22;
	add.s32 	%r24, %r2, %r29;
	add.s32 	%r23, %r24, -4;
	// begin inline asm
	ld.shared.f32 %r21, [%r23];
ld.shared.f32 %r22, [%r24];
	// end inline asm
	st.u32 	[%rd9], %r21;
	st.u32 	[%rd9+4], %r22;
	and.b64  	%rd27, %rd26, 1;
	setp.eq.s32 	%p8, %r28, 1;
	@%p8 bra 	$L__BB17_9;
	and.b64  	%rd90, %rd26, 4094;
$L__BB17_8:
	sub.s64 	%rd61, %rd7, %rd93;
	shr.u64 	%rd62, %rd61, 3;
	setp.gt.u64 	%p9, %rd62, %rd8;
	add.s64 	%rd65, %rd93, %rd13;
	add.s64 	%rd66, %rd65, -8;
	selp.b64 	%rd67, %rd65, %rd7, %p9;
	selp.b64 	%rd68, %rd66, 0, %p9;
	add.s64 	%rd69, %rd94, %rd8;
	setp.lt.u64 	%p10, %rd69, %rd94;
	setp.gt.u64 	%p11, %rd69, 4095;
	add.s64 	%rd70, %rd69, 1;
	selp.b64 	%rd71, 4096, %rd70, %p11;
	selp.b64 	%rd72, 4096, %rd71, %p10;
	cvt.u16.u64 	%rs23, %rd69;
	and.b16  	%rs24, %rs23, 3840;
	xor.b16  	%rs25, %rs24, 6144;
	shl.b16 	%rs26, %rs23, 8;
	shr.u16 	%rs27, %rs25, 8;
	or.b16  	%rs28, %rs26, %rs27;
	and.b16  	%rs29, %rs28, 3855;
	shl.b16 	%rs30, %rs29, 4;
	shr.u16 	%rs31, %rs28, 4;
	and.b16  	%rs32, %rs31, 3841;
	or.b16  	%rs33, %rs32, %rs30;
	and.b16  	%rs34, %rs33, 13107;
	shl.b16 	%rs35, %rs34, 2;
	shr.u16 	%rs36, %rs33, 2;
	and.b16  	%rs37, %rs36, 13107;
	or.b16  	%rs38, %rs37, %rs35;
	and.b16  	%rs39, %rs38, 21845;
	shl.b16 	%rs40, %rs39, 1;
	shr.u16 	%rs41, %rs38, 1;
	and.b16  	%rs42, %rs41, 21845;
	or.b16  	%rs43, %rs42, %rs40;
	shr.u16 	%rs44, %rs43, 1;
	cvt.u32.u16 	%r38, %rs44;
	add.s32 	%r33, %r2, %r38;
	add.s32 	%r32, %r33, -4;
	// begin inline asm
	ld.shared.f32 %r30, [%r32];
ld.shared.f32 %r31, [%r33];
	// end inline asm
	st.u32 	[%rd68], %r30;
	st.u32 	[%rd68+4], %r31;
	add.s64 	%rd93, %rd67, %rd13;
	add.s64 	%rd73, %rd72, %rd8;
	setp.lt.u64 	%p12, %rd73, %rd72;
	setp.gt.u64 	%p13, %rd73, 4095;
	add.s64 	%rd75, %rd73, 1;
	selp.b64 	%rd76, 4096, %rd75, %p13;
	selp.b64 	%rd94, 4096, %rd76, %p12;
	cvt.u16.u64 	%rs45, %rd73;
	and.b16  	%rs46, %rs45, 3840;
	xor.b16  	%rs47, %rs46, 6144;
	shl.b16 	%rs48, %rs45, 8;
	shr.u16 	%rs49, %rs47, 8;
	or.b16  	%rs50, %rs48, %rs49;
	and.b16  	%rs51, %rs50, 3855;
	shl.b16 	%rs52, %rs51, 4;
	shr.u16 	%rs53, %rs50, 4;
	and.b16  	%rs54, %rs53, 3841;
	or.b16  	%rs55, %rs54, %rs52;
	and.b16  	%rs56, %rs55, 13107;
	shl.b16 	%rs57, %rs56, 2;
	shr.u16 	%rs58, %rs55, 2;
	and.b16  	%rs59, %rs58, 13107;
	or.b16  	%rs60, %rs59, %rs57;
	and.b16  	%rs61, %rs60, 21845;
	shl.b16 	%rs62, %rs61, 1;
	shr.u16 	%rs63, %rs60, 1;
	and.b16  	%rs64, %rs63, 21845;
	or.b16  	%rs65, %rs64, %rs62;
	shr.u16 	%rs66, %rs65, 1;
	cvt.u32.u16 	%r39, %rs66;
	add.s32 	%r37, %r2, %r39;
	add.s32 	%r36, %r37, -4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd93+-8], %r34;
	st.u32 	[%rd93+-4], %r35;
	add.s64 	%rd90, %rd90, -2;
	setp.ne.s64 	%p14, %rd90, 0;
	@%p14 bra 	$L__BB17_8;
$L__BB17_9:
	setp.eq.s64 	%p15, %rd27, 0;
	@%p15 bra 	$L__BB17_11;
	sub.s64 	%rd77, %rd7, %rd93;
	shr.u64 	%rd78, %rd77, 3;
	setp.gt.u64 	%p16, %rd78, %rd8;
	add.s64 	%rd80, %rd93, %rd13;
	add.s64 	%rd81, %rd80, -8;
	selp.b64 	%rd82, %rd81, 0, %p16;
	add.s64 	%rd83, %rd94, %rd8;
	setp.lt.u64 	%p17, %rd83, %rd94;
	cvt.u16.u64 	%rs67, %rd83;
	and.b16  	%rs68, %rs67, 4095;
	xor.b16  	%rs69, %rs68, 6144;
	selp.b16 	%rs70, 6144, %rs69, %p17;
	shr.u16 	%rs71, %rs70, 8;
	shl.b16 	%rs72, %rs70, 8;
	or.b16  	%rs73, %rs72, %rs71;
	and.b16  	%rs74, %rs73, 3855;
	shl.b16 	%rs75, %rs74, 4;
	shr.u16 	%rs76, %rs73, 4;
	and.b16  	%rs77, %rs76, 3841;
	or.b16  	%rs78, %rs77, %rs75;
	and.b16  	%rs79, %rs78, 13107;
	shl.b16 	%rs80, %rs79, 2;
	shr.u16 	%rs81, %rs78, 2;
	and.b16  	%rs82, %rs81, 13107;
	or.b16  	%rs83, %rs82, %rs80;
	and.b16  	%rs84, %rs83, 21845;
	shl.b16 	%rs85, %rs84, 1;
	shr.u16 	%rs86, %rs83, 1;
	and.b16  	%rs87, %rs86, 21845;
	or.b16  	%rs88, %rs87, %rs85;
	shr.u16 	%rs89, %rs88, 1;
	cvt.u32.u16 	%r44, %rs89;
	add.s32 	%r43, %r2, %r44;
	add.s32 	%r42, %r43, -4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	st.u32 	[%rd82], %r40;
	st.u32 	[%rd82+4], %r41;
$L__BB17_11:
	ret;
$L__BB17_4:
	mov.u64 	%rd52, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd53, %rd52;
	mov.u64 	%rd54, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd55, %rd54;
	{ // callseq 63, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd53;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd55;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 63
$L__BB17_1:
	mov.u64 	%rd84, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_7;
	cvta.global.u64 	%rd85, %rd84;
	{ // callseq 65, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd85;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 65

}
	// .globl	fft_backward_shifted_128_kernel
.visible .entry fft_backward_shifted_128_kernel(
	.param .u64 fft_backward_shifted_128_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot18[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<36>;
	.reg .b32 	%r<68>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<127>;

	mov.u64 	%SPL, __local_depot18;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd57, [fft_backward_shifted_128_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd57;
	add.u64 	%rd59, %SP, 16;
	add.u64 	%rd60, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[1024];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd60], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd61, [%rd1+8];
	and.b64  	%rd62, %rd61, -128;
	mul.wide.u32 	%rd63, %r5, 128;
	setp.lt.u64 	%p2, %rd63, %rd62;
	sub.s64 	%rd65, %rd61, %rd63;
	setp.gt.u64 	%p3, %rd65, 127;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB18_2;
	bra.uni 	$L__BB18_1;
$L__BB18_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd67, %rd3, 128;
	setp.gt.u32 	%p5, %r6, 127;
	not.b64 	%rd68, %rd3;
	add.s64 	%rd10, %rd68, %rd67;
	mov.u64 	%rd115, 0;
	and.b64  	%rd111, %rd10, -4294967296;
	mov.u64 	%rd113, %rd115;
	@%p5 bra 	$L__BB18_7;
	setp.ne.s64 	%p6, %rd111, 0;
	@%p6 bra 	$L__BB18_5;
	bra.uni 	$L__BB18_4;
$L__BB18_5:
	div.u64 	%rd112, %rd10, %rd4;
	bra.uni 	$L__BB18_6;
$L__BB18_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd10;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd112, %r9;
$L__BB18_6:
	add.s64 	%rd113, %rd112, 1;
$L__BB18_7:
	shl.b64 	%rd64, %rd63, 3;
	@%p5 bra 	$L__BB18_12;
	setp.ne.s64 	%p8, %rd111, 0;
	@%p8 bra 	$L__BB18_10;
	bra.uni 	$L__BB18_9;
$L__BB18_10:
	div.u64 	%rd114, %rd10, %rd4;
	bra.uni 	$L__BB18_11;
$L__BB18_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd10;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd114, %r13;
$L__BB18_11:
	add.s64 	%rd115, %rd114, 1;
$L__BB18_12:
	add.s64 	%rd7, %rd6, %rd64;
	add.s64 	%rd9, %rd4, -1;
	min.u64 	%rd21, %rd113, %rd115;
	setp.eq.s64 	%p9, %rd21, 0;
	shl.b64 	%rd107, %rd3, 3;
	shl.b64 	%rd108, %rd4, 3;
	@%p9 bra 	$L__BB18_18;
	add.s64 	%rd73, %rd7, %rd107;
	ld.u32 	%r15, [%rd73+4];
	ld.u32 	%r18, [%rd73];
	// begin inline asm
	neg.ftz.f32 %r14, %r15;
	// end inline asm
	shl.b32 	%r21, %r6, 3;
	add.s32 	%r16, %r2, %r21;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	st.shared.f32 [%r16], %r18;
st.shared.f32 [%r17], %r14;
	// end inline asm
	setp.eq.s64 	%p10, %rd21, 1;
	@%p10 bra 	$L__BB18_18;
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd119, %rd3, 1;
	add.s64 	%rd118, %rd21, -1;
	shl.b64 	%rd74, %rd5, 10;
	add.s64 	%rd75, %rd74, %rd108;
	add.s64 	%rd77, %rd75, %rd107;
	add.s64 	%rd117, %rd6, %rd77;
	mov.u64 	%rd78, 1016;
	sub.s64 	%rd116, %rd78, %rd107;
$L__BB18_15:
	shr.u64 	%rd79, %rd116, 3;
	setp.gt.u64 	%p11, %rd79, %rd9;
	selp.b64 	%rd80, %rd117, 0, %p11;
	add.s64 	%rd31, %rd119, %rd9;
	ld.f32 	%f1, [%rd80];
	ld.u32 	%r23, [%rd80+4];
	// begin inline asm
	neg.ftz.f32 %r22, %r23;
	// end inline asm
	setp.lt.u64 	%p12, %rd31, 128;
	@%p12 bra 	$L__BB18_17;
	bra.uni 	$L__BB18_16;
$L__BB18_17:
	setp.lt.u64 	%p1, %rd31, %rd119;
	mov.b32 	%f2, %r22;
	add.s64 	%rd85, %rd119, %rd4;
	selp.b64 	%rd119, 128, %rd85, %p1;
	cvt.u32.u64 	%r28, %rd31;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r24, %r29, %r2;
	add.s32 	%r25, %r24, 4;
	mov.b32 	%r26, %f1;
	// begin inline asm
	st.shared.f32 [%r24], %r26;
st.shared.f32 [%r25], %r22;
	// end inline asm
	add.s64 	%rd118, %rd118, -1;
	add.s64 	%rd117, %rd117, %rd108;
	sub.s64 	%rd116, %rd116, %rd108;
	setp.ne.s64 	%p13, %rd118, 0;
	@%p13 bra 	$L__BB18_15;
$L__BB18_18:
	add.u64 	%rd58, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd87, [%rd1+16];
	ld.global.nc.u64 	%rd88, [%rd1+24];
	st.local.u64 	[%rd2], %rd87;
	st.local.u64 	[%rd2+8], %rd88;
	bar.sync 	0;
	{ // callseq 67, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd58;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd59;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 67
	mov.b32 	%r31, 1124073472;
	// begin inline asm
	rcp.approx.ftz.f32 %r48, %r31;
	// end inline asm
	bar.sync 	0;
	mov.u64 	%rd123, 0;
	mov.u64 	%rd121, %rd123;
	@%p5 bra 	$L__BB18_23;
	setp.ne.s64 	%p15, %rd111, 0;
	@%p15 bra 	$L__BB18_21;
	bra.uni 	$L__BB18_20;
$L__BB18_21:
	div.u64 	%rd120, %rd10, %rd4;
	bra.uni 	$L__BB18_22;
$L__BB18_20:
	cvt.u32.u64 	%r33, %rd4;
	cvt.u32.u64 	%r34, %rd10;
	div.u32 	%r35, %r34, %r33;
	cvt.u64.u32 	%rd120, %r35;
$L__BB18_22:
	add.s64 	%rd121, %rd120, 1;
$L__BB18_23:
	@%p5 bra 	$L__BB18_28;
	setp.ne.s64 	%p17, %rd111, 0;
	@%p17 bra 	$L__BB18_26;
	bra.uni 	$L__BB18_25;
$L__BB18_26:
	div.u64 	%rd122, %rd10, %rd4;
	bra.uni 	$L__BB18_27;
$L__BB18_25:
	cvt.u32.u64 	%r37, %rd4;
	cvt.u32.u64 	%r38, %rd10;
	div.u32 	%r39, %r38, %r37;
	cvt.u64.u32 	%rd122, %r39;
$L__BB18_27:
	add.s64 	%rd123, %rd122, 1;
$L__BB18_28:
	min.u64 	%rd46, %rd121, %rd123;
	setp.eq.s64 	%p18, %rd46, 0;
	@%p18 bra 	$L__BB18_32;
	mov.b32 	%f3, %r48;
	add.s64 	%rd47, %rd7, %rd107;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, -64;
	and.b16  	%rs3, %rs2, 240;
	and.b16  	%rs4, %rs2, 15;
	shl.b16 	%rs5, %rs4, 4;
	shr.u16 	%rs6, %rs3, 4;
	or.b16  	%rs7, %rs6, %rs5;
	and.b16  	%rs8, %rs7, 51;
	shl.b16 	%rs9, %rs8, 2;
	shr.u16 	%rs10, %rs7, 2;
	and.b16  	%rs11, %rs10, 51;
	or.b16  	%rs12, %rs11, %rs9;
	and.b16  	%rs13, %rs12, 85;
	shl.b16 	%rs14, %rs13, 1;
	shr.u16 	%rs15, %rs12, 1;
	and.b16  	%rs16, %rs15, 85;
	or.b16  	%rs17, %rs16, %rs14;
	mul.wide.u16 	%r52, %rs17, 4;
	add.s32 	%r43, %r2, %r52;
	add.s32 	%r42, %r43, -4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r44, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r46, %r40, %r48;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r44, %r48;
	// end inline asm
	st.u32 	[%rd47], %r46;
	st.u32 	[%rd47+4], %r49;
	setp.eq.s64 	%p19, %rd46, 1;
	@%p19 bra 	$L__BB18_32;
	add.s64 	%rd8, %rd7, 1024;
	add.s64 	%rd126, %rd3, 1;
	add.s64 	%rd125, %rd47, 8;
	add.s64 	%rd124, %rd46, -1;
$L__BB18_31:
	sub.s64 	%rd95, %rd8, %rd125;
	shr.u64 	%rd96, %rd95, 3;
	setp.gt.u64 	%p20, %rd96, %rd9;
	add.s64 	%rd99, %rd125, %rd108;
	add.s64 	%rd100, %rd99, -8;
	selp.b64 	%rd125, %rd99, %rd8, %p20;
	selp.b64 	%rd101, %rd100, 0, %p20;
	add.s64 	%rd102, %rd126, %rd9;
	setp.lt.u64 	%p21, %rd102, %rd126;
	setp.gt.u64 	%p22, %rd102, 127;
	add.s64 	%rd103, %rd102, 1;
	selp.b64 	%rd104, 128, %rd103, %p22;
	selp.b64 	%rd126, 128, %rd104, %p21;
	cvt.u16.u64 	%rs18, %rd102;
	and.b16  	%rs19, %rs18, 127;
	xor.b16  	%rs20, %rs19, -64;
	and.b16  	%rs21, %rs20, 240;
	and.b16  	%rs22, %rs20, 15;
	shl.b16 	%rs23, %rs22, 4;
	shr.u16 	%rs24, %rs21, 4;
	or.b16  	%rs25, %rs24, %rs23;
	and.b16  	%rs26, %rs25, 51;
	shl.b16 	%rs27, %rs26, 2;
	shr.u16 	%rs28, %rs25, 2;
	and.b16  	%rs29, %rs28, 51;
	or.b16  	%rs30, %rs29, %rs27;
	and.b16  	%rs31, %rs30, 85;
	shl.b16 	%rs32, %rs31, 1;
	shr.u16 	%rs33, %rs30, 1;
	and.b16  	%rs34, %rs33, 85;
	or.b16  	%rs35, %rs34, %rs32;
	mul.wide.u16 	%r65, %rs35, 4;
	add.s32 	%r56, %r65, %r2;
	add.s32 	%r55, %r56, -4;
	// begin inline asm
	ld.shared.f32 %r53, [%r55];
ld.shared.f32 %r54, [%r56];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r57, %r54;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r59, %r53, %r48;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r62, %r57, %r48;
	// end inline asm
	st.u32 	[%rd101], %r59;
	st.u32 	[%rd101+4], %r62;
	add.s64 	%rd124, %rd124, -1;
	setp.ne.s64 	%p23, %rd124, 0;
	@%p23 bra 	$L__BB18_31;
$L__BB18_32:
	ret;
$L__BB18_16:
	mov.u64 	%rd81, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd82, %rd81;
	mov.u64 	%rd83, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd84, %rd83;
	{ // callseq 66, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd82;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd84;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 66
$L__BB18_1:
	mov.u64 	%rd105, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_9;
	cvta.global.u64 	%rd106, %rd105;
	{ // callseq 68, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd106;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 68

}
	// .globl	fft_backward_shifted_256_kernel
.visible .entry fft_backward_shifted_256_kernel(
	.param .u64 fft_backward_shifted_256_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot19[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<39>;
	.reg .b32 	%r<68>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<127>;

	mov.u64 	%SPL, __local_depot19;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd57, [fft_backward_shifted_256_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd57;
	add.u64 	%rd59, %SP, 16;
	add.u64 	%rd60, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[2048];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd60], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd61, [%rd1+8];
	and.b64  	%rd62, %rd61, -256;
	mul.wide.u32 	%rd63, %r5, 256;
	setp.lt.u64 	%p2, %rd63, %rd62;
	sub.s64 	%rd65, %rd61, %rd63;
	setp.gt.u64 	%p3, %rd65, 255;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB19_2;
	bra.uni 	$L__BB19_1;
$L__BB19_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd67, %rd3, 256;
	setp.gt.u32 	%p5, %r6, 255;
	not.b64 	%rd68, %rd3;
	add.s64 	%rd10, %rd68, %rd67;
	mov.u64 	%rd115, 0;
	and.b64  	%rd111, %rd10, -4294967296;
	mov.u64 	%rd113, %rd115;
	@%p5 bra 	$L__BB19_7;
	setp.ne.s64 	%p6, %rd111, 0;
	@%p6 bra 	$L__BB19_5;
	bra.uni 	$L__BB19_4;
$L__BB19_5:
	div.u64 	%rd112, %rd10, %rd4;
	bra.uni 	$L__BB19_6;
$L__BB19_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd10;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd112, %r9;
$L__BB19_6:
	add.s64 	%rd113, %rd112, 1;
$L__BB19_7:
	shl.b64 	%rd64, %rd63, 3;
	@%p5 bra 	$L__BB19_12;
	setp.ne.s64 	%p8, %rd111, 0;
	@%p8 bra 	$L__BB19_10;
	bra.uni 	$L__BB19_9;
$L__BB19_10:
	div.u64 	%rd114, %rd10, %rd4;
	bra.uni 	$L__BB19_11;
$L__BB19_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd10;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd114, %r13;
$L__BB19_11:
	add.s64 	%rd115, %rd114, 1;
$L__BB19_12:
	add.s64 	%rd7, %rd6, %rd64;
	add.s64 	%rd9, %rd4, -1;
	min.u64 	%rd21, %rd113, %rd115;
	setp.eq.s64 	%p9, %rd21, 0;
	shl.b64 	%rd107, %rd3, 3;
	shl.b64 	%rd108, %rd4, 3;
	@%p9 bra 	$L__BB19_18;
	add.s64 	%rd73, %rd7, %rd107;
	ld.u32 	%r15, [%rd73+4];
	ld.u32 	%r18, [%rd73];
	// begin inline asm
	neg.ftz.f32 %r14, %r15;
	// end inline asm
	shl.b32 	%r21, %r6, 3;
	add.s32 	%r16, %r2, %r21;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	st.shared.f32 [%r16], %r18;
st.shared.f32 [%r17], %r14;
	// end inline asm
	setp.eq.s64 	%p10, %rd21, 1;
	@%p10 bra 	$L__BB19_18;
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd119, %rd3, 1;
	add.s64 	%rd118, %rd21, -1;
	shl.b64 	%rd74, %rd5, 11;
	add.s64 	%rd75, %rd74, %rd108;
	add.s64 	%rd77, %rd75, %rd107;
	add.s64 	%rd117, %rd6, %rd77;
	mov.u64 	%rd78, 2040;
	sub.s64 	%rd116, %rd78, %rd107;
$L__BB19_15:
	shr.u64 	%rd79, %rd116, 3;
	setp.gt.u64 	%p11, %rd79, %rd9;
	selp.b64 	%rd80, %rd117, 0, %p11;
	add.s64 	%rd31, %rd119, %rd9;
	ld.f32 	%f1, [%rd80];
	ld.u32 	%r23, [%rd80+4];
	// begin inline asm
	neg.ftz.f32 %r22, %r23;
	// end inline asm
	setp.lt.u64 	%p12, %rd31, 256;
	@%p12 bra 	$L__BB19_17;
	bra.uni 	$L__BB19_16;
$L__BB19_17:
	setp.lt.u64 	%p1, %rd31, %rd119;
	mov.b32 	%f2, %r22;
	add.s64 	%rd85, %rd119, %rd4;
	selp.b64 	%rd119, 256, %rd85, %p1;
	cvt.u32.u64 	%r28, %rd31;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r24, %r29, %r2;
	add.s32 	%r25, %r24, 4;
	mov.b32 	%r26, %f1;
	// begin inline asm
	st.shared.f32 [%r24], %r26;
st.shared.f32 [%r25], %r22;
	// end inline asm
	add.s64 	%rd118, %rd118, -1;
	add.s64 	%rd117, %rd117, %rd108;
	sub.s64 	%rd116, %rd116, %rd108;
	setp.ne.s64 	%p13, %rd118, 0;
	@%p13 bra 	$L__BB19_15;
$L__BB19_18:
	add.u64 	%rd58, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd87, [%rd1+16];
	ld.global.nc.u64 	%rd88, [%rd1+24];
	st.local.u64 	[%rd2], %rd87;
	st.local.u64 	[%rd2+8], %rd88;
	bar.sync 	0;
	{ // callseq 70, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd58;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd59;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 70
	mov.b32 	%r31, 1132462080;
	// begin inline asm
	rcp.approx.ftz.f32 %r48, %r31;
	// end inline asm
	bar.sync 	0;
	mov.u64 	%rd123, 0;
	mov.u64 	%rd121, %rd123;
	@%p5 bra 	$L__BB19_23;
	setp.ne.s64 	%p15, %rd111, 0;
	@%p15 bra 	$L__BB19_21;
	bra.uni 	$L__BB19_20;
$L__BB19_21:
	div.u64 	%rd120, %rd10, %rd4;
	bra.uni 	$L__BB19_22;
$L__BB19_20:
	cvt.u32.u64 	%r33, %rd4;
	cvt.u32.u64 	%r34, %rd10;
	div.u32 	%r35, %r34, %r33;
	cvt.u64.u32 	%rd120, %r35;
$L__BB19_22:
	add.s64 	%rd121, %rd120, 1;
$L__BB19_23:
	@%p5 bra 	$L__BB19_28;
	setp.ne.s64 	%p17, %rd111, 0;
	@%p17 bra 	$L__BB19_26;
	bra.uni 	$L__BB19_25;
$L__BB19_26:
	div.u64 	%rd122, %rd10, %rd4;
	bra.uni 	$L__BB19_27;
$L__BB19_25:
	cvt.u32.u64 	%r37, %rd4;
	cvt.u32.u64 	%r38, %rd10;
	div.u32 	%r39, %r38, %r37;
	cvt.u64.u32 	%rd122, %r39;
$L__BB19_27:
	add.s64 	%rd123, %rd122, 1;
$L__BB19_28:
	min.u64 	%rd46, %rd121, %rd123;
	setp.eq.s64 	%p18, %rd46, 0;
	@%p18 bra 	$L__BB19_32;
	mov.b32 	%f3, %r48;
	add.s64 	%rd47, %rd7, %rd107;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 384;
	shl.b16 	%rs3, %rs1, 8;
	shr.u16 	%rs4, %rs2, 8;
	or.b16  	%rs5, %rs3, %rs4;
	shl.b16 	%rs6, %rs5, 4;
	shl.b16 	%rs7, %rs2, 4;
	and.b16  	%rs8, %rs7, 3840;
	or.b16  	%rs9, %rs8, %rs6;
	and.b16  	%rs10, %rs9, 13107;
	shl.b16 	%rs11, %rs10, 2;
	shr.u16 	%rs12, %rs9, 2;
	and.b16  	%rs13, %rs12, 13107;
	or.b16  	%rs14, %rs13, %rs11;
	and.b16  	%rs15, %rs14, 21845;
	shl.b16 	%rs16, %rs15, 1;
	shr.u16 	%rs17, %rs14, 1;
	and.b16  	%rs18, %rs17, 21845;
	or.b16  	%rs19, %rs18, %rs16;
	shr.u16 	%rs20, %rs19, 5;
	cvt.u32.u16 	%r52, %rs20;
	add.s32 	%r43, %r2, %r52;
	add.s32 	%r42, %r43, -4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r44, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r46, %r40, %r48;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r44, %r48;
	// end inline asm
	st.u32 	[%rd47], %r46;
	st.u32 	[%rd47+4], %r49;
	setp.eq.s64 	%p19, %rd46, 1;
	@%p19 bra 	$L__BB19_32;
	add.s64 	%rd8, %rd7, 2048;
	add.s64 	%rd126, %rd3, 1;
	add.s64 	%rd125, %rd47, 8;
	add.s64 	%rd124, %rd46, -1;
$L__BB19_31:
	sub.s64 	%rd95, %rd8, %rd125;
	shr.u64 	%rd96, %rd95, 3;
	setp.gt.u64 	%p20, %rd96, %rd9;
	add.s64 	%rd99, %rd125, %rd108;
	add.s64 	%rd100, %rd99, -8;
	selp.b64 	%rd125, %rd99, %rd8, %p20;
	selp.b64 	%rd101, %rd100, 0, %p20;
	add.s64 	%rd102, %rd126, %rd9;
	setp.lt.u64 	%p21, %rd102, %rd126;
	setp.gt.u64 	%p22, %rd102, 255;
	add.s64 	%rd103, %rd102, 1;
	selp.b64 	%rd104, 256, %rd103, %p22;
	selp.b64 	%rd126, 256, %rd104, %p21;
	cvt.u16.u64 	%rs21, %rd102;
	xor.b16  	%rs22, %rs21, 128;
	shl.b16 	%rs23, %rs21, 12;
	shl.b16 	%rs24, %rs22, 4;
	and.b16  	%rs25, %rs24, 3840;
	or.b16  	%rs26, %rs23, %rs25;
	shr.u16 	%rs27, %rs26, 2;
	and.b16  	%rs28, %rs27, 13056;
	or.b16  	%rs29, %rs26, 16;
	and.b16  	%rs30, %rs29, 13072;
	shl.b16 	%rs31, %rs30, 2;
	or.b16  	%rs32, %rs28, %rs31;
	and.b16  	%rs33, %rs32, 21824;
	shl.b16 	%rs34, %rs33, 1;
	shr.u16 	%rs35, %rs32, 1;
	and.b16  	%rs36, %rs35, 21760;
	or.b16  	%rs37, %rs36, %rs34;
	shr.u16 	%rs38, %rs37, 5;
	cvt.u32.u16 	%r65, %rs38;
	add.s32 	%r56, %r2, %r65;
	add.s32 	%r55, %r56, -4;
	// begin inline asm
	ld.shared.f32 %r53, [%r55];
ld.shared.f32 %r54, [%r56];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r57, %r54;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r59, %r53, %r48;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r62, %r57, %r48;
	// end inline asm
	st.u32 	[%rd101], %r59;
	st.u32 	[%rd101+4], %r62;
	add.s64 	%rd124, %rd124, -1;
	setp.ne.s64 	%p23, %rd124, 0;
	@%p23 bra 	$L__BB19_31;
$L__BB19_32:
	ret;
$L__BB19_16:
	mov.u64 	%rd81, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd82, %rd81;
	mov.u64 	%rd83, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd84, %rd83;
	{ // callseq 69, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd82;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd84;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 69
$L__BB19_1:
	mov.u64 	%rd105, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_9;
	cvta.global.u64 	%rd106, %rd105;
	{ // callseq 71, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd106;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 71

}
	// .globl	fft_backward_shifted_512_kernel
.visible .entry fft_backward_shifted_512_kernel(
	.param .u64 fft_backward_shifted_512_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot20[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<41>;
	.reg .b32 	%r<68>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<127>;

	mov.u64 	%SPL, __local_depot20;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd57, [fft_backward_shifted_512_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd57;
	add.u64 	%rd59, %SP, 16;
	add.u64 	%rd60, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[4096];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd60], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd61, [%rd1+8];
	and.b64  	%rd62, %rd61, -512;
	mul.wide.u32 	%rd63, %r5, 512;
	setp.lt.u64 	%p2, %rd63, %rd62;
	sub.s64 	%rd65, %rd61, %rd63;
	setp.gt.u64 	%p3, %rd65, 511;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB20_2;
	bra.uni 	$L__BB20_1;
$L__BB20_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd67, %rd3, 512;
	setp.gt.u32 	%p5, %r6, 511;
	not.b64 	%rd68, %rd3;
	add.s64 	%rd10, %rd68, %rd67;
	mov.u64 	%rd115, 0;
	and.b64  	%rd111, %rd10, -4294967296;
	mov.u64 	%rd113, %rd115;
	@%p5 bra 	$L__BB20_7;
	setp.ne.s64 	%p6, %rd111, 0;
	@%p6 bra 	$L__BB20_5;
	bra.uni 	$L__BB20_4;
$L__BB20_5:
	div.u64 	%rd112, %rd10, %rd4;
	bra.uni 	$L__BB20_6;
$L__BB20_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd10;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd112, %r9;
$L__BB20_6:
	add.s64 	%rd113, %rd112, 1;
$L__BB20_7:
	shl.b64 	%rd64, %rd63, 3;
	@%p5 bra 	$L__BB20_12;
	setp.ne.s64 	%p8, %rd111, 0;
	@%p8 bra 	$L__BB20_10;
	bra.uni 	$L__BB20_9;
$L__BB20_10:
	div.u64 	%rd114, %rd10, %rd4;
	bra.uni 	$L__BB20_11;
$L__BB20_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd10;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd114, %r13;
$L__BB20_11:
	add.s64 	%rd115, %rd114, 1;
$L__BB20_12:
	add.s64 	%rd7, %rd6, %rd64;
	add.s64 	%rd9, %rd4, -1;
	min.u64 	%rd21, %rd113, %rd115;
	setp.eq.s64 	%p9, %rd21, 0;
	shl.b64 	%rd107, %rd3, 3;
	shl.b64 	%rd108, %rd4, 3;
	@%p9 bra 	$L__BB20_18;
	add.s64 	%rd73, %rd7, %rd107;
	ld.u32 	%r15, [%rd73+4];
	ld.u32 	%r18, [%rd73];
	// begin inline asm
	neg.ftz.f32 %r14, %r15;
	// end inline asm
	shl.b32 	%r21, %r6, 3;
	add.s32 	%r16, %r2, %r21;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	st.shared.f32 [%r16], %r18;
st.shared.f32 [%r17], %r14;
	// end inline asm
	setp.eq.s64 	%p10, %rd21, 1;
	@%p10 bra 	$L__BB20_18;
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd119, %rd3, 1;
	add.s64 	%rd118, %rd21, -1;
	shl.b64 	%rd74, %rd5, 12;
	add.s64 	%rd75, %rd74, %rd108;
	add.s64 	%rd77, %rd75, %rd107;
	add.s64 	%rd117, %rd6, %rd77;
	mov.u64 	%rd78, 4088;
	sub.s64 	%rd116, %rd78, %rd107;
$L__BB20_15:
	shr.u64 	%rd79, %rd116, 3;
	setp.gt.u64 	%p11, %rd79, %rd9;
	selp.b64 	%rd80, %rd117, 0, %p11;
	add.s64 	%rd31, %rd119, %rd9;
	ld.f32 	%f1, [%rd80];
	ld.u32 	%r23, [%rd80+4];
	// begin inline asm
	neg.ftz.f32 %r22, %r23;
	// end inline asm
	setp.lt.u64 	%p12, %rd31, 512;
	@%p12 bra 	$L__BB20_17;
	bra.uni 	$L__BB20_16;
$L__BB20_17:
	setp.lt.u64 	%p1, %rd31, %rd119;
	mov.b32 	%f2, %r22;
	add.s64 	%rd85, %rd119, %rd4;
	selp.b64 	%rd119, 512, %rd85, %p1;
	cvt.u32.u64 	%r28, %rd31;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r24, %r29, %r2;
	add.s32 	%r25, %r24, 4;
	mov.b32 	%r26, %f1;
	// begin inline asm
	st.shared.f32 [%r24], %r26;
st.shared.f32 [%r25], %r22;
	// end inline asm
	add.s64 	%rd118, %rd118, -1;
	add.s64 	%rd117, %rd117, %rd108;
	sub.s64 	%rd116, %rd116, %rd108;
	setp.ne.s64 	%p13, %rd118, 0;
	@%p13 bra 	$L__BB20_15;
$L__BB20_18:
	add.u64 	%rd58, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd87, [%rd1+16];
	ld.global.nc.u64 	%rd88, [%rd1+24];
	st.local.u64 	[%rd2], %rd87;
	st.local.u64 	[%rd2+8], %rd88;
	bar.sync 	0;
	{ // callseq 73, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd58;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd59;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 73
	mov.b32 	%r31, 1140850688;
	// begin inline asm
	rcp.approx.ftz.f32 %r48, %r31;
	// end inline asm
	bar.sync 	0;
	mov.u64 	%rd123, 0;
	mov.u64 	%rd121, %rd123;
	@%p5 bra 	$L__BB20_23;
	setp.ne.s64 	%p15, %rd111, 0;
	@%p15 bra 	$L__BB20_21;
	bra.uni 	$L__BB20_20;
$L__BB20_21:
	div.u64 	%rd120, %rd10, %rd4;
	bra.uni 	$L__BB20_22;
$L__BB20_20:
	cvt.u32.u64 	%r33, %rd4;
	cvt.u32.u64 	%r34, %rd10;
	div.u32 	%r35, %r34, %r33;
	cvt.u64.u32 	%rd120, %r35;
$L__BB20_22:
	add.s64 	%rd121, %rd120, 1;
$L__BB20_23:
	@%p5 bra 	$L__BB20_28;
	setp.ne.s64 	%p17, %rd111, 0;
	@%p17 bra 	$L__BB20_26;
	bra.uni 	$L__BB20_25;
$L__BB20_26:
	div.u64 	%rd122, %rd10, %rd4;
	bra.uni 	$L__BB20_27;
$L__BB20_25:
	cvt.u32.u64 	%r37, %rd4;
	cvt.u32.u64 	%r38, %rd10;
	div.u32 	%r39, %r38, %r37;
	cvt.u64.u32 	%rd122, %r39;
$L__BB20_27:
	add.s64 	%rd123, %rd122, 1;
$L__BB20_28:
	min.u64 	%rd46, %rd121, %rd123;
	setp.eq.s64 	%p18, %rd46, 0;
	@%p18 bra 	$L__BB20_32;
	mov.b32 	%f3, %r48;
	add.s64 	%rd47, %rd7, %rd107;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 768;
	shl.b16 	%rs3, %rs1, 8;
	shr.u16 	%rs4, %rs2, 8;
	or.b16  	%rs5, %rs3, %rs4;
	shl.b16 	%rs6, %rs5, 4;
	shl.b16 	%rs7, %rs1, 4;
	and.b16  	%rs8, %rs7, 3840;
	or.b16  	%rs9, %rs8, %rs6;
	and.b16  	%rs10, %rs9, 13107;
	shl.b16 	%rs11, %rs10, 2;
	shr.u16 	%rs12, %rs9, 2;
	and.b16  	%rs13, %rs12, 13107;
	or.b16  	%rs14, %rs13, %rs11;
	and.b16  	%rs15, %rs14, 21845;
	shl.b16 	%rs16, %rs15, 1;
	shr.u16 	%rs17, %rs14, 1;
	and.b16  	%rs18, %rs17, 21845;
	or.b16  	%rs19, %rs18, %rs16;
	shr.u16 	%rs20, %rs19, 4;
	cvt.u32.u16 	%r52, %rs20;
	add.s32 	%r43, %r2, %r52;
	add.s32 	%r42, %r43, -4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r44, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r46, %r40, %r48;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r44, %r48;
	// end inline asm
	st.u32 	[%rd47], %r46;
	st.u32 	[%rd47+4], %r49;
	setp.eq.s64 	%p19, %rd46, 1;
	@%p19 bra 	$L__BB20_32;
	add.s64 	%rd8, %rd7, 4096;
	add.s64 	%rd126, %rd3, 1;
	add.s64 	%rd125, %rd47, 8;
	add.s64 	%rd124, %rd46, -1;
$L__BB20_31:
	sub.s64 	%rd95, %rd8, %rd125;
	shr.u64 	%rd96, %rd95, 3;
	setp.gt.u64 	%p20, %rd96, %rd9;
	add.s64 	%rd99, %rd125, %rd108;
	add.s64 	%rd100, %rd99, -8;
	selp.b64 	%rd125, %rd99, %rd8, %p20;
	selp.b64 	%rd101, %rd100, 0, %p20;
	add.s64 	%rd102, %rd126, %rd9;
	setp.lt.u64 	%p21, %rd102, %rd126;
	setp.gt.u64 	%p22, %rd102, 511;
	add.s64 	%rd103, %rd102, 1;
	selp.b64 	%rd104, 512, %rd103, %p22;
	selp.b64 	%rd126, 512, %rd104, %p21;
	cvt.u16.u64 	%rs21, %rd102;
	and.b16  	%rs22, %rs21, 256;
	xor.b16  	%rs23, %rs22, 768;
	shl.b16 	%rs24, %rs21, 12;
	shr.u16 	%rs25, %rs23, 4;
	or.b16  	%rs26, %rs24, %rs25;
	shl.b16 	%rs27, %rs21, 4;
	and.b16  	%rs28, %rs27, 3840;
	or.b16  	%rs29, %rs28, %rs26;
	and.b16  	%rs30, %rs29, 13107;
	shl.b16 	%rs31, %rs30, 2;
	shr.u16 	%rs32, %rs29, 2;
	and.b16  	%rs33, %rs32, 13107;
	or.b16  	%rs34, %rs33, %rs31;
	and.b16  	%rs35, %rs34, 21845;
	shl.b16 	%rs36, %rs35, 1;
	shr.u16 	%rs37, %rs34, 1;
	and.b16  	%rs38, %rs37, 21845;
	or.b16  	%rs39, %rs38, %rs36;
	shr.u16 	%rs40, %rs39, 4;
	cvt.u32.u16 	%r65, %rs40;
	add.s32 	%r56, %r2, %r65;
	add.s32 	%r55, %r56, -4;
	// begin inline asm
	ld.shared.f32 %r53, [%r55];
ld.shared.f32 %r54, [%r56];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r57, %r54;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r59, %r53, %r48;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r62, %r57, %r48;
	// end inline asm
	st.u32 	[%rd101], %r59;
	st.u32 	[%rd101+4], %r62;
	add.s64 	%rd124, %rd124, -1;
	setp.ne.s64 	%p23, %rd124, 0;
	@%p23 bra 	$L__BB20_31;
$L__BB20_32:
	ret;
$L__BB20_16:
	mov.u64 	%rd81, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd82, %rd81;
	mov.u64 	%rd83, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd84, %rd83;
	{ // callseq 72, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd82;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd84;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 72
$L__BB20_1:
	mov.u64 	%rd105, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_9;
	cvta.global.u64 	%rd106, %rd105;
	{ // callseq 74, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd106;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 74

}
	// .globl	fft_backward_shifted_1024_kernel
.visible .entry fft_backward_shifted_1024_kernel(
	.param .u64 fft_backward_shifted_1024_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot21[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<43>;
	.reg .b32 	%r<59>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<79>;

	mov.u64 	%SPL, __local_depot21;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd33, [fft_backward_shifted_1024_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd33;
	add.u64 	%rd35, %SP, 16;
	add.u64 	%rd36, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[8192];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd36], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd37, [%rd1+8];
	and.b64  	%rd38, %rd37, -1024;
	mul.wide.u32 	%rd6, %r5, 1024;
	setp.lt.u64 	%p2, %rd6, %rd38;
	sub.s64 	%rd39, %rd37, %rd6;
	setp.gt.u64 	%p3, %rd39, 1023;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB21_2;
	bra.uni 	$L__BB21_1;
$L__BB21_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r12, %rd3;
	ld.global.nc.u64 	%rd7, [%rd1];
	shl.b64 	%rd40, %rd6, 3;
	add.s64 	%rd41, %rd7, %rd40;
	add.s64 	%rd9, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 1023;
	cvt.u16.u64 	%rs3, %rd4;
	shl.b64 	%rd42, %rd3, 3;
	add.s64 	%rd10, %rd41, %rd42;
	shl.b32 	%r13, %r12, 3;
	ld.u32 	%r7, [%rd10+4];
	ld.u32 	%r10, [%rd10];
	// begin inline asm
	neg.ftz.f32 %r6, %r7;
	// end inline asm
	add.s32 	%r8, %r2, %r13;
	add.s32 	%r9, %r8, 4;
	// begin inline asm
	st.shared.f32 [%r8], %r10;
st.shared.f32 [%r9], %r6;
	// end inline asm
	setp.lt.u16 	%p5, %rs2, %rs3;
	add.s64 	%rd78, %rd3, 1;
	cvt.u32.u64 	%r56, %rd4;
	shl.b64 	%rd71, %rd4, 3;
	@%p5 bra 	$L__BB21_7;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u16 	%r14, %rs2;
	div.u32 	%r16, %r14, %r56;
	cvt.u64.u32 	%rd74, %r16;
	shl.b64 	%rd43, %rd5, 13;
	add.s64 	%rd44, %rd43, %rd71;
	add.s64 	%rd46, %rd44, %rd42;
	add.s64 	%rd73, %rd7, %rd46;
	xor.b64  	%rd72, %rd42, 8184;
	mov.u64 	%rd75, %rd78;
$L__BB21_4:
	shr.u64 	%rd47, %rd72, 3;
	setp.gt.u64 	%p6, %rd47, %rd9;
	selp.b64 	%rd48, %rd73, 0, %p6;
	add.s64 	%rd20, %rd75, %rd9;
	ld.f32 	%f1, [%rd48];
	ld.u32 	%r18, [%rd48+4];
	// begin inline asm
	neg.ftz.f32 %r17, %r18;
	// end inline asm
	setp.lt.u64 	%p7, %rd20, 1024;
	@%p7 bra 	$L__BB21_6;
	bra.uni 	$L__BB21_5;
$L__BB21_6:
	setp.lt.u64 	%p1, %rd20, %rd75;
	mov.b32 	%f2, %r17;
	add.s64 	%rd53, %rd75, %rd4;
	selp.b64 	%rd75, 1024, %rd53, %p1;
	cvt.u32.u64 	%r23, %rd20;
	shl.b32 	%r24, %r23, 3;
	add.s32 	%r19, %r24, %r2;
	add.s32 	%r20, %r19, 4;
	mov.b32 	%r21, %f1;
	// begin inline asm
	st.shared.f32 [%r19], %r21;
st.shared.f32 [%r20], %r17;
	// end inline asm
	add.s64 	%rd74, %rd74, -1;
	add.s64 	%rd73, %rd73, %rd71;
	sub.s64 	%rd72, %rd72, %rd71;
	setp.ne.s64 	%p8, %rd74, 0;
	@%p8 bra 	$L__BB21_4;
$L__BB21_7:
	add.u64 	%rd34, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd54, [%rd1+16];
	ld.global.nc.u64 	%rd55, [%rd1+24];
	st.local.u64 	[%rd2], %rd54;
	st.local.u64 	[%rd2+8], %rd55;
	bar.sync 	0;
	{ // callseq 76, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd34;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd35;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 76
	mov.b32 	%r26, 1149239296;
	// begin inline asm
	rcp.approx.ftz.f32 %r35, %r26;
	// end inline asm
	bar.sync 	0;
	xor.b64  	%rd58, %rd3, 1023;
	xor.b16  	%rs4, %rs1, 1536;
	shl.b16 	%rs5, %rs1, 12;
	shr.u16 	%rs6, %rs4, 4;
	and.b16  	%rs7, %rs6, 112;
	or.b16  	%rs8, %rs5, %rs7;
	shl.b16 	%rs9, %rs1, 4;
	and.b16  	%rs10, %rs9, 3840;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 13107;
	shl.b16 	%rs13, %rs12, 2;
	shr.u16 	%rs14, %rs11, 2;
	and.b16  	%rs15, %rs14, 13107;
	or.b16  	%rs16, %rs15, %rs13;
	and.b16  	%rs17, %rs16, 21845;
	shl.b16 	%rs18, %rs17, 1;
	shr.u16 	%rs19, %rs16, 1;
	and.b16  	%rs20, %rs19, 21845;
	or.b16  	%rs21, %rs20, %rs18;
	shr.u16 	%rs22, %rs21, 3;
	cvt.u32.u16 	%r42, %rs22;
	add.s32 	%r30, %r2, %r42;
	add.s32 	%r29, %r30, -4;
	// begin inline asm
	ld.shared.f32 %r27, [%r29];
ld.shared.f32 %r28, [%r30];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r31, %r28;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r33, %r27, %r35;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r36, %r31, %r35;
	// end inline asm
	st.u32 	[%rd10], %r33;
	st.u32 	[%rd10+4], %r36;
	setp.lt.u64 	%p9, %rd58, %rd4;
	@%p9 bra 	$L__BB21_10;
	add.s64 	%rd8, %rd41, 8192;
	mov.b32 	%f3, %r35;
	cvt.u32.u64 	%r39, %rd58;
	div.u32 	%r41, %r39, %r56;
	cvt.u64.u32 	%rd76, %r41;
	add.s64 	%rd77, %rd10, 8;
$L__BB21_9:
	sub.s64 	%rd59, %rd8, %rd77;
	shr.u64 	%rd60, %rd59, 3;
	setp.gt.u64 	%p10, %rd60, %rd9;
	add.s64 	%rd63, %rd77, %rd71;
	add.s64 	%rd64, %rd63, -8;
	selp.b64 	%rd77, %rd63, %rd8, %p10;
	selp.b64 	%rd65, %rd64, 0, %p10;
	add.s64 	%rd66, %rd78, %rd9;
	setp.lt.u64 	%p11, %rd66, %rd78;
	setp.gt.u64 	%p12, %rd66, 1023;
	add.s64 	%rd67, %rd66, 1;
	selp.b64 	%rd68, 1024, %rd67, %p12;
	selp.b64 	%rd78, 1024, %rd68, %p11;
	cvt.u16.u64 	%rs23, %rd66;
	and.b16  	%rs24, %rs23, 768;
	xor.b16  	%rs25, %rs24, 1536;
	shl.b16 	%rs26, %rs23, 12;
	shr.u16 	%rs27, %rs25, 4;
	or.b16  	%rs28, %rs26, %rs27;
	shl.b16 	%rs29, %rs23, 4;
	and.b16  	%rs30, %rs29, 3840;
	or.b16  	%rs31, %rs30, %rs28;
	and.b16  	%rs32, %rs31, 13107;
	shl.b16 	%rs33, %rs32, 2;
	shr.u16 	%rs34, %rs31, 2;
	and.b16  	%rs35, %rs34, 13107;
	or.b16  	%rs36, %rs35, %rs33;
	and.b16  	%rs37, %rs36, 21845;
	shl.b16 	%rs38, %rs37, 1;
	shr.u16 	%rs39, %rs36, 1;
	and.b16  	%rs40, %rs39, 21845;
	or.b16  	%rs41, %rs40, %rs38;
	shr.u16 	%rs42, %rs41, 3;
	cvt.u32.u16 	%r55, %rs42;
	add.s32 	%r46, %r2, %r55;
	add.s32 	%r45, %r46, -4;
	// begin inline asm
	ld.shared.f32 %r43, [%r45];
ld.shared.f32 %r44, [%r46];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r47, %r44;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r43, %r35;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r52, %r47, %r35;
	// end inline asm
	st.u32 	[%rd65], %r49;
	st.u32 	[%rd65+4], %r52;
	add.s64 	%rd76, %rd76, -1;
	setp.ne.s64 	%p13, %rd76, 0;
	@%p13 bra 	$L__BB21_9;
$L__BB21_10:
	ret;
$L__BB21_5:
	mov.u64 	%rd49, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd50, %rd49;
	mov.u64 	%rd51, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd52, %rd51;
	{ // callseq 75, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd50;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd52;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 75
$L__BB21_1:
	mov.u64 	%rd69, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_9;
	cvta.global.u64 	%rd70, %rd69;
	{ // callseq 77, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd70;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 77

}
	// .globl	fft_backward_shifted_2048_kernel
.visible .entry fft_backward_shifted_2048_kernel(
	.param .u64 fft_backward_shifted_2048_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot22[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<42>;
	.reg .b32 	%r<59>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<76>;

	mov.u64 	%SPL, __local_depot22;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd32, [fft_backward_shifted_2048_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd32;
	add.u64 	%rd34, %SP, 16;
	add.u64 	%rd35, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[16384];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd35], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd36, [%rd1+8];
	and.b64  	%rd37, %rd36, -2048;
	mul.wide.u32 	%rd6, %r5, 2048;
	setp.lt.u64 	%p2, %rd6, %rd37;
	sub.s64 	%rd38, %rd36, %rd6;
	setp.gt.u64 	%p3, %rd38, 2047;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB22_2;
	bra.uni 	$L__BB22_1;
$L__BB22_2:
	add.u64 	%rd33, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r12, %rd3;
	ld.global.nc.u64 	%rd39, [%rd1];
	shl.b64 	%rd40, %rd6, 3;
	add.s64 	%rd41, %rd39, %rd40;
	add.s64 	%rd7, %rd41, 16384;
	add.s64 	%rd8, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 2047;
	shl.b64 	%rd42, %rd3, 3;
	add.s64 	%rd9, %rd41, %rd42;
	shl.b32 	%r13, %r12, 3;
	ld.u32 	%r7, [%rd9+4];
	ld.u32 	%r10, [%rd9];
	// begin inline asm
	neg.ftz.f32 %r11, %r7;
	// end inline asm
	add.s32 	%r8, %r2, %r13;
	add.s32 	%r9, %r8, 4;
	// begin inline asm
	st.shared.f32 [%r8], %r10;
st.shared.f32 [%r9], %r11;
	// end inline asm
	add.s64 	%rd75, %rd3, 1;
	add.s64 	%rd74, %rd9, 8;
	cvt.u32.u16 	%r14, %rs2;
	cvt.u32.u64 	%r15, %rd4;
	div.u32 	%r16, %r14, %r15;
	cvt.u64.u32 	%rd71, %r16;
	shl.b64 	%rd43, %rd5, 14;
	shl.b64 	%rd13, %rd4, 3;
	add.s64 	%rd44, %rd43, %rd13;
	add.s64 	%rd45, %rd44, %rd42;
	add.s64 	%rd70, %rd39, %rd45;
	xor.b64  	%rd69, %rd42, 16376;
	mov.u64 	%rd72, %rd75;
$L__BB22_3:
	shr.u64 	%rd46, %rd69, 3;
	setp.gt.u64 	%p5, %rd46, %rd8;
	selp.b64 	%rd47, %rd70, 0, %p5;
	add.s64 	%rd20, %rd72, %rd8;
	ld.f32 	%f1, [%rd47];
	ld.u32 	%r18, [%rd47+4];
	// begin inline asm
	neg.ftz.f32 %r17, %r18;
	// end inline asm
	setp.lt.u64 	%p6, %rd20, 2048;
	@%p6 bra 	$L__BB22_5;
	bra.uni 	$L__BB22_4;
$L__BB22_5:
	setp.lt.u64 	%p1, %rd20, %rd72;
	mov.b32 	%f2, %r17;
	add.s64 	%rd52, %rd72, %rd4;
	selp.b64 	%rd72, 2048, %rd52, %p1;
	cvt.u32.u64 	%r23, %rd20;
	shl.b32 	%r24, %r23, 3;
	add.s32 	%r19, %r24, %r2;
	add.s32 	%r20, %r19, 4;
	mov.b32 	%r21, %f1;
	// begin inline asm
	st.shared.f32 [%r19], %r21;
st.shared.f32 [%r20], %r17;
	// end inline asm
	add.s64 	%rd71, %rd71, -1;
	add.s64 	%rd70, %rd70, %rd13;
	sub.s64 	%rd69, %rd69, %rd13;
	setp.ne.s64 	%p7, %rd71, 0;
	@%p7 bra 	$L__BB22_3;
	ld.global.nc.u64 	%rd53, [%rd1+16];
	ld.global.nc.u64 	%rd54, [%rd1+24];
	st.local.u64 	[%rd2], %rd53;
	st.local.u64 	[%rd2+8], %rd54;
	bar.sync 	0;
	{ // callseq 79, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd33;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd34;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 79
	mov.b32 	%r26, 1157627904;
	// begin inline asm
	rcp.approx.ftz.f32 %r35, %r26;
	// end inline asm
	mov.b32 	%f3, %r35;
	bar.sync 	0;
	xor.b32  	%r40, %r12, 2047;
	div.u32 	%r42, %r40, %r15;
	cvt.u64.u32 	%rd73, %r42;
	or.b16  	%rs3, %rs1, 3072;
	shl.b16 	%rs4, %rs1, 12;
	shr.u16 	%rs5, %rs3, 4;
	and.b16  	%rs6, %rs5, 240;
	or.b16  	%rs7, %rs4, %rs6;
	shl.b16 	%rs8, %rs1, 4;
	and.b16  	%rs9, %rs8, 3840;
	or.b16  	%rs10, %rs9, %rs7;
	and.b16  	%rs11, %rs10, 13107;
	shl.b16 	%rs12, %rs11, 2;
	shr.u16 	%rs13, %rs10, 2;
	and.b16  	%rs14, %rs13, 13107;
	or.b16  	%rs15, %rs14, %rs12;
	and.b16  	%rs16, %rs15, 21845;
	shl.b16 	%rs17, %rs16, 1;
	shr.u16 	%rs18, %rs15, 1;
	and.b16  	%rs19, %rs18, 21845;
	or.b16  	%rs20, %rs19, %rs17;
	shr.u16 	%rs21, %rs20, 2;
	cvt.u32.u16 	%r43, %rs21;
	add.s32 	%r30, %r2, %r43;
	add.s32 	%r29, %r30, -4;
	// begin inline asm
	ld.shared.f32 %r34, [%r29];
ld.shared.f32 %r32, [%r30];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r37, %r32;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r33, %r34, %r35;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r36, %r37, %r35;
	// end inline asm
	st.u32 	[%rd9], %r33;
	st.u32 	[%rd9+4], %r36;
$L__BB22_7:
	sub.s64 	%rd57, %rd7, %rd74;
	shr.u64 	%rd58, %rd57, 3;
	setp.gt.u64 	%p8, %rd58, %rd8;
	add.s64 	%rd61, %rd74, %rd13;
	add.s64 	%rd62, %rd61, -8;
	selp.b64 	%rd74, %rd61, %rd7, %p8;
	selp.b64 	%rd63, %rd62, 0, %p8;
	add.s64 	%rd64, %rd75, %rd8;
	setp.lt.u64 	%p9, %rd64, %rd75;
	setp.gt.u64 	%p10, %rd64, 2047;
	add.s64 	%rd65, %rd64, 1;
	selp.b64 	%rd66, 2048, %rd65, %p10;
	selp.b64 	%rd75, 2048, %rd66, %p9;
	cvt.u16.u64 	%rs22, %rd64;
	and.b16  	%rs23, %rs22, 1792;
	xor.b16  	%rs24, %rs23, 3072;
	shl.b16 	%rs25, %rs22, 12;
	shr.u16 	%rs26, %rs24, 4;
	or.b16  	%rs27, %rs25, %rs26;
	shl.b16 	%rs28, %rs22, 4;
	and.b16  	%rs29, %rs28, 3840;
	or.b16  	%rs30, %rs29, %rs27;
	and.b16  	%rs31, %rs30, 13107;
	shl.b16 	%rs32, %rs31, 2;
	shr.u16 	%rs33, %rs30, 2;
	and.b16  	%rs34, %rs33, 13107;
	or.b16  	%rs35, %rs34, %rs32;
	and.b16  	%rs36, %rs35, 21845;
	shl.b16 	%rs37, %rs36, 1;
	shr.u16 	%rs38, %rs35, 1;
	and.b16  	%rs39, %rs38, 21845;
	or.b16  	%rs40, %rs39, %rs37;
	shr.u16 	%rs41, %rs40, 2;
	cvt.u32.u16 	%r56, %rs41;
	add.s32 	%r47, %r2, %r56;
	add.s32 	%r46, %r47, -4;
	// begin inline asm
	ld.shared.f32 %r44, [%r46];
ld.shared.f32 %r45, [%r47];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r48, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r50, %r44, %r35;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r53, %r48, %r35;
	// end inline asm
	st.u32 	[%rd63], %r50;
	st.u32 	[%rd63+4], %r53;
	add.s64 	%rd73, %rd73, -1;
	setp.ne.s64 	%p11, %rd73, 0;
	@%p11 bra 	$L__BB22_7;
	ret;
$L__BB22_4:
	mov.u64 	%rd48, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd49, %rd48;
	mov.u64 	%rd50, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd51, %rd50;
	{ // callseq 78, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd49;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd51;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 78
$L__BB22_1:
	mov.u64 	%rd67, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_9;
	cvta.global.u64 	%rd68, %rd67;
	{ // callseq 80, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd68;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 80

}
	// .globl	fft_backward_shifted_4096_kernel
.visible .entry fft_backward_shifted_4096_kernel(
	.param .u64 fft_backward_shifted_4096_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot23[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<45>;
	.reg .b32 	%r<59>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<76>;

	mov.u64 	%SPL, __local_depot23;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd32, [fft_backward_shifted_4096_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd32;
	add.u64 	%rd34, %SP, 16;
	add.u64 	%rd35, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[32768];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd35], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd36, [%rd1+8];
	and.b64  	%rd37, %rd36, -4096;
	mul.wide.u32 	%rd6, %r5, 4096;
	setp.lt.u64 	%p2, %rd6, %rd37;
	sub.s64 	%rd38, %rd36, %rd6;
	setp.gt.u64 	%p3, %rd38, 4095;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB23_2;
	bra.uni 	$L__BB23_1;
$L__BB23_2:
	add.u64 	%rd33, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r12, %rd3;
	ld.global.nc.u64 	%rd39, [%rd1];
	shl.b64 	%rd40, %rd6, 3;
	add.s64 	%rd41, %rd39, %rd40;
	add.s64 	%rd7, %rd41, 32768;
	add.s64 	%rd8, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 4095;
	shl.b64 	%rd42, %rd3, 3;
	add.s64 	%rd9, %rd41, %rd42;
	add.s64 	%rd74, %rd9, 8;
	add.s64 	%rd75, %rd3, 1;
	shl.b32 	%r13, %r12, 3;
	ld.u32 	%r7, [%rd9+4];
	ld.u32 	%r10, [%rd9];
	// begin inline asm
	neg.ftz.f32 %r11, %r7;
	// end inline asm
	add.s32 	%r8, %r2, %r13;
	add.s32 	%r9, %r8, 4;
	// begin inline asm
	st.shared.f32 [%r8], %r10;
st.shared.f32 [%r9], %r11;
	// end inline asm
	cvt.u32.u16 	%r14, %rs2;
	cvt.u32.u64 	%r15, %rd4;
	div.u32 	%r16, %r14, %r15;
	cvt.u64.u32 	%rd71, %r16;
	shl.b64 	%rd43, %rd5, 15;
	shl.b64 	%rd13, %rd4, 3;
	add.s64 	%rd44, %rd43, %rd13;
	add.s64 	%rd45, %rd44, %rd42;
	add.s64 	%rd70, %rd39, %rd45;
	xor.b64  	%rd69, %rd42, 32760;
	mov.u64 	%rd72, %rd75;
$L__BB23_3:
	shr.u64 	%rd46, %rd69, 3;
	setp.gt.u64 	%p5, %rd46, %rd8;
	selp.b64 	%rd47, %rd70, 0, %p5;
	add.s64 	%rd20, %rd72, %rd8;
	ld.f32 	%f1, [%rd47];
	ld.u32 	%r18, [%rd47+4];
	// begin inline asm
	neg.ftz.f32 %r17, %r18;
	// end inline asm
	setp.lt.u64 	%p6, %rd20, 4096;
	@%p6 bra 	$L__BB23_5;
	bra.uni 	$L__BB23_4;
$L__BB23_5:
	setp.lt.u64 	%p1, %rd20, %rd72;
	mov.b32 	%f2, %r17;
	add.s64 	%rd52, %rd72, %rd4;
	selp.b64 	%rd72, 4096, %rd52, %p1;
	cvt.u32.u64 	%r23, %rd20;
	shl.b32 	%r24, %r23, 3;
	add.s32 	%r19, %r24, %r2;
	add.s32 	%r20, %r19, 4;
	mov.b32 	%r21, %f1;
	// begin inline asm
	st.shared.f32 [%r19], %r21;
st.shared.f32 [%r20], %r17;
	// end inline asm
	add.s64 	%rd71, %rd71, -1;
	add.s64 	%rd70, %rd70, %rd13;
	sub.s64 	%rd69, %rd69, %rd13;
	setp.ne.s64 	%p7, %rd71, 0;
	@%p7 bra 	$L__BB23_3;
	ld.global.nc.u64 	%rd53, [%rd1+16];
	ld.global.nc.u64 	%rd54, [%rd1+24];
	st.local.u64 	[%rd2], %rd53;
	st.local.u64 	[%rd2+8], %rd54;
	bar.sync 	0;
	{ // callseq 82, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd33;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd34;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 82
	mov.b32 	%r26, 1166016512;
	// begin inline asm
	rcp.approx.ftz.f32 %r35, %r26;
	// end inline asm
	mov.b32 	%f3, %r35;
	bar.sync 	0;
	xor.b32  	%r40, %r12, 4095;
	div.u32 	%r42, %r40, %r15;
	cvt.u64.u32 	%rd73, %r42;
	or.b16  	%rs3, %rs1, 6144;
	shl.b16 	%rs4, %rs1, 8;
	shr.u16 	%rs5, %rs3, 8;
	or.b16  	%rs6, %rs4, %rs5;
	and.b16  	%rs7, %rs6, 3851;
	shl.b16 	%rs8, %rs7, 4;
	shr.u16 	%rs9, %rs6, 4;
	and.b16  	%rs10, %rs9, 3841;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 13107;
	shl.b16 	%rs13, %rs12, 2;
	shr.u16 	%rs14, %rs11, 2;
	and.b16  	%rs15, %rs14, 13107;
	or.b16  	%rs16, %rs15, %rs13;
	and.b16  	%rs17, %rs16, 21845;
	shl.b16 	%rs18, %rs17, 1;
	shr.u16 	%rs19, %rs16, 1;
	and.b16  	%rs20, %rs19, 21845;
	or.b16  	%rs21, %rs20, %rs18;
	shr.u16 	%rs22, %rs21, 1;
	cvt.u32.u16 	%r43, %rs22;
	add.s32 	%r30, %r2, %r43;
	add.s32 	%r29, %r30, -4;
	// begin inline asm
	ld.shared.f32 %r34, [%r29];
ld.shared.f32 %r32, [%r30];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r37, %r32;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r33, %r34, %r35;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r36, %r37, %r35;
	// end inline asm
	st.u32 	[%rd9], %r33;
	st.u32 	[%rd9+4], %r36;
$L__BB23_7:
	sub.s64 	%rd57, %rd7, %rd74;
	shr.u64 	%rd58, %rd57, 3;
	setp.gt.u64 	%p8, %rd58, %rd8;
	add.s64 	%rd61, %rd74, %rd13;
	add.s64 	%rd62, %rd61, -8;
	selp.b64 	%rd74, %rd61, %rd7, %p8;
	selp.b64 	%rd63, %rd62, 0, %p8;
	add.s64 	%rd64, %rd75, %rd8;
	setp.lt.u64 	%p9, %rd64, %rd75;
	setp.gt.u64 	%p10, %rd64, 4095;
	add.s64 	%rd65, %rd64, 1;
	selp.b64 	%rd66, 4096, %rd65, %p10;
	selp.b64 	%rd75, 4096, %rd66, %p9;
	cvt.u16.u64 	%rs23, %rd64;
	and.b16  	%rs24, %rs23, 3840;
	xor.b16  	%rs25, %rs24, 6144;
	shl.b16 	%rs26, %rs23, 8;
	shr.u16 	%rs27, %rs25, 8;
	or.b16  	%rs28, %rs26, %rs27;
	and.b16  	%rs29, %rs28, 3855;
	shl.b16 	%rs30, %rs29, 4;
	shr.u16 	%rs31, %rs28, 4;
	and.b16  	%rs32, %rs31, 3841;
	or.b16  	%rs33, %rs32, %rs30;
	and.b16  	%rs34, %rs33, 13107;
	shl.b16 	%rs35, %rs34, 2;
	shr.u16 	%rs36, %rs33, 2;
	and.b16  	%rs37, %rs36, 13107;
	or.b16  	%rs38, %rs37, %rs35;
	and.b16  	%rs39, %rs38, 21845;
	shl.b16 	%rs40, %rs39, 1;
	shr.u16 	%rs41, %rs38, 1;
	and.b16  	%rs42, %rs41, 21845;
	or.b16  	%rs43, %rs42, %rs40;
	shr.u16 	%rs44, %rs43, 1;
	cvt.u32.u16 	%r56, %rs44;
	add.s32 	%r47, %r2, %r56;
	add.s32 	%r46, %r47, -4;
	// begin inline asm
	ld.shared.f32 %r44, [%r46];
ld.shared.f32 %r45, [%r47];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r48, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r50, %r44, %r35;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r53, %r48, %r35;
	// end inline asm
	st.u32 	[%rd63], %r50;
	st.u32 	[%rd63+4], %r53;
	add.s64 	%rd73, %rd73, -1;
	setp.ne.s64 	%p11, %rd73, 0;
	@%p11 bra 	$L__BB23_7;
	ret;
$L__BB23_4:
	mov.u64 	%rd48, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd49, %rd48;
	mov.u64 	%rd50, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd51, %rd50;
	{ // callseq 81, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd49;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd51;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 81
$L__BB23_1:
	mov.u64 	%rd67, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_9;
	cvta.global.u64 	%rd68, %rd67;
	{ // callseq 83, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd68;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 83

}
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_3
)
{
	.reg .pred 	%p<63>;
	.reg .b32 	%r<399>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<180>;

	ld.param.u64 	%rd57, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_2];
	setp.eq.s64 	%p8, %rd57, 0;
	@%p8 bra 	$L__BB24_5;
	ld.param.u64 	%rd56, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_1];
	ld.param.u64 	%rd58, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_0];
	add.s64 	%rd1, %rd57, -1;
	setp.gt.u64 	%p9, %rd56, 63;
	ld.param.u64 	%rd59, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_3];
	add.s64 	%rd60, %rd56, 1;
	selp.b64 	%rd61, 64, %rd60, %p9;
	ld.u64 	%rd62, [%rd58];
	ld.u64 	%rd63, [%rd58+8];
	ld.u32 	%r1, [%rd59];
	shl.b64 	%rd64, %rd63, 3;
	add.s64 	%rd2, %rd62, %rd64;
	setp.eq.s64 	%p10, %rd63, 0;
	selp.b64 	%rd3, 0, %rd62, %p10;
	selp.b64 	%rd65, 0, 8, %p10;
	add.s64 	%rd4, %rd62, %rd65;
	add.s64 	%rd66, %rd61, %rd1;
	setp.lt.u64 	%p11, %rd66, %rd61;
	setp.gt.u64 	%p12, %rd66, 63;
	or.pred  	%p1, %p11, %p12;
	mov.u64 	%rd7, 1;
	mov.u64 	%rd8, 7;
	bra.uni 	$L__BB24_2;
$L__BB24_32:
	shl.b64 	%rd7, %rd7, 1;
	bar.sync 	0;
	setp.gt.u64 	%p37, %rd8, 2;
	@%p37 bra 	$L__BB24_2;
	bra.uni 	$L__BB24_33;
$L__BB24_2:
	add.s64 	%rd8, %rd8, -1;
	@%p9 bra 	$L__BB24_32;
	cvt.u32.u64 	%r13, %rd8;
	shr.u64 	%rd69, %rd56, %r13;
	mov.u64 	%rd70, 1;
	shl.b64 	%rd27, %rd70, %r13;
	mov.u64 	%rd71, 2;
	shl.b64 	%rd28, %rd71, %r13;
	add.s64 	%rd29, %rd27, -1;
	and.b64  	%rd172, %rd29, %rd56;
	mul.lo.s64 	%rd72, %rd69, %rd28;
	add.s64 	%rd31, %rd172, %rd72;
	setp.lt.u64 	%p14, %rd31, 128;
	@%p14 bra 	$L__BB24_20;
	bra.uni 	$L__BB24_4;
$L__BB24_20:
	cvt.u32.u64 	%r18, %rd31;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r16, %r1, %r19;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	ld.shared.f32 %r14, [%r16];
ld.shared.f32 %r15, [%r17];
	// end inline asm
	add.s64 	%rd32, %rd31, %rd27;
	setp.lt.u64 	%p15, %rd32, 128;
	@%p15 bra 	$L__BB24_22;
	bra.uni 	$L__BB24_21;
$L__BB24_22:
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r22, %r1, %r25;
	add.s32 	%r23, %r22, 4;
	// begin inline asm
	ld.shared.f32 %r20, [%r22];
ld.shared.f32 %r21, [%r23];
	// end inline asm
	setp.eq.s64 	%p16, %rd172, 0;
	mov.u64 	%rd162, -1;
	mov.u64 	%rd177, %rd3;
	@%p16 bra 	$L__BB24_29;
	mul.lo.s64 	%rd176, %rd172, %rd7;
	mul.hi.u64 	%rd81, %rd172, %rd7;
	setp.eq.s64 	%p17, %rd81, 0;
	mov.u64 	%rd174, %rd4;
	@%p17 bra 	$L__BB24_28;
	mov.u64 	%rd173, %rd7;
	mov.u64 	%rd174, %rd4;
$L__BB24_25:
	setp.eq.s64 	%p18, %rd172, 0;
	@%p18 bra 	$L__BB24_55;
	setp.eq.s64 	%p19, %rd173, 0;
	@%p19 bra 	$L__BB24_56;
	div.u64 	%rd83, %rd162, %rd172;
	div.u64 	%rd84, %rd162, %rd173;
	mul.lo.s64 	%rd85, %rd83, %rd172;
	mul.lo.s64 	%rd86, %rd84, %rd173;
	setp.gt.u64 	%p20, %rd85, %rd86;
	max.u64 	%rd87, %rd85, %rd86;
	selp.b64 	%rd88, %rd83, 0, %p20;
	sub.s64 	%rd173, %rd173, %rd88;
	selp.b64 	%rd89, 0, %rd84, %p20;
	sub.s64 	%rd172, %rd172, %rd89;
	add.s64 	%rd90, %rd87, -1;
	sub.s64 	%rd91, %rd2, %rd174;
	shr.u64 	%rd92, %rd91, 3;
	setp.gt.u64 	%p21, %rd92, %rd90;
	shl.b64 	%rd93, %rd87, 3;
	add.s64 	%rd94, %rd174, %rd93;
	selp.b64 	%rd174, %rd94, %rd2, %p21;
	mul.lo.s64 	%rd176, %rd172, %rd173;
	mul.hi.u64 	%rd95, %rd172, %rd173;
	setp.ne.s64 	%p6, %rd95, 0;
	@%p6 bra 	$L__BB24_25;
$L__BB24_28:
	add.s64 	%rd96, %rd176, -1;
	sub.s64 	%rd97, %rd2, %rd174;
	shr.u64 	%rd98, %rd97, 3;
	setp.gt.u64 	%p22, %rd98, %rd96;
	shl.b64 	%rd99, %rd176, 3;
	add.s64 	%rd100, %rd174, %rd99;
	add.s64 	%rd101, %rd100, -8;
	selp.b64 	%rd177, %rd101, 0, %p22;
$L__BB24_29:
	setp.ne.s64 	%p23, %rd177, 0;
	@%p23 bra 	$L__BB24_31;
	bra.uni 	$L__BB24_30;
$L__BB24_31:
	mov.b32 	%f6, %r15;
	mov.b32 	%f5, %r14;
	mov.b32 	%f8, %r21;
	mov.b32 	%f7, %r20;
	// begin inline asm
	add.rn.ftz.f32 %r26, %r14, %r20;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r29, %r15, %r21;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r16], %r26;
st.shared.f32 [%r17], %r29;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r43, %r14, %r20;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r46, %r15, %r21;
	// end inline asm
	ld.u32 	%r56, [%rd177];
	ld.u32 	%r53, [%rd177+4];
	// begin inline asm
	mul.rn.ftz.f32 %r42, %r43, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r46, %r53;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r48, %r42, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r51, %r43, %r53;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r54, %r46, %r56;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r57, %r51, %r54;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r22], %r48;
st.shared.f32 [%r23], %r57;
	// end inline asm
	mov.u64 	%rd9, %rd66;
	@%p1 bra 	$L__BB24_32;
$L__BB24_7:
	shr.u64 	%rd106, %rd9, %r13;
	and.b64  	%rd166, %rd9, %rd29;
	mul.lo.s64 	%rd107, %rd106, %rd28;
	add.s64 	%rd12, %rd107, %rd166;
	setp.lt.u64 	%p24, %rd12, 128;
	@%p24 bra 	$L__BB24_9;
	bra.uni 	$L__BB24_8;
$L__BB24_9:
	cvt.u32.u64 	%r69, %rd12;
	shl.b32 	%r70, %r69, 3;
	add.s32 	%r67, %r70, %r1;
	add.s32 	%r68, %r67, 4;
	// begin inline asm
	ld.shared.f32 %r65, [%r67];
ld.shared.f32 %r66, [%r68];
	// end inline asm
	add.s64 	%rd13, %rd12, %rd27;
	setp.lt.u64 	%p25, %rd13, 128;
	@%p25 bra 	$L__BB24_11;
	bra.uni 	$L__BB24_10;
$L__BB24_11:
	cvt.u32.u64 	%r75, %rd13;
	shl.b32 	%r76, %r75, 3;
	add.s32 	%r73, %r76, %r1;
	add.s32 	%r74, %r73, 4;
	// begin inline asm
	ld.shared.f32 %r71, [%r73];
ld.shared.f32 %r72, [%r74];
	// end inline asm
	setp.eq.s64 	%p26, %rd166, 0;
	mov.u64 	%rd171, %rd3;
	@%p26 bra 	$L__BB24_18;
	mul.lo.s64 	%rd170, %rd166, %rd7;
	mul.hi.u64 	%rd116, %rd166, %rd7;
	setp.eq.s64 	%p27, %rd116, 0;
	mov.u64 	%rd168, %rd4;
	@%p27 bra 	$L__BB24_17;
	mov.u64 	%rd167, %rd7;
	mov.u64 	%rd168, %rd4;
$L__BB24_14:
	setp.eq.s64 	%p28, %rd166, 0;
	@%p28 bra 	$L__BB24_53;
	setp.eq.s64 	%p29, %rd167, 0;
	@%p29 bra 	$L__BB24_54;
	div.u64 	%rd118, %rd162, %rd166;
	div.u64 	%rd119, %rd162, %rd167;
	mul.lo.s64 	%rd120, %rd118, %rd166;
	mul.lo.s64 	%rd121, %rd119, %rd167;
	setp.gt.u64 	%p30, %rd120, %rd121;
	max.u64 	%rd122, %rd120, %rd121;
	selp.b64 	%rd123, %rd118, 0, %p30;
	sub.s64 	%rd167, %rd167, %rd123;
	selp.b64 	%rd124, 0, %rd119, %p30;
	sub.s64 	%rd166, %rd166, %rd124;
	add.s64 	%rd125, %rd122, -1;
	sub.s64 	%rd126, %rd2, %rd168;
	shr.u64 	%rd127, %rd126, 3;
	setp.gt.u64 	%p31, %rd127, %rd125;
	shl.b64 	%rd128, %rd122, 3;
	add.s64 	%rd129, %rd168, %rd128;
	selp.b64 	%rd168, %rd129, %rd2, %p31;
	mul.lo.s64 	%rd170, %rd166, %rd167;
	mul.hi.u64 	%rd130, %rd166, %rd167;
	setp.ne.s64 	%p3, %rd130, 0;
	@%p3 bra 	$L__BB24_14;
$L__BB24_17:
	add.s64 	%rd131, %rd170, -1;
	sub.s64 	%rd132, %rd2, %rd168;
	shr.u64 	%rd133, %rd132, 3;
	setp.gt.u64 	%p32, %rd133, %rd131;
	shl.b64 	%rd134, %rd170, 3;
	add.s64 	%rd135, %rd168, %rd134;
	add.s64 	%rd136, %rd135, -8;
	selp.b64 	%rd171, %rd136, 0, %p32;
$L__BB24_18:
	setp.ne.s64 	%p33, %rd171, 0;
	@%p33 bra 	$L__BB24_6;
	bra.uni 	$L__BB24_19;
$L__BB24_6:
	add.s64 	%rd10, %rd9, 1;
	mov.b32 	%f2, %r66;
	mov.b32 	%f1, %r65;
	mov.b32 	%f4, %r72;
	mov.b32 	%f3, %r71;
	// begin inline asm
	add.rn.ftz.f32 %r77, %r65, %r71;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r66, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r67], %r77;
st.shared.f32 [%r68], %r80;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r94, %r65, %r71;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r97, %r66, %r72;
	// end inline asm
	ld.u32 	%r107, [%rd171];
	ld.u32 	%r104, [%rd171+4];
	// begin inline asm
	mul.rn.ftz.f32 %r93, %r94, %r107;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r96, %r97, %r104;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r99, %r93, %r96;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r102, %r94, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r105, %r97, %r107;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r108, %r102, %r105;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r73], %r99;
st.shared.f32 [%r74], %r108;
	// end inline asm
	add.s64 	%rd141, %rd10, %rd1;
	setp.lt.u64 	%p34, %rd141, %rd10;
	add.s64 	%rd9, %rd9, %rd57;
	setp.gt.u64 	%p35, %rd9, 63;
	or.pred  	%p36, %p34, %p35;
	@%p36 bra 	$L__BB24_32;
	bra.uni 	$L__BB24_7;
$L__BB24_33:
	@%p9 bra 	$L__BB24_34;
	and.b64  	%rd146, %rd56, 1;
	setp.eq.b64 	%p39, %rd146, 1;
	mov.pred 	%p40, 0;
	xor.pred  	%p41, %p39, %p40;
	cvt.u32.u64 	%r115, %rd56;
	shl.b32 	%r12, %r115, 4;
	@%p41 bra 	$L__BB24_36;
	add.s32 	%r151, %r1, %r12;
	add.s32 	%r152, %r151, 4;
	// begin inline asm
	ld.shared.f32 %r158, [%r151];
ld.shared.f32 %r161, [%r152];
	// end inline asm
	add.s32 	%r173, %r151, 16;
	add.s32 	%r174, %r151, 20;
	// begin inline asm
	ld.shared.f32 %r159, [%r173];
ld.shared.f32 %r162, [%r174];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r165, %r158, %r159;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r166, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r151], %r165;
st.shared.f32 [%r152], %r166;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r175, %r158, %r159;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r176, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r173], %r175;
st.shared.f32 [%r174], %r176;
	// end inline asm
	bra.uni 	$L__BB24_37;
$L__BB24_36:
	and.b32  	%r146, %r12, 992;
	add.s32 	%r147, %r1, %r146;
	add.s32 	%r130, %r147, 8;
	add.s32 	%r131, %r147, 12;
	// begin inline asm
	ld.shared.f32 %r125, [%r130];
ld.shared.f32 %r128, [%r131];
	// end inline asm
	or.b32  	%r148, %r12, 24;
	add.s32 	%r122, %r1, %r148;
	add.s32 	%r123, %r122, 4;
	// begin inline asm
	ld.shared.f32 %r126, [%r122];
ld.shared.f32 %r129, [%r123];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r132, %r125, %r126;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r133, %r128, %r129;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r130], %r132;
st.shared.f32 [%r131], %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r125, %r126;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r128, %r129;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r145, %r141;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r122], %r144;
st.shared.f32 [%r123], %r145;
	// end inline asm
$L__BB24_37:
	@%p1 bra 	$L__BB24_34;
	add.s64 	%rd46, %rd66, 1;
	and.b64  	%rd147, %rd66, 1;
	setp.eq.b64 	%p42, %rd147, 1;
	xor.pred  	%p44, %p42, %p40;
	not.pred 	%p45, %p44;
	cvt.u32.u64 	%r177, %rd66;
	shl.b32 	%r10, %r177, 4;
	@%p45 bra 	$L__BB24_40;
	bra.uni 	$L__BB24_39;
$L__BB24_40:
	add.s32 	%r213, %r1, %r10;
	add.s32 	%r214, %r213, 4;
	// begin inline asm
	ld.shared.f32 %r220, [%r213];
ld.shared.f32 %r223, [%r214];
	// end inline asm
	add.s32 	%r235, %r213, 16;
	add.s32 	%r236, %r213, 20;
	// begin inline asm
	ld.shared.f32 %r221, [%r235];
ld.shared.f32 %r224, [%r236];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r227, %r220, %r221;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r228, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r213], %r227;
st.shared.f32 [%r214], %r228;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r237, %r220, %r221;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r238, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r235], %r237;
st.shared.f32 [%r236], %r238;
	// end inline asm
	bra.uni 	$L__BB24_41;
$L__BB24_39:
	and.b32  	%r208, %r10, 992;
	add.s32 	%r209, %r1, %r208;
	add.s32 	%r192, %r209, 8;
	add.s32 	%r193, %r209, 12;
	// begin inline asm
	ld.shared.f32 %r187, [%r192];
ld.shared.f32 %r190, [%r193];
	// end inline asm
	or.b32  	%r210, %r10, 24;
	add.s32 	%r184, %r1, %r210;
	add.s32 	%r185, %r184, 4;
	// begin inline asm
	ld.shared.f32 %r188, [%r184];
ld.shared.f32 %r191, [%r185];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r194, %r187, %r188;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r195, %r190, %r191;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r192], %r194;
st.shared.f32 [%r193], %r195;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r203, %r187, %r188;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r206, %r190, %r191;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r207, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r184], %r206;
st.shared.f32 [%r185], %r207;
	// end inline asm
$L__BB24_41:
	add.s64 	%rd148, %rd46, %rd1;
	setp.lt.u64 	%p46, %rd148, %rd46;
	add.s64 	%rd178, %rd66, %rd57;
	setp.gt.u64 	%p47, %rd178, 63;
	or.pred  	%p48, %p46, %p47;
	@%p48 bra 	$L__BB24_34;
	bra.uni 	$L__BB24_42;
$L__BB24_34:
	bar.sync 	0;
	@%p9 bra 	$L__BB24_51;
	cvt.u32.u64 	%r329, %rd56;
	shl.b32 	%r330, %r329, 4;
	add.s32 	%r303, %r1, %r330;
	add.s32 	%r304, %r303, 4;
	// begin inline asm
	ld.shared.f32 %r310, [%r303];
ld.shared.f32 %r313, [%r304];
	// end inline asm
	add.s32 	%r307, %r303, 8;
	add.s32 	%r308, %r303, 12;
	// begin inline asm
	ld.shared.f32 %r311, [%r307];
ld.shared.f32 %r314, [%r308];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r309, %r310, %r311;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r312, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r303], %r309;
st.shared.f32 [%r304], %r312;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r319, %r310, %r311;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r322, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r307], %r319;
st.shared.f32 [%r308], %r322;
	// end inline asm
	@!%p1 bra 	$L__BB24_48;
	bra.uni 	$L__BB24_51;
$L__BB24_48:
	shl.b64 	%rd55, %rd66, 1;
	setp.gt.u64 	%p56, %rd55, 127;
	@%p56 bra 	$L__BB24_52;
	add.s64 	%rd54, %rd66, 1;
	cvt.u32.u64 	%r359, %rd55;
	shl.b32 	%r360, %r359, 3;
	add.s32 	%r333, %r1, %r360;
	add.s32 	%r334, %r333, 4;
	// begin inline asm
	ld.shared.f32 %r340, [%r333];
ld.shared.f32 %r343, [%r334];
	// end inline asm
	add.s32 	%r337, %r333, 8;
	add.s32 	%r338, %r333, 12;
	// begin inline asm
	ld.shared.f32 %r341, [%r337];
ld.shared.f32 %r344, [%r338];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r339, %r340, %r341;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r342, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r333], %r339;
st.shared.f32 [%r334], %r342;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r349, %r340, %r341;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r352, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r337], %r349;
st.shared.f32 [%r338], %r352;
	// end inline asm
	add.s64 	%rd155, %rd54, %rd1;
	setp.lt.u64 	%p57, %rd155, %rd54;
	add.s64 	%rd179, %rd66, %rd57;
	setp.gt.u64 	%p58, %rd179, 63;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB24_51;
$L__BB24_50:
	add.s64 	%rd156, %rd179, 1;
	cvt.u32.u64 	%r389, %rd179;
	shl.b32 	%r390, %r389, 4;
	add.s32 	%r363, %r390, %r1;
	add.s32 	%r364, %r363, 4;
	// begin inline asm
	ld.shared.f32 %r370, [%r363];
ld.shared.f32 %r373, [%r364];
	// end inline asm
	add.s32 	%r367, %r363, 8;
	add.s32 	%r368, %r363, 12;
	// begin inline asm
	ld.shared.f32 %r371, [%r367];
ld.shared.f32 %r374, [%r368];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r369, %r370, %r371;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r372, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r363], %r369;
st.shared.f32 [%r364], %r372;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r379, %r370, %r371;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r382, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r367], %r379;
st.shared.f32 [%r368], %r382;
	// end inline asm
	add.s64 	%rd157, %rd156, %rd1;
	setp.lt.u64 	%p60, %rd157, %rd156;
	add.s64 	%rd179, %rd179, %rd57;
	setp.gt.u64 	%p61, %rd179, 63;
	or.pred  	%p62, %p60, %p61;
	@%p62 bra 	$L__BB24_51;
	bra.uni 	$L__BB24_50;
$L__BB24_51:
	ret;
$L__BB24_44:
	and.b32  	%r270, %r11, 992;
	add.s32 	%r271, %r1, %r270;
	add.s32 	%r254, %r271, 8;
	add.s32 	%r255, %r271, 12;
	// begin inline asm
	ld.shared.f32 %r249, [%r254];
ld.shared.f32 %r252, [%r255];
	// end inline asm
	or.b32  	%r272, %r11, 24;
	add.s32 	%r246, %r272, %r1;
	add.s32 	%r247, %r246, 4;
	// begin inline asm
	ld.shared.f32 %r250, [%r246];
ld.shared.f32 %r253, [%r247];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r256, %r249, %r250;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r257, %r252, %r253;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r254], %r256;
st.shared.f32 [%r255], %r257;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r265, %r249, %r250;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r268, %r252, %r253;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r269, %r265;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r246], %r268;
st.shared.f32 [%r247], %r269;
	// end inline asm
$L__BB24_45:
	add.s64 	%rd49, %rd178, 1;
	add.s64 	%rd150, %rd49, %rd1;
	setp.lt.u64 	%p52, %rd150, %rd49;
	add.s64 	%rd178, %rd178, %rd57;
	setp.gt.u64 	%p53, %rd178, 63;
	or.pred  	%p54, %p52, %p53;
	@%p54 bra 	$L__BB24_34;
$L__BB24_42:
	and.b64  	%rd149, %rd178, 1;
	setp.eq.b64 	%p49, %rd149, 1;
	xor.pred  	%p51, %p49, %p40;
	cvt.u32.u64 	%r239, %rd178;
	shl.b32 	%r11, %r239, 4;
	@%p51 bra 	$L__BB24_44;
	add.s32 	%r275, %r11, %r1;
	add.s32 	%r276, %r275, 4;
	// begin inline asm
	ld.shared.f32 %r282, [%r275];
ld.shared.f32 %r285, [%r276];
	// end inline asm
	add.s32 	%r297, %r275, 16;
	add.s32 	%r298, %r275, 20;
	// begin inline asm
	ld.shared.f32 %r283, [%r297];
ld.shared.f32 %r286, [%r298];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r289, %r282, %r283;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r290, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r275], %r289;
st.shared.f32 [%r276], %r290;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r299, %r282, %r283;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r300, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r297], %r299;
st.shared.f32 [%r298], %r300;
	// end inline asm
	bra.uni 	$L__BB24_45;
$L__BB24_54:
	mov.u64 	%rd137, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd138, %rd137;
	{ // callseq 90, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd138;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 90
$L__BB24_53:
	mov.u64 	%rd139, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd140, %rd139;
	{ // callseq 91, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 91
$L__BB24_10:
	mov.u64 	%rd112, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd113, %rd112;
	mov.u64 	%rd114, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd115, %rd114;
	{ // callseq 89, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd113;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd115;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 89
$L__BB24_8:
	mov.u64 	%rd108, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd109, %rd108;
	mov.u64 	%rd110, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd111, %rd110;
	{ // callseq 88, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd109;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd111;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 88
$L__BB24_19:
	mov.u64 	%rd142, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_10;
	cvta.global.u64 	%rd143, %rd142;
	{ // callseq 92, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd143;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 92
$L__BB24_55:
	mov.u64 	%rd104, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd105, %rd104;
	{ // callseq 87, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd105;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 87
$L__BB24_56:
	mov.u64 	%rd102, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd103, %rd102;
	{ // callseq 86, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd103;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 86
$L__BB24_30:
	mov.u64 	%rd144, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_10;
	cvta.global.u64 	%rd145, %rd144;
	{ // callseq 93, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd145;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 93
$L__BB24_4:
	mov.u64 	%rd73, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd74, %rd73;
	mov.u64 	%rd75, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd76, %rd75;
	{ // callseq 84, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd74;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd76;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 84
$L__BB24_21:
	mov.u64 	%rd77, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd78, %rd77;
	mov.u64 	%rd79, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd80, %rd79;
	{ // callseq 85, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd80;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 85
$L__BB24_5:
	mov.u64 	%rd158, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_5;
	cvta.global.u64 	%rd159, %rd158;
	mov.u64 	%rd160, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_6;
	cvta.global.u64 	%rd161, %rd160;
	{ // callseq 95, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd159;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd161;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 95
$L__BB24_52:
	mov.u64 	%rd151, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd152, %rd151;
	mov.u64 	%rd153, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd154, %rd153;
	{ // callseq 94, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd152;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd154;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 94

}
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_3
)
{
	.reg .pred 	%p<63>;
	.reg .b32 	%r<399>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<180>;

	ld.param.u64 	%rd57, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_2];
	setp.eq.s64 	%p8, %rd57, 0;
	@%p8 bra 	$L__BB25_5;
	ld.param.u64 	%rd56, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_1];
	ld.param.u64 	%rd58, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_0];
	add.s64 	%rd1, %rd57, -1;
	setp.gt.u64 	%p9, %rd56, 2047;
	ld.param.u64 	%rd59, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_3];
	add.s64 	%rd60, %rd56, 1;
	selp.b64 	%rd61, 2048, %rd60, %p9;
	ld.u64 	%rd62, [%rd58];
	ld.u64 	%rd63, [%rd58+8];
	ld.u32 	%r1, [%rd59];
	shl.b64 	%rd64, %rd63, 3;
	add.s64 	%rd2, %rd62, %rd64;
	setp.eq.s64 	%p10, %rd63, 0;
	selp.b64 	%rd3, 0, %rd62, %p10;
	selp.b64 	%rd65, 0, 8, %p10;
	add.s64 	%rd4, %rd62, %rd65;
	add.s64 	%rd66, %rd61, %rd1;
	setp.lt.u64 	%p11, %rd66, %rd61;
	setp.gt.u64 	%p12, %rd66, 2047;
	or.pred  	%p1, %p11, %p12;
	mov.u64 	%rd7, 1;
	mov.u64 	%rd8, 12;
	bra.uni 	$L__BB25_2;
$L__BB25_32:
	shl.b64 	%rd7, %rd7, 1;
	bar.sync 	0;
	setp.gt.u64 	%p37, %rd8, 2;
	@%p37 bra 	$L__BB25_2;
	bra.uni 	$L__BB25_33;
$L__BB25_2:
	add.s64 	%rd8, %rd8, -1;
	@%p9 bra 	$L__BB25_32;
	cvt.u32.u64 	%r13, %rd8;
	shr.u64 	%rd69, %rd56, %r13;
	mov.u64 	%rd70, 1;
	shl.b64 	%rd27, %rd70, %r13;
	mov.u64 	%rd71, 2;
	shl.b64 	%rd28, %rd71, %r13;
	add.s64 	%rd29, %rd27, -1;
	and.b64  	%rd172, %rd29, %rd56;
	mul.lo.s64 	%rd72, %rd69, %rd28;
	add.s64 	%rd31, %rd172, %rd72;
	setp.lt.u64 	%p14, %rd31, 4096;
	@%p14 bra 	$L__BB25_20;
	bra.uni 	$L__BB25_4;
$L__BB25_20:
	cvt.u32.u64 	%r18, %rd31;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r16, %r1, %r19;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	ld.shared.f32 %r14, [%r16];
ld.shared.f32 %r15, [%r17];
	// end inline asm
	add.s64 	%rd32, %rd31, %rd27;
	setp.lt.u64 	%p15, %rd32, 4096;
	@%p15 bra 	$L__BB25_22;
	bra.uni 	$L__BB25_21;
$L__BB25_22:
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r22, %r1, %r25;
	add.s32 	%r23, %r22, 4;
	// begin inline asm
	ld.shared.f32 %r20, [%r22];
ld.shared.f32 %r21, [%r23];
	// end inline asm
	setp.eq.s64 	%p16, %rd172, 0;
	mov.u64 	%rd162, -1;
	mov.u64 	%rd177, %rd3;
	@%p16 bra 	$L__BB25_29;
	mul.lo.s64 	%rd176, %rd172, %rd7;
	mul.hi.u64 	%rd81, %rd172, %rd7;
	setp.eq.s64 	%p17, %rd81, 0;
	mov.u64 	%rd174, %rd4;
	@%p17 bra 	$L__BB25_28;
	mov.u64 	%rd173, %rd7;
	mov.u64 	%rd174, %rd4;
$L__BB25_25:
	setp.eq.s64 	%p18, %rd172, 0;
	@%p18 bra 	$L__BB25_55;
	setp.eq.s64 	%p19, %rd173, 0;
	@%p19 bra 	$L__BB25_56;
	div.u64 	%rd83, %rd162, %rd172;
	div.u64 	%rd84, %rd162, %rd173;
	mul.lo.s64 	%rd85, %rd83, %rd172;
	mul.lo.s64 	%rd86, %rd84, %rd173;
	setp.gt.u64 	%p20, %rd85, %rd86;
	max.u64 	%rd87, %rd85, %rd86;
	selp.b64 	%rd88, %rd83, 0, %p20;
	sub.s64 	%rd173, %rd173, %rd88;
	selp.b64 	%rd89, 0, %rd84, %p20;
	sub.s64 	%rd172, %rd172, %rd89;
	add.s64 	%rd90, %rd87, -1;
	sub.s64 	%rd91, %rd2, %rd174;
	shr.u64 	%rd92, %rd91, 3;
	setp.gt.u64 	%p21, %rd92, %rd90;
	shl.b64 	%rd93, %rd87, 3;
	add.s64 	%rd94, %rd174, %rd93;
	selp.b64 	%rd174, %rd94, %rd2, %p21;
	mul.lo.s64 	%rd176, %rd172, %rd173;
	mul.hi.u64 	%rd95, %rd172, %rd173;
	setp.ne.s64 	%p6, %rd95, 0;
	@%p6 bra 	$L__BB25_25;
$L__BB25_28:
	add.s64 	%rd96, %rd176, -1;
	sub.s64 	%rd97, %rd2, %rd174;
	shr.u64 	%rd98, %rd97, 3;
	setp.gt.u64 	%p22, %rd98, %rd96;
	shl.b64 	%rd99, %rd176, 3;
	add.s64 	%rd100, %rd174, %rd99;
	add.s64 	%rd101, %rd100, -8;
	selp.b64 	%rd177, %rd101, 0, %p22;
$L__BB25_29:
	setp.ne.s64 	%p23, %rd177, 0;
	@%p23 bra 	$L__BB25_31;
	bra.uni 	$L__BB25_30;
$L__BB25_31:
	mov.b32 	%f6, %r15;
	mov.b32 	%f5, %r14;
	mov.b32 	%f8, %r21;
	mov.b32 	%f7, %r20;
	// begin inline asm
	add.rn.ftz.f32 %r26, %r14, %r20;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r29, %r15, %r21;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r16], %r26;
st.shared.f32 [%r17], %r29;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r43, %r14, %r20;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r46, %r15, %r21;
	// end inline asm
	ld.u32 	%r56, [%rd177];
	ld.u32 	%r53, [%rd177+4];
	// begin inline asm
	mul.rn.ftz.f32 %r42, %r43, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r46, %r53;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r48, %r42, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r51, %r43, %r53;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r54, %r46, %r56;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r57, %r51, %r54;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r22], %r48;
st.shared.f32 [%r23], %r57;
	// end inline asm
	mov.u64 	%rd9, %rd66;
	@%p1 bra 	$L__BB25_32;
$L__BB25_7:
	shr.u64 	%rd106, %rd9, %r13;
	and.b64  	%rd166, %rd9, %rd29;
	mul.lo.s64 	%rd107, %rd106, %rd28;
	add.s64 	%rd12, %rd107, %rd166;
	setp.lt.u64 	%p24, %rd12, 4096;
	@%p24 bra 	$L__BB25_9;
	bra.uni 	$L__BB25_8;
$L__BB25_9:
	cvt.u32.u64 	%r69, %rd12;
	shl.b32 	%r70, %r69, 3;
	add.s32 	%r67, %r70, %r1;
	add.s32 	%r68, %r67, 4;
	// begin inline asm
	ld.shared.f32 %r65, [%r67];
ld.shared.f32 %r66, [%r68];
	// end inline asm
	add.s64 	%rd13, %rd12, %rd27;
	setp.lt.u64 	%p25, %rd13, 4096;
	@%p25 bra 	$L__BB25_11;
	bra.uni 	$L__BB25_10;
$L__BB25_11:
	cvt.u32.u64 	%r75, %rd13;
	shl.b32 	%r76, %r75, 3;
	add.s32 	%r73, %r76, %r1;
	add.s32 	%r74, %r73, 4;
	// begin inline asm
	ld.shared.f32 %r71, [%r73];
ld.shared.f32 %r72, [%r74];
	// end inline asm
	setp.eq.s64 	%p26, %rd166, 0;
	mov.u64 	%rd171, %rd3;
	@%p26 bra 	$L__BB25_18;
	mul.lo.s64 	%rd170, %rd166, %rd7;
	mul.hi.u64 	%rd116, %rd166, %rd7;
	setp.eq.s64 	%p27, %rd116, 0;
	mov.u64 	%rd168, %rd4;
	@%p27 bra 	$L__BB25_17;
	mov.u64 	%rd167, %rd7;
	mov.u64 	%rd168, %rd4;
$L__BB25_14:
	setp.eq.s64 	%p28, %rd166, 0;
	@%p28 bra 	$L__BB25_53;
	setp.eq.s64 	%p29, %rd167, 0;
	@%p29 bra 	$L__BB25_54;
	div.u64 	%rd118, %rd162, %rd166;
	div.u64 	%rd119, %rd162, %rd167;
	mul.lo.s64 	%rd120, %rd118, %rd166;
	mul.lo.s64 	%rd121, %rd119, %rd167;
	setp.gt.u64 	%p30, %rd120, %rd121;
	max.u64 	%rd122, %rd120, %rd121;
	selp.b64 	%rd123, %rd118, 0, %p30;
	sub.s64 	%rd167, %rd167, %rd123;
	selp.b64 	%rd124, 0, %rd119, %p30;
	sub.s64 	%rd166, %rd166, %rd124;
	add.s64 	%rd125, %rd122, -1;
	sub.s64 	%rd126, %rd2, %rd168;
	shr.u64 	%rd127, %rd126, 3;
	setp.gt.u64 	%p31, %rd127, %rd125;
	shl.b64 	%rd128, %rd122, 3;
	add.s64 	%rd129, %rd168, %rd128;
	selp.b64 	%rd168, %rd129, %rd2, %p31;
	mul.lo.s64 	%rd170, %rd166, %rd167;
	mul.hi.u64 	%rd130, %rd166, %rd167;
	setp.ne.s64 	%p3, %rd130, 0;
	@%p3 bra 	$L__BB25_14;
$L__BB25_17:
	add.s64 	%rd131, %rd170, -1;
	sub.s64 	%rd132, %rd2, %rd168;
	shr.u64 	%rd133, %rd132, 3;
	setp.gt.u64 	%p32, %rd133, %rd131;
	shl.b64 	%rd134, %rd170, 3;
	add.s64 	%rd135, %rd168, %rd134;
	add.s64 	%rd136, %rd135, -8;
	selp.b64 	%rd171, %rd136, 0, %p32;
$L__BB25_18:
	setp.ne.s64 	%p33, %rd171, 0;
	@%p33 bra 	$L__BB25_6;
	bra.uni 	$L__BB25_19;
$L__BB25_6:
	add.s64 	%rd10, %rd9, 1;
	mov.b32 	%f2, %r66;
	mov.b32 	%f1, %r65;
	mov.b32 	%f4, %r72;
	mov.b32 	%f3, %r71;
	// begin inline asm
	add.rn.ftz.f32 %r77, %r65, %r71;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r66, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r67], %r77;
st.shared.f32 [%r68], %r80;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r94, %r65, %r71;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r97, %r66, %r72;
	// end inline asm
	ld.u32 	%r107, [%rd171];
	ld.u32 	%r104, [%rd171+4];
	// begin inline asm
	mul.rn.ftz.f32 %r93, %r94, %r107;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r96, %r97, %r104;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r99, %r93, %r96;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r102, %r94, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r105, %r97, %r107;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r108, %r102, %r105;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r73], %r99;
st.shared.f32 [%r74], %r108;
	// end inline asm
	add.s64 	%rd141, %rd10, %rd1;
	setp.lt.u64 	%p34, %rd141, %rd10;
	add.s64 	%rd9, %rd9, %rd57;
	setp.gt.u64 	%p35, %rd9, 2047;
	or.pred  	%p36, %p34, %p35;
	@%p36 bra 	$L__BB25_32;
	bra.uni 	$L__BB25_7;
$L__BB25_33:
	@%p9 bra 	$L__BB25_34;
	and.b64  	%rd146, %rd56, 1;
	setp.eq.b64 	%p39, %rd146, 1;
	mov.pred 	%p40, 0;
	xor.pred  	%p41, %p39, %p40;
	cvt.u32.u64 	%r115, %rd56;
	shl.b32 	%r12, %r115, 4;
	@%p41 bra 	$L__BB25_36;
	add.s32 	%r151, %r1, %r12;
	add.s32 	%r152, %r151, 4;
	// begin inline asm
	ld.shared.f32 %r158, [%r151];
ld.shared.f32 %r161, [%r152];
	// end inline asm
	add.s32 	%r173, %r151, 16;
	add.s32 	%r174, %r151, 20;
	// begin inline asm
	ld.shared.f32 %r159, [%r173];
ld.shared.f32 %r162, [%r174];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r165, %r158, %r159;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r166, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r151], %r165;
st.shared.f32 [%r152], %r166;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r175, %r158, %r159;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r176, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r173], %r175;
st.shared.f32 [%r174], %r176;
	// end inline asm
	bra.uni 	$L__BB25_37;
$L__BB25_36:
	and.b32  	%r146, %r12, 32736;
	add.s32 	%r147, %r1, %r146;
	add.s32 	%r130, %r147, 8;
	add.s32 	%r131, %r147, 12;
	// begin inline asm
	ld.shared.f32 %r125, [%r130];
ld.shared.f32 %r128, [%r131];
	// end inline asm
	or.b32  	%r148, %r12, 24;
	add.s32 	%r122, %r1, %r148;
	add.s32 	%r123, %r122, 4;
	// begin inline asm
	ld.shared.f32 %r126, [%r122];
ld.shared.f32 %r129, [%r123];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r132, %r125, %r126;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r133, %r128, %r129;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r130], %r132;
st.shared.f32 [%r131], %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r125, %r126;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r128, %r129;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r145, %r141;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r122], %r144;
st.shared.f32 [%r123], %r145;
	// end inline asm
$L__BB25_37:
	@%p1 bra 	$L__BB25_34;
	add.s64 	%rd46, %rd66, 1;
	and.b64  	%rd147, %rd66, 1;
	setp.eq.b64 	%p42, %rd147, 1;
	xor.pred  	%p44, %p42, %p40;
	not.pred 	%p45, %p44;
	cvt.u32.u64 	%r177, %rd66;
	shl.b32 	%r10, %r177, 4;
	@%p45 bra 	$L__BB25_40;
	bra.uni 	$L__BB25_39;
$L__BB25_40:
	add.s32 	%r213, %r1, %r10;
	add.s32 	%r214, %r213, 4;
	// begin inline asm
	ld.shared.f32 %r220, [%r213];
ld.shared.f32 %r223, [%r214];
	// end inline asm
	add.s32 	%r235, %r213, 16;
	add.s32 	%r236, %r213, 20;
	// begin inline asm
	ld.shared.f32 %r221, [%r235];
ld.shared.f32 %r224, [%r236];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r227, %r220, %r221;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r228, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r213], %r227;
st.shared.f32 [%r214], %r228;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r237, %r220, %r221;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r238, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r235], %r237;
st.shared.f32 [%r236], %r238;
	// end inline asm
	bra.uni 	$L__BB25_41;
$L__BB25_39:
	and.b32  	%r208, %r10, 32736;
	add.s32 	%r209, %r1, %r208;
	add.s32 	%r192, %r209, 8;
	add.s32 	%r193, %r209, 12;
	// begin inline asm
	ld.shared.f32 %r187, [%r192];
ld.shared.f32 %r190, [%r193];
	// end inline asm
	or.b32  	%r210, %r10, 24;
	add.s32 	%r184, %r1, %r210;
	add.s32 	%r185, %r184, 4;
	// begin inline asm
	ld.shared.f32 %r188, [%r184];
ld.shared.f32 %r191, [%r185];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r194, %r187, %r188;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r195, %r190, %r191;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r192], %r194;
st.shared.f32 [%r193], %r195;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r203, %r187, %r188;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r206, %r190, %r191;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r207, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r184], %r206;
st.shared.f32 [%r185], %r207;
	// end inline asm
$L__BB25_41:
	add.s64 	%rd148, %rd46, %rd1;
	setp.lt.u64 	%p46, %rd148, %rd46;
	add.s64 	%rd178, %rd66, %rd57;
	setp.gt.u64 	%p47, %rd178, 2047;
	or.pred  	%p48, %p46, %p47;
	@%p48 bra 	$L__BB25_34;
	bra.uni 	$L__BB25_42;
$L__BB25_34:
	bar.sync 	0;
	@%p9 bra 	$L__BB25_51;
	cvt.u32.u64 	%r329, %rd56;
	shl.b32 	%r330, %r329, 4;
	add.s32 	%r303, %r1, %r330;
	add.s32 	%r304, %r303, 4;
	// begin inline asm
	ld.shared.f32 %r310, [%r303];
ld.shared.f32 %r313, [%r304];
	// end inline asm
	add.s32 	%r307, %r303, 8;
	add.s32 	%r308, %r303, 12;
	// begin inline asm
	ld.shared.f32 %r311, [%r307];
ld.shared.f32 %r314, [%r308];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r309, %r310, %r311;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r312, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r303], %r309;
st.shared.f32 [%r304], %r312;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r319, %r310, %r311;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r322, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r307], %r319;
st.shared.f32 [%r308], %r322;
	// end inline asm
	@!%p1 bra 	$L__BB25_48;
	bra.uni 	$L__BB25_51;
$L__BB25_48:
	shl.b64 	%rd55, %rd66, 1;
	setp.gt.u64 	%p56, %rd55, 4095;
	@%p56 bra 	$L__BB25_52;
	add.s64 	%rd54, %rd66, 1;
	cvt.u32.u64 	%r359, %rd55;
	shl.b32 	%r360, %r359, 3;
	add.s32 	%r333, %r1, %r360;
	add.s32 	%r334, %r333, 4;
	// begin inline asm
	ld.shared.f32 %r340, [%r333];
ld.shared.f32 %r343, [%r334];
	// end inline asm
	add.s32 	%r337, %r333, 8;
	add.s32 	%r338, %r333, 12;
	// begin inline asm
	ld.shared.f32 %r341, [%r337];
ld.shared.f32 %r344, [%r338];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r339, %r340, %r341;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r342, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r333], %r339;
st.shared.f32 [%r334], %r342;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r349, %r340, %r341;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r352, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r337], %r349;
st.shared.f32 [%r338], %r352;
	// end inline asm
	add.s64 	%rd155, %rd54, %rd1;
	setp.lt.u64 	%p57, %rd155, %rd54;
	add.s64 	%rd179, %rd66, %rd57;
	setp.gt.u64 	%p58, %rd179, 2047;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB25_51;
$L__BB25_50:
	add.s64 	%rd156, %rd179, 1;
	cvt.u32.u64 	%r389, %rd179;
	shl.b32 	%r390, %r389, 4;
	add.s32 	%r363, %r390, %r1;
	add.s32 	%r364, %r363, 4;
	// begin inline asm
	ld.shared.f32 %r370, [%r363];
ld.shared.f32 %r373, [%r364];
	// end inline asm
	add.s32 	%r367, %r363, 8;
	add.s32 	%r368, %r363, 12;
	// begin inline asm
	ld.shared.f32 %r371, [%r367];
ld.shared.f32 %r374, [%r368];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r369, %r370, %r371;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r372, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r363], %r369;
st.shared.f32 [%r364], %r372;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r379, %r370, %r371;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r382, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r367], %r379;
st.shared.f32 [%r368], %r382;
	// end inline asm
	add.s64 	%rd157, %rd156, %rd1;
	setp.lt.u64 	%p60, %rd157, %rd156;
	add.s64 	%rd179, %rd179, %rd57;
	setp.gt.u64 	%p61, %rd179, 2047;
	or.pred  	%p62, %p60, %p61;
	@%p62 bra 	$L__BB25_51;
	bra.uni 	$L__BB25_50;
$L__BB25_51:
	ret;
$L__BB25_44:
	and.b32  	%r270, %r11, 32736;
	add.s32 	%r271, %r1, %r270;
	add.s32 	%r254, %r271, 8;
	add.s32 	%r255, %r271, 12;
	// begin inline asm
	ld.shared.f32 %r249, [%r254];
ld.shared.f32 %r252, [%r255];
	// end inline asm
	or.b32  	%r272, %r11, 24;
	add.s32 	%r246, %r272, %r1;
	add.s32 	%r247, %r246, 4;
	// begin inline asm
	ld.shared.f32 %r250, [%r246];
ld.shared.f32 %r253, [%r247];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r256, %r249, %r250;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r257, %r252, %r253;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r254], %r256;
st.shared.f32 [%r255], %r257;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r265, %r249, %r250;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r268, %r252, %r253;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r269, %r265;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r246], %r268;
st.shared.f32 [%r247], %r269;
	// end inline asm
$L__BB25_45:
	add.s64 	%rd49, %rd178, 1;
	add.s64 	%rd150, %rd49, %rd1;
	setp.lt.u64 	%p52, %rd150, %rd49;
	add.s64 	%rd178, %rd178, %rd57;
	setp.gt.u64 	%p53, %rd178, 2047;
	or.pred  	%p54, %p52, %p53;
	@%p54 bra 	$L__BB25_34;
$L__BB25_42:
	and.b64  	%rd149, %rd178, 1;
	setp.eq.b64 	%p49, %rd149, 1;
	xor.pred  	%p51, %p49, %p40;
	cvt.u32.u64 	%r239, %rd178;
	shl.b32 	%r11, %r239, 4;
	@%p51 bra 	$L__BB25_44;
	add.s32 	%r275, %r11, %r1;
	add.s32 	%r276, %r275, 4;
	// begin inline asm
	ld.shared.f32 %r282, [%r275];
ld.shared.f32 %r285, [%r276];
	// end inline asm
	add.s32 	%r297, %r275, 16;
	add.s32 	%r298, %r275, 20;
	// begin inline asm
	ld.shared.f32 %r283, [%r297];
ld.shared.f32 %r286, [%r298];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r289, %r282, %r283;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r290, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r275], %r289;
st.shared.f32 [%r276], %r290;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r299, %r282, %r283;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r300, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r297], %r299;
st.shared.f32 [%r298], %r300;
	// end inline asm
	bra.uni 	$L__BB25_45;
$L__BB25_54:
	mov.u64 	%rd137, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd138, %rd137;
	{ // callseq 102, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd138;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 102
$L__BB25_53:
	mov.u64 	%rd139, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd140, %rd139;
	{ // callseq 103, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 103
$L__BB25_10:
	mov.u64 	%rd112, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd113, %rd112;
	mov.u64 	%rd114, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd115, %rd114;
	{ // callseq 101, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd113;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd115;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 101
$L__BB25_8:
	mov.u64 	%rd108, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd109, %rd108;
	mov.u64 	%rd110, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd111, %rd110;
	{ // callseq 100, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd109;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd111;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 100
$L__BB25_19:
	mov.u64 	%rd142, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_10;
	cvta.global.u64 	%rd143, %rd142;
	{ // callseq 104, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd143;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 104
$L__BB25_55:
	mov.u64 	%rd104, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd105, %rd104;
	{ // callseq 99, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd105;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 99
$L__BB25_56:
	mov.u64 	%rd102, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd103, %rd102;
	{ // callseq 98, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd103;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 98
$L__BB25_30:
	mov.u64 	%rd144, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_10;
	cvta.global.u64 	%rd145, %rd144;
	{ // callseq 105, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd145;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 105
$L__BB25_4:
	mov.u64 	%rd73, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd74, %rd73;
	mov.u64 	%rd75, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd76, %rd75;
	{ // callseq 96, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd74;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd76;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 96
$L__BB25_21:
	mov.u64 	%rd77, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd78, %rd77;
	mov.u64 	%rd79, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd80, %rd79;
	{ // callseq 97, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd80;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 97
$L__BB25_5:
	mov.u64 	%rd158, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_5;
	cvta.global.u64 	%rd159, %rd158;
	mov.u64 	%rd160, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_6;
	cvta.global.u64 	%rd161, %rd160;
	{ // callseq 107, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd159;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd161;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 107
$L__BB25_52:
	mov.u64 	%rd151, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd152, %rd151;
	mov.u64 	%rd153, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd154, %rd153;
	{ // callseq 106, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd152;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd154;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 106

}
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_3
)
{
	.reg .pred 	%p<63>;
	.reg .b32 	%r<399>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<180>;

	ld.param.u64 	%rd57, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_2];
	setp.eq.s64 	%p8, %rd57, 0;
	@%p8 bra 	$L__BB26_5;
	ld.param.u64 	%rd56, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_1];
	ld.param.u64 	%rd58, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_0];
	add.s64 	%rd1, %rd57, -1;
	setp.gt.u64 	%p9, %rd56, 1023;
	ld.param.u64 	%rd59, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_3];
	add.s64 	%rd60, %rd56, 1;
	selp.b64 	%rd61, 1024, %rd60, %p9;
	ld.u64 	%rd62, [%rd58];
	ld.u64 	%rd63, [%rd58+8];
	ld.u32 	%r1, [%rd59];
	shl.b64 	%rd64, %rd63, 3;
	add.s64 	%rd2, %rd62, %rd64;
	setp.eq.s64 	%p10, %rd63, 0;
	selp.b64 	%rd3, 0, %rd62, %p10;
	selp.b64 	%rd65, 0, 8, %p10;
	add.s64 	%rd4, %rd62, %rd65;
	add.s64 	%rd66, %rd61, %rd1;
	setp.lt.u64 	%p11, %rd66, %rd61;
	setp.gt.u64 	%p12, %rd66, 1023;
	or.pred  	%p1, %p11, %p12;
	mov.u64 	%rd7, 1;
	mov.u64 	%rd8, 11;
	bra.uni 	$L__BB26_2;
$L__BB26_32:
	shl.b64 	%rd7, %rd7, 1;
	bar.sync 	0;
	setp.gt.u64 	%p37, %rd8, 2;
	@%p37 bra 	$L__BB26_2;
	bra.uni 	$L__BB26_33;
$L__BB26_2:
	add.s64 	%rd8, %rd8, -1;
	@%p9 bra 	$L__BB26_32;
	cvt.u32.u64 	%r13, %rd8;
	shr.u64 	%rd69, %rd56, %r13;
	mov.u64 	%rd70, 1;
	shl.b64 	%rd27, %rd70, %r13;
	mov.u64 	%rd71, 2;
	shl.b64 	%rd28, %rd71, %r13;
	add.s64 	%rd29, %rd27, -1;
	and.b64  	%rd172, %rd29, %rd56;
	mul.lo.s64 	%rd72, %rd69, %rd28;
	add.s64 	%rd31, %rd172, %rd72;
	setp.lt.u64 	%p14, %rd31, 2048;
	@%p14 bra 	$L__BB26_20;
	bra.uni 	$L__BB26_4;
$L__BB26_20:
	cvt.u32.u64 	%r18, %rd31;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r16, %r1, %r19;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	ld.shared.f32 %r14, [%r16];
ld.shared.f32 %r15, [%r17];
	// end inline asm
	add.s64 	%rd32, %rd31, %rd27;
	setp.lt.u64 	%p15, %rd32, 2048;
	@%p15 bra 	$L__BB26_22;
	bra.uni 	$L__BB26_21;
$L__BB26_22:
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r22, %r1, %r25;
	add.s32 	%r23, %r22, 4;
	// begin inline asm
	ld.shared.f32 %r20, [%r22];
ld.shared.f32 %r21, [%r23];
	// end inline asm
	setp.eq.s64 	%p16, %rd172, 0;
	mov.u64 	%rd162, -1;
	mov.u64 	%rd177, %rd3;
	@%p16 bra 	$L__BB26_29;
	mul.lo.s64 	%rd176, %rd172, %rd7;
	mul.hi.u64 	%rd81, %rd172, %rd7;
	setp.eq.s64 	%p17, %rd81, 0;
	mov.u64 	%rd174, %rd4;
	@%p17 bra 	$L__BB26_28;
	mov.u64 	%rd173, %rd7;
	mov.u64 	%rd174, %rd4;
$L__BB26_25:
	setp.eq.s64 	%p18, %rd172, 0;
	@%p18 bra 	$L__BB26_55;
	setp.eq.s64 	%p19, %rd173, 0;
	@%p19 bra 	$L__BB26_56;
	div.u64 	%rd83, %rd162, %rd172;
	div.u64 	%rd84, %rd162, %rd173;
	mul.lo.s64 	%rd85, %rd83, %rd172;
	mul.lo.s64 	%rd86, %rd84, %rd173;
	setp.gt.u64 	%p20, %rd85, %rd86;
	max.u64 	%rd87, %rd85, %rd86;
	selp.b64 	%rd88, %rd83, 0, %p20;
	sub.s64 	%rd173, %rd173, %rd88;
	selp.b64 	%rd89, 0, %rd84, %p20;
	sub.s64 	%rd172, %rd172, %rd89;
	add.s64 	%rd90, %rd87, -1;
	sub.s64 	%rd91, %rd2, %rd174;
	shr.u64 	%rd92, %rd91, 3;
	setp.gt.u64 	%p21, %rd92, %rd90;
	shl.b64 	%rd93, %rd87, 3;
	add.s64 	%rd94, %rd174, %rd93;
	selp.b64 	%rd174, %rd94, %rd2, %p21;
	mul.lo.s64 	%rd176, %rd172, %rd173;
	mul.hi.u64 	%rd95, %rd172, %rd173;
	setp.ne.s64 	%p6, %rd95, 0;
	@%p6 bra 	$L__BB26_25;
$L__BB26_28:
	add.s64 	%rd96, %rd176, -1;
	sub.s64 	%rd97, %rd2, %rd174;
	shr.u64 	%rd98, %rd97, 3;
	setp.gt.u64 	%p22, %rd98, %rd96;
	shl.b64 	%rd99, %rd176, 3;
	add.s64 	%rd100, %rd174, %rd99;
	add.s64 	%rd101, %rd100, -8;
	selp.b64 	%rd177, %rd101, 0, %p22;
$L__BB26_29:
	setp.ne.s64 	%p23, %rd177, 0;
	@%p23 bra 	$L__BB26_31;
	bra.uni 	$L__BB26_30;
$L__BB26_31:
	mov.b32 	%f6, %r15;
	mov.b32 	%f5, %r14;
	mov.b32 	%f8, %r21;
	mov.b32 	%f7, %r20;
	// begin inline asm
	add.rn.ftz.f32 %r26, %r14, %r20;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r29, %r15, %r21;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r16], %r26;
st.shared.f32 [%r17], %r29;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r43, %r14, %r20;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r46, %r15, %r21;
	// end inline asm
	ld.u32 	%r56, [%rd177];
	ld.u32 	%r53, [%rd177+4];
	// begin inline asm
	mul.rn.ftz.f32 %r42, %r43, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r46, %r53;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r48, %r42, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r51, %r43, %r53;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r54, %r46, %r56;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r57, %r51, %r54;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r22], %r48;
st.shared.f32 [%r23], %r57;
	// end inline asm
	mov.u64 	%rd9, %rd66;
	@%p1 bra 	$L__BB26_32;
$L__BB26_7:
	shr.u64 	%rd106, %rd9, %r13;
	and.b64  	%rd166, %rd9, %rd29;
	mul.lo.s64 	%rd107, %rd106, %rd28;
	add.s64 	%rd12, %rd107, %rd166;
	setp.lt.u64 	%p24, %rd12, 2048;
	@%p24 bra 	$L__BB26_9;
	bra.uni 	$L__BB26_8;
$L__BB26_9:
	cvt.u32.u64 	%r69, %rd12;
	shl.b32 	%r70, %r69, 3;
	add.s32 	%r67, %r70, %r1;
	add.s32 	%r68, %r67, 4;
	// begin inline asm
	ld.shared.f32 %r65, [%r67];
ld.shared.f32 %r66, [%r68];
	// end inline asm
	add.s64 	%rd13, %rd12, %rd27;
	setp.lt.u64 	%p25, %rd13, 2048;
	@%p25 bra 	$L__BB26_11;
	bra.uni 	$L__BB26_10;
$L__BB26_11:
	cvt.u32.u64 	%r75, %rd13;
	shl.b32 	%r76, %r75, 3;
	add.s32 	%r73, %r76, %r1;
	add.s32 	%r74, %r73, 4;
	// begin inline asm
	ld.shared.f32 %r71, [%r73];
ld.shared.f32 %r72, [%r74];
	// end inline asm
	setp.eq.s64 	%p26, %rd166, 0;
	mov.u64 	%rd171, %rd3;
	@%p26 bra 	$L__BB26_18;
	mul.lo.s64 	%rd170, %rd166, %rd7;
	mul.hi.u64 	%rd116, %rd166, %rd7;
	setp.eq.s64 	%p27, %rd116, 0;
	mov.u64 	%rd168, %rd4;
	@%p27 bra 	$L__BB26_17;
	mov.u64 	%rd167, %rd7;
	mov.u64 	%rd168, %rd4;
$L__BB26_14:
	setp.eq.s64 	%p28, %rd166, 0;
	@%p28 bra 	$L__BB26_53;
	setp.eq.s64 	%p29, %rd167, 0;
	@%p29 bra 	$L__BB26_54;
	div.u64 	%rd118, %rd162, %rd166;
	div.u64 	%rd119, %rd162, %rd167;
	mul.lo.s64 	%rd120, %rd118, %rd166;
	mul.lo.s64 	%rd121, %rd119, %rd167;
	setp.gt.u64 	%p30, %rd120, %rd121;
	max.u64 	%rd122, %rd120, %rd121;
	selp.b64 	%rd123, %rd118, 0, %p30;
	sub.s64 	%rd167, %rd167, %rd123;
	selp.b64 	%rd124, 0, %rd119, %p30;
	sub.s64 	%rd166, %rd166, %rd124;
	add.s64 	%rd125, %rd122, -1;
	sub.s64 	%rd126, %rd2, %rd168;
	shr.u64 	%rd127, %rd126, 3;
	setp.gt.u64 	%p31, %rd127, %rd125;
	shl.b64 	%rd128, %rd122, 3;
	add.s64 	%rd129, %rd168, %rd128;
	selp.b64 	%rd168, %rd129, %rd2, %p31;
	mul.lo.s64 	%rd170, %rd166, %rd167;
	mul.hi.u64 	%rd130, %rd166, %rd167;
	setp.ne.s64 	%p3, %rd130, 0;
	@%p3 bra 	$L__BB26_14;
$L__BB26_17:
	add.s64 	%rd131, %rd170, -1;
	sub.s64 	%rd132, %rd2, %rd168;
	shr.u64 	%rd133, %rd132, 3;
	setp.gt.u64 	%p32, %rd133, %rd131;
	shl.b64 	%rd134, %rd170, 3;
	add.s64 	%rd135, %rd168, %rd134;
	add.s64 	%rd136, %rd135, -8;
	selp.b64 	%rd171, %rd136, 0, %p32;
$L__BB26_18:
	setp.ne.s64 	%p33, %rd171, 0;
	@%p33 bra 	$L__BB26_6;
	bra.uni 	$L__BB26_19;
$L__BB26_6:
	add.s64 	%rd10, %rd9, 1;
	mov.b32 	%f2, %r66;
	mov.b32 	%f1, %r65;
	mov.b32 	%f4, %r72;
	mov.b32 	%f3, %r71;
	// begin inline asm
	add.rn.ftz.f32 %r77, %r65, %r71;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r66, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r67], %r77;
st.shared.f32 [%r68], %r80;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r94, %r65, %r71;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r97, %r66, %r72;
	// end inline asm
	ld.u32 	%r107, [%rd171];
	ld.u32 	%r104, [%rd171+4];
	// begin inline asm
	mul.rn.ftz.f32 %r93, %r94, %r107;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r96, %r97, %r104;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r99, %r93, %r96;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r102, %r94, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r105, %r97, %r107;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r108, %r102, %r105;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r73], %r99;
st.shared.f32 [%r74], %r108;
	// end inline asm
	add.s64 	%rd141, %rd10, %rd1;
	setp.lt.u64 	%p34, %rd141, %rd10;
	add.s64 	%rd9, %rd9, %rd57;
	setp.gt.u64 	%p35, %rd9, 1023;
	or.pred  	%p36, %p34, %p35;
	@%p36 bra 	$L__BB26_32;
	bra.uni 	$L__BB26_7;
$L__BB26_33:
	@%p9 bra 	$L__BB26_34;
	and.b64  	%rd146, %rd56, 1;
	setp.eq.b64 	%p39, %rd146, 1;
	mov.pred 	%p40, 0;
	xor.pred  	%p41, %p39, %p40;
	cvt.u32.u64 	%r115, %rd56;
	shl.b32 	%r12, %r115, 4;
	@%p41 bra 	$L__BB26_36;
	add.s32 	%r151, %r1, %r12;
	add.s32 	%r152, %r151, 4;
	// begin inline asm
	ld.shared.f32 %r158, [%r151];
ld.shared.f32 %r161, [%r152];
	// end inline asm
	add.s32 	%r173, %r151, 16;
	add.s32 	%r174, %r151, 20;
	// begin inline asm
	ld.shared.f32 %r159, [%r173];
ld.shared.f32 %r162, [%r174];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r165, %r158, %r159;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r166, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r151], %r165;
st.shared.f32 [%r152], %r166;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r175, %r158, %r159;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r176, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r173], %r175;
st.shared.f32 [%r174], %r176;
	// end inline asm
	bra.uni 	$L__BB26_37;
$L__BB26_36:
	and.b32  	%r146, %r12, 16352;
	add.s32 	%r147, %r1, %r146;
	add.s32 	%r130, %r147, 8;
	add.s32 	%r131, %r147, 12;
	// begin inline asm
	ld.shared.f32 %r125, [%r130];
ld.shared.f32 %r128, [%r131];
	// end inline asm
	or.b32  	%r148, %r12, 24;
	add.s32 	%r122, %r1, %r148;
	add.s32 	%r123, %r122, 4;
	// begin inline asm
	ld.shared.f32 %r126, [%r122];
ld.shared.f32 %r129, [%r123];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r132, %r125, %r126;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r133, %r128, %r129;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r130], %r132;
st.shared.f32 [%r131], %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r125, %r126;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r128, %r129;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r145, %r141;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r122], %r144;
st.shared.f32 [%r123], %r145;
	// end inline asm
$L__BB26_37:
	@%p1 bra 	$L__BB26_34;
	add.s64 	%rd46, %rd66, 1;
	and.b64  	%rd147, %rd66, 1;
	setp.eq.b64 	%p42, %rd147, 1;
	xor.pred  	%p44, %p42, %p40;
	not.pred 	%p45, %p44;
	cvt.u32.u64 	%r177, %rd66;
	shl.b32 	%r10, %r177, 4;
	@%p45 bra 	$L__BB26_40;
	bra.uni 	$L__BB26_39;
$L__BB26_40:
	add.s32 	%r213, %r1, %r10;
	add.s32 	%r214, %r213, 4;
	// begin inline asm
	ld.shared.f32 %r220, [%r213];
ld.shared.f32 %r223, [%r214];
	// end inline asm
	add.s32 	%r235, %r213, 16;
	add.s32 	%r236, %r213, 20;
	// begin inline asm
	ld.shared.f32 %r221, [%r235];
ld.shared.f32 %r224, [%r236];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r227, %r220, %r221;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r228, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r213], %r227;
st.shared.f32 [%r214], %r228;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r237, %r220, %r221;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r238, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r235], %r237;
st.shared.f32 [%r236], %r238;
	// end inline asm
	bra.uni 	$L__BB26_41;
$L__BB26_39:
	and.b32  	%r208, %r10, 16352;
	add.s32 	%r209, %r1, %r208;
	add.s32 	%r192, %r209, 8;
	add.s32 	%r193, %r209, 12;
	// begin inline asm
	ld.shared.f32 %r187, [%r192];
ld.shared.f32 %r190, [%r193];
	// end inline asm
	or.b32  	%r210, %r10, 24;
	add.s32 	%r184, %r1, %r210;
	add.s32 	%r185, %r184, 4;
	// begin inline asm
	ld.shared.f32 %r188, [%r184];
ld.shared.f32 %r191, [%r185];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r194, %r187, %r188;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r195, %r190, %r191;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r192], %r194;
st.shared.f32 [%r193], %r195;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r203, %r187, %r188;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r206, %r190, %r191;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r207, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r184], %r206;
st.shared.f32 [%r185], %r207;
	// end inline asm
$L__BB26_41:
	add.s64 	%rd148, %rd46, %rd1;
	setp.lt.u64 	%p46, %rd148, %rd46;
	add.s64 	%rd178, %rd66, %rd57;
	setp.gt.u64 	%p47, %rd178, 1023;
	or.pred  	%p48, %p46, %p47;
	@%p48 bra 	$L__BB26_34;
	bra.uni 	$L__BB26_42;
$L__BB26_34:
	bar.sync 	0;
	@%p9 bra 	$L__BB26_51;
	cvt.u32.u64 	%r329, %rd56;
	shl.b32 	%r330, %r329, 4;
	add.s32 	%r303, %r1, %r330;
	add.s32 	%r304, %r303, 4;
	// begin inline asm
	ld.shared.f32 %r310, [%r303];
ld.shared.f32 %r313, [%r304];
	// end inline asm
	add.s32 	%r307, %r303, 8;
	add.s32 	%r308, %r303, 12;
	// begin inline asm
	ld.shared.f32 %r311, [%r307];
ld.shared.f32 %r314, [%r308];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r309, %r310, %r311;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r312, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r303], %r309;
st.shared.f32 [%r304], %r312;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r319, %r310, %r311;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r322, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r307], %r319;
st.shared.f32 [%r308], %r322;
	// end inline asm
	@!%p1 bra 	$L__BB26_48;
	bra.uni 	$L__BB26_51;
$L__BB26_48:
	shl.b64 	%rd55, %rd66, 1;
	setp.gt.u64 	%p56, %rd55, 2047;
	@%p56 bra 	$L__BB26_52;
	add.s64 	%rd54, %rd66, 1;
	cvt.u32.u64 	%r359, %rd55;
	shl.b32 	%r360, %r359, 3;
	add.s32 	%r333, %r1, %r360;
	add.s32 	%r334, %r333, 4;
	// begin inline asm
	ld.shared.f32 %r340, [%r333];
ld.shared.f32 %r343, [%r334];
	// end inline asm
	add.s32 	%r337, %r333, 8;
	add.s32 	%r338, %r333, 12;
	// begin inline asm
	ld.shared.f32 %r341, [%r337];
ld.shared.f32 %r344, [%r338];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r339, %r340, %r341;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r342, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r333], %r339;
st.shared.f32 [%r334], %r342;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r349, %r340, %r341;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r352, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r337], %r349;
st.shared.f32 [%r338], %r352;
	// end inline asm
	add.s64 	%rd155, %rd54, %rd1;
	setp.lt.u64 	%p57, %rd155, %rd54;
	add.s64 	%rd179, %rd66, %rd57;
	setp.gt.u64 	%p58, %rd179, 1023;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB26_51;
$L__BB26_50:
	add.s64 	%rd156, %rd179, 1;
	cvt.u32.u64 	%r389, %rd179;
	shl.b32 	%r390, %r389, 4;
	add.s32 	%r363, %r390, %r1;
	add.s32 	%r364, %r363, 4;
	// begin inline asm
	ld.shared.f32 %r370, [%r363];
ld.shared.f32 %r373, [%r364];
	// end inline asm
	add.s32 	%r367, %r363, 8;
	add.s32 	%r368, %r363, 12;
	// begin inline asm
	ld.shared.f32 %r371, [%r367];
ld.shared.f32 %r374, [%r368];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r369, %r370, %r371;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r372, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r363], %r369;
st.shared.f32 [%r364], %r372;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r379, %r370, %r371;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r382, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r367], %r379;
st.shared.f32 [%r368], %r382;
	// end inline asm
	add.s64 	%rd157, %rd156, %rd1;
	setp.lt.u64 	%p60, %rd157, %rd156;
	add.s64 	%rd179, %rd179, %rd57;
	setp.gt.u64 	%p61, %rd179, 1023;
	or.pred  	%p62, %p60, %p61;
	@%p62 bra 	$L__BB26_51;
	bra.uni 	$L__BB26_50;
$L__BB26_51:
	ret;
$L__BB26_44:
	and.b32  	%r270, %r11, 16352;
	add.s32 	%r271, %r1, %r270;
	add.s32 	%r254, %r271, 8;
	add.s32 	%r255, %r271, 12;
	// begin inline asm
	ld.shared.f32 %r249, [%r254];
ld.shared.f32 %r252, [%r255];
	// end inline asm
	or.b32  	%r272, %r11, 24;
	add.s32 	%r246, %r272, %r1;
	add.s32 	%r247, %r246, 4;
	// begin inline asm
	ld.shared.f32 %r250, [%r246];
ld.shared.f32 %r253, [%r247];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r256, %r249, %r250;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r257, %r252, %r253;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r254], %r256;
st.shared.f32 [%r255], %r257;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r265, %r249, %r250;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r268, %r252, %r253;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r269, %r265;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r246], %r268;
st.shared.f32 [%r247], %r269;
	// end inline asm
$L__BB26_45:
	add.s64 	%rd49, %rd178, 1;
	add.s64 	%rd150, %rd49, %rd1;
	setp.lt.u64 	%p52, %rd150, %rd49;
	add.s64 	%rd178, %rd178, %rd57;
	setp.gt.u64 	%p53, %rd178, 1023;
	or.pred  	%p54, %p52, %p53;
	@%p54 bra 	$L__BB26_34;
$L__BB26_42:
	and.b64  	%rd149, %rd178, 1;
	setp.eq.b64 	%p49, %rd149, 1;
	xor.pred  	%p51, %p49, %p40;
	cvt.u32.u64 	%r239, %rd178;
	shl.b32 	%r11, %r239, 4;
	@%p51 bra 	$L__BB26_44;
	add.s32 	%r275, %r11, %r1;
	add.s32 	%r276, %r275, 4;
	// begin inline asm
	ld.shared.f32 %r282, [%r275];
ld.shared.f32 %r285, [%r276];
	// end inline asm
	add.s32 	%r297, %r275, 16;
	add.s32 	%r298, %r275, 20;
	// begin inline asm
	ld.shared.f32 %r283, [%r297];
ld.shared.f32 %r286, [%r298];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r289, %r282, %r283;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r290, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r275], %r289;
st.shared.f32 [%r276], %r290;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r299, %r282, %r283;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r300, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r297], %r299;
st.shared.f32 [%r298], %r300;
	// end inline asm
	bra.uni 	$L__BB26_45;
$L__BB26_54:
	mov.u64 	%rd137, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd138, %rd137;
	{ // callseq 114, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd138;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 114
$L__BB26_53:
	mov.u64 	%rd139, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd140, %rd139;
	{ // callseq 115, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 115
$L__BB26_10:
	mov.u64 	%rd112, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd113, %rd112;
	mov.u64 	%rd114, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd115, %rd114;
	{ // callseq 113, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd113;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd115;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 113
$L__BB26_8:
	mov.u64 	%rd108, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd109, %rd108;
	mov.u64 	%rd110, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd111, %rd110;
	{ // callseq 112, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd109;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd111;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 112
$L__BB26_19:
	mov.u64 	%rd142, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_10;
	cvta.global.u64 	%rd143, %rd142;
	{ // callseq 116, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd143;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 116
$L__BB26_55:
	mov.u64 	%rd104, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd105, %rd104;
	{ // callseq 111, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd105;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 111
$L__BB26_56:
	mov.u64 	%rd102, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd103, %rd102;
	{ // callseq 110, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd103;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 110
$L__BB26_30:
	mov.u64 	%rd144, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_10;
	cvta.global.u64 	%rd145, %rd144;
	{ // callseq 117, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd145;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 117
$L__BB26_4:
	mov.u64 	%rd73, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd74, %rd73;
	mov.u64 	%rd75, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd76, %rd75;
	{ // callseq 108, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd74;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd76;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 108
$L__BB26_21:
	mov.u64 	%rd77, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd78, %rd77;
	mov.u64 	%rd79, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd80, %rd79;
	{ // callseq 109, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd80;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 109
$L__BB26_5:
	mov.u64 	%rd158, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_5;
	cvta.global.u64 	%rd159, %rd158;
	mov.u64 	%rd160, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_6;
	cvta.global.u64 	%rd161, %rd160;
	{ // callseq 119, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd159;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd161;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 119
$L__BB26_52:
	mov.u64 	%rd151, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd152, %rd151;
	mov.u64 	%rd153, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd154, %rd153;
	{ // callseq 118, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd152;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd154;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 118

}
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_3
)
{
	.reg .pred 	%p<63>;
	.reg .b32 	%r<399>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<180>;

	ld.param.u64 	%rd57, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_2];
	setp.eq.s64 	%p8, %rd57, 0;
	@%p8 bra 	$L__BB27_5;
	ld.param.u64 	%rd56, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_1];
	ld.param.u64 	%rd58, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_0];
	add.s64 	%rd1, %rd57, -1;
	setp.gt.u64 	%p9, %rd56, 127;
	ld.param.u64 	%rd59, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_3];
	add.s64 	%rd60, %rd56, 1;
	selp.b64 	%rd61, 128, %rd60, %p9;
	ld.u64 	%rd62, [%rd58];
	ld.u64 	%rd63, [%rd58+8];
	ld.u32 	%r1, [%rd59];
	shl.b64 	%rd64, %rd63, 3;
	add.s64 	%rd2, %rd62, %rd64;
	setp.eq.s64 	%p10, %rd63, 0;
	selp.b64 	%rd3, 0, %rd62, %p10;
	selp.b64 	%rd65, 0, 8, %p10;
	add.s64 	%rd4, %rd62, %rd65;
	add.s64 	%rd66, %rd61, %rd1;
	setp.lt.u64 	%p11, %rd66, %rd61;
	setp.gt.u64 	%p12, %rd66, 127;
	or.pred  	%p1, %p11, %p12;
	mov.u64 	%rd7, 1;
	mov.u64 	%rd8, 8;
	bra.uni 	$L__BB27_2;
$L__BB27_32:
	shl.b64 	%rd7, %rd7, 1;
	bar.sync 	0;
	setp.gt.u64 	%p37, %rd8, 2;
	@%p37 bra 	$L__BB27_2;
	bra.uni 	$L__BB27_33;
$L__BB27_2:
	add.s64 	%rd8, %rd8, -1;
	@%p9 bra 	$L__BB27_32;
	cvt.u32.u64 	%r13, %rd8;
	shr.u64 	%rd69, %rd56, %r13;
	mov.u64 	%rd70, 1;
	shl.b64 	%rd27, %rd70, %r13;
	mov.u64 	%rd71, 2;
	shl.b64 	%rd28, %rd71, %r13;
	add.s64 	%rd29, %rd27, -1;
	and.b64  	%rd172, %rd29, %rd56;
	mul.lo.s64 	%rd72, %rd69, %rd28;
	add.s64 	%rd31, %rd172, %rd72;
	setp.lt.u64 	%p14, %rd31, 256;
	@%p14 bra 	$L__BB27_20;
	bra.uni 	$L__BB27_4;
$L__BB27_20:
	cvt.u32.u64 	%r18, %rd31;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r16, %r1, %r19;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	ld.shared.f32 %r14, [%r16];
ld.shared.f32 %r15, [%r17];
	// end inline asm
	add.s64 	%rd32, %rd31, %rd27;
	setp.lt.u64 	%p15, %rd32, 256;
	@%p15 bra 	$L__BB27_22;
	bra.uni 	$L__BB27_21;
$L__BB27_22:
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r22, %r1, %r25;
	add.s32 	%r23, %r22, 4;
	// begin inline asm
	ld.shared.f32 %r20, [%r22];
ld.shared.f32 %r21, [%r23];
	// end inline asm
	setp.eq.s64 	%p16, %rd172, 0;
	mov.u64 	%rd162, -1;
	mov.u64 	%rd177, %rd3;
	@%p16 bra 	$L__BB27_29;
	mul.lo.s64 	%rd176, %rd172, %rd7;
	mul.hi.u64 	%rd81, %rd172, %rd7;
	setp.eq.s64 	%p17, %rd81, 0;
	mov.u64 	%rd174, %rd4;
	@%p17 bra 	$L__BB27_28;
	mov.u64 	%rd173, %rd7;
	mov.u64 	%rd174, %rd4;
$L__BB27_25:
	setp.eq.s64 	%p18, %rd172, 0;
	@%p18 bra 	$L__BB27_55;
	setp.eq.s64 	%p19, %rd173, 0;
	@%p19 bra 	$L__BB27_56;
	div.u64 	%rd83, %rd162, %rd172;
	div.u64 	%rd84, %rd162, %rd173;
	mul.lo.s64 	%rd85, %rd83, %rd172;
	mul.lo.s64 	%rd86, %rd84, %rd173;
	setp.gt.u64 	%p20, %rd85, %rd86;
	max.u64 	%rd87, %rd85, %rd86;
	selp.b64 	%rd88, %rd83, 0, %p20;
	sub.s64 	%rd173, %rd173, %rd88;
	selp.b64 	%rd89, 0, %rd84, %p20;
	sub.s64 	%rd172, %rd172, %rd89;
	add.s64 	%rd90, %rd87, -1;
	sub.s64 	%rd91, %rd2, %rd174;
	shr.u64 	%rd92, %rd91, 3;
	setp.gt.u64 	%p21, %rd92, %rd90;
	shl.b64 	%rd93, %rd87, 3;
	add.s64 	%rd94, %rd174, %rd93;
	selp.b64 	%rd174, %rd94, %rd2, %p21;
	mul.lo.s64 	%rd176, %rd172, %rd173;
	mul.hi.u64 	%rd95, %rd172, %rd173;
	setp.ne.s64 	%p6, %rd95, 0;
	@%p6 bra 	$L__BB27_25;
$L__BB27_28:
	add.s64 	%rd96, %rd176, -1;
	sub.s64 	%rd97, %rd2, %rd174;
	shr.u64 	%rd98, %rd97, 3;
	setp.gt.u64 	%p22, %rd98, %rd96;
	shl.b64 	%rd99, %rd176, 3;
	add.s64 	%rd100, %rd174, %rd99;
	add.s64 	%rd101, %rd100, -8;
	selp.b64 	%rd177, %rd101, 0, %p22;
$L__BB27_29:
	setp.ne.s64 	%p23, %rd177, 0;
	@%p23 bra 	$L__BB27_31;
	bra.uni 	$L__BB27_30;
$L__BB27_31:
	mov.b32 	%f6, %r15;
	mov.b32 	%f5, %r14;
	mov.b32 	%f8, %r21;
	mov.b32 	%f7, %r20;
	// begin inline asm
	add.rn.ftz.f32 %r26, %r14, %r20;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r29, %r15, %r21;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r16], %r26;
st.shared.f32 [%r17], %r29;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r43, %r14, %r20;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r46, %r15, %r21;
	// end inline asm
	ld.u32 	%r56, [%rd177];
	ld.u32 	%r53, [%rd177+4];
	// begin inline asm
	mul.rn.ftz.f32 %r42, %r43, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r46, %r53;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r48, %r42, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r51, %r43, %r53;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r54, %r46, %r56;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r57, %r51, %r54;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r22], %r48;
st.shared.f32 [%r23], %r57;
	// end inline asm
	mov.u64 	%rd9, %rd66;
	@%p1 bra 	$L__BB27_32;
$L__BB27_7:
	shr.u64 	%rd106, %rd9, %r13;
	and.b64  	%rd166, %rd9, %rd29;
	mul.lo.s64 	%rd107, %rd106, %rd28;
	add.s64 	%rd12, %rd107, %rd166;
	setp.lt.u64 	%p24, %rd12, 256;
	@%p24 bra 	$L__BB27_9;
	bra.uni 	$L__BB27_8;
$L__BB27_9:
	cvt.u32.u64 	%r69, %rd12;
	shl.b32 	%r70, %r69, 3;
	add.s32 	%r67, %r70, %r1;
	add.s32 	%r68, %r67, 4;
	// begin inline asm
	ld.shared.f32 %r65, [%r67];
ld.shared.f32 %r66, [%r68];
	// end inline asm
	add.s64 	%rd13, %rd12, %rd27;
	setp.lt.u64 	%p25, %rd13, 256;
	@%p25 bra 	$L__BB27_11;
	bra.uni 	$L__BB27_10;
$L__BB27_11:
	cvt.u32.u64 	%r75, %rd13;
	shl.b32 	%r76, %r75, 3;
	add.s32 	%r73, %r76, %r1;
	add.s32 	%r74, %r73, 4;
	// begin inline asm
	ld.shared.f32 %r71, [%r73];
ld.shared.f32 %r72, [%r74];
	// end inline asm
	setp.eq.s64 	%p26, %rd166, 0;
	mov.u64 	%rd171, %rd3;
	@%p26 bra 	$L__BB27_18;
	mul.lo.s64 	%rd170, %rd166, %rd7;
	mul.hi.u64 	%rd116, %rd166, %rd7;
	setp.eq.s64 	%p27, %rd116, 0;
	mov.u64 	%rd168, %rd4;
	@%p27 bra 	$L__BB27_17;
	mov.u64 	%rd167, %rd7;
	mov.u64 	%rd168, %rd4;
$L__BB27_14:
	setp.eq.s64 	%p28, %rd166, 0;
	@%p28 bra 	$L__BB27_53;
	setp.eq.s64 	%p29, %rd167, 0;
	@%p29 bra 	$L__BB27_54;
	div.u64 	%rd118, %rd162, %rd166;
	div.u64 	%rd119, %rd162, %rd167;
	mul.lo.s64 	%rd120, %rd118, %rd166;
	mul.lo.s64 	%rd121, %rd119, %rd167;
	setp.gt.u64 	%p30, %rd120, %rd121;
	max.u64 	%rd122, %rd120, %rd121;
	selp.b64 	%rd123, %rd118, 0, %p30;
	sub.s64 	%rd167, %rd167, %rd123;
	selp.b64 	%rd124, 0, %rd119, %p30;
	sub.s64 	%rd166, %rd166, %rd124;
	add.s64 	%rd125, %rd122, -1;
	sub.s64 	%rd126, %rd2, %rd168;
	shr.u64 	%rd127, %rd126, 3;
	setp.gt.u64 	%p31, %rd127, %rd125;
	shl.b64 	%rd128, %rd122, 3;
	add.s64 	%rd129, %rd168, %rd128;
	selp.b64 	%rd168, %rd129, %rd2, %p31;
	mul.lo.s64 	%rd170, %rd166, %rd167;
	mul.hi.u64 	%rd130, %rd166, %rd167;
	setp.ne.s64 	%p3, %rd130, 0;
	@%p3 bra 	$L__BB27_14;
$L__BB27_17:
	add.s64 	%rd131, %rd170, -1;
	sub.s64 	%rd132, %rd2, %rd168;
	shr.u64 	%rd133, %rd132, 3;
	setp.gt.u64 	%p32, %rd133, %rd131;
	shl.b64 	%rd134, %rd170, 3;
	add.s64 	%rd135, %rd168, %rd134;
	add.s64 	%rd136, %rd135, -8;
	selp.b64 	%rd171, %rd136, 0, %p32;
$L__BB27_18:
	setp.ne.s64 	%p33, %rd171, 0;
	@%p33 bra 	$L__BB27_6;
	bra.uni 	$L__BB27_19;
$L__BB27_6:
	add.s64 	%rd10, %rd9, 1;
	mov.b32 	%f2, %r66;
	mov.b32 	%f1, %r65;
	mov.b32 	%f4, %r72;
	mov.b32 	%f3, %r71;
	// begin inline asm
	add.rn.ftz.f32 %r77, %r65, %r71;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r66, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r67], %r77;
st.shared.f32 [%r68], %r80;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r94, %r65, %r71;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r97, %r66, %r72;
	// end inline asm
	ld.u32 	%r107, [%rd171];
	ld.u32 	%r104, [%rd171+4];
	// begin inline asm
	mul.rn.ftz.f32 %r93, %r94, %r107;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r96, %r97, %r104;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r99, %r93, %r96;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r102, %r94, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r105, %r97, %r107;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r108, %r102, %r105;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r73], %r99;
st.shared.f32 [%r74], %r108;
	// end inline asm
	add.s64 	%rd141, %rd10, %rd1;
	setp.lt.u64 	%p34, %rd141, %rd10;
	add.s64 	%rd9, %rd9, %rd57;
	setp.gt.u64 	%p35, %rd9, 127;
	or.pred  	%p36, %p34, %p35;
	@%p36 bra 	$L__BB27_32;
	bra.uni 	$L__BB27_7;
$L__BB27_33:
	@%p9 bra 	$L__BB27_34;
	and.b64  	%rd146, %rd56, 1;
	setp.eq.b64 	%p39, %rd146, 1;
	mov.pred 	%p40, 0;
	xor.pred  	%p41, %p39, %p40;
	cvt.u32.u64 	%r115, %rd56;
	shl.b32 	%r12, %r115, 4;
	@%p41 bra 	$L__BB27_36;
	add.s32 	%r151, %r1, %r12;
	add.s32 	%r152, %r151, 4;
	// begin inline asm
	ld.shared.f32 %r158, [%r151];
ld.shared.f32 %r161, [%r152];
	// end inline asm
	add.s32 	%r173, %r151, 16;
	add.s32 	%r174, %r151, 20;
	// begin inline asm
	ld.shared.f32 %r159, [%r173];
ld.shared.f32 %r162, [%r174];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r165, %r158, %r159;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r166, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r151], %r165;
st.shared.f32 [%r152], %r166;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r175, %r158, %r159;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r176, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r173], %r175;
st.shared.f32 [%r174], %r176;
	// end inline asm
	bra.uni 	$L__BB27_37;
$L__BB27_36:
	and.b32  	%r146, %r12, 2016;
	add.s32 	%r147, %r1, %r146;
	add.s32 	%r130, %r147, 8;
	add.s32 	%r131, %r147, 12;
	// begin inline asm
	ld.shared.f32 %r125, [%r130];
ld.shared.f32 %r128, [%r131];
	// end inline asm
	or.b32  	%r148, %r12, 24;
	add.s32 	%r122, %r1, %r148;
	add.s32 	%r123, %r122, 4;
	// begin inline asm
	ld.shared.f32 %r126, [%r122];
ld.shared.f32 %r129, [%r123];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r132, %r125, %r126;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r133, %r128, %r129;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r130], %r132;
st.shared.f32 [%r131], %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r125, %r126;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r128, %r129;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r145, %r141;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r122], %r144;
st.shared.f32 [%r123], %r145;
	// end inline asm
$L__BB27_37:
	@%p1 bra 	$L__BB27_34;
	add.s64 	%rd46, %rd66, 1;
	and.b64  	%rd147, %rd66, 1;
	setp.eq.b64 	%p42, %rd147, 1;
	xor.pred  	%p44, %p42, %p40;
	not.pred 	%p45, %p44;
	cvt.u32.u64 	%r177, %rd66;
	shl.b32 	%r10, %r177, 4;
	@%p45 bra 	$L__BB27_40;
	bra.uni 	$L__BB27_39;
$L__BB27_40:
	add.s32 	%r213, %r1, %r10;
	add.s32 	%r214, %r213, 4;
	// begin inline asm
	ld.shared.f32 %r220, [%r213];
ld.shared.f32 %r223, [%r214];
	// end inline asm
	add.s32 	%r235, %r213, 16;
	add.s32 	%r236, %r213, 20;
	// begin inline asm
	ld.shared.f32 %r221, [%r235];
ld.shared.f32 %r224, [%r236];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r227, %r220, %r221;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r228, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r213], %r227;
st.shared.f32 [%r214], %r228;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r237, %r220, %r221;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r238, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r235], %r237;
st.shared.f32 [%r236], %r238;
	// end inline asm
	bra.uni 	$L__BB27_41;
$L__BB27_39:
	and.b32  	%r208, %r10, 2016;
	add.s32 	%r209, %r1, %r208;
	add.s32 	%r192, %r209, 8;
	add.s32 	%r193, %r209, 12;
	// begin inline asm
	ld.shared.f32 %r187, [%r192];
ld.shared.f32 %r190, [%r193];
	// end inline asm
	or.b32  	%r210, %r10, 24;
	add.s32 	%r184, %r1, %r210;
	add.s32 	%r185, %r184, 4;
	// begin inline asm
	ld.shared.f32 %r188, [%r184];
ld.shared.f32 %r191, [%r185];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r194, %r187, %r188;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r195, %r190, %r191;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r192], %r194;
st.shared.f32 [%r193], %r195;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r203, %r187, %r188;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r206, %r190, %r191;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r207, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r184], %r206;
st.shared.f32 [%r185], %r207;
	// end inline asm
$L__BB27_41:
	add.s64 	%rd148, %rd46, %rd1;
	setp.lt.u64 	%p46, %rd148, %rd46;
	add.s64 	%rd178, %rd66, %rd57;
	setp.gt.u64 	%p47, %rd178, 127;
	or.pred  	%p48, %p46, %p47;
	@%p48 bra 	$L__BB27_34;
	bra.uni 	$L__BB27_42;
$L__BB27_34:
	bar.sync 	0;
	@%p9 bra 	$L__BB27_51;
	cvt.u32.u64 	%r329, %rd56;
	shl.b32 	%r330, %r329, 4;
	add.s32 	%r303, %r1, %r330;
	add.s32 	%r304, %r303, 4;
	// begin inline asm
	ld.shared.f32 %r310, [%r303];
ld.shared.f32 %r313, [%r304];
	// end inline asm
	add.s32 	%r307, %r303, 8;
	add.s32 	%r308, %r303, 12;
	// begin inline asm
	ld.shared.f32 %r311, [%r307];
ld.shared.f32 %r314, [%r308];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r309, %r310, %r311;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r312, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r303], %r309;
st.shared.f32 [%r304], %r312;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r319, %r310, %r311;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r322, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r307], %r319;
st.shared.f32 [%r308], %r322;
	// end inline asm
	@!%p1 bra 	$L__BB27_48;
	bra.uni 	$L__BB27_51;
$L__BB27_48:
	shl.b64 	%rd55, %rd66, 1;
	setp.gt.u64 	%p56, %rd55, 255;
	@%p56 bra 	$L__BB27_52;
	add.s64 	%rd54, %rd66, 1;
	cvt.u32.u64 	%r359, %rd55;
	shl.b32 	%r360, %r359, 3;
	add.s32 	%r333, %r1, %r360;
	add.s32 	%r334, %r333, 4;
	// begin inline asm
	ld.shared.f32 %r340, [%r333];
ld.shared.f32 %r343, [%r334];
	// end inline asm
	add.s32 	%r337, %r333, 8;
	add.s32 	%r338, %r333, 12;
	// begin inline asm
	ld.shared.f32 %r341, [%r337];
ld.shared.f32 %r344, [%r338];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r339, %r340, %r341;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r342, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r333], %r339;
st.shared.f32 [%r334], %r342;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r349, %r340, %r341;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r352, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r337], %r349;
st.shared.f32 [%r338], %r352;
	// end inline asm
	add.s64 	%rd155, %rd54, %rd1;
	setp.lt.u64 	%p57, %rd155, %rd54;
	add.s64 	%rd179, %rd66, %rd57;
	setp.gt.u64 	%p58, %rd179, 127;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB27_51;
$L__BB27_50:
	add.s64 	%rd156, %rd179, 1;
	cvt.u32.u64 	%r389, %rd179;
	shl.b32 	%r390, %r389, 4;
	add.s32 	%r363, %r390, %r1;
	add.s32 	%r364, %r363, 4;
	// begin inline asm
	ld.shared.f32 %r370, [%r363];
ld.shared.f32 %r373, [%r364];
	// end inline asm
	add.s32 	%r367, %r363, 8;
	add.s32 	%r368, %r363, 12;
	// begin inline asm
	ld.shared.f32 %r371, [%r367];
ld.shared.f32 %r374, [%r368];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r369, %r370, %r371;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r372, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r363], %r369;
st.shared.f32 [%r364], %r372;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r379, %r370, %r371;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r382, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r367], %r379;
st.shared.f32 [%r368], %r382;
	// end inline asm
	add.s64 	%rd157, %rd156, %rd1;
	setp.lt.u64 	%p60, %rd157, %rd156;
	add.s64 	%rd179, %rd179, %rd57;
	setp.gt.u64 	%p61, %rd179, 127;
	or.pred  	%p62, %p60, %p61;
	@%p62 bra 	$L__BB27_51;
	bra.uni 	$L__BB27_50;
$L__BB27_51:
	ret;
$L__BB27_44:
	and.b32  	%r270, %r11, 2016;
	add.s32 	%r271, %r1, %r270;
	add.s32 	%r254, %r271, 8;
	add.s32 	%r255, %r271, 12;
	// begin inline asm
	ld.shared.f32 %r249, [%r254];
ld.shared.f32 %r252, [%r255];
	// end inline asm
	or.b32  	%r272, %r11, 24;
	add.s32 	%r246, %r272, %r1;
	add.s32 	%r247, %r246, 4;
	// begin inline asm
	ld.shared.f32 %r250, [%r246];
ld.shared.f32 %r253, [%r247];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r256, %r249, %r250;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r257, %r252, %r253;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r254], %r256;
st.shared.f32 [%r255], %r257;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r265, %r249, %r250;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r268, %r252, %r253;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r269, %r265;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r246], %r268;
st.shared.f32 [%r247], %r269;
	// end inline asm
$L__BB27_45:
	add.s64 	%rd49, %rd178, 1;
	add.s64 	%rd150, %rd49, %rd1;
	setp.lt.u64 	%p52, %rd150, %rd49;
	add.s64 	%rd178, %rd178, %rd57;
	setp.gt.u64 	%p53, %rd178, 127;
	or.pred  	%p54, %p52, %p53;
	@%p54 bra 	$L__BB27_34;
$L__BB27_42:
	and.b64  	%rd149, %rd178, 1;
	setp.eq.b64 	%p49, %rd149, 1;
	xor.pred  	%p51, %p49, %p40;
	cvt.u32.u64 	%r239, %rd178;
	shl.b32 	%r11, %r239, 4;
	@%p51 bra 	$L__BB27_44;
	add.s32 	%r275, %r11, %r1;
	add.s32 	%r276, %r275, 4;
	// begin inline asm
	ld.shared.f32 %r282, [%r275];
ld.shared.f32 %r285, [%r276];
	// end inline asm
	add.s32 	%r297, %r275, 16;
	add.s32 	%r298, %r275, 20;
	// begin inline asm
	ld.shared.f32 %r283, [%r297];
ld.shared.f32 %r286, [%r298];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r289, %r282, %r283;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r290, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r275], %r289;
st.shared.f32 [%r276], %r290;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r299, %r282, %r283;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r300, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r297], %r299;
st.shared.f32 [%r298], %r300;
	// end inline asm
	bra.uni 	$L__BB27_45;
$L__BB27_54:
	mov.u64 	%rd137, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd138, %rd137;
	{ // callseq 126, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd138;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 126
$L__BB27_53:
	mov.u64 	%rd139, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd140, %rd139;
	{ // callseq 127, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 127
$L__BB27_10:
	mov.u64 	%rd112, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd113, %rd112;
	mov.u64 	%rd114, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd115, %rd114;
	{ // callseq 125, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd113;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd115;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 125
$L__BB27_8:
	mov.u64 	%rd108, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd109, %rd108;
	mov.u64 	%rd110, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd111, %rd110;
	{ // callseq 124, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd109;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd111;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 124
$L__BB27_19:
	mov.u64 	%rd142, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_10;
	cvta.global.u64 	%rd143, %rd142;
	{ // callseq 128, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd143;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 128
$L__BB27_55:
	mov.u64 	%rd104, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd105, %rd104;
	{ // callseq 123, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd105;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 123
$L__BB27_56:
	mov.u64 	%rd102, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd103, %rd102;
	{ // callseq 122, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd103;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 122
$L__BB27_30:
	mov.u64 	%rd144, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_10;
	cvta.global.u64 	%rd145, %rd144;
	{ // callseq 129, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd145;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 129
$L__BB27_4:
	mov.u64 	%rd73, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd74, %rd73;
	mov.u64 	%rd75, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd76, %rd75;
	{ // callseq 120, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd74;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd76;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 120
$L__BB27_21:
	mov.u64 	%rd77, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd78, %rd77;
	mov.u64 	%rd79, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd80, %rd79;
	{ // callseq 121, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd80;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 121
$L__BB27_5:
	mov.u64 	%rd158, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_5;
	cvta.global.u64 	%rd159, %rd158;
	mov.u64 	%rd160, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_6;
	cvta.global.u64 	%rd161, %rd160;
	{ // callseq 131, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd159;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd161;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 131
$L__BB27_52:
	mov.u64 	%rd151, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd152, %rd151;
	mov.u64 	%rd153, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd154, %rd153;
	{ // callseq 130, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd152;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd154;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 130

}
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_3
)
{
	.reg .pred 	%p<63>;
	.reg .b32 	%r<399>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<180>;

	ld.param.u64 	%rd57, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_2];
	setp.eq.s64 	%p8, %rd57, 0;
	@%p8 bra 	$L__BB28_5;
	ld.param.u64 	%rd56, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_1];
	ld.param.u64 	%rd58, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_0];
	add.s64 	%rd1, %rd57, -1;
	setp.gt.u64 	%p9, %rd56, 255;
	ld.param.u64 	%rd59, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_3];
	add.s64 	%rd60, %rd56, 1;
	selp.b64 	%rd61, 256, %rd60, %p9;
	ld.u64 	%rd62, [%rd58];
	ld.u64 	%rd63, [%rd58+8];
	ld.u32 	%r1, [%rd59];
	shl.b64 	%rd64, %rd63, 3;
	add.s64 	%rd2, %rd62, %rd64;
	setp.eq.s64 	%p10, %rd63, 0;
	selp.b64 	%rd3, 0, %rd62, %p10;
	selp.b64 	%rd65, 0, 8, %p10;
	add.s64 	%rd4, %rd62, %rd65;
	add.s64 	%rd66, %rd61, %rd1;
	setp.lt.u64 	%p11, %rd66, %rd61;
	setp.gt.u64 	%p12, %rd66, 255;
	or.pred  	%p1, %p11, %p12;
	mov.u64 	%rd7, 1;
	mov.u64 	%rd8, 9;
	bra.uni 	$L__BB28_2;
$L__BB28_32:
	shl.b64 	%rd7, %rd7, 1;
	bar.sync 	0;
	setp.gt.u64 	%p37, %rd8, 2;
	@%p37 bra 	$L__BB28_2;
	bra.uni 	$L__BB28_33;
$L__BB28_2:
	add.s64 	%rd8, %rd8, -1;
	@%p9 bra 	$L__BB28_32;
	cvt.u32.u64 	%r13, %rd8;
	shr.u64 	%rd69, %rd56, %r13;
	mov.u64 	%rd70, 1;
	shl.b64 	%rd27, %rd70, %r13;
	mov.u64 	%rd71, 2;
	shl.b64 	%rd28, %rd71, %r13;
	add.s64 	%rd29, %rd27, -1;
	and.b64  	%rd172, %rd29, %rd56;
	mul.lo.s64 	%rd72, %rd69, %rd28;
	add.s64 	%rd31, %rd172, %rd72;
	setp.lt.u64 	%p14, %rd31, 512;
	@%p14 bra 	$L__BB28_20;
	bra.uni 	$L__BB28_4;
$L__BB28_20:
	cvt.u32.u64 	%r18, %rd31;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r16, %r1, %r19;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	ld.shared.f32 %r14, [%r16];
ld.shared.f32 %r15, [%r17];
	// end inline asm
	add.s64 	%rd32, %rd31, %rd27;
	setp.lt.u64 	%p15, %rd32, 512;
	@%p15 bra 	$L__BB28_22;
	bra.uni 	$L__BB28_21;
$L__BB28_22:
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r22, %r1, %r25;
	add.s32 	%r23, %r22, 4;
	// begin inline asm
	ld.shared.f32 %r20, [%r22];
ld.shared.f32 %r21, [%r23];
	// end inline asm
	setp.eq.s64 	%p16, %rd172, 0;
	mov.u64 	%rd162, -1;
	mov.u64 	%rd177, %rd3;
	@%p16 bra 	$L__BB28_29;
	mul.lo.s64 	%rd176, %rd172, %rd7;
	mul.hi.u64 	%rd81, %rd172, %rd7;
	setp.eq.s64 	%p17, %rd81, 0;
	mov.u64 	%rd174, %rd4;
	@%p17 bra 	$L__BB28_28;
	mov.u64 	%rd173, %rd7;
	mov.u64 	%rd174, %rd4;
$L__BB28_25:
	setp.eq.s64 	%p18, %rd172, 0;
	@%p18 bra 	$L__BB28_55;
	setp.eq.s64 	%p19, %rd173, 0;
	@%p19 bra 	$L__BB28_56;
	div.u64 	%rd83, %rd162, %rd172;
	div.u64 	%rd84, %rd162, %rd173;
	mul.lo.s64 	%rd85, %rd83, %rd172;
	mul.lo.s64 	%rd86, %rd84, %rd173;
	setp.gt.u64 	%p20, %rd85, %rd86;
	max.u64 	%rd87, %rd85, %rd86;
	selp.b64 	%rd88, %rd83, 0, %p20;
	sub.s64 	%rd173, %rd173, %rd88;
	selp.b64 	%rd89, 0, %rd84, %p20;
	sub.s64 	%rd172, %rd172, %rd89;
	add.s64 	%rd90, %rd87, -1;
	sub.s64 	%rd91, %rd2, %rd174;
	shr.u64 	%rd92, %rd91, 3;
	setp.gt.u64 	%p21, %rd92, %rd90;
	shl.b64 	%rd93, %rd87, 3;
	add.s64 	%rd94, %rd174, %rd93;
	selp.b64 	%rd174, %rd94, %rd2, %p21;
	mul.lo.s64 	%rd176, %rd172, %rd173;
	mul.hi.u64 	%rd95, %rd172, %rd173;
	setp.ne.s64 	%p6, %rd95, 0;
	@%p6 bra 	$L__BB28_25;
$L__BB28_28:
	add.s64 	%rd96, %rd176, -1;
	sub.s64 	%rd97, %rd2, %rd174;
	shr.u64 	%rd98, %rd97, 3;
	setp.gt.u64 	%p22, %rd98, %rd96;
	shl.b64 	%rd99, %rd176, 3;
	add.s64 	%rd100, %rd174, %rd99;
	add.s64 	%rd101, %rd100, -8;
	selp.b64 	%rd177, %rd101, 0, %p22;
$L__BB28_29:
	setp.ne.s64 	%p23, %rd177, 0;
	@%p23 bra 	$L__BB28_31;
	bra.uni 	$L__BB28_30;
$L__BB28_31:
	mov.b32 	%f6, %r15;
	mov.b32 	%f5, %r14;
	mov.b32 	%f8, %r21;
	mov.b32 	%f7, %r20;
	// begin inline asm
	add.rn.ftz.f32 %r26, %r14, %r20;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r29, %r15, %r21;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r16], %r26;
st.shared.f32 [%r17], %r29;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r43, %r14, %r20;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r46, %r15, %r21;
	// end inline asm
	ld.u32 	%r56, [%rd177];
	ld.u32 	%r53, [%rd177+4];
	// begin inline asm
	mul.rn.ftz.f32 %r42, %r43, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r46, %r53;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r48, %r42, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r51, %r43, %r53;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r54, %r46, %r56;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r57, %r51, %r54;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r22], %r48;
st.shared.f32 [%r23], %r57;
	// end inline asm
	mov.u64 	%rd9, %rd66;
	@%p1 bra 	$L__BB28_32;
$L__BB28_7:
	shr.u64 	%rd106, %rd9, %r13;
	and.b64  	%rd166, %rd9, %rd29;
	mul.lo.s64 	%rd107, %rd106, %rd28;
	add.s64 	%rd12, %rd107, %rd166;
	setp.lt.u64 	%p24, %rd12, 512;
	@%p24 bra 	$L__BB28_9;
	bra.uni 	$L__BB28_8;
$L__BB28_9:
	cvt.u32.u64 	%r69, %rd12;
	shl.b32 	%r70, %r69, 3;
	add.s32 	%r67, %r70, %r1;
	add.s32 	%r68, %r67, 4;
	// begin inline asm
	ld.shared.f32 %r65, [%r67];
ld.shared.f32 %r66, [%r68];
	// end inline asm
	add.s64 	%rd13, %rd12, %rd27;
	setp.lt.u64 	%p25, %rd13, 512;
	@%p25 bra 	$L__BB28_11;
	bra.uni 	$L__BB28_10;
$L__BB28_11:
	cvt.u32.u64 	%r75, %rd13;
	shl.b32 	%r76, %r75, 3;
	add.s32 	%r73, %r76, %r1;
	add.s32 	%r74, %r73, 4;
	// begin inline asm
	ld.shared.f32 %r71, [%r73];
ld.shared.f32 %r72, [%r74];
	// end inline asm
	setp.eq.s64 	%p26, %rd166, 0;
	mov.u64 	%rd171, %rd3;
	@%p26 bra 	$L__BB28_18;
	mul.lo.s64 	%rd170, %rd166, %rd7;
	mul.hi.u64 	%rd116, %rd166, %rd7;
	setp.eq.s64 	%p27, %rd116, 0;
	mov.u64 	%rd168, %rd4;
	@%p27 bra 	$L__BB28_17;
	mov.u64 	%rd167, %rd7;
	mov.u64 	%rd168, %rd4;
$L__BB28_14:
	setp.eq.s64 	%p28, %rd166, 0;
	@%p28 bra 	$L__BB28_53;
	setp.eq.s64 	%p29, %rd167, 0;
	@%p29 bra 	$L__BB28_54;
	div.u64 	%rd118, %rd162, %rd166;
	div.u64 	%rd119, %rd162, %rd167;
	mul.lo.s64 	%rd120, %rd118, %rd166;
	mul.lo.s64 	%rd121, %rd119, %rd167;
	setp.gt.u64 	%p30, %rd120, %rd121;
	max.u64 	%rd122, %rd120, %rd121;
	selp.b64 	%rd123, %rd118, 0, %p30;
	sub.s64 	%rd167, %rd167, %rd123;
	selp.b64 	%rd124, 0, %rd119, %p30;
	sub.s64 	%rd166, %rd166, %rd124;
	add.s64 	%rd125, %rd122, -1;
	sub.s64 	%rd126, %rd2, %rd168;
	shr.u64 	%rd127, %rd126, 3;
	setp.gt.u64 	%p31, %rd127, %rd125;
	shl.b64 	%rd128, %rd122, 3;
	add.s64 	%rd129, %rd168, %rd128;
	selp.b64 	%rd168, %rd129, %rd2, %p31;
	mul.lo.s64 	%rd170, %rd166, %rd167;
	mul.hi.u64 	%rd130, %rd166, %rd167;
	setp.ne.s64 	%p3, %rd130, 0;
	@%p3 bra 	$L__BB28_14;
$L__BB28_17:
	add.s64 	%rd131, %rd170, -1;
	sub.s64 	%rd132, %rd2, %rd168;
	shr.u64 	%rd133, %rd132, 3;
	setp.gt.u64 	%p32, %rd133, %rd131;
	shl.b64 	%rd134, %rd170, 3;
	add.s64 	%rd135, %rd168, %rd134;
	add.s64 	%rd136, %rd135, -8;
	selp.b64 	%rd171, %rd136, 0, %p32;
$L__BB28_18:
	setp.ne.s64 	%p33, %rd171, 0;
	@%p33 bra 	$L__BB28_6;
	bra.uni 	$L__BB28_19;
$L__BB28_6:
	add.s64 	%rd10, %rd9, 1;
	mov.b32 	%f2, %r66;
	mov.b32 	%f1, %r65;
	mov.b32 	%f4, %r72;
	mov.b32 	%f3, %r71;
	// begin inline asm
	add.rn.ftz.f32 %r77, %r65, %r71;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r66, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r67], %r77;
st.shared.f32 [%r68], %r80;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r94, %r65, %r71;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r97, %r66, %r72;
	// end inline asm
	ld.u32 	%r107, [%rd171];
	ld.u32 	%r104, [%rd171+4];
	// begin inline asm
	mul.rn.ftz.f32 %r93, %r94, %r107;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r96, %r97, %r104;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r99, %r93, %r96;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r102, %r94, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r105, %r97, %r107;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r108, %r102, %r105;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r73], %r99;
st.shared.f32 [%r74], %r108;
	// end inline asm
	add.s64 	%rd141, %rd10, %rd1;
	setp.lt.u64 	%p34, %rd141, %rd10;
	add.s64 	%rd9, %rd9, %rd57;
	setp.gt.u64 	%p35, %rd9, 255;
	or.pred  	%p36, %p34, %p35;
	@%p36 bra 	$L__BB28_32;
	bra.uni 	$L__BB28_7;
$L__BB28_33:
	@%p9 bra 	$L__BB28_34;
	and.b64  	%rd146, %rd56, 1;
	setp.eq.b64 	%p39, %rd146, 1;
	mov.pred 	%p40, 0;
	xor.pred  	%p41, %p39, %p40;
	cvt.u32.u64 	%r115, %rd56;
	shl.b32 	%r12, %r115, 4;
	@%p41 bra 	$L__BB28_36;
	add.s32 	%r151, %r1, %r12;
	add.s32 	%r152, %r151, 4;
	// begin inline asm
	ld.shared.f32 %r158, [%r151];
ld.shared.f32 %r161, [%r152];
	// end inline asm
	add.s32 	%r173, %r151, 16;
	add.s32 	%r174, %r151, 20;
	// begin inline asm
	ld.shared.f32 %r159, [%r173];
ld.shared.f32 %r162, [%r174];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r165, %r158, %r159;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r166, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r151], %r165;
st.shared.f32 [%r152], %r166;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r175, %r158, %r159;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r176, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r173], %r175;
st.shared.f32 [%r174], %r176;
	// end inline asm
	bra.uni 	$L__BB28_37;
$L__BB28_36:
	and.b32  	%r146, %r12, 4064;
	add.s32 	%r147, %r1, %r146;
	add.s32 	%r130, %r147, 8;
	add.s32 	%r131, %r147, 12;
	// begin inline asm
	ld.shared.f32 %r125, [%r130];
ld.shared.f32 %r128, [%r131];
	// end inline asm
	or.b32  	%r148, %r12, 24;
	add.s32 	%r122, %r1, %r148;
	add.s32 	%r123, %r122, 4;
	// begin inline asm
	ld.shared.f32 %r126, [%r122];
ld.shared.f32 %r129, [%r123];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r132, %r125, %r126;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r133, %r128, %r129;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r130], %r132;
st.shared.f32 [%r131], %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r125, %r126;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r128, %r129;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r145, %r141;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r122], %r144;
st.shared.f32 [%r123], %r145;
	// end inline asm
$L__BB28_37:
	@%p1 bra 	$L__BB28_34;
	add.s64 	%rd46, %rd66, 1;
	and.b64  	%rd147, %rd66, 1;
	setp.eq.b64 	%p42, %rd147, 1;
	xor.pred  	%p44, %p42, %p40;
	not.pred 	%p45, %p44;
	cvt.u32.u64 	%r177, %rd66;
	shl.b32 	%r10, %r177, 4;
	@%p45 bra 	$L__BB28_40;
	bra.uni 	$L__BB28_39;
$L__BB28_40:
	add.s32 	%r213, %r1, %r10;
	add.s32 	%r214, %r213, 4;
	// begin inline asm
	ld.shared.f32 %r220, [%r213];
ld.shared.f32 %r223, [%r214];
	// end inline asm
	add.s32 	%r235, %r213, 16;
	add.s32 	%r236, %r213, 20;
	// begin inline asm
	ld.shared.f32 %r221, [%r235];
ld.shared.f32 %r224, [%r236];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r227, %r220, %r221;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r228, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r213], %r227;
st.shared.f32 [%r214], %r228;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r237, %r220, %r221;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r238, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r235], %r237;
st.shared.f32 [%r236], %r238;
	// end inline asm
	bra.uni 	$L__BB28_41;
$L__BB28_39:
	and.b32  	%r208, %r10, 4064;
	add.s32 	%r209, %r1, %r208;
	add.s32 	%r192, %r209, 8;
	add.s32 	%r193, %r209, 12;
	// begin inline asm
	ld.shared.f32 %r187, [%r192];
ld.shared.f32 %r190, [%r193];
	// end inline asm
	or.b32  	%r210, %r10, 24;
	add.s32 	%r184, %r1, %r210;
	add.s32 	%r185, %r184, 4;
	// begin inline asm
	ld.shared.f32 %r188, [%r184];
ld.shared.f32 %r191, [%r185];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r194, %r187, %r188;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r195, %r190, %r191;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r192], %r194;
st.shared.f32 [%r193], %r195;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r203, %r187, %r188;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r206, %r190, %r191;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r207, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r184], %r206;
st.shared.f32 [%r185], %r207;
	// end inline asm
$L__BB28_41:
	add.s64 	%rd148, %rd46, %rd1;
	setp.lt.u64 	%p46, %rd148, %rd46;
	add.s64 	%rd178, %rd66, %rd57;
	setp.gt.u64 	%p47, %rd178, 255;
	or.pred  	%p48, %p46, %p47;
	@%p48 bra 	$L__BB28_34;
	bra.uni 	$L__BB28_42;
$L__BB28_34:
	bar.sync 	0;
	@%p9 bra 	$L__BB28_51;
	cvt.u32.u64 	%r329, %rd56;
	shl.b32 	%r330, %r329, 4;
	add.s32 	%r303, %r1, %r330;
	add.s32 	%r304, %r303, 4;
	// begin inline asm
	ld.shared.f32 %r310, [%r303];
ld.shared.f32 %r313, [%r304];
	// end inline asm
	add.s32 	%r307, %r303, 8;
	add.s32 	%r308, %r303, 12;
	// begin inline asm
	ld.shared.f32 %r311, [%r307];
ld.shared.f32 %r314, [%r308];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r309, %r310, %r311;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r312, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r303], %r309;
st.shared.f32 [%r304], %r312;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r319, %r310, %r311;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r322, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r307], %r319;
st.shared.f32 [%r308], %r322;
	// end inline asm
	@!%p1 bra 	$L__BB28_48;
	bra.uni 	$L__BB28_51;
$L__BB28_48:
	shl.b64 	%rd55, %rd66, 1;
	setp.gt.u64 	%p56, %rd55, 511;
	@%p56 bra 	$L__BB28_52;
	add.s64 	%rd54, %rd66, 1;
	cvt.u32.u64 	%r359, %rd55;
	shl.b32 	%r360, %r359, 3;
	add.s32 	%r333, %r1, %r360;
	add.s32 	%r334, %r333, 4;
	// begin inline asm
	ld.shared.f32 %r340, [%r333];
ld.shared.f32 %r343, [%r334];
	// end inline asm
	add.s32 	%r337, %r333, 8;
	add.s32 	%r338, %r333, 12;
	// begin inline asm
	ld.shared.f32 %r341, [%r337];
ld.shared.f32 %r344, [%r338];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r339, %r340, %r341;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r342, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r333], %r339;
st.shared.f32 [%r334], %r342;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r349, %r340, %r341;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r352, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r337], %r349;
st.shared.f32 [%r338], %r352;
	// end inline asm
	add.s64 	%rd155, %rd54, %rd1;
	setp.lt.u64 	%p57, %rd155, %rd54;
	add.s64 	%rd179, %rd66, %rd57;
	setp.gt.u64 	%p58, %rd179, 255;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB28_51;
$L__BB28_50:
	add.s64 	%rd156, %rd179, 1;
	cvt.u32.u64 	%r389, %rd179;
	shl.b32 	%r390, %r389, 4;
	add.s32 	%r363, %r390, %r1;
	add.s32 	%r364, %r363, 4;
	// begin inline asm
	ld.shared.f32 %r370, [%r363];
ld.shared.f32 %r373, [%r364];
	// end inline asm
	add.s32 	%r367, %r363, 8;
	add.s32 	%r368, %r363, 12;
	// begin inline asm
	ld.shared.f32 %r371, [%r367];
ld.shared.f32 %r374, [%r368];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r369, %r370, %r371;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r372, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r363], %r369;
st.shared.f32 [%r364], %r372;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r379, %r370, %r371;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r382, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r367], %r379;
st.shared.f32 [%r368], %r382;
	// end inline asm
	add.s64 	%rd157, %rd156, %rd1;
	setp.lt.u64 	%p60, %rd157, %rd156;
	add.s64 	%rd179, %rd179, %rd57;
	setp.gt.u64 	%p61, %rd179, 255;
	or.pred  	%p62, %p60, %p61;
	@%p62 bra 	$L__BB28_51;
	bra.uni 	$L__BB28_50;
$L__BB28_51:
	ret;
$L__BB28_44:
	and.b32  	%r270, %r11, 4064;
	add.s32 	%r271, %r1, %r270;
	add.s32 	%r254, %r271, 8;
	add.s32 	%r255, %r271, 12;
	// begin inline asm
	ld.shared.f32 %r249, [%r254];
ld.shared.f32 %r252, [%r255];
	// end inline asm
	or.b32  	%r272, %r11, 24;
	add.s32 	%r246, %r272, %r1;
	add.s32 	%r247, %r246, 4;
	// begin inline asm
	ld.shared.f32 %r250, [%r246];
ld.shared.f32 %r253, [%r247];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r256, %r249, %r250;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r257, %r252, %r253;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r254], %r256;
st.shared.f32 [%r255], %r257;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r265, %r249, %r250;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r268, %r252, %r253;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r269, %r265;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r246], %r268;
st.shared.f32 [%r247], %r269;
	// end inline asm
$L__BB28_45:
	add.s64 	%rd49, %rd178, 1;
	add.s64 	%rd150, %rd49, %rd1;
	setp.lt.u64 	%p52, %rd150, %rd49;
	add.s64 	%rd178, %rd178, %rd57;
	setp.gt.u64 	%p53, %rd178, 255;
	or.pred  	%p54, %p52, %p53;
	@%p54 bra 	$L__BB28_34;
$L__BB28_42:
	and.b64  	%rd149, %rd178, 1;
	setp.eq.b64 	%p49, %rd149, 1;
	xor.pred  	%p51, %p49, %p40;
	cvt.u32.u64 	%r239, %rd178;
	shl.b32 	%r11, %r239, 4;
	@%p51 bra 	$L__BB28_44;
	add.s32 	%r275, %r11, %r1;
	add.s32 	%r276, %r275, 4;
	// begin inline asm
	ld.shared.f32 %r282, [%r275];
ld.shared.f32 %r285, [%r276];
	// end inline asm
	add.s32 	%r297, %r275, 16;
	add.s32 	%r298, %r275, 20;
	// begin inline asm
	ld.shared.f32 %r283, [%r297];
ld.shared.f32 %r286, [%r298];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r289, %r282, %r283;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r290, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r275], %r289;
st.shared.f32 [%r276], %r290;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r299, %r282, %r283;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r300, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r297], %r299;
st.shared.f32 [%r298], %r300;
	// end inline asm
	bra.uni 	$L__BB28_45;
$L__BB28_54:
	mov.u64 	%rd137, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd138, %rd137;
	{ // callseq 138, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd138;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 138
$L__BB28_53:
	mov.u64 	%rd139, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd140, %rd139;
	{ // callseq 139, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 139
$L__BB28_10:
	mov.u64 	%rd112, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd113, %rd112;
	mov.u64 	%rd114, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd115, %rd114;
	{ // callseq 137, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd113;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd115;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 137
$L__BB28_8:
	mov.u64 	%rd108, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd109, %rd108;
	mov.u64 	%rd110, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd111, %rd110;
	{ // callseq 136, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd109;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd111;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 136
$L__BB28_19:
	mov.u64 	%rd142, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_10;
	cvta.global.u64 	%rd143, %rd142;
	{ // callseq 140, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd143;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 140
$L__BB28_55:
	mov.u64 	%rd104, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd105, %rd104;
	{ // callseq 135, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd105;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 135
$L__BB28_56:
	mov.u64 	%rd102, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd103, %rd102;
	{ // callseq 134, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd103;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 134
$L__BB28_30:
	mov.u64 	%rd144, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_10;
	cvta.global.u64 	%rd145, %rd144;
	{ // callseq 141, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd145;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 141
$L__BB28_4:
	mov.u64 	%rd73, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd74, %rd73;
	mov.u64 	%rd75, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd76, %rd75;
	{ // callseq 132, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd74;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd76;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 132
$L__BB28_21:
	mov.u64 	%rd77, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd78, %rd77;
	mov.u64 	%rd79, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd80, %rd79;
	{ // callseq 133, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd80;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 133
$L__BB28_5:
	mov.u64 	%rd158, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_5;
	cvta.global.u64 	%rd159, %rd158;
	mov.u64 	%rd160, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_6;
	cvta.global.u64 	%rd161, %rd160;
	{ // callseq 143, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd159;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd161;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 143
$L__BB28_52:
	mov.u64 	%rd151, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd152, %rd151;
	mov.u64 	%rd153, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd154, %rd153;
	{ // callseq 142, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd152;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd154;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 142

}
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_3
)
{
	.reg .pred 	%p<63>;
	.reg .b32 	%r<399>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<180>;

	ld.param.u64 	%rd57, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_2];
	setp.eq.s64 	%p8, %rd57, 0;
	@%p8 bra 	$L__BB29_5;
	ld.param.u64 	%rd56, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_1];
	ld.param.u64 	%rd58, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_0];
	add.s64 	%rd1, %rd57, -1;
	setp.gt.u64 	%p9, %rd56, 511;
	ld.param.u64 	%rd59, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_3];
	add.s64 	%rd60, %rd56, 1;
	selp.b64 	%rd61, 512, %rd60, %p9;
	ld.u64 	%rd62, [%rd58];
	ld.u64 	%rd63, [%rd58+8];
	ld.u32 	%r1, [%rd59];
	shl.b64 	%rd64, %rd63, 3;
	add.s64 	%rd2, %rd62, %rd64;
	setp.eq.s64 	%p10, %rd63, 0;
	selp.b64 	%rd3, 0, %rd62, %p10;
	selp.b64 	%rd65, 0, 8, %p10;
	add.s64 	%rd4, %rd62, %rd65;
	add.s64 	%rd66, %rd61, %rd1;
	setp.lt.u64 	%p11, %rd66, %rd61;
	setp.gt.u64 	%p12, %rd66, 511;
	or.pred  	%p1, %p11, %p12;
	mov.u64 	%rd7, 1;
	mov.u64 	%rd8, 10;
	bra.uni 	$L__BB29_2;
$L__BB29_32:
	shl.b64 	%rd7, %rd7, 1;
	bar.sync 	0;
	setp.gt.u64 	%p37, %rd8, 2;
	@%p37 bra 	$L__BB29_2;
	bra.uni 	$L__BB29_33;
$L__BB29_2:
	add.s64 	%rd8, %rd8, -1;
	@%p9 bra 	$L__BB29_32;
	cvt.u32.u64 	%r13, %rd8;
	shr.u64 	%rd69, %rd56, %r13;
	mov.u64 	%rd70, 1;
	shl.b64 	%rd27, %rd70, %r13;
	mov.u64 	%rd71, 2;
	shl.b64 	%rd28, %rd71, %r13;
	add.s64 	%rd29, %rd27, -1;
	and.b64  	%rd172, %rd29, %rd56;
	mul.lo.s64 	%rd72, %rd69, %rd28;
	add.s64 	%rd31, %rd172, %rd72;
	setp.lt.u64 	%p14, %rd31, 1024;
	@%p14 bra 	$L__BB29_20;
	bra.uni 	$L__BB29_4;
$L__BB29_20:
	cvt.u32.u64 	%r18, %rd31;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r16, %r1, %r19;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	ld.shared.f32 %r14, [%r16];
ld.shared.f32 %r15, [%r17];
	// end inline asm
	add.s64 	%rd32, %rd31, %rd27;
	setp.lt.u64 	%p15, %rd32, 1024;
	@%p15 bra 	$L__BB29_22;
	bra.uni 	$L__BB29_21;
$L__BB29_22:
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r22, %r1, %r25;
	add.s32 	%r23, %r22, 4;
	// begin inline asm
	ld.shared.f32 %r20, [%r22];
ld.shared.f32 %r21, [%r23];
	// end inline asm
	setp.eq.s64 	%p16, %rd172, 0;
	mov.u64 	%rd162, -1;
	mov.u64 	%rd177, %rd3;
	@%p16 bra 	$L__BB29_29;
	mul.lo.s64 	%rd176, %rd172, %rd7;
	mul.hi.u64 	%rd81, %rd172, %rd7;
	setp.eq.s64 	%p17, %rd81, 0;
	mov.u64 	%rd174, %rd4;
	@%p17 bra 	$L__BB29_28;
	mov.u64 	%rd173, %rd7;
	mov.u64 	%rd174, %rd4;
$L__BB29_25:
	setp.eq.s64 	%p18, %rd172, 0;
	@%p18 bra 	$L__BB29_55;
	setp.eq.s64 	%p19, %rd173, 0;
	@%p19 bra 	$L__BB29_56;
	div.u64 	%rd83, %rd162, %rd172;
	div.u64 	%rd84, %rd162, %rd173;
	mul.lo.s64 	%rd85, %rd83, %rd172;
	mul.lo.s64 	%rd86, %rd84, %rd173;
	setp.gt.u64 	%p20, %rd85, %rd86;
	max.u64 	%rd87, %rd85, %rd86;
	selp.b64 	%rd88, %rd83, 0, %p20;
	sub.s64 	%rd173, %rd173, %rd88;
	selp.b64 	%rd89, 0, %rd84, %p20;
	sub.s64 	%rd172, %rd172, %rd89;
	add.s64 	%rd90, %rd87, -1;
	sub.s64 	%rd91, %rd2, %rd174;
	shr.u64 	%rd92, %rd91, 3;
	setp.gt.u64 	%p21, %rd92, %rd90;
	shl.b64 	%rd93, %rd87, 3;
	add.s64 	%rd94, %rd174, %rd93;
	selp.b64 	%rd174, %rd94, %rd2, %p21;
	mul.lo.s64 	%rd176, %rd172, %rd173;
	mul.hi.u64 	%rd95, %rd172, %rd173;
	setp.ne.s64 	%p6, %rd95, 0;
	@%p6 bra 	$L__BB29_25;
$L__BB29_28:
	add.s64 	%rd96, %rd176, -1;
	sub.s64 	%rd97, %rd2, %rd174;
	shr.u64 	%rd98, %rd97, 3;
	setp.gt.u64 	%p22, %rd98, %rd96;
	shl.b64 	%rd99, %rd176, 3;
	add.s64 	%rd100, %rd174, %rd99;
	add.s64 	%rd101, %rd100, -8;
	selp.b64 	%rd177, %rd101, 0, %p22;
$L__BB29_29:
	setp.ne.s64 	%p23, %rd177, 0;
	@%p23 bra 	$L__BB29_31;
	bra.uni 	$L__BB29_30;
$L__BB29_31:
	mov.b32 	%f6, %r15;
	mov.b32 	%f5, %r14;
	mov.b32 	%f8, %r21;
	mov.b32 	%f7, %r20;
	// begin inline asm
	add.rn.ftz.f32 %r26, %r14, %r20;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r29, %r15, %r21;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r16], %r26;
st.shared.f32 [%r17], %r29;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r43, %r14, %r20;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r46, %r15, %r21;
	// end inline asm
	ld.u32 	%r56, [%rd177];
	ld.u32 	%r53, [%rd177+4];
	// begin inline asm
	mul.rn.ftz.f32 %r42, %r43, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r46, %r53;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r48, %r42, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r51, %r43, %r53;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r54, %r46, %r56;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r57, %r51, %r54;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r22], %r48;
st.shared.f32 [%r23], %r57;
	// end inline asm
	mov.u64 	%rd9, %rd66;
	@%p1 bra 	$L__BB29_32;
$L__BB29_7:
	shr.u64 	%rd106, %rd9, %r13;
	and.b64  	%rd166, %rd9, %rd29;
	mul.lo.s64 	%rd107, %rd106, %rd28;
	add.s64 	%rd12, %rd107, %rd166;
	setp.lt.u64 	%p24, %rd12, 1024;
	@%p24 bra 	$L__BB29_9;
	bra.uni 	$L__BB29_8;
$L__BB29_9:
	cvt.u32.u64 	%r69, %rd12;
	shl.b32 	%r70, %r69, 3;
	add.s32 	%r67, %r70, %r1;
	add.s32 	%r68, %r67, 4;
	// begin inline asm
	ld.shared.f32 %r65, [%r67];
ld.shared.f32 %r66, [%r68];
	// end inline asm
	add.s64 	%rd13, %rd12, %rd27;
	setp.lt.u64 	%p25, %rd13, 1024;
	@%p25 bra 	$L__BB29_11;
	bra.uni 	$L__BB29_10;
$L__BB29_11:
	cvt.u32.u64 	%r75, %rd13;
	shl.b32 	%r76, %r75, 3;
	add.s32 	%r73, %r76, %r1;
	add.s32 	%r74, %r73, 4;
	// begin inline asm
	ld.shared.f32 %r71, [%r73];
ld.shared.f32 %r72, [%r74];
	// end inline asm
	setp.eq.s64 	%p26, %rd166, 0;
	mov.u64 	%rd171, %rd3;
	@%p26 bra 	$L__BB29_18;
	mul.lo.s64 	%rd170, %rd166, %rd7;
	mul.hi.u64 	%rd116, %rd166, %rd7;
	setp.eq.s64 	%p27, %rd116, 0;
	mov.u64 	%rd168, %rd4;
	@%p27 bra 	$L__BB29_17;
	mov.u64 	%rd167, %rd7;
	mov.u64 	%rd168, %rd4;
$L__BB29_14:
	setp.eq.s64 	%p28, %rd166, 0;
	@%p28 bra 	$L__BB29_53;
	setp.eq.s64 	%p29, %rd167, 0;
	@%p29 bra 	$L__BB29_54;
	div.u64 	%rd118, %rd162, %rd166;
	div.u64 	%rd119, %rd162, %rd167;
	mul.lo.s64 	%rd120, %rd118, %rd166;
	mul.lo.s64 	%rd121, %rd119, %rd167;
	setp.gt.u64 	%p30, %rd120, %rd121;
	max.u64 	%rd122, %rd120, %rd121;
	selp.b64 	%rd123, %rd118, 0, %p30;
	sub.s64 	%rd167, %rd167, %rd123;
	selp.b64 	%rd124, 0, %rd119, %p30;
	sub.s64 	%rd166, %rd166, %rd124;
	add.s64 	%rd125, %rd122, -1;
	sub.s64 	%rd126, %rd2, %rd168;
	shr.u64 	%rd127, %rd126, 3;
	setp.gt.u64 	%p31, %rd127, %rd125;
	shl.b64 	%rd128, %rd122, 3;
	add.s64 	%rd129, %rd168, %rd128;
	selp.b64 	%rd168, %rd129, %rd2, %p31;
	mul.lo.s64 	%rd170, %rd166, %rd167;
	mul.hi.u64 	%rd130, %rd166, %rd167;
	setp.ne.s64 	%p3, %rd130, 0;
	@%p3 bra 	$L__BB29_14;
$L__BB29_17:
	add.s64 	%rd131, %rd170, -1;
	sub.s64 	%rd132, %rd2, %rd168;
	shr.u64 	%rd133, %rd132, 3;
	setp.gt.u64 	%p32, %rd133, %rd131;
	shl.b64 	%rd134, %rd170, 3;
	add.s64 	%rd135, %rd168, %rd134;
	add.s64 	%rd136, %rd135, -8;
	selp.b64 	%rd171, %rd136, 0, %p32;
$L__BB29_18:
	setp.ne.s64 	%p33, %rd171, 0;
	@%p33 bra 	$L__BB29_6;
	bra.uni 	$L__BB29_19;
$L__BB29_6:
	add.s64 	%rd10, %rd9, 1;
	mov.b32 	%f2, %r66;
	mov.b32 	%f1, %r65;
	mov.b32 	%f4, %r72;
	mov.b32 	%f3, %r71;
	// begin inline asm
	add.rn.ftz.f32 %r77, %r65, %r71;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r66, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r67], %r77;
st.shared.f32 [%r68], %r80;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r94, %r65, %r71;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r97, %r66, %r72;
	// end inline asm
	ld.u32 	%r107, [%rd171];
	ld.u32 	%r104, [%rd171+4];
	// begin inline asm
	mul.rn.ftz.f32 %r93, %r94, %r107;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r96, %r97, %r104;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r99, %r93, %r96;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r102, %r94, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r105, %r97, %r107;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r108, %r102, %r105;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r73], %r99;
st.shared.f32 [%r74], %r108;
	// end inline asm
	add.s64 	%rd141, %rd10, %rd1;
	setp.lt.u64 	%p34, %rd141, %rd10;
	add.s64 	%rd9, %rd9, %rd57;
	setp.gt.u64 	%p35, %rd9, 511;
	or.pred  	%p36, %p34, %p35;
	@%p36 bra 	$L__BB29_32;
	bra.uni 	$L__BB29_7;
$L__BB29_33:
	@%p9 bra 	$L__BB29_34;
	and.b64  	%rd146, %rd56, 1;
	setp.eq.b64 	%p39, %rd146, 1;
	mov.pred 	%p40, 0;
	xor.pred  	%p41, %p39, %p40;
	cvt.u32.u64 	%r115, %rd56;
	shl.b32 	%r12, %r115, 4;
	@%p41 bra 	$L__BB29_36;
	add.s32 	%r151, %r1, %r12;
	add.s32 	%r152, %r151, 4;
	// begin inline asm
	ld.shared.f32 %r158, [%r151];
ld.shared.f32 %r161, [%r152];
	// end inline asm
	add.s32 	%r173, %r151, 16;
	add.s32 	%r174, %r151, 20;
	// begin inline asm
	ld.shared.f32 %r159, [%r173];
ld.shared.f32 %r162, [%r174];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r165, %r158, %r159;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r166, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r151], %r165;
st.shared.f32 [%r152], %r166;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r175, %r158, %r159;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r176, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r173], %r175;
st.shared.f32 [%r174], %r176;
	// end inline asm
	bra.uni 	$L__BB29_37;
$L__BB29_36:
	and.b32  	%r146, %r12, 8160;
	add.s32 	%r147, %r1, %r146;
	add.s32 	%r130, %r147, 8;
	add.s32 	%r131, %r147, 12;
	// begin inline asm
	ld.shared.f32 %r125, [%r130];
ld.shared.f32 %r128, [%r131];
	// end inline asm
	or.b32  	%r148, %r12, 24;
	add.s32 	%r122, %r1, %r148;
	add.s32 	%r123, %r122, 4;
	// begin inline asm
	ld.shared.f32 %r126, [%r122];
ld.shared.f32 %r129, [%r123];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r132, %r125, %r126;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r133, %r128, %r129;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r130], %r132;
st.shared.f32 [%r131], %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r125, %r126;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r128, %r129;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r145, %r141;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r122], %r144;
st.shared.f32 [%r123], %r145;
	// end inline asm
$L__BB29_37:
	@%p1 bra 	$L__BB29_34;
	add.s64 	%rd46, %rd66, 1;
	and.b64  	%rd147, %rd66, 1;
	setp.eq.b64 	%p42, %rd147, 1;
	xor.pred  	%p44, %p42, %p40;
	not.pred 	%p45, %p44;
	cvt.u32.u64 	%r177, %rd66;
	shl.b32 	%r10, %r177, 4;
	@%p45 bra 	$L__BB29_40;
	bra.uni 	$L__BB29_39;
$L__BB29_40:
	add.s32 	%r213, %r1, %r10;
	add.s32 	%r214, %r213, 4;
	// begin inline asm
	ld.shared.f32 %r220, [%r213];
ld.shared.f32 %r223, [%r214];
	// end inline asm
	add.s32 	%r235, %r213, 16;
	add.s32 	%r236, %r213, 20;
	// begin inline asm
	ld.shared.f32 %r221, [%r235];
ld.shared.f32 %r224, [%r236];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r227, %r220, %r221;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r228, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r213], %r227;
st.shared.f32 [%r214], %r228;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r237, %r220, %r221;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r238, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r235], %r237;
st.shared.f32 [%r236], %r238;
	// end inline asm
	bra.uni 	$L__BB29_41;
$L__BB29_39:
	and.b32  	%r208, %r10, 8160;
	add.s32 	%r209, %r1, %r208;
	add.s32 	%r192, %r209, 8;
	add.s32 	%r193, %r209, 12;
	// begin inline asm
	ld.shared.f32 %r187, [%r192];
ld.shared.f32 %r190, [%r193];
	// end inline asm
	or.b32  	%r210, %r10, 24;
	add.s32 	%r184, %r1, %r210;
	add.s32 	%r185, %r184, 4;
	// begin inline asm
	ld.shared.f32 %r188, [%r184];
ld.shared.f32 %r191, [%r185];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r194, %r187, %r188;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r195, %r190, %r191;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r192], %r194;
st.shared.f32 [%r193], %r195;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r203, %r187, %r188;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r206, %r190, %r191;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r207, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r184], %r206;
st.shared.f32 [%r185], %r207;
	// end inline asm
$L__BB29_41:
	add.s64 	%rd148, %rd46, %rd1;
	setp.lt.u64 	%p46, %rd148, %rd46;
	add.s64 	%rd178, %rd66, %rd57;
	setp.gt.u64 	%p47, %rd178, 511;
	or.pred  	%p48, %p46, %p47;
	@%p48 bra 	$L__BB29_34;
	bra.uni 	$L__BB29_42;
$L__BB29_34:
	bar.sync 	0;
	@%p9 bra 	$L__BB29_51;
	cvt.u32.u64 	%r329, %rd56;
	shl.b32 	%r330, %r329, 4;
	add.s32 	%r303, %r1, %r330;
	add.s32 	%r304, %r303, 4;
	// begin inline asm
	ld.shared.f32 %r310, [%r303];
ld.shared.f32 %r313, [%r304];
	// end inline asm
	add.s32 	%r307, %r303, 8;
	add.s32 	%r308, %r303, 12;
	// begin inline asm
	ld.shared.f32 %r311, [%r307];
ld.shared.f32 %r314, [%r308];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r309, %r310, %r311;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r312, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r303], %r309;
st.shared.f32 [%r304], %r312;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r319, %r310, %r311;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r322, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r307], %r319;
st.shared.f32 [%r308], %r322;
	// end inline asm
	@!%p1 bra 	$L__BB29_48;
	bra.uni 	$L__BB29_51;
$L__BB29_48:
	shl.b64 	%rd55, %rd66, 1;
	setp.gt.u64 	%p56, %rd55, 1023;
	@%p56 bra 	$L__BB29_52;
	add.s64 	%rd54, %rd66, 1;
	cvt.u32.u64 	%r359, %rd55;
	shl.b32 	%r360, %r359, 3;
	add.s32 	%r333, %r1, %r360;
	add.s32 	%r334, %r333, 4;
	// begin inline asm
	ld.shared.f32 %r340, [%r333];
ld.shared.f32 %r343, [%r334];
	// end inline asm
	add.s32 	%r337, %r333, 8;
	add.s32 	%r338, %r333, 12;
	// begin inline asm
	ld.shared.f32 %r341, [%r337];
ld.shared.f32 %r344, [%r338];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r339, %r340, %r341;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r342, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r333], %r339;
st.shared.f32 [%r334], %r342;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r349, %r340, %r341;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r352, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r337], %r349;
st.shared.f32 [%r338], %r352;
	// end inline asm
	add.s64 	%rd155, %rd54, %rd1;
	setp.lt.u64 	%p57, %rd155, %rd54;
	add.s64 	%rd179, %rd66, %rd57;
	setp.gt.u64 	%p58, %rd179, 511;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB29_51;
$L__BB29_50:
	add.s64 	%rd156, %rd179, 1;
	cvt.u32.u64 	%r389, %rd179;
	shl.b32 	%r390, %r389, 4;
	add.s32 	%r363, %r390, %r1;
	add.s32 	%r364, %r363, 4;
	// begin inline asm
	ld.shared.f32 %r370, [%r363];
ld.shared.f32 %r373, [%r364];
	// end inline asm
	add.s32 	%r367, %r363, 8;
	add.s32 	%r368, %r363, 12;
	// begin inline asm
	ld.shared.f32 %r371, [%r367];
ld.shared.f32 %r374, [%r368];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r369, %r370, %r371;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r372, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r363], %r369;
st.shared.f32 [%r364], %r372;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r379, %r370, %r371;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r382, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r367], %r379;
st.shared.f32 [%r368], %r382;
	// end inline asm
	add.s64 	%rd157, %rd156, %rd1;
	setp.lt.u64 	%p60, %rd157, %rd156;
	add.s64 	%rd179, %rd179, %rd57;
	setp.gt.u64 	%p61, %rd179, 511;
	or.pred  	%p62, %p60, %p61;
	@%p62 bra 	$L__BB29_51;
	bra.uni 	$L__BB29_50;
$L__BB29_51:
	ret;
$L__BB29_44:
	and.b32  	%r270, %r11, 8160;
	add.s32 	%r271, %r1, %r270;
	add.s32 	%r254, %r271, 8;
	add.s32 	%r255, %r271, 12;
	// begin inline asm
	ld.shared.f32 %r249, [%r254];
ld.shared.f32 %r252, [%r255];
	// end inline asm
	or.b32  	%r272, %r11, 24;
	add.s32 	%r246, %r272, %r1;
	add.s32 	%r247, %r246, 4;
	// begin inline asm
	ld.shared.f32 %r250, [%r246];
ld.shared.f32 %r253, [%r247];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r256, %r249, %r250;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r257, %r252, %r253;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r254], %r256;
st.shared.f32 [%r255], %r257;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r265, %r249, %r250;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r268, %r252, %r253;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r269, %r265;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r246], %r268;
st.shared.f32 [%r247], %r269;
	// end inline asm
$L__BB29_45:
	add.s64 	%rd49, %rd178, 1;
	add.s64 	%rd150, %rd49, %rd1;
	setp.lt.u64 	%p52, %rd150, %rd49;
	add.s64 	%rd178, %rd178, %rd57;
	setp.gt.u64 	%p53, %rd178, 511;
	or.pred  	%p54, %p52, %p53;
	@%p54 bra 	$L__BB29_34;
$L__BB29_42:
	and.b64  	%rd149, %rd178, 1;
	setp.eq.b64 	%p49, %rd149, 1;
	xor.pred  	%p51, %p49, %p40;
	cvt.u32.u64 	%r239, %rd178;
	shl.b32 	%r11, %r239, 4;
	@%p51 bra 	$L__BB29_44;
	add.s32 	%r275, %r11, %r1;
	add.s32 	%r276, %r275, 4;
	// begin inline asm
	ld.shared.f32 %r282, [%r275];
ld.shared.f32 %r285, [%r276];
	// end inline asm
	add.s32 	%r297, %r275, 16;
	add.s32 	%r298, %r275, 20;
	// begin inline asm
	ld.shared.f32 %r283, [%r297];
ld.shared.f32 %r286, [%r298];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r289, %r282, %r283;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r290, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r275], %r289;
st.shared.f32 [%r276], %r290;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r299, %r282, %r283;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r300, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r297], %r299;
st.shared.f32 [%r298], %r300;
	// end inline asm
	bra.uni 	$L__BB29_45;
$L__BB29_54:
	mov.u64 	%rd137, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd138, %rd137;
	{ // callseq 150, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd138;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 150
$L__BB29_53:
	mov.u64 	%rd139, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd140, %rd139;
	{ // callseq 151, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 151
$L__BB29_10:
	mov.u64 	%rd112, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd113, %rd112;
	mov.u64 	%rd114, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd115, %rd114;
	{ // callseq 149, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd113;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd115;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 149
$L__BB29_8:
	mov.u64 	%rd108, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd109, %rd108;
	mov.u64 	%rd110, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd111, %rd110;
	{ // callseq 148, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd109;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd111;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 148
$L__BB29_19:
	mov.u64 	%rd142, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_10;
	cvta.global.u64 	%rd143, %rd142;
	{ // callseq 152, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd143;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 152
$L__BB29_55:
	mov.u64 	%rd104, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd105, %rd104;
	{ // callseq 147, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd105;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 147
$L__BB29_56:
	mov.u64 	%rd102, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd103, %rd102;
	{ // callseq 146, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd103;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 146
$L__BB29_30:
	mov.u64 	%rd144, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_10;
	cvta.global.u64 	%rd145, %rd144;
	{ // callseq 153, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd145;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 153
$L__BB29_4:
	mov.u64 	%rd73, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd74, %rd73;
	mov.u64 	%rd75, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd76, %rd75;
	{ // callseq 144, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd74;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd76;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 144
$L__BB29_21:
	mov.u64 	%rd77, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd78, %rd77;
	mov.u64 	%rd79, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd80, %rd79;
	{ // callseq 145, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd80;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 145
$L__BB29_5:
	mov.u64 	%rd158, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_5;
	cvta.global.u64 	%rd159, %rd158;
	mov.u64 	%rd160, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_6;
	cvta.global.u64 	%rd161, %rd160;
	{ // callseq 155, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd159;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd161;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 155
$L__BB29_52:
	mov.u64 	%rd151, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd152, %rd151;
	mov.u64 	%rd153, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd154, %rd153;
	{ // callseq 154, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd152;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd154;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 154

}
	// .globl	daubechies_first_forward_128_kernel
.visible .entry daubechies_first_forward_128_kernel(
	.param .u64 daubechies_first_forward_128_kernel_param_0
)
{
	.reg .pred 	%p<104>;
	.reg .b32 	%r<262>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<407>;

	ld.param.u64 	%rd161, [daubechies_first_forward_128_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd161;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[1024];
    mov.u32 %r6, nonphysical;
	// end inline asm
	mov.u32 	%r9, %ctaid.x;
	ld.global.nc.u64 	%rd5, [%rd1+16];
	ld.global.nc.u64 	%rd162, [%rd1+24];
	and.b64  	%rd163, %rd162, -128;
	mul.wide.u32 	%rd6, %r9, 128;
	setp.ge.u64 	%p16, %rd6, %rd163;
	sub.s64 	%rd165, %rd162, %rd6;
	setp.lt.u64 	%p17, %rd165, 128;
	or.pred  	%p18, %p16, %p17;
	@!%p18 bra 	$L__BB30_2;
	bra.uni 	$L__BB30_1;
$L__BB30_2:
	mov.u32 	%r7, %tid.x;
	cvt.u64.u32 	%rd95, %r7;
	mov.u32 	%r8, %ntid.x;
	cvt.u64.u32 	%rd3, %r8;
	cvt.u32.u64 	%r10, %rd95;
	max.u64 	%rd168, %rd95, 128;
	setp.gt.u32 	%p19, %r10, 127;
	not.b64 	%rd169, %rd95;
	add.s64 	%rd9, %rd169, %rd168;
	mov.u64 	%rd350, 0;
	mov.u64 	%rd348, %rd350;
	@%p19 bra 	$L__BB30_7;
	and.b64  	%rd170, %rd9, -4294967296;
	setp.ne.s64 	%p20, %rd170, 0;
	@%p20 bra 	$L__BB30_5;
	bra.uni 	$L__BB30_4;
$L__BB30_5:
	div.u64 	%rd347, %rd9, %rd3;
	bra.uni 	$L__BB30_6;
$L__BB30_4:
	cvt.u32.u64 	%r11, %rd3;
	cvt.u32.u64 	%r12, %rd9;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd347, %r13;
$L__BB30_6:
	add.s64 	%rd348, %rd347, 1;
$L__BB30_7:
	@%p19 bra 	$L__BB30_12;
	and.b64  	%rd172, %rd9, -4294967296;
	setp.ne.s64 	%p22, %rd172, 0;
	@%p22 bra 	$L__BB30_10;
	bra.uni 	$L__BB30_9;
$L__BB30_10:
	div.u64 	%rd349, %rd9, %rd3;
	bra.uni 	$L__BB30_11;
$L__BB30_9:
	cvt.u32.u64 	%r15, %rd3;
	cvt.u32.u64 	%r16, %rd9;
	div.u32 	%r17, %r16, %r15;
	cvt.u64.u32 	%rd349, %r17;
$L__BB30_11:
	add.s64 	%rd350, %rd349, 1;
$L__BB30_12:
	shl.b64 	%rd164, %rd6, 3;
	add.s64 	%rd8, %rd3, -1;
	min.u64 	%rd20, %rd348, %rd350;
	setp.ne.s64 	%p23, %rd20, 0;
	shl.b64 	%rd346, %rd3, 3;
	@%p23 bra 	$L__BB30_14;
	bra.uni 	$L__BB30_13;
$L__BB30_14:
	add.s64 	%rd7, %rd5, %rd164;
	shl.b64 	%rd173, %rd95, 3;
	add.s64 	%rd174, %rd7, %rd173;
	add.s64 	%rd355, %rd95, 1;
	shl.b32 	%r23, %r10, 3;
	add.s32 	%r18, %r6, %r23;
	add.s32 	%r19, %r18, 4;
	ld.u32 	%r20, [%rd174];
	ld.u32 	%r21, [%rd174+4];
	// begin inline asm
	st.shared.f32 [%r18], %r20;
st.shared.f32 [%r19], %r21;
	// end inline asm
	setp.eq.s64 	%p24, %rd20, 1;
	@%p24 bra 	$L__BB30_19;
	cvt.u64.u32 	%rd4, %r9;
	add.s64 	%rd353, %rd20, -1;
	shl.b64 	%rd175, %rd4, 10;
	add.s64 	%rd176, %rd175, %rd346;
	add.s64 	%rd178, %rd176, %rd173;
	add.s64 	%rd352, %rd5, %rd178;
	mov.u64 	%rd179, 1016;
	sub.s64 	%rd351, %rd179, %rd173;
	mov.u64 	%rd354, %rd355;
$L__BB30_16:
	add.s64 	%rd32, %rd354, %rd8;
	setp.lt.u64 	%p26, %rd32, 128;
	@%p26 bra 	$L__BB30_18;
	bra.uni 	$L__BB30_17;
$L__BB30_18:
	shr.u64 	%rd180, %rd351, 3;
	setp.gt.u64 	%p25, %rd180, %rd8;
	selp.b64 	%rd31, %rd352, 0, %p25;
	setp.lt.u64 	%p1, %rd32, %rd354;
	add.s64 	%rd185, %rd354, %rd3;
	selp.b64 	%rd354, 128, %rd185, %p1;
	cvt.u32.u64 	%r28, %rd32;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r24, %r29, %r6;
	add.s32 	%r25, %r24, 4;
	ld.u32 	%r26, [%rd31];
	ld.u32 	%r27, [%rd31+4];
	// begin inline asm
	st.shared.f32 [%r24], %r26;
st.shared.f32 [%r25], %r27;
	// end inline asm
	add.s64 	%rd353, %rd353, -1;
	add.s64 	%rd352, %rd352, %rd346;
	sub.s64 	%rd351, %rd351, %rd346;
	setp.ne.s64 	%p27, %rd353, 0;
	@%p27 bra 	$L__BB30_16;
	bra.uni 	$L__BB30_19;
$L__BB30_13:
	add.s64 	%rd355, %rd95, 1;
$L__BB30_19:
	ld.global.nc.u64 	%rd38, [%rd1];
	ld.global.nc.u64 	%rd39, [%rd1+8];
	bar.sync 	0;
	setp.lt.u32 	%p28, %r10, 64;
	@%p28 bra 	$L__BB30_20;
	bra.uni 	$L__BB30_25;
$L__BB30_20:
	shl.b32 	%r2, %r10, 4;
	add.s32 	%r33, %r6, %r2;
	add.s32 	%r34, %r33, 4;
	// begin inline asm
	ld.shared.f32 %r31, [%r33];
ld.shared.f32 %r32, [%r34];
	// end inline asm
	setp.ne.s64 	%p29, %rd39, 0;
	@%p29 bra 	$L__BB30_24;
	bra.uni 	$L__BB30_21;
$L__BB30_24:
	mov.b32 	%f2, %r32;
	mov.b32 	%f1, %r31;
	ld.u32 	%r38, [%rd38];
	ld.u32 	%r41, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r36, %r31, %r38;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r39, %r32, %r41;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r77, %r36, %r39;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r31, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r48, %r32, %r38;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r45, %r48;
	// end inline asm
	add.s32 	%r92, %r33, 8;
	add.s32 	%r93, %r33, 12;
	// begin inline asm
	ld.shared.f32 %r59, [%r92];
ld.shared.f32 %r62, [%r93];
	// end inline asm
	ld.u32 	%r72, [%rd38];
	ld.u32 	%r69, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r58, %r59, %r72;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r61, %r62, %r69;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r78, %r58, %r61;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r67, %r59, %r69;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r70, %r62, %r72;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r81, %r67, %r70;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r76, %r77, %r78;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r79, %r80, %r81;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r33], %r76;
st.shared.f32 [%r34], %r79;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r86, %r77, %r78;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r89, %r80, %r81;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r92], %r86;
st.shared.f32 [%r93], %r89;
	// end inline asm
	add.s64 	%rd43, %rd95, %rd3;
	setp.lt.u64 	%p30, %rd43, 64;
	@%p30 bra 	$L__BB30_22;
	bra.uni 	$L__BB30_25;
$L__BB30_22:
	cvt.u32.u64 	%r161, %rd43;
	shl.b32 	%r162, %r161, 4;
	add.s32 	%r99, %r6, %r162;
	add.s32 	%r100, %r99, 4;
	// begin inline asm
	ld.shared.f32 %r102, [%r99];
ld.shared.f32 %r105, [%r100];
	// end inline asm
	ld.u32 	%r115, [%rd38];
	ld.u32 	%r112, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r101, %r102, %r115;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r104, %r105, %r112;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r142, %r101, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r110, %r102, %r112;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r113, %r105, %r115;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r145, %r110, %r113;
	// end inline asm
	add.s32 	%r157, %r99, 8;
	add.s32 	%r158, %r99, 12;
	// begin inline asm
	ld.shared.f32 %r124, [%r157];
ld.shared.f32 %r127, [%r158];
	// end inline asm
	ld.u32 	%r137, [%rd38];
	ld.u32 	%r134, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r123, %r124, %r137;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r126, %r127, %r134;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r143, %r123, %r126;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r132, %r124, %r134;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r135, %r127, %r137;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r146, %r132, %r135;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r142, %r143;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r145, %r146;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r99], %r141;
st.shared.f32 [%r100], %r144;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r151, %r142, %r143;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r154, %r145, %r146;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r157], %r151;
st.shared.f32 [%r158], %r154;
	// end inline asm
	add.s64 	%rd356, %rd43, %rd3;
	setp.gt.u64 	%p31, %rd356, 63;
	@%p31 bra 	$L__BB30_25;
$L__BB30_23:
	add.s64 	%rd186, %rd356, 1;
	cvt.u32.u64 	%r227, %rd356;
	shl.b32 	%r228, %r227, 4;
	add.s32 	%r165, %r228, %r6;
	add.s32 	%r166, %r165, 4;
	// begin inline asm
	ld.shared.f32 %r168, [%r165];
ld.shared.f32 %r171, [%r166];
	// end inline asm
	ld.u32 	%r181, [%rd38];
	ld.u32 	%r178, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r167, %r168, %r181;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r170, %r171, %r178;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r208, %r167, %r170;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r176, %r168, %r178;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r179, %r171, %r181;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r211, %r176, %r179;
	// end inline asm
	add.s32 	%r223, %r165, 8;
	add.s32 	%r224, %r165, 12;
	// begin inline asm
	ld.shared.f32 %r190, [%r223];
ld.shared.f32 %r193, [%r224];
	// end inline asm
	ld.u32 	%r203, [%rd38];
	ld.u32 	%r200, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r189, %r190, %r203;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r192, %r193, %r200;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r209, %r189, %r192;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r198, %r190, %r200;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r201, %r193, %r203;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r212, %r198, %r201;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r207, %r208, %r209;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r210, %r211, %r212;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r165], %r207;
st.shared.f32 [%r166], %r210;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r217, %r208, %r209;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r220, %r211, %r212;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r223], %r217;
st.shared.f32 [%r224], %r220;
	// end inline asm
	add.s64 	%rd187, %rd186, %rd8;
	setp.lt.u64 	%p32, %rd187, %rd186;
	add.s64 	%rd356, %rd356, %rd3;
	setp.gt.u64 	%p33, %rd356, 63;
	or.pred  	%p34, %p32, %p33;
	@%p34 bra 	$L__BB30_25;
	bra.uni 	$L__BB30_23;
$L__BB30_25:
	ld.global.nc.u64 	%rd189, [%rd1+32];
	ld.global.nc.u64 	%rd190, [%rd1+40];
	and.b64  	%rd191, %rd190, -128;
	setp.lt.u64 	%p35, %rd6, %rd191;
	sub.s64 	%rd193, %rd190, %rd6;
	setp.gt.u64 	%p36, %rd193, 127;
	and.pred  	%p37, %p35, %p36;
	@%p37 bra 	$L__BB30_27;
	bra.uni 	$L__BB30_26;
$L__BB30_27:
	setp.gt.u32 	%p38, %r10, 63;
	bar.sync 	0;
	mov.u64 	%rd363, 0;
	mov.u64 	%rd93, %rd363;
	@%p38 bra 	$L__BB30_31;
	setp.eq.s32 	%p39, %r10, 0;
	selp.b64 	%rd198, 1, %rd355, %p39;
	add.s64 	%rd357, %rd198, %rd8;
	setp.gt.u64 	%p40, %rd357, 63;
	mov.u64 	%rd93, 1;
	@%p40 bra 	$L__BB30_31;
	mov.u64 	%rd93, 1;
$L__BB30_30:
	add.s64 	%rd200, %rd357, 1;
	add.s64 	%rd93, %rd93, 1;
	add.s64 	%rd201, %rd200, %rd8;
	setp.lt.u64 	%p41, %rd201, %rd200;
	add.s64 	%rd357, %rd357, %rd3;
	setp.gt.u64 	%p42, %rd357, 63;
	or.pred  	%p43, %p41, %p42;
	@!%p43 bra 	$L__BB30_30;
	bra.uni 	$L__BB30_31;
$L__BB30_31:
	setp.eq.s64 	%p99, %rd93, 0;
	mov.u64 	%rd362, %rd363;
	@%p99 bra 	$L__BB30_38;
	mov.u64 	%rd361, 0;
	@%p19 bra 	$L__BB30_37;
	and.b64  	%rd205, %rd9, -4294967296;
	setp.ne.s64 	%p45, %rd205, 0;
	@%p45 bra 	$L__BB30_35;
	bra.uni 	$L__BB30_34;
$L__BB30_35:
	div.u64 	%rd360, %rd9, %rd3;
	bra.uni 	$L__BB30_36;
$L__BB30_34:
	cvt.u32.u64 	%r232, %rd3;
	cvt.u32.u64 	%r233, %rd9;
	div.u32 	%r234, %r233, %r232;
	cvt.u64.u32 	%rd360, %r234;
$L__BB30_36:
	add.s64 	%rd361, %rd360, 1;
$L__BB30_37:
	min.u64 	%rd362, %rd361, %rd93;
$L__BB30_38:
	add.s64 	%rd44, %rd189, %rd164;
	@%p38 bra 	$L__BB30_40;
	mov.u64 	%rd207, 63;
	sub.s64 	%rd60, %rd207, %rd95;
	cvt.u32.u64 	%r236, %rd60;
	cvt.u32.u64 	%r237, %rd3;
	div.u32 	%r238, %r236, %r237;
	cvt.u64.u32 	%rd208, %r238;
	add.s64 	%rd363, %rd208, 1;
$L__BB30_40:
	selp.b64 	%rd396, %rd44, 0, %p37;
	add.s64 	%rd46, %rd44, 1024;
	min.u64 	%rd63, %rd362, %rd363;
	setp.eq.s64 	%p47, %rd63, 0;
	@%p47 bra 	$L__BB30_55;
	setp.eq.s64 	%p48, %rd93, 0;
	mov.u64 	%rd369, 0;
	mov.u64 	%rd374, %rd396;
	mov.u64 	%rd375, %rd95;
	mov.u64 	%rd367, %rd369;
	@%p48 bra 	$L__BB30_45;
	add.s64 	%rd369, %rd93, -1;
	setp.eq.s32 	%p49, %r10, 0;
	@%p49 bra 	$L__BB30_44;
	setp.lt.u32 	%p50, %r10, 128;
	shl.b64 	%rd212, %rd95, 3;
	add.s64 	%rd213, %rd396, %rd212;
	add.s64 	%rd214, %rd213, 8;
	selp.b64 	%rd374, %rd214, %rd46, %p50;
	selp.b64 	%rd367, %rd213, 0, %p50;
	mov.u64 	%rd375, 0;
	bra.uni 	$L__BB30_45;
$L__BB30_44:
	add.s64 	%rd374, %rd396, 8;
	mov.u64 	%rd375, 0;
	mov.u64 	%rd367, %rd396;
$L__BB30_45:
	@%p38 bra 	$L__BB30_53;
	add.s32 	%r5, %r6, 8;
	shl.b32 	%r247, %r10, 4;
	add.s32 	%r244, %r5, %r247;
	add.s32 	%r245, %r244, 4;
	// begin inline asm
	ld.shared.f32 %r242, [%r244];
ld.shared.f32 %r243, [%r245];
	// end inline asm
	st.u32 	[%rd367], %r242;
	st.u32 	[%rd367+4], %r243;
	setp.eq.s64 	%p52, %rd63, 1;
	@%p52 bra 	$L__BB30_55;
	mov.pred 	%p97, 0;
	add.s64 	%rd372, %rd95, 1;
	add.s64 	%rd368, %rd63, -1;
	mov.u64 	%rd217, 0;
$L__BB30_48:
	setp.eq.s64 	%p53, %rd369, 0;
	mov.u64 	%rd373, %rd217;
	mov.u64 	%rd376, %rd217;
	@%p53 bra 	$L__BB30_52;
	add.s64 	%rd373, %rd369, -1;
	selp.b64 	%rd80, 0, %rd8, %p99;
	setp.ne.s64 	%p54, %rd375, 0;
	sub.s64 	%rd344, %rd46, %rd374;
	@%p54 bra 	$L__BB30_51;
	shr.u64 	%rd227, %rd344, 3;
	setp.gt.u64 	%p58, %rd227, %rd80;
	shl.b64 	%rd228, %rd80, 3;
	add.s64 	%rd229, %rd374, %rd228;
	add.s64 	%rd230, %rd229, 8;
	selp.b64 	%rd374, %rd230, %rd46, %p58;
	selp.b64 	%rd376, %rd229, 0, %p58;
	mov.u64 	%rd375, 0;
	mov.pred 	%p99, %p97;
	bra.uni 	$L__BB30_52;
$L__BB30_51:
	add.s64 	%rd219, %rd80, %rd375;
	shr.u64 	%rd221, %rd344, 3;
	setp.gt.u64 	%p56, %rd221, %rd219;
	shl.b64 	%rd222, %rd219, 3;
	add.s64 	%rd223, %rd374, %rd222;
	add.s64 	%rd224, %rd223, 8;
	selp.b64 	%rd374, %rd224, %rd46, %p56;
	selp.b64 	%rd376, %rd223, 0, %p56;
	mov.u64 	%rd375, 0;
	mov.pred 	%p99, %p97;
$L__BB30_52:
	add.s64 	%rd89, %rd372, %rd8;
	shl.b64 	%rd90, %rd89, 1;
	setp.lt.u64 	%p59, %rd90, 128;
	@%p59 bra 	$L__BB30_54;
	bra.uni 	$L__BB30_53;
$L__BB30_54:
	setp.lt.u64 	%p5, %rd89, %rd372;
	setp.gt.u64 	%p60, %rd89, 63;
	add.s64 	%rd235, %rd89, 1;
	selp.b64 	%rd236, 64, %rd235, %p60;
	selp.b64 	%rd372, 64, %rd236, %p5;
	cvt.u32.u64 	%r252, %rd90;
	shl.b32 	%r253, %r252, 3;
	add.s32 	%r250, %r253, %r5;
	add.s32 	%r251, %r250, 4;
	// begin inline asm
	ld.shared.f32 %r248, [%r250];
ld.shared.f32 %r249, [%r251];
	// end inline asm
	st.u32 	[%rd376], %r248;
	st.u32 	[%rd376+4], %r249;
	add.s64 	%rd368, %rd368, -1;
	setp.ne.s64 	%p61, %rd368, 0;
	mov.u64 	%rd369, %rd373;
	@%p61 bra 	$L__BB30_48;
$L__BB30_55:
	mov.u64 	%rd403, 0;
	mov.pred 	%p100, -1;
	mov.u64 	%rd404, %rd95;
	mov.pred 	%p103, %p100;
$L__BB30_56:
	setp.ne.s64 	%p63, %rd93, 0;
	@%p63 bra 	$L__BB30_60;
	bra.uni 	$L__BB30_57;
$L__BB30_60:
	@%p100 bra 	$L__BB30_63;
	bra.uni 	$L__BB30_61;
$L__BB30_63:
	setp.ne.s64 	%p64, %rd95, 0;
	@%p64 bra 	$L__BB30_65;
	bra.uni 	$L__BB30_64;
$L__BB30_65:
	sub.s64 	%rd238, %rd46, %rd396;
	shr.u64 	%rd239, %rd238, 3;
	setp.gt.u64 	%p65, %rd239, %rd95;
	shl.b64 	%rd240, %rd95, 3;
	add.s64 	%rd241, %rd396, %rd240;
	add.s64 	%rd242, %rd241, 8;
	selp.b64 	%rd396, %rd242, %rd46, %p65;
	bra.uni 	$L__BB30_66;
$L__BB30_57:
	selp.b64 	%rd98, 0, %rd8, %p100;
	setp.ne.s64 	%p86, %rd95, 0;
	@%p86 bra 	$L__BB30_59;
	bra.uni 	$L__BB30_58;
$L__BB30_59:
	add.s64 	%rd317, %rd98, %rd95;
	sub.s64 	%rd318, %rd46, %rd396;
	shr.u64 	%rd319, %rd318, 3;
	setp.gt.u64 	%p87, %rd319, %rd317;
	shl.b64 	%rd320, %rd317, 3;
	add.s64 	%rd321, %rd396, %rd320;
	add.s64 	%rd322, %rd321, 8;
	selp.b64 	%rd401, %rd322, %rd46, %p87;
	selp.b64 	%rd402, %rd321, 0, %p87;
	bra.uni 	$L__BB30_88;
$L__BB30_58:
	sub.s64 	%rd323, %rd46, %rd396;
	shr.u64 	%rd324, %rd323, 3;
	setp.gt.u64 	%p88, %rd324, %rd98;
	shl.b64 	%rd325, %rd98, 3;
	add.s64 	%rd326, %rd396, %rd325;
	add.s64 	%rd327, %rd326, 8;
	selp.b64 	%rd401, %rd327, %rd46, %p88;
	selp.b64 	%rd402, %rd326, 0, %p88;
	bra.uni 	$L__BB30_88;
$L__BB30_64:
	setp.eq.s64 	%p66, %rd396, %rd46;
	selp.b64 	%rd243, 0, 8, %p66;
	add.s64 	%rd396, %rd396, %rd243;
$L__BB30_66:
	add.s64 	%rd93, %rd93, -1;
	mov.u64 	%rd95, 0;
$L__BB30_61:
	setp.eq.s64 	%p67, %rd93, -1;
	@%p67 bra 	$L__BB30_67;
	add.s64 	%rd388, %rd93, 1;
	bra.uni 	$L__BB30_70;
$L__BB30_67:
	setp.ne.s64 	%p68, %rd95, 0;
	sub.s64 	%rd345, %rd46, %rd396;
	@%p68 bra 	$L__BB30_69;
	bra.uni 	$L__BB30_68;
$L__BB30_69:
	add.s64 	%rd247, %rd95, %rd8;
	shr.u64 	%rd249, %rd345, 3;
	setp.gt.u64 	%p69, %rd249, %rd247;
	add.s64 	%rd250, %rd95, %rd3;
	shl.b64 	%rd251, %rd250, 3;
	add.s64 	%rd252, %rd396, %rd251;
	selp.b64 	%rd396, %rd252, %rd46, %p69;
	mov.u64 	%rd388, -1;
	mov.u64 	%rd95, 0;
	bra.uni 	$L__BB30_70;
$L__BB30_68:
	shr.u64 	%rd256, %rd345, 3;
	setp.gt.u64 	%p70, %rd256, %rd8;
	add.s64 	%rd258, %rd396, %rd346;
	selp.b64 	%rd396, %rd258, %rd46, %p70;
	mov.u64 	%rd388, -1;
	mov.u64 	%rd95, 0;
$L__BB30_70:
	mul.lo.s64 	%rd398, %rd388, %rd3;
	mul.hi.u64 	%rd259, %rd388, %rd3;
	setp.eq.s64 	%p71, %rd259, 0;
	@%p71 bra 	$L__BB30_82;
	mov.u64 	%rd260, -1;
	div.u64 	%rd261, %rd260, %rd388;
	div.u64 	%rd262, %rd260, %rd3;
	mul.lo.s64 	%rd263, %rd261, %rd388;
	mul.lo.s64 	%rd264, %rd262, %rd3;
	max.u64 	%rd117, %rd263, %rd264;
	add.s64 	%rd390, %rd117, -1;
	setp.eq.s64 	%p73, %rd95, 0;
	@%p73 bra 	$L__BB30_76;
	add.s64 	%rd267, %rd95, %rd390;
	setp.lt.u64 	%p74, %rd267, %rd95;
	@%p74 bra 	$L__BB30_74;
	bra.uni 	$L__BB30_73;
$L__BB30_74:
	add.s64 	%rd268, %rd95, -1;
	sub.s64 	%rd269, %rd46, %rd396;
	shr.u64 	%rd270, %rd269, 3;
	setp.le.u64 	%p75, %rd270, %rd268;
	shl.b64 	%rd271, %rd95, 3;
	add.s64 	%rd389, %rd396, %rd271;
	mov.u64 	%rd396, %rd46;
	@%p75 bra 	$L__BB30_78;
	bra.uni 	$L__BB30_75;
$L__BB30_76:
	sub.s64 	%rd275, %rd46, %rd396;
	shr.u64 	%rd276, %rd275, 3;
	setp.gt.u64 	%p102, %rd276, %rd390;
	shl.b64 	%rd277, %rd117, 3;
	add.s64 	%rd278, %rd396, %rd277;
	add.s64 	%rd391, %rd278, -8;
	bra.uni 	$L__BB30_77;
$L__BB30_73:
	add.s64 	%rd390, %rd390, %rd95;
	mov.u64 	%rd389, %rd396;
$L__BB30_75:
	sub.s64 	%rd272, %rd46, %rd389;
	shr.u64 	%rd273, %rd272, 3;
	setp.gt.u64 	%p102, %rd273, %rd390;
	shl.b64 	%rd274, %rd390, 3;
	add.s64 	%rd391, %rd389, %rd274;
$L__BB30_77:
	add.s64 	%rd279, %rd391, 8;
	selp.b64 	%rd396, %rd279, %rd46, %p102;
$L__BB30_78:
	setp.gt.u64 	%p72, %rd263, %rd264;
	selp.b64 	%rd265, %rd261, 0, %p72;
	sub.s64 	%rd394, %rd3, %rd265;
	selp.b64 	%rd266, 0, %rd262, %p72;
	sub.s64 	%rd393, %rd388, %rd266;
	mul.lo.s64 	%rd398, %rd393, %rd394;
	mul.hi.u64 	%rd281, %rd393, %rd394;
	mov.u64 	%rd95, 0;
	setp.eq.s64 	%p76, %rd281, 0;
	@%p76 bra 	$L__BB30_82;
$L__BB30_79:
	setp.eq.s64 	%p77, %rd393, 0;
	@%p77 bra 	$L__BB30_95;
	setp.eq.s64 	%p78, %rd394, 0;
	@%p78 bra 	$L__BB30_96;
	div.u64 	%rd284, %rd260, %rd393;
	div.u64 	%rd285, %rd260, %rd394;
	mul.lo.s64 	%rd286, %rd284, %rd393;
	mul.lo.s64 	%rd287, %rd285, %rd394;
	setp.gt.u64 	%p79, %rd286, %rd287;
	selp.b64 	%rd288, %rd284, 0, %p79;
	sub.s64 	%rd394, %rd394, %rd288;
	selp.b64 	%rd289, 0, %rd285, %p79;
	sub.s64 	%rd393, %rd393, %rd289;
	max.u64 	%rd290, %rd286, %rd287;
	add.s64 	%rd291, %rd290, -1;
	shl.b64 	%rd292, %rd290, 3;
	add.s64 	%rd293, %rd396, %rd292;
	sub.s64 	%rd294, %rd46, %rd396;
	shr.u64 	%rd295, %rd294, 3;
	setp.gt.u64 	%p80, %rd295, %rd291;
	selp.b64 	%rd396, %rd293, %rd46, %p80;
	mul.lo.s64 	%rd398, %rd393, %rd394;
	mul.hi.u64 	%rd296, %rd393, %rd394;
	setp.ne.s64 	%p13, %rd296, 0;
	@%p13 bra 	$L__BB30_79;
$L__BB30_82:
	add.s64 	%rd400, %rd398, -1;
	setp.ne.s64 	%p81, %rd95, 0;
	@%p81 bra 	$L__BB30_84;
	bra.uni 	$L__BB30_83;
$L__BB30_84:
	add.s64 	%rd297, %rd95, %rd400;
	setp.lt.u64 	%p82, %rd297, %rd95;
	@%p82 bra 	$L__BB30_86;
	bra.uni 	$L__BB30_85;
$L__BB30_86:
	add.s64 	%rd299, %rd95, -1;
	sub.s64 	%rd300, %rd46, %rd396;
	shr.u64 	%rd301, %rd300, 3;
	setp.le.u64 	%p83, %rd301, %rd299;
	shl.b64 	%rd302, %rd95, 3;
	add.s64 	%rd396, %rd396, %rd302;
	mov.u64 	%rd402, 0;
	mov.u64 	%rd401, %rd46;
	@%p83 bra 	$L__BB30_88;
	bra.uni 	$L__BB30_87;
$L__BB30_83:
	sub.s64 	%rd308, %rd46, %rd396;
	shr.u64 	%rd309, %rd308, 3;
	setp.gt.u64 	%p85, %rd309, %rd400;
	shl.b64 	%rd310, %rd398, 3;
	add.s64 	%rd311, %rd396, %rd310;
	add.s64 	%rd312, %rd311, -8;
	selp.b64 	%rd401, %rd311, %rd46, %p85;
	selp.b64 	%rd402, %rd312, 0, %p85;
	bra.uni 	$L__BB30_88;
$L__BB30_85:
	add.s64 	%rd400, %rd400, %rd95;
$L__BB30_87:
	sub.s64 	%rd303, %rd46, %rd396;
	shr.u64 	%rd304, %rd303, 3;
	setp.gt.u64 	%p84, %rd304, %rd400;
	shl.b64 	%rd305, %rd400, 3;
	add.s64 	%rd306, %rd396, %rd305;
	add.s64 	%rd307, %rd306, 8;
	selp.b64 	%rd401, %rd307, %rd46, %p84;
	selp.b64 	%rd402, %rd306, 0, %p84;
$L__BB30_88:
	setp.eq.s64 	%p89, %rd402, 0;
	mov.u64 	%rd406, 0;
	@%p89 bra 	$L__BB30_90;
	selp.b64 	%rd331, 0, %rd8, %p103;
	add.s64 	%rd333, %rd331, %rd404;
	add.s64 	%rd405, %rd333, %rd403;
	setp.lt.u64 	%p91, %rd405, %rd403;
	setp.gt.u64 	%p92, %rd405, 63;
	or.pred  	%p93, %p91, %p92;
	add.s64 	%rd334, %rd405, 1;
	selp.b64 	%rd403, 64, %rd334, %p93;
	selp.b64 	%rd406, 0, %rd402, %p93;
	mov.pred 	%p103, 0;
	mov.u64 	%rd404, 0;
$L__BB30_90:
	setp.eq.s64 	%p94, %rd406, 0;
	@%p94 bra 	$L__BB30_94;
	shl.b64 	%rd160, %rd405, 1;
	setp.lt.u64 	%p95, %rd160, 128;
	@%p95 bra 	$L__BB30_93;
	bra.uni 	$L__BB30_92;
$L__BB30_93:
	cvt.u32.u64 	%r258, %rd160;
	shl.b32 	%r259, %r258, 3;
	add.s32 	%r256, %r259, %r6;
	add.s32 	%r257, %r256, 4;
	// begin inline asm
	ld.shared.f32 %r254, [%r256];
ld.shared.f32 %r255, [%r257];
	// end inline asm
	st.u32 	[%rd406], %r254;
	st.u32 	[%rd406+4], %r255;
	mov.pred 	%p100, 0;
	mov.u64 	%rd93, 0;
	mov.u64 	%rd396, %rd401;
	mov.u64 	%rd95, %rd93;
	bra.uni 	$L__BB30_56;
$L__BB30_94:
	ret;
$L__BB30_95:
	mov.u64 	%rd315, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd316, %rd315;
	{ // callseq 160, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd316;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 160
$L__BB30_96:
	mov.u64 	%rd313, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd314, %rd313;
	{ // callseq 159, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd314;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 159
$L__BB30_92:
	mov.u64 	%rd335, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd336, %rd335;
	mov.u64 	%rd337, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd338, %rd337;
	{ // callseq 161, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd336;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd338;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 161
$L__BB30_53:
	mov.u64 	%rd231, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd232, %rd231;
	mov.u64 	%rd233, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd234, %rd233;
	{ // callseq 158, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd232;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd234;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 158
$L__BB30_17:
	mov.u64 	%rd181, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd182, %rd181;
	mov.u64 	%rd183, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd184, %rd183;
	{ // callseq 156, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd182;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd184;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 156
$L__BB30_1:
	mov.u64 	%rd342, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_12;
	cvta.global.u64 	%rd343, %rd342;
	{ // callseq 163, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd343;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 163
$L__BB30_26:
	mov.u64 	%rd194, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_13;
	cvta.global.u64 	%rd195, %rd194;
	{ // callseq 157, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd195;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 157
$L__BB30_21:
	mov.u64 	%rd340, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_16;
	cvta.global.u64 	%rd341, %rd340;
	{ // callseq 162, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd341;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 162

}
	// .globl	daubechies_first_forward_256_kernel
.visible .entry daubechies_first_forward_256_kernel(
	.param .u64 daubechies_first_forward_256_kernel_param_0
)
{
	.reg .pred 	%p<104>;
	.reg .b32 	%r<262>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<407>;

	ld.param.u64 	%rd161, [daubechies_first_forward_256_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd161;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[2048];
    mov.u32 %r6, nonphysical;
	// end inline asm
	mov.u32 	%r9, %ctaid.x;
	ld.global.nc.u64 	%rd5, [%rd1+16];
	ld.global.nc.u64 	%rd162, [%rd1+24];
	and.b64  	%rd163, %rd162, -256;
	mul.wide.u32 	%rd6, %r9, 256;
	setp.ge.u64 	%p16, %rd6, %rd163;
	sub.s64 	%rd165, %rd162, %rd6;
	setp.lt.u64 	%p17, %rd165, 256;
	or.pred  	%p18, %p16, %p17;
	@!%p18 bra 	$L__BB31_2;
	bra.uni 	$L__BB31_1;
$L__BB31_2:
	mov.u32 	%r7, %tid.x;
	cvt.u64.u32 	%rd95, %r7;
	mov.u32 	%r8, %ntid.x;
	cvt.u64.u32 	%rd3, %r8;
	cvt.u32.u64 	%r10, %rd95;
	max.u64 	%rd168, %rd95, 256;
	setp.gt.u32 	%p19, %r10, 255;
	not.b64 	%rd169, %rd95;
	add.s64 	%rd9, %rd169, %rd168;
	mov.u64 	%rd350, 0;
	mov.u64 	%rd348, %rd350;
	@%p19 bra 	$L__BB31_7;
	and.b64  	%rd170, %rd9, -4294967296;
	setp.ne.s64 	%p20, %rd170, 0;
	@%p20 bra 	$L__BB31_5;
	bra.uni 	$L__BB31_4;
$L__BB31_5:
	div.u64 	%rd347, %rd9, %rd3;
	bra.uni 	$L__BB31_6;
$L__BB31_4:
	cvt.u32.u64 	%r11, %rd3;
	cvt.u32.u64 	%r12, %rd9;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd347, %r13;
$L__BB31_6:
	add.s64 	%rd348, %rd347, 1;
$L__BB31_7:
	@%p19 bra 	$L__BB31_12;
	and.b64  	%rd172, %rd9, -4294967296;
	setp.ne.s64 	%p22, %rd172, 0;
	@%p22 bra 	$L__BB31_10;
	bra.uni 	$L__BB31_9;
$L__BB31_10:
	div.u64 	%rd349, %rd9, %rd3;
	bra.uni 	$L__BB31_11;
$L__BB31_9:
	cvt.u32.u64 	%r15, %rd3;
	cvt.u32.u64 	%r16, %rd9;
	div.u32 	%r17, %r16, %r15;
	cvt.u64.u32 	%rd349, %r17;
$L__BB31_11:
	add.s64 	%rd350, %rd349, 1;
$L__BB31_12:
	shl.b64 	%rd164, %rd6, 3;
	add.s64 	%rd8, %rd3, -1;
	min.u64 	%rd20, %rd348, %rd350;
	setp.ne.s64 	%p23, %rd20, 0;
	shl.b64 	%rd346, %rd3, 3;
	@%p23 bra 	$L__BB31_14;
	bra.uni 	$L__BB31_13;
$L__BB31_14:
	add.s64 	%rd7, %rd5, %rd164;
	shl.b64 	%rd173, %rd95, 3;
	add.s64 	%rd174, %rd7, %rd173;
	add.s64 	%rd355, %rd95, 1;
	shl.b32 	%r23, %r10, 3;
	add.s32 	%r18, %r6, %r23;
	add.s32 	%r19, %r18, 4;
	ld.u32 	%r20, [%rd174];
	ld.u32 	%r21, [%rd174+4];
	// begin inline asm
	st.shared.f32 [%r18], %r20;
st.shared.f32 [%r19], %r21;
	// end inline asm
	setp.eq.s64 	%p24, %rd20, 1;
	@%p24 bra 	$L__BB31_19;
	cvt.u64.u32 	%rd4, %r9;
	add.s64 	%rd353, %rd20, -1;
	shl.b64 	%rd175, %rd4, 11;
	add.s64 	%rd176, %rd175, %rd346;
	add.s64 	%rd178, %rd176, %rd173;
	add.s64 	%rd352, %rd5, %rd178;
	mov.u64 	%rd179, 2040;
	sub.s64 	%rd351, %rd179, %rd173;
	mov.u64 	%rd354, %rd355;
$L__BB31_16:
	add.s64 	%rd32, %rd354, %rd8;
	setp.lt.u64 	%p26, %rd32, 256;
	@%p26 bra 	$L__BB31_18;
	bra.uni 	$L__BB31_17;
$L__BB31_18:
	shr.u64 	%rd180, %rd351, 3;
	setp.gt.u64 	%p25, %rd180, %rd8;
	selp.b64 	%rd31, %rd352, 0, %p25;
	setp.lt.u64 	%p1, %rd32, %rd354;
	add.s64 	%rd185, %rd354, %rd3;
	selp.b64 	%rd354, 256, %rd185, %p1;
	cvt.u32.u64 	%r28, %rd32;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r24, %r29, %r6;
	add.s32 	%r25, %r24, 4;
	ld.u32 	%r26, [%rd31];
	ld.u32 	%r27, [%rd31+4];
	// begin inline asm
	st.shared.f32 [%r24], %r26;
st.shared.f32 [%r25], %r27;
	// end inline asm
	add.s64 	%rd353, %rd353, -1;
	add.s64 	%rd352, %rd352, %rd346;
	sub.s64 	%rd351, %rd351, %rd346;
	setp.ne.s64 	%p27, %rd353, 0;
	@%p27 bra 	$L__BB31_16;
	bra.uni 	$L__BB31_19;
$L__BB31_13:
	add.s64 	%rd355, %rd95, 1;
$L__BB31_19:
	ld.global.nc.u64 	%rd38, [%rd1];
	ld.global.nc.u64 	%rd39, [%rd1+8];
	bar.sync 	0;
	setp.lt.u32 	%p28, %r10, 128;
	@%p28 bra 	$L__BB31_20;
	bra.uni 	$L__BB31_25;
$L__BB31_20:
	shl.b32 	%r2, %r10, 4;
	add.s32 	%r33, %r6, %r2;
	add.s32 	%r34, %r33, 4;
	// begin inline asm
	ld.shared.f32 %r31, [%r33];
ld.shared.f32 %r32, [%r34];
	// end inline asm
	setp.ne.s64 	%p29, %rd39, 0;
	@%p29 bra 	$L__BB31_24;
	bra.uni 	$L__BB31_21;
$L__BB31_24:
	mov.b32 	%f2, %r32;
	mov.b32 	%f1, %r31;
	ld.u32 	%r38, [%rd38];
	ld.u32 	%r41, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r36, %r31, %r38;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r39, %r32, %r41;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r77, %r36, %r39;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r31, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r48, %r32, %r38;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r45, %r48;
	// end inline asm
	add.s32 	%r92, %r33, 8;
	add.s32 	%r93, %r33, 12;
	// begin inline asm
	ld.shared.f32 %r59, [%r92];
ld.shared.f32 %r62, [%r93];
	// end inline asm
	ld.u32 	%r72, [%rd38];
	ld.u32 	%r69, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r58, %r59, %r72;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r61, %r62, %r69;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r78, %r58, %r61;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r67, %r59, %r69;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r70, %r62, %r72;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r81, %r67, %r70;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r76, %r77, %r78;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r79, %r80, %r81;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r33], %r76;
st.shared.f32 [%r34], %r79;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r86, %r77, %r78;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r89, %r80, %r81;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r92], %r86;
st.shared.f32 [%r93], %r89;
	// end inline asm
	add.s64 	%rd43, %rd95, %rd3;
	setp.lt.u64 	%p30, %rd43, 128;
	@%p30 bra 	$L__BB31_22;
	bra.uni 	$L__BB31_25;
$L__BB31_22:
	cvt.u32.u64 	%r161, %rd43;
	shl.b32 	%r162, %r161, 4;
	add.s32 	%r99, %r6, %r162;
	add.s32 	%r100, %r99, 4;
	// begin inline asm
	ld.shared.f32 %r102, [%r99];
ld.shared.f32 %r105, [%r100];
	// end inline asm
	ld.u32 	%r115, [%rd38];
	ld.u32 	%r112, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r101, %r102, %r115;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r104, %r105, %r112;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r142, %r101, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r110, %r102, %r112;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r113, %r105, %r115;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r145, %r110, %r113;
	// end inline asm
	add.s32 	%r157, %r99, 8;
	add.s32 	%r158, %r99, 12;
	// begin inline asm
	ld.shared.f32 %r124, [%r157];
ld.shared.f32 %r127, [%r158];
	// end inline asm
	ld.u32 	%r137, [%rd38];
	ld.u32 	%r134, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r123, %r124, %r137;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r126, %r127, %r134;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r143, %r123, %r126;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r132, %r124, %r134;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r135, %r127, %r137;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r146, %r132, %r135;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r142, %r143;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r145, %r146;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r99], %r141;
st.shared.f32 [%r100], %r144;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r151, %r142, %r143;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r154, %r145, %r146;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r157], %r151;
st.shared.f32 [%r158], %r154;
	// end inline asm
	add.s64 	%rd356, %rd43, %rd3;
	setp.gt.u64 	%p31, %rd356, 127;
	@%p31 bra 	$L__BB31_25;
$L__BB31_23:
	add.s64 	%rd186, %rd356, 1;
	cvt.u32.u64 	%r227, %rd356;
	shl.b32 	%r228, %r227, 4;
	add.s32 	%r165, %r228, %r6;
	add.s32 	%r166, %r165, 4;
	// begin inline asm
	ld.shared.f32 %r168, [%r165];
ld.shared.f32 %r171, [%r166];
	// end inline asm
	ld.u32 	%r181, [%rd38];
	ld.u32 	%r178, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r167, %r168, %r181;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r170, %r171, %r178;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r208, %r167, %r170;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r176, %r168, %r178;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r179, %r171, %r181;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r211, %r176, %r179;
	// end inline asm
	add.s32 	%r223, %r165, 8;
	add.s32 	%r224, %r165, 12;
	// begin inline asm
	ld.shared.f32 %r190, [%r223];
ld.shared.f32 %r193, [%r224];
	// end inline asm
	ld.u32 	%r203, [%rd38];
	ld.u32 	%r200, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r189, %r190, %r203;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r192, %r193, %r200;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r209, %r189, %r192;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r198, %r190, %r200;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r201, %r193, %r203;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r212, %r198, %r201;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r207, %r208, %r209;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r210, %r211, %r212;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r165], %r207;
st.shared.f32 [%r166], %r210;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r217, %r208, %r209;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r220, %r211, %r212;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r223], %r217;
st.shared.f32 [%r224], %r220;
	// end inline asm
	add.s64 	%rd187, %rd186, %rd8;
	setp.lt.u64 	%p32, %rd187, %rd186;
	add.s64 	%rd356, %rd356, %rd3;
	setp.gt.u64 	%p33, %rd356, 127;
	or.pred  	%p34, %p32, %p33;
	@%p34 bra 	$L__BB31_25;
	bra.uni 	$L__BB31_23;
$L__BB31_25:
	ld.global.nc.u64 	%rd189, [%rd1+32];
	ld.global.nc.u64 	%rd190, [%rd1+40];
	and.b64  	%rd191, %rd190, -256;
	setp.lt.u64 	%p35, %rd6, %rd191;
	sub.s64 	%rd193, %rd190, %rd6;
	setp.gt.u64 	%p36, %rd193, 255;
	and.pred  	%p37, %p35, %p36;
	@%p37 bra 	$L__BB31_27;
	bra.uni 	$L__BB31_26;
$L__BB31_27:
	setp.gt.u32 	%p38, %r10, 127;
	bar.sync 	0;
	mov.u64 	%rd363, 0;
	mov.u64 	%rd93, %rd363;
	@%p38 bra 	$L__BB31_31;
	setp.eq.s32 	%p39, %r10, 0;
	selp.b64 	%rd198, 1, %rd355, %p39;
	add.s64 	%rd357, %rd198, %rd8;
	setp.gt.u64 	%p40, %rd357, 127;
	mov.u64 	%rd93, 1;
	@%p40 bra 	$L__BB31_31;
	mov.u64 	%rd93, 1;
$L__BB31_30:
	add.s64 	%rd200, %rd357, 1;
	add.s64 	%rd93, %rd93, 1;
	add.s64 	%rd201, %rd200, %rd8;
	setp.lt.u64 	%p41, %rd201, %rd200;
	add.s64 	%rd357, %rd357, %rd3;
	setp.gt.u64 	%p42, %rd357, 127;
	or.pred  	%p43, %p41, %p42;
	@!%p43 bra 	$L__BB31_30;
	bra.uni 	$L__BB31_31;
$L__BB31_31:
	setp.eq.s64 	%p99, %rd93, 0;
	mov.u64 	%rd362, %rd363;
	@%p99 bra 	$L__BB31_38;
	mov.u64 	%rd361, 0;
	@%p19 bra 	$L__BB31_37;
	and.b64  	%rd205, %rd9, -4294967296;
	setp.ne.s64 	%p45, %rd205, 0;
	@%p45 bra 	$L__BB31_35;
	bra.uni 	$L__BB31_34;
$L__BB31_35:
	div.u64 	%rd360, %rd9, %rd3;
	bra.uni 	$L__BB31_36;
$L__BB31_34:
	cvt.u32.u64 	%r232, %rd3;
	cvt.u32.u64 	%r233, %rd9;
	div.u32 	%r234, %r233, %r232;
	cvt.u64.u32 	%rd360, %r234;
$L__BB31_36:
	add.s64 	%rd361, %rd360, 1;
$L__BB31_37:
	min.u64 	%rd362, %rd361, %rd93;
$L__BB31_38:
	add.s64 	%rd44, %rd189, %rd164;
	@%p38 bra 	$L__BB31_40;
	mov.u64 	%rd207, 127;
	sub.s64 	%rd60, %rd207, %rd95;
	cvt.u32.u64 	%r236, %rd60;
	cvt.u32.u64 	%r237, %rd3;
	div.u32 	%r238, %r236, %r237;
	cvt.u64.u32 	%rd208, %r238;
	add.s64 	%rd363, %rd208, 1;
$L__BB31_40:
	selp.b64 	%rd396, %rd44, 0, %p37;
	add.s64 	%rd46, %rd44, 2048;
	min.u64 	%rd63, %rd362, %rd363;
	setp.eq.s64 	%p47, %rd63, 0;
	@%p47 bra 	$L__BB31_55;
	setp.eq.s64 	%p48, %rd93, 0;
	mov.u64 	%rd369, 0;
	mov.u64 	%rd374, %rd396;
	mov.u64 	%rd375, %rd95;
	mov.u64 	%rd367, %rd369;
	@%p48 bra 	$L__BB31_45;
	add.s64 	%rd369, %rd93, -1;
	setp.eq.s32 	%p49, %r10, 0;
	@%p49 bra 	$L__BB31_44;
	setp.lt.u32 	%p50, %r10, 256;
	shl.b64 	%rd212, %rd95, 3;
	add.s64 	%rd213, %rd396, %rd212;
	add.s64 	%rd214, %rd213, 8;
	selp.b64 	%rd374, %rd214, %rd46, %p50;
	selp.b64 	%rd367, %rd213, 0, %p50;
	mov.u64 	%rd375, 0;
	bra.uni 	$L__BB31_45;
$L__BB31_44:
	add.s64 	%rd374, %rd396, 8;
	mov.u64 	%rd375, 0;
	mov.u64 	%rd367, %rd396;
$L__BB31_45:
	@%p38 bra 	$L__BB31_53;
	add.s32 	%r5, %r6, 8;
	shl.b32 	%r247, %r10, 4;
	add.s32 	%r244, %r5, %r247;
	add.s32 	%r245, %r244, 4;
	// begin inline asm
	ld.shared.f32 %r242, [%r244];
ld.shared.f32 %r243, [%r245];
	// end inline asm
	st.u32 	[%rd367], %r242;
	st.u32 	[%rd367+4], %r243;
	setp.eq.s64 	%p52, %rd63, 1;
	@%p52 bra 	$L__BB31_55;
	mov.pred 	%p97, 0;
	add.s64 	%rd372, %rd95, 1;
	add.s64 	%rd368, %rd63, -1;
	mov.u64 	%rd217, 0;
$L__BB31_48:
	setp.eq.s64 	%p53, %rd369, 0;
	mov.u64 	%rd373, %rd217;
	mov.u64 	%rd376, %rd217;
	@%p53 bra 	$L__BB31_52;
	add.s64 	%rd373, %rd369, -1;
	selp.b64 	%rd80, 0, %rd8, %p99;
	setp.ne.s64 	%p54, %rd375, 0;
	sub.s64 	%rd344, %rd46, %rd374;
	@%p54 bra 	$L__BB31_51;
	shr.u64 	%rd227, %rd344, 3;
	setp.gt.u64 	%p58, %rd227, %rd80;
	shl.b64 	%rd228, %rd80, 3;
	add.s64 	%rd229, %rd374, %rd228;
	add.s64 	%rd230, %rd229, 8;
	selp.b64 	%rd374, %rd230, %rd46, %p58;
	selp.b64 	%rd376, %rd229, 0, %p58;
	mov.u64 	%rd375, 0;
	mov.pred 	%p99, %p97;
	bra.uni 	$L__BB31_52;
$L__BB31_51:
	add.s64 	%rd219, %rd80, %rd375;
	shr.u64 	%rd221, %rd344, 3;
	setp.gt.u64 	%p56, %rd221, %rd219;
	shl.b64 	%rd222, %rd219, 3;
	add.s64 	%rd223, %rd374, %rd222;
	add.s64 	%rd224, %rd223, 8;
	selp.b64 	%rd374, %rd224, %rd46, %p56;
	selp.b64 	%rd376, %rd223, 0, %p56;
	mov.u64 	%rd375, 0;
	mov.pred 	%p99, %p97;
$L__BB31_52:
	add.s64 	%rd89, %rd372, %rd8;
	shl.b64 	%rd90, %rd89, 1;
	setp.lt.u64 	%p59, %rd90, 256;
	@%p59 bra 	$L__BB31_54;
	bra.uni 	$L__BB31_53;
$L__BB31_54:
	setp.lt.u64 	%p5, %rd89, %rd372;
	setp.gt.u64 	%p60, %rd89, 127;
	add.s64 	%rd235, %rd89, 1;
	selp.b64 	%rd236, 128, %rd235, %p60;
	selp.b64 	%rd372, 128, %rd236, %p5;
	cvt.u32.u64 	%r252, %rd90;
	shl.b32 	%r253, %r252, 3;
	add.s32 	%r250, %r253, %r5;
	add.s32 	%r251, %r250, 4;
	// begin inline asm
	ld.shared.f32 %r248, [%r250];
ld.shared.f32 %r249, [%r251];
	// end inline asm
	st.u32 	[%rd376], %r248;
	st.u32 	[%rd376+4], %r249;
	add.s64 	%rd368, %rd368, -1;
	setp.ne.s64 	%p61, %rd368, 0;
	mov.u64 	%rd369, %rd373;
	@%p61 bra 	$L__BB31_48;
$L__BB31_55:
	mov.u64 	%rd403, 0;
	mov.pred 	%p100, -1;
	mov.u64 	%rd404, %rd95;
	mov.pred 	%p103, %p100;
$L__BB31_56:
	setp.ne.s64 	%p63, %rd93, 0;
	@%p63 bra 	$L__BB31_60;
	bra.uni 	$L__BB31_57;
$L__BB31_60:
	@%p100 bra 	$L__BB31_63;
	bra.uni 	$L__BB31_61;
$L__BB31_63:
	setp.ne.s64 	%p64, %rd95, 0;
	@%p64 bra 	$L__BB31_65;
	bra.uni 	$L__BB31_64;
$L__BB31_65:
	sub.s64 	%rd238, %rd46, %rd396;
	shr.u64 	%rd239, %rd238, 3;
	setp.gt.u64 	%p65, %rd239, %rd95;
	shl.b64 	%rd240, %rd95, 3;
	add.s64 	%rd241, %rd396, %rd240;
	add.s64 	%rd242, %rd241, 8;
	selp.b64 	%rd396, %rd242, %rd46, %p65;
	bra.uni 	$L__BB31_66;
$L__BB31_57:
	selp.b64 	%rd98, 0, %rd8, %p100;
	setp.ne.s64 	%p86, %rd95, 0;
	@%p86 bra 	$L__BB31_59;
	bra.uni 	$L__BB31_58;
$L__BB31_59:
	add.s64 	%rd317, %rd98, %rd95;
	sub.s64 	%rd318, %rd46, %rd396;
	shr.u64 	%rd319, %rd318, 3;
	setp.gt.u64 	%p87, %rd319, %rd317;
	shl.b64 	%rd320, %rd317, 3;
	add.s64 	%rd321, %rd396, %rd320;
	add.s64 	%rd322, %rd321, 8;
	selp.b64 	%rd401, %rd322, %rd46, %p87;
	selp.b64 	%rd402, %rd321, 0, %p87;
	bra.uni 	$L__BB31_88;
$L__BB31_58:
	sub.s64 	%rd323, %rd46, %rd396;
	shr.u64 	%rd324, %rd323, 3;
	setp.gt.u64 	%p88, %rd324, %rd98;
	shl.b64 	%rd325, %rd98, 3;
	add.s64 	%rd326, %rd396, %rd325;
	add.s64 	%rd327, %rd326, 8;
	selp.b64 	%rd401, %rd327, %rd46, %p88;
	selp.b64 	%rd402, %rd326, 0, %p88;
	bra.uni 	$L__BB31_88;
$L__BB31_64:
	setp.eq.s64 	%p66, %rd396, %rd46;
	selp.b64 	%rd243, 0, 8, %p66;
	add.s64 	%rd396, %rd396, %rd243;
$L__BB31_66:
	add.s64 	%rd93, %rd93, -1;
	mov.u64 	%rd95, 0;
$L__BB31_61:
	setp.eq.s64 	%p67, %rd93, -1;
	@%p67 bra 	$L__BB31_67;
	add.s64 	%rd388, %rd93, 1;
	bra.uni 	$L__BB31_70;
$L__BB31_67:
	setp.ne.s64 	%p68, %rd95, 0;
	sub.s64 	%rd345, %rd46, %rd396;
	@%p68 bra 	$L__BB31_69;
	bra.uni 	$L__BB31_68;
$L__BB31_69:
	add.s64 	%rd247, %rd95, %rd8;
	shr.u64 	%rd249, %rd345, 3;
	setp.gt.u64 	%p69, %rd249, %rd247;
	add.s64 	%rd250, %rd95, %rd3;
	shl.b64 	%rd251, %rd250, 3;
	add.s64 	%rd252, %rd396, %rd251;
	selp.b64 	%rd396, %rd252, %rd46, %p69;
	mov.u64 	%rd388, -1;
	mov.u64 	%rd95, 0;
	bra.uni 	$L__BB31_70;
$L__BB31_68:
	shr.u64 	%rd256, %rd345, 3;
	setp.gt.u64 	%p70, %rd256, %rd8;
	add.s64 	%rd258, %rd396, %rd346;
	selp.b64 	%rd396, %rd258, %rd46, %p70;
	mov.u64 	%rd388, -1;
	mov.u64 	%rd95, 0;
$L__BB31_70:
	mul.lo.s64 	%rd398, %rd388, %rd3;
	mul.hi.u64 	%rd259, %rd388, %rd3;
	setp.eq.s64 	%p71, %rd259, 0;
	@%p71 bra 	$L__BB31_82;
	mov.u64 	%rd260, -1;
	div.u64 	%rd261, %rd260, %rd388;
	div.u64 	%rd262, %rd260, %rd3;
	mul.lo.s64 	%rd263, %rd261, %rd388;
	mul.lo.s64 	%rd264, %rd262, %rd3;
	max.u64 	%rd117, %rd263, %rd264;
	add.s64 	%rd390, %rd117, -1;
	setp.eq.s64 	%p73, %rd95, 0;
	@%p73 bra 	$L__BB31_76;
	add.s64 	%rd267, %rd95, %rd390;
	setp.lt.u64 	%p74, %rd267, %rd95;
	@%p74 bra 	$L__BB31_74;
	bra.uni 	$L__BB31_73;
$L__BB31_74:
	add.s64 	%rd268, %rd95, -1;
	sub.s64 	%rd269, %rd46, %rd396;
	shr.u64 	%rd270, %rd269, 3;
	setp.le.u64 	%p75, %rd270, %rd268;
	shl.b64 	%rd271, %rd95, 3;
	add.s64 	%rd389, %rd396, %rd271;
	mov.u64 	%rd396, %rd46;
	@%p75 bra 	$L__BB31_78;
	bra.uni 	$L__BB31_75;
$L__BB31_76:
	sub.s64 	%rd275, %rd46, %rd396;
	shr.u64 	%rd276, %rd275, 3;
	setp.gt.u64 	%p102, %rd276, %rd390;
	shl.b64 	%rd277, %rd117, 3;
	add.s64 	%rd278, %rd396, %rd277;
	add.s64 	%rd391, %rd278, -8;
	bra.uni 	$L__BB31_77;
$L__BB31_73:
	add.s64 	%rd390, %rd390, %rd95;
	mov.u64 	%rd389, %rd396;
$L__BB31_75:
	sub.s64 	%rd272, %rd46, %rd389;
	shr.u64 	%rd273, %rd272, 3;
	setp.gt.u64 	%p102, %rd273, %rd390;
	shl.b64 	%rd274, %rd390, 3;
	add.s64 	%rd391, %rd389, %rd274;
$L__BB31_77:
	add.s64 	%rd279, %rd391, 8;
	selp.b64 	%rd396, %rd279, %rd46, %p102;
$L__BB31_78:
	setp.gt.u64 	%p72, %rd263, %rd264;
	selp.b64 	%rd265, %rd261, 0, %p72;
	sub.s64 	%rd394, %rd3, %rd265;
	selp.b64 	%rd266, 0, %rd262, %p72;
	sub.s64 	%rd393, %rd388, %rd266;
	mul.lo.s64 	%rd398, %rd393, %rd394;
	mul.hi.u64 	%rd281, %rd393, %rd394;
	mov.u64 	%rd95, 0;
	setp.eq.s64 	%p76, %rd281, 0;
	@%p76 bra 	$L__BB31_82;
$L__BB31_79:
	setp.eq.s64 	%p77, %rd393, 0;
	@%p77 bra 	$L__BB31_95;
	setp.eq.s64 	%p78, %rd394, 0;
	@%p78 bra 	$L__BB31_96;
	div.u64 	%rd284, %rd260, %rd393;
	div.u64 	%rd285, %rd260, %rd394;
	mul.lo.s64 	%rd286, %rd284, %rd393;
	mul.lo.s64 	%rd287, %rd285, %rd394;
	setp.gt.u64 	%p79, %rd286, %rd287;
	selp.b64 	%rd288, %rd284, 0, %p79;
	sub.s64 	%rd394, %rd394, %rd288;
	selp.b64 	%rd289, 0, %rd285, %p79;
	sub.s64 	%rd393, %rd393, %rd289;
	max.u64 	%rd290, %rd286, %rd287;
	add.s64 	%rd291, %rd290, -1;
	shl.b64 	%rd292, %rd290, 3;
	add.s64 	%rd293, %rd396, %rd292;
	sub.s64 	%rd294, %rd46, %rd396;
	shr.u64 	%rd295, %rd294, 3;
	setp.gt.u64 	%p80, %rd295, %rd291;
	selp.b64 	%rd396, %rd293, %rd46, %p80;
	mul.lo.s64 	%rd398, %rd393, %rd394;
	mul.hi.u64 	%rd296, %rd393, %rd394;
	setp.ne.s64 	%p13, %rd296, 0;
	@%p13 bra 	$L__BB31_79;
$L__BB31_82:
	add.s64 	%rd400, %rd398, -1;
	setp.ne.s64 	%p81, %rd95, 0;
	@%p81 bra 	$L__BB31_84;
	bra.uni 	$L__BB31_83;
$L__BB31_84:
	add.s64 	%rd297, %rd95, %rd400;
	setp.lt.u64 	%p82, %rd297, %rd95;
	@%p82 bra 	$L__BB31_86;
	bra.uni 	$L__BB31_85;
$L__BB31_86:
	add.s64 	%rd299, %rd95, -1;
	sub.s64 	%rd300, %rd46, %rd396;
	shr.u64 	%rd301, %rd300, 3;
	setp.le.u64 	%p83, %rd301, %rd299;
	shl.b64 	%rd302, %rd95, 3;
	add.s64 	%rd396, %rd396, %rd302;
	mov.u64 	%rd402, 0;
	mov.u64 	%rd401, %rd46;
	@%p83 bra 	$L__BB31_88;
	bra.uni 	$L__BB31_87;
$L__BB31_83:
	sub.s64 	%rd308, %rd46, %rd396;
	shr.u64 	%rd309, %rd308, 3;
	setp.gt.u64 	%p85, %rd309, %rd400;
	shl.b64 	%rd310, %rd398, 3;
	add.s64 	%rd311, %rd396, %rd310;
	add.s64 	%rd312, %rd311, -8;
	selp.b64 	%rd401, %rd311, %rd46, %p85;
	selp.b64 	%rd402, %rd312, 0, %p85;
	bra.uni 	$L__BB31_88;
$L__BB31_85:
	add.s64 	%rd400, %rd400, %rd95;
$L__BB31_87:
	sub.s64 	%rd303, %rd46, %rd396;
	shr.u64 	%rd304, %rd303, 3;
	setp.gt.u64 	%p84, %rd304, %rd400;
	shl.b64 	%rd305, %rd400, 3;
	add.s64 	%rd306, %rd396, %rd305;
	add.s64 	%rd307, %rd306, 8;
	selp.b64 	%rd401, %rd307, %rd46, %p84;
	selp.b64 	%rd402, %rd306, 0, %p84;
$L__BB31_88:
	setp.eq.s64 	%p89, %rd402, 0;
	mov.u64 	%rd406, 0;
	@%p89 bra 	$L__BB31_90;
	selp.b64 	%rd331, 0, %rd8, %p103;
	add.s64 	%rd333, %rd331, %rd404;
	add.s64 	%rd405, %rd333, %rd403;
	setp.lt.u64 	%p91, %rd405, %rd403;
	setp.gt.u64 	%p92, %rd405, 127;
	or.pred  	%p93, %p91, %p92;
	add.s64 	%rd334, %rd405, 1;
	selp.b64 	%rd403, 128, %rd334, %p93;
	selp.b64 	%rd406, 0, %rd402, %p93;
	mov.pred 	%p103, 0;
	mov.u64 	%rd404, 0;
$L__BB31_90:
	setp.eq.s64 	%p94, %rd406, 0;
	@%p94 bra 	$L__BB31_94;
	shl.b64 	%rd160, %rd405, 1;
	setp.lt.u64 	%p95, %rd160, 256;
	@%p95 bra 	$L__BB31_93;
	bra.uni 	$L__BB31_92;
$L__BB31_93:
	cvt.u32.u64 	%r258, %rd160;
	shl.b32 	%r259, %r258, 3;
	add.s32 	%r256, %r259, %r6;
	add.s32 	%r257, %r256, 4;
	// begin inline asm
	ld.shared.f32 %r254, [%r256];
ld.shared.f32 %r255, [%r257];
	// end inline asm
	st.u32 	[%rd406], %r254;
	st.u32 	[%rd406+4], %r255;
	mov.pred 	%p100, 0;
	mov.u64 	%rd93, 0;
	mov.u64 	%rd396, %rd401;
	mov.u64 	%rd95, %rd93;
	bra.uni 	$L__BB31_56;
$L__BB31_94:
	ret;
$L__BB31_95:
	mov.u64 	%rd315, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd316, %rd315;
	{ // callseq 168, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd316;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 168
$L__BB31_96:
	mov.u64 	%rd313, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd314, %rd313;
	{ // callseq 167, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd314;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 167
$L__BB31_92:
	mov.u64 	%rd335, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd336, %rd335;
	mov.u64 	%rd337, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd338, %rd337;
	{ // callseq 169, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd336;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd338;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 169
$L__BB31_53:
	mov.u64 	%rd231, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd232, %rd231;
	mov.u64 	%rd233, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd234, %rd233;
	{ // callseq 166, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd232;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd234;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 166
$L__BB31_17:
	mov.u64 	%rd181, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd182, %rd181;
	mov.u64 	%rd183, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd184, %rd183;
	{ // callseq 164, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd182;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd184;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 164
$L__BB31_1:
	mov.u64 	%rd342, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_12;
	cvta.global.u64 	%rd343, %rd342;
	{ // callseq 171, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd343;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 171
$L__BB31_26:
	mov.u64 	%rd194, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_13;
	cvta.global.u64 	%rd195, %rd194;
	{ // callseq 165, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd195;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 165
$L__BB31_21:
	mov.u64 	%rd340, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_16;
	cvta.global.u64 	%rd341, %rd340;
	{ // callseq 170, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd341;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 170

}
	// .globl	daubechies_first_forward_512_kernel
.visible .entry daubechies_first_forward_512_kernel(
	.param .u64 daubechies_first_forward_512_kernel_param_0
)
{
	.reg .pred 	%p<104>;
	.reg .b32 	%r<262>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<407>;

	ld.param.u64 	%rd161, [daubechies_first_forward_512_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd161;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[4096];
    mov.u32 %r6, nonphysical;
	// end inline asm
	mov.u32 	%r9, %ctaid.x;
	ld.global.nc.u64 	%rd5, [%rd1+16];
	ld.global.nc.u64 	%rd162, [%rd1+24];
	and.b64  	%rd163, %rd162, -512;
	mul.wide.u32 	%rd6, %r9, 512;
	setp.ge.u64 	%p16, %rd6, %rd163;
	sub.s64 	%rd165, %rd162, %rd6;
	setp.lt.u64 	%p17, %rd165, 512;
	or.pred  	%p18, %p16, %p17;
	@!%p18 bra 	$L__BB32_2;
	bra.uni 	$L__BB32_1;
$L__BB32_2:
	mov.u32 	%r7, %tid.x;
	cvt.u64.u32 	%rd95, %r7;
	mov.u32 	%r8, %ntid.x;
	cvt.u64.u32 	%rd3, %r8;
	cvt.u32.u64 	%r10, %rd95;
	max.u64 	%rd168, %rd95, 512;
	setp.gt.u32 	%p19, %r10, 511;
	not.b64 	%rd169, %rd95;
	add.s64 	%rd9, %rd169, %rd168;
	mov.u64 	%rd350, 0;
	mov.u64 	%rd348, %rd350;
	@%p19 bra 	$L__BB32_7;
	and.b64  	%rd170, %rd9, -4294967296;
	setp.ne.s64 	%p20, %rd170, 0;
	@%p20 bra 	$L__BB32_5;
	bra.uni 	$L__BB32_4;
$L__BB32_5:
	div.u64 	%rd347, %rd9, %rd3;
	bra.uni 	$L__BB32_6;
$L__BB32_4:
	cvt.u32.u64 	%r11, %rd3;
	cvt.u32.u64 	%r12, %rd9;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd347, %r13;
$L__BB32_6:
	add.s64 	%rd348, %rd347, 1;
$L__BB32_7:
	@%p19 bra 	$L__BB32_12;
	and.b64  	%rd172, %rd9, -4294967296;
	setp.ne.s64 	%p22, %rd172, 0;
	@%p22 bra 	$L__BB32_10;
	bra.uni 	$L__BB32_9;
$L__BB32_10:
	div.u64 	%rd349, %rd9, %rd3;
	bra.uni 	$L__BB32_11;
$L__BB32_9:
	cvt.u32.u64 	%r15, %rd3;
	cvt.u32.u64 	%r16, %rd9;
	div.u32 	%r17, %r16, %r15;
	cvt.u64.u32 	%rd349, %r17;
$L__BB32_11:
	add.s64 	%rd350, %rd349, 1;
$L__BB32_12:
	shl.b64 	%rd164, %rd6, 3;
	add.s64 	%rd8, %rd3, -1;
	min.u64 	%rd20, %rd348, %rd350;
	setp.ne.s64 	%p23, %rd20, 0;
	shl.b64 	%rd346, %rd3, 3;
	@%p23 bra 	$L__BB32_14;
	bra.uni 	$L__BB32_13;
$L__BB32_14:
	add.s64 	%rd7, %rd5, %rd164;
	shl.b64 	%rd173, %rd95, 3;
	add.s64 	%rd174, %rd7, %rd173;
	add.s64 	%rd355, %rd95, 1;
	shl.b32 	%r23, %r10, 3;
	add.s32 	%r18, %r6, %r23;
	add.s32 	%r19, %r18, 4;
	ld.u32 	%r20, [%rd174];
	ld.u32 	%r21, [%rd174+4];
	// begin inline asm
	st.shared.f32 [%r18], %r20;
st.shared.f32 [%r19], %r21;
	// end inline asm
	setp.eq.s64 	%p24, %rd20, 1;
	@%p24 bra 	$L__BB32_19;
	cvt.u64.u32 	%rd4, %r9;
	add.s64 	%rd353, %rd20, -1;
	shl.b64 	%rd175, %rd4, 12;
	add.s64 	%rd176, %rd175, %rd346;
	add.s64 	%rd178, %rd176, %rd173;
	add.s64 	%rd352, %rd5, %rd178;
	mov.u64 	%rd179, 4088;
	sub.s64 	%rd351, %rd179, %rd173;
	mov.u64 	%rd354, %rd355;
$L__BB32_16:
	add.s64 	%rd32, %rd354, %rd8;
	setp.lt.u64 	%p26, %rd32, 512;
	@%p26 bra 	$L__BB32_18;
	bra.uni 	$L__BB32_17;
$L__BB32_18:
	shr.u64 	%rd180, %rd351, 3;
	setp.gt.u64 	%p25, %rd180, %rd8;
	selp.b64 	%rd31, %rd352, 0, %p25;
	setp.lt.u64 	%p1, %rd32, %rd354;
	add.s64 	%rd185, %rd354, %rd3;
	selp.b64 	%rd354, 512, %rd185, %p1;
	cvt.u32.u64 	%r28, %rd32;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r24, %r29, %r6;
	add.s32 	%r25, %r24, 4;
	ld.u32 	%r26, [%rd31];
	ld.u32 	%r27, [%rd31+4];
	// begin inline asm
	st.shared.f32 [%r24], %r26;
st.shared.f32 [%r25], %r27;
	// end inline asm
	add.s64 	%rd353, %rd353, -1;
	add.s64 	%rd352, %rd352, %rd346;
	sub.s64 	%rd351, %rd351, %rd346;
	setp.ne.s64 	%p27, %rd353, 0;
	@%p27 bra 	$L__BB32_16;
	bra.uni 	$L__BB32_19;
$L__BB32_13:
	add.s64 	%rd355, %rd95, 1;
$L__BB32_19:
	ld.global.nc.u64 	%rd38, [%rd1];
	ld.global.nc.u64 	%rd39, [%rd1+8];
	bar.sync 	0;
	setp.lt.u32 	%p28, %r10, 256;
	@%p28 bra 	$L__BB32_20;
	bra.uni 	$L__BB32_25;
$L__BB32_20:
	shl.b32 	%r2, %r10, 4;
	add.s32 	%r33, %r6, %r2;
	add.s32 	%r34, %r33, 4;
	// begin inline asm
	ld.shared.f32 %r31, [%r33];
ld.shared.f32 %r32, [%r34];
	// end inline asm
	setp.ne.s64 	%p29, %rd39, 0;
	@%p29 bra 	$L__BB32_24;
	bra.uni 	$L__BB32_21;
$L__BB32_24:
	mov.b32 	%f2, %r32;
	mov.b32 	%f1, %r31;
	ld.u32 	%r38, [%rd38];
	ld.u32 	%r41, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r36, %r31, %r38;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r39, %r32, %r41;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r77, %r36, %r39;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r31, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r48, %r32, %r38;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r45, %r48;
	// end inline asm
	add.s32 	%r92, %r33, 8;
	add.s32 	%r93, %r33, 12;
	// begin inline asm
	ld.shared.f32 %r59, [%r92];
ld.shared.f32 %r62, [%r93];
	// end inline asm
	ld.u32 	%r72, [%rd38];
	ld.u32 	%r69, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r58, %r59, %r72;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r61, %r62, %r69;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r78, %r58, %r61;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r67, %r59, %r69;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r70, %r62, %r72;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r81, %r67, %r70;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r76, %r77, %r78;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r79, %r80, %r81;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r33], %r76;
st.shared.f32 [%r34], %r79;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r86, %r77, %r78;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r89, %r80, %r81;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r92], %r86;
st.shared.f32 [%r93], %r89;
	// end inline asm
	add.s64 	%rd43, %rd95, %rd3;
	setp.lt.u64 	%p30, %rd43, 256;
	@%p30 bra 	$L__BB32_22;
	bra.uni 	$L__BB32_25;
$L__BB32_22:
	cvt.u32.u64 	%r161, %rd43;
	shl.b32 	%r162, %r161, 4;
	add.s32 	%r99, %r6, %r162;
	add.s32 	%r100, %r99, 4;
	// begin inline asm
	ld.shared.f32 %r102, [%r99];
ld.shared.f32 %r105, [%r100];
	// end inline asm
	ld.u32 	%r115, [%rd38];
	ld.u32 	%r112, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r101, %r102, %r115;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r104, %r105, %r112;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r142, %r101, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r110, %r102, %r112;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r113, %r105, %r115;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r145, %r110, %r113;
	// end inline asm
	add.s32 	%r157, %r99, 8;
	add.s32 	%r158, %r99, 12;
	// begin inline asm
	ld.shared.f32 %r124, [%r157];
ld.shared.f32 %r127, [%r158];
	// end inline asm
	ld.u32 	%r137, [%rd38];
	ld.u32 	%r134, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r123, %r124, %r137;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r126, %r127, %r134;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r143, %r123, %r126;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r132, %r124, %r134;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r135, %r127, %r137;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r146, %r132, %r135;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r142, %r143;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r145, %r146;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r99], %r141;
st.shared.f32 [%r100], %r144;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r151, %r142, %r143;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r154, %r145, %r146;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r157], %r151;
st.shared.f32 [%r158], %r154;
	// end inline asm
	add.s64 	%rd356, %rd43, %rd3;
	setp.gt.u64 	%p31, %rd356, 255;
	@%p31 bra 	$L__BB32_25;
$L__BB32_23:
	add.s64 	%rd186, %rd356, 1;
	cvt.u32.u64 	%r227, %rd356;
	shl.b32 	%r228, %r227, 4;
	add.s32 	%r165, %r228, %r6;
	add.s32 	%r166, %r165, 4;
	// begin inline asm
	ld.shared.f32 %r168, [%r165];
ld.shared.f32 %r171, [%r166];
	// end inline asm
	ld.u32 	%r181, [%rd38];
	ld.u32 	%r178, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r167, %r168, %r181;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r170, %r171, %r178;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r208, %r167, %r170;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r176, %r168, %r178;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r179, %r171, %r181;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r211, %r176, %r179;
	// end inline asm
	add.s32 	%r223, %r165, 8;
	add.s32 	%r224, %r165, 12;
	// begin inline asm
	ld.shared.f32 %r190, [%r223];
ld.shared.f32 %r193, [%r224];
	// end inline asm
	ld.u32 	%r203, [%rd38];
	ld.u32 	%r200, [%rd38+4];
	// begin inline asm
	mul.rn.ftz.f32 %r189, %r190, %r203;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r192, %r193, %r200;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r209, %r189, %r192;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r198, %r190, %r200;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r201, %r193, %r203;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r212, %r198, %r201;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r207, %r208, %r209;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r210, %r211, %r212;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r165], %r207;
st.shared.f32 [%r166], %r210;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r217, %r208, %r209;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r220, %r211, %r212;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r223], %r217;
st.shared.f32 [%r224], %r220;
	// end inline asm
	add.s64 	%rd187, %rd186, %rd8;
	setp.lt.u64 	%p32, %rd187, %rd186;
	add.s64 	%rd356, %rd356, %rd3;
	setp.gt.u64 	%p33, %rd356, 255;
	or.pred  	%p34, %p32, %p33;
	@%p34 bra 	$L__BB32_25;
	bra.uni 	$L__BB32_23;
$L__BB32_25:
	ld.global.nc.u64 	%rd189, [%rd1+32];
	ld.global.nc.u64 	%rd190, [%rd1+40];
	and.b64  	%rd191, %rd190, -512;
	setp.lt.u64 	%p35, %rd6, %rd191;
	sub.s64 	%rd193, %rd190, %rd6;
	setp.gt.u64 	%p36, %rd193, 511;
	and.pred  	%p37, %p35, %p36;
	@%p37 bra 	$L__BB32_27;
	bra.uni 	$L__BB32_26;
$L__BB32_27:
	setp.gt.u32 	%p38, %r10, 255;
	bar.sync 	0;
	mov.u64 	%rd363, 0;
	mov.u64 	%rd93, %rd363;
	@%p38 bra 	$L__BB32_31;
	setp.eq.s32 	%p39, %r10, 0;
	selp.b64 	%rd198, 1, %rd355, %p39;
	add.s64 	%rd357, %rd198, %rd8;
	setp.gt.u64 	%p40, %rd357, 255;
	mov.u64 	%rd93, 1;
	@%p40 bra 	$L__BB32_31;
	mov.u64 	%rd93, 1;
$L__BB32_30:
	add.s64 	%rd200, %rd357, 1;
	add.s64 	%rd93, %rd93, 1;
	add.s64 	%rd201, %rd200, %rd8;
	setp.lt.u64 	%p41, %rd201, %rd200;
	add.s64 	%rd357, %rd357, %rd3;
	setp.gt.u64 	%p42, %rd357, 255;
	or.pred  	%p43, %p41, %p42;
	@!%p43 bra 	$L__BB32_30;
	bra.uni 	$L__BB32_31;
$L__BB32_31:
	setp.eq.s64 	%p99, %rd93, 0;
	mov.u64 	%rd362, %rd363;
	@%p99 bra 	$L__BB32_38;
	mov.u64 	%rd361, 0;
	@%p19 bra 	$L__BB32_37;
	and.b64  	%rd205, %rd9, -4294967296;
	setp.ne.s64 	%p45, %rd205, 0;
	@%p45 bra 	$L__BB32_35;
	bra.uni 	$L__BB32_34;
$L__BB32_35:
	div.u64 	%rd360, %rd9, %rd3;
	bra.uni 	$L__BB32_36;
$L__BB32_34:
	cvt.u32.u64 	%r232, %rd3;
	cvt.u32.u64 	%r233, %rd9;
	div.u32 	%r234, %r233, %r232;
	cvt.u64.u32 	%rd360, %r234;
$L__BB32_36:
	add.s64 	%rd361, %rd360, 1;
$L__BB32_37:
	min.u64 	%rd362, %rd361, %rd93;
$L__BB32_38:
	add.s64 	%rd44, %rd189, %rd164;
	@%p38 bra 	$L__BB32_40;
	mov.u64 	%rd207, 255;
	sub.s64 	%rd60, %rd207, %rd95;
	cvt.u32.u64 	%r236, %rd60;
	cvt.u32.u64 	%r237, %rd3;
	div.u32 	%r238, %r236, %r237;
	cvt.u64.u32 	%rd208, %r238;
	add.s64 	%rd363, %rd208, 1;
$L__BB32_40:
	selp.b64 	%rd396, %rd44, 0, %p37;
	add.s64 	%rd46, %rd44, 4096;
	min.u64 	%rd63, %rd362, %rd363;
	setp.eq.s64 	%p47, %rd63, 0;
	@%p47 bra 	$L__BB32_55;
	setp.eq.s64 	%p48, %rd93, 0;
	mov.u64 	%rd369, 0;
	mov.u64 	%rd374, %rd396;
	mov.u64 	%rd375, %rd95;
	mov.u64 	%rd367, %rd369;
	@%p48 bra 	$L__BB32_45;
	add.s64 	%rd369, %rd93, -1;
	setp.eq.s32 	%p49, %r10, 0;
	@%p49 bra 	$L__BB32_44;
	setp.lt.u32 	%p50, %r10, 512;
	shl.b64 	%rd212, %rd95, 3;
	add.s64 	%rd213, %rd396, %rd212;
	add.s64 	%rd214, %rd213, 8;
	selp.b64 	%rd374, %rd214, %rd46, %p50;
	selp.b64 	%rd367, %rd213, 0, %p50;
	mov.u64 	%rd375, 0;
	bra.uni 	$L__BB32_45;
$L__BB32_44:
	add.s64 	%rd374, %rd396, 8;
	mov.u64 	%rd375, 0;
	mov.u64 	%rd367, %rd396;
$L__BB32_45:
	@%p38 bra 	$L__BB32_53;
	add.s32 	%r5, %r6, 8;
	shl.b32 	%r247, %r10, 4;
	add.s32 	%r244, %r5, %r247;
	add.s32 	%r245, %r244, 4;
	// begin inline asm
	ld.shared.f32 %r242, [%r244];
ld.shared.f32 %r243, [%r245];
	// end inline asm
	st.u32 	[%rd367], %r242;
	st.u32 	[%rd367+4], %r243;
	setp.eq.s64 	%p52, %rd63, 1;
	@%p52 bra 	$L__BB32_55;
	mov.pred 	%p97, 0;
	add.s64 	%rd372, %rd95, 1;
	add.s64 	%rd368, %rd63, -1;
	mov.u64 	%rd217, 0;
$L__BB32_48:
	setp.eq.s64 	%p53, %rd369, 0;
	mov.u64 	%rd373, %rd217;
	mov.u64 	%rd376, %rd217;
	@%p53 bra 	$L__BB32_52;
	add.s64 	%rd373, %rd369, -1;
	selp.b64 	%rd80, 0, %rd8, %p99;
	setp.ne.s64 	%p54, %rd375, 0;
	sub.s64 	%rd344, %rd46, %rd374;
	@%p54 bra 	$L__BB32_51;
	shr.u64 	%rd227, %rd344, 3;
	setp.gt.u64 	%p58, %rd227, %rd80;
	shl.b64 	%rd228, %rd80, 3;
	add.s64 	%rd229, %rd374, %rd228;
	add.s64 	%rd230, %rd229, 8;
	selp.b64 	%rd374, %rd230, %rd46, %p58;
	selp.b64 	%rd376, %rd229, 0, %p58;
	mov.u64 	%rd375, 0;
	mov.pred 	%p99, %p97;
	bra.uni 	$L__BB32_52;
$L__BB32_51:
	add.s64 	%rd219, %rd80, %rd375;
	shr.u64 	%rd221, %rd344, 3;
	setp.gt.u64 	%p56, %rd221, %rd219;
	shl.b64 	%rd222, %rd219, 3;
	add.s64 	%rd223, %rd374, %rd222;
	add.s64 	%rd224, %rd223, 8;
	selp.b64 	%rd374, %rd224, %rd46, %p56;
	selp.b64 	%rd376, %rd223, 0, %p56;
	mov.u64 	%rd375, 0;
	mov.pred 	%p99, %p97;
$L__BB32_52:
	add.s64 	%rd89, %rd372, %rd8;
	shl.b64 	%rd90, %rd89, 1;
	setp.lt.u64 	%p59, %rd90, 512;
	@%p59 bra 	$L__BB32_54;
	bra.uni 	$L__BB32_53;
$L__BB32_54:
	setp.lt.u64 	%p5, %rd89, %rd372;
	setp.gt.u64 	%p60, %rd89, 255;
	add.s64 	%rd235, %rd89, 1;
	selp.b64 	%rd236, 256, %rd235, %p60;
	selp.b64 	%rd372, 256, %rd236, %p5;
	cvt.u32.u64 	%r252, %rd90;
	shl.b32 	%r253, %r252, 3;
	add.s32 	%r250, %r253, %r5;
	add.s32 	%r251, %r250, 4;
	// begin inline asm
	ld.shared.f32 %r248, [%r250];
ld.shared.f32 %r249, [%r251];
	// end inline asm
	st.u32 	[%rd376], %r248;
	st.u32 	[%rd376+4], %r249;
	add.s64 	%rd368, %rd368, -1;
	setp.ne.s64 	%p61, %rd368, 0;
	mov.u64 	%rd369, %rd373;
	@%p61 bra 	$L__BB32_48;
$L__BB32_55:
	mov.u64 	%rd403, 0;
	mov.pred 	%p100, -1;
	mov.u64 	%rd404, %rd95;
	mov.pred 	%p103, %p100;
$L__BB32_56:
	setp.ne.s64 	%p63, %rd93, 0;
	@%p63 bra 	$L__BB32_60;
	bra.uni 	$L__BB32_57;
$L__BB32_60:
	@%p100 bra 	$L__BB32_63;
	bra.uni 	$L__BB32_61;
$L__BB32_63:
	setp.ne.s64 	%p64, %rd95, 0;
	@%p64 bra 	$L__BB32_65;
	bra.uni 	$L__BB32_64;
$L__BB32_65:
	sub.s64 	%rd238, %rd46, %rd396;
	shr.u64 	%rd239, %rd238, 3;
	setp.gt.u64 	%p65, %rd239, %rd95;
	shl.b64 	%rd240, %rd95, 3;
	add.s64 	%rd241, %rd396, %rd240;
	add.s64 	%rd242, %rd241, 8;
	selp.b64 	%rd396, %rd242, %rd46, %p65;
	bra.uni 	$L__BB32_66;
$L__BB32_57:
	selp.b64 	%rd98, 0, %rd8, %p100;
	setp.ne.s64 	%p86, %rd95, 0;
	@%p86 bra 	$L__BB32_59;
	bra.uni 	$L__BB32_58;
$L__BB32_59:
	add.s64 	%rd317, %rd98, %rd95;
	sub.s64 	%rd318, %rd46, %rd396;
	shr.u64 	%rd319, %rd318, 3;
	setp.gt.u64 	%p87, %rd319, %rd317;
	shl.b64 	%rd320, %rd317, 3;
	add.s64 	%rd321, %rd396, %rd320;
	add.s64 	%rd322, %rd321, 8;
	selp.b64 	%rd401, %rd322, %rd46, %p87;
	selp.b64 	%rd402, %rd321, 0, %p87;
	bra.uni 	$L__BB32_88;
$L__BB32_58:
	sub.s64 	%rd323, %rd46, %rd396;
	shr.u64 	%rd324, %rd323, 3;
	setp.gt.u64 	%p88, %rd324, %rd98;
	shl.b64 	%rd325, %rd98, 3;
	add.s64 	%rd326, %rd396, %rd325;
	add.s64 	%rd327, %rd326, 8;
	selp.b64 	%rd401, %rd327, %rd46, %p88;
	selp.b64 	%rd402, %rd326, 0, %p88;
	bra.uni 	$L__BB32_88;
$L__BB32_64:
	setp.eq.s64 	%p66, %rd396, %rd46;
	selp.b64 	%rd243, 0, 8, %p66;
	add.s64 	%rd396, %rd396, %rd243;
$L__BB32_66:
	add.s64 	%rd93, %rd93, -1;
	mov.u64 	%rd95, 0;
$L__BB32_61:
	setp.eq.s64 	%p67, %rd93, -1;
	@%p67 bra 	$L__BB32_67;
	add.s64 	%rd388, %rd93, 1;
	bra.uni 	$L__BB32_70;
$L__BB32_67:
	setp.ne.s64 	%p68, %rd95, 0;
	sub.s64 	%rd345, %rd46, %rd396;
	@%p68 bra 	$L__BB32_69;
	bra.uni 	$L__BB32_68;
$L__BB32_69:
	add.s64 	%rd247, %rd95, %rd8;
	shr.u64 	%rd249, %rd345, 3;
	setp.gt.u64 	%p69, %rd249, %rd247;
	add.s64 	%rd250, %rd95, %rd3;
	shl.b64 	%rd251, %rd250, 3;
	add.s64 	%rd252, %rd396, %rd251;
	selp.b64 	%rd396, %rd252, %rd46, %p69;
	mov.u64 	%rd388, -1;
	mov.u64 	%rd95, 0;
	bra.uni 	$L__BB32_70;
$L__BB32_68:
	shr.u64 	%rd256, %rd345, 3;
	setp.gt.u64 	%p70, %rd256, %rd8;
	add.s64 	%rd258, %rd396, %rd346;
	selp.b64 	%rd396, %rd258, %rd46, %p70;
	mov.u64 	%rd388, -1;
	mov.u64 	%rd95, 0;
$L__BB32_70:
	mul.lo.s64 	%rd398, %rd388, %rd3;
	mul.hi.u64 	%rd259, %rd388, %rd3;
	setp.eq.s64 	%p71, %rd259, 0;
	@%p71 bra 	$L__BB32_82;
	mov.u64 	%rd260, -1;
	div.u64 	%rd261, %rd260, %rd388;
	div.u64 	%rd262, %rd260, %rd3;
	mul.lo.s64 	%rd263, %rd261, %rd388;
	mul.lo.s64 	%rd264, %rd262, %rd3;
	max.u64 	%rd117, %rd263, %rd264;
	add.s64 	%rd390, %rd117, -1;
	setp.eq.s64 	%p73, %rd95, 0;
	@%p73 bra 	$L__BB32_76;
	add.s64 	%rd267, %rd95, %rd390;
	setp.lt.u64 	%p74, %rd267, %rd95;
	@%p74 bra 	$L__BB32_74;
	bra.uni 	$L__BB32_73;
$L__BB32_74:
	add.s64 	%rd268, %rd95, -1;
	sub.s64 	%rd269, %rd46, %rd396;
	shr.u64 	%rd270, %rd269, 3;
	setp.le.u64 	%p75, %rd270, %rd268;
	shl.b64 	%rd271, %rd95, 3;
	add.s64 	%rd389, %rd396, %rd271;
	mov.u64 	%rd396, %rd46;
	@%p75 bra 	$L__BB32_78;
	bra.uni 	$L__BB32_75;
$L__BB32_76:
	sub.s64 	%rd275, %rd46, %rd396;
	shr.u64 	%rd276, %rd275, 3;
	setp.gt.u64 	%p102, %rd276, %rd390;
	shl.b64 	%rd277, %rd117, 3;
	add.s64 	%rd278, %rd396, %rd277;
	add.s64 	%rd391, %rd278, -8;
	bra.uni 	$L__BB32_77;
$L__BB32_73:
	add.s64 	%rd390, %rd390, %rd95;
	mov.u64 	%rd389, %rd396;
$L__BB32_75:
	sub.s64 	%rd272, %rd46, %rd389;
	shr.u64 	%rd273, %rd272, 3;
	setp.gt.u64 	%p102, %rd273, %rd390;
	shl.b64 	%rd274, %rd390, 3;
	add.s64 	%rd391, %rd389, %rd274;
$L__BB32_77:
	add.s64 	%rd279, %rd391, 8;
	selp.b64 	%rd396, %rd279, %rd46, %p102;
$L__BB32_78:
	setp.gt.u64 	%p72, %rd263, %rd264;
	selp.b64 	%rd265, %rd261, 0, %p72;
	sub.s64 	%rd394, %rd3, %rd265;
	selp.b64 	%rd266, 0, %rd262, %p72;
	sub.s64 	%rd393, %rd388, %rd266;
	mul.lo.s64 	%rd398, %rd393, %rd394;
	mul.hi.u64 	%rd281, %rd393, %rd394;
	mov.u64 	%rd95, 0;
	setp.eq.s64 	%p76, %rd281, 0;
	@%p76 bra 	$L__BB32_82;
$L__BB32_79:
	setp.eq.s64 	%p77, %rd393, 0;
	@%p77 bra 	$L__BB32_95;
	setp.eq.s64 	%p78, %rd394, 0;
	@%p78 bra 	$L__BB32_96;
	div.u64 	%rd284, %rd260, %rd393;
	div.u64 	%rd285, %rd260, %rd394;
	mul.lo.s64 	%rd286, %rd284, %rd393;
	mul.lo.s64 	%rd287, %rd285, %rd394;
	setp.gt.u64 	%p79, %rd286, %rd287;
	selp.b64 	%rd288, %rd284, 0, %p79;
	sub.s64 	%rd394, %rd394, %rd288;
	selp.b64 	%rd289, 0, %rd285, %p79;
	sub.s64 	%rd393, %rd393, %rd289;
	max.u64 	%rd290, %rd286, %rd287;
	add.s64 	%rd291, %rd290, -1;
	shl.b64 	%rd292, %rd290, 3;
	add.s64 	%rd293, %rd396, %rd292;
	sub.s64 	%rd294, %rd46, %rd396;
	shr.u64 	%rd295, %rd294, 3;
	setp.gt.u64 	%p80, %rd295, %rd291;
	selp.b64 	%rd396, %rd293, %rd46, %p80;
	mul.lo.s64 	%rd398, %rd393, %rd394;
	mul.hi.u64 	%rd296, %rd393, %rd394;
	setp.ne.s64 	%p13, %rd296, 0;
	@%p13 bra 	$L__BB32_79;
$L__BB32_82:
	add.s64 	%rd400, %rd398, -1;
	setp.ne.s64 	%p81, %rd95, 0;
	@%p81 bra 	$L__BB32_84;
	bra.uni 	$L__BB32_83;
$L__BB32_84:
	add.s64 	%rd297, %rd95, %rd400;
	setp.lt.u64 	%p82, %rd297, %rd95;
	@%p82 bra 	$L__BB32_86;
	bra.uni 	$L__BB32_85;
$L__BB32_86:
	add.s64 	%rd299, %rd95, -1;
	sub.s64 	%rd300, %rd46, %rd396;
	shr.u64 	%rd301, %rd300, 3;
	setp.le.u64 	%p83, %rd301, %rd299;
	shl.b64 	%rd302, %rd95, 3;
	add.s64 	%rd396, %rd396, %rd302;
	mov.u64 	%rd402, 0;
	mov.u64 	%rd401, %rd46;
	@%p83 bra 	$L__BB32_88;
	bra.uni 	$L__BB32_87;
$L__BB32_83:
	sub.s64 	%rd308, %rd46, %rd396;
	shr.u64 	%rd309, %rd308, 3;
	setp.gt.u64 	%p85, %rd309, %rd400;
	shl.b64 	%rd310, %rd398, 3;
	add.s64 	%rd311, %rd396, %rd310;
	add.s64 	%rd312, %rd311, -8;
	selp.b64 	%rd401, %rd311, %rd46, %p85;
	selp.b64 	%rd402, %rd312, 0, %p85;
	bra.uni 	$L__BB32_88;
$L__BB32_85:
	add.s64 	%rd400, %rd400, %rd95;
$L__BB32_87:
	sub.s64 	%rd303, %rd46, %rd396;
	shr.u64 	%rd304, %rd303, 3;
	setp.gt.u64 	%p84, %rd304, %rd400;
	shl.b64 	%rd305, %rd400, 3;
	add.s64 	%rd306, %rd396, %rd305;
	add.s64 	%rd307, %rd306, 8;
	selp.b64 	%rd401, %rd307, %rd46, %p84;
	selp.b64 	%rd402, %rd306, 0, %p84;
$L__BB32_88:
	setp.eq.s64 	%p89, %rd402, 0;
	mov.u64 	%rd406, 0;
	@%p89 bra 	$L__BB32_90;
	selp.b64 	%rd331, 0, %rd8, %p103;
	add.s64 	%rd333, %rd331, %rd404;
	add.s64 	%rd405, %rd333, %rd403;
	setp.lt.u64 	%p91, %rd405, %rd403;
	setp.gt.u64 	%p92, %rd405, 255;
	or.pred  	%p93, %p91, %p92;
	add.s64 	%rd334, %rd405, 1;
	selp.b64 	%rd403, 256, %rd334, %p93;
	selp.b64 	%rd406, 0, %rd402, %p93;
	mov.pred 	%p103, 0;
	mov.u64 	%rd404, 0;
$L__BB32_90:
	setp.eq.s64 	%p94, %rd406, 0;
	@%p94 bra 	$L__BB32_94;
	shl.b64 	%rd160, %rd405, 1;
	setp.lt.u64 	%p95, %rd160, 512;
	@%p95 bra 	$L__BB32_93;
	bra.uni 	$L__BB32_92;
$L__BB32_93:
	cvt.u32.u64 	%r258, %rd160;
	shl.b32 	%r259, %r258, 3;
	add.s32 	%r256, %r259, %r6;
	add.s32 	%r257, %r256, 4;
	// begin inline asm
	ld.shared.f32 %r254, [%r256];
ld.shared.f32 %r255, [%r257];
	// end inline asm
	st.u32 	[%rd406], %r254;
	st.u32 	[%rd406+4], %r255;
	mov.pred 	%p100, 0;
	mov.u64 	%rd93, 0;
	mov.u64 	%rd396, %rd401;
	mov.u64 	%rd95, %rd93;
	bra.uni 	$L__BB32_56;
$L__BB32_94:
	ret;
$L__BB32_95:
	mov.u64 	%rd315, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd316, %rd315;
	{ // callseq 176, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd316;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 176
$L__BB32_96:
	mov.u64 	%rd313, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd314, %rd313;
	{ // callseq 175, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd314;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 175
$L__BB32_92:
	mov.u64 	%rd335, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd336, %rd335;
	mov.u64 	%rd337, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd338, %rd337;
	{ // callseq 177, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd336;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd338;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 177
$L__BB32_53:
	mov.u64 	%rd231, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd232, %rd231;
	mov.u64 	%rd233, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd234, %rd233;
	{ // callseq 174, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd232;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd234;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 174
$L__BB32_17:
	mov.u64 	%rd181, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd182, %rd181;
	mov.u64 	%rd183, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd184, %rd183;
	{ // callseq 172, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd182;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd184;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 172
$L__BB32_1:
	mov.u64 	%rd342, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_12;
	cvta.global.u64 	%rd343, %rd342;
	{ // callseq 179, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd343;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 179
$L__BB32_26:
	mov.u64 	%rd194, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_13;
	cvta.global.u64 	%rd195, %rd194;
	{ // callseq 173, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd195;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 173
$L__BB32_21:
	mov.u64 	%rd340, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_16;
	cvta.global.u64 	%rd341, %rd340;
	{ // callseq 178, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd341;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 178

}
	// .globl	daubechies_first_forward_1024_kernel
.visible .entry daubechies_first_forward_1024_kernel(
	.param .u64 daubechies_first_forward_1024_kernel_param_0
)
{
	.reg .pred 	%p<95>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<256>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<372>;

	ld.param.u64 	%rd142, [daubechies_first_forward_1024_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd142;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[8192];
    mov.u32 %r8, nonphysical;
	// end inline asm
	mov.u32 	%r11, %ctaid.x;
	ld.global.nc.u64 	%rd143, [%rd1+24];
	and.b64  	%rd144, %rd143, -1024;
	mul.wide.u32 	%rd5, %r11, 1024;
	setp.ge.u64 	%p16, %rd5, %rd144;
	sub.s64 	%rd145, %rd143, %rd5;
	setp.lt.u64 	%p17, %rd145, 1024;
	or.pred  	%p18, %p16, %p17;
	@!%p18 bra 	$L__BB33_2;
	bra.uni 	$L__BB33_1;
$L__BB33_2:
	mov.u32 	%r9, %tid.x;
	cvt.u64.u32 	%rd76, %r9;
	mov.u32 	%r10, %ntid.x;
	cvt.u64.u32 	%rd3, %r10;
	cvt.u32.u64 	%r16, %rd76;
	ld.global.nc.u64 	%rd6, [%rd1+16];
	shl.b64 	%rd147, %rd5, 3;
	add.s64 	%rd148, %rd6, %rd147;
	add.s64 	%rd7, %rd3, -1;
	cvt.u16.u64 	%rs2, %rd76;
	xor.b16  	%rs1, %rs2, 1023;
	cvt.u16.u64 	%rs3, %rd3;
	shl.b64 	%rd149, %rd76, 3;
	add.s64 	%rd150, %rd148, %rd149;
	shl.b32 	%r2, %r16, 3;
	add.s32 	%r12, %r8, %r2;
	add.s32 	%r13, %r12, 4;
	ld.u32 	%r14, [%rd150];
	ld.u32 	%r15, [%rd150+4];
	// begin inline asm
	st.shared.f32 [%r12], %r14;
st.shared.f32 [%r13], %r15;
	// end inline asm
	setp.lt.u16 	%p19, %rs1, %rs3;
	cvt.u32.u64 	%r253, %rd3;
	shl.b64 	%rd318, %rd3, 3;
	@%p19 bra 	$L__BB33_7;
	cvt.u64.u32 	%rd4, %r11;
	add.s64 	%rd322, %rd76, 1;
	cvt.u32.u16 	%r17, %rs1;
	div.u32 	%r19, %r17, %r253;
	cvt.u64.u32 	%rd321, %r19;
	shl.b64 	%rd151, %rd4, 13;
	add.s64 	%rd152, %rd151, %rd318;
	add.s64 	%rd154, %rd152, %rd149;
	add.s64 	%rd320, %rd6, %rd154;
	xor.b64  	%rd319, %rd149, 8184;
$L__BB33_4:
	add.s64 	%rd18, %rd322, %rd7;
	setp.lt.u64 	%p21, %rd18, 1024;
	@%p21 bra 	$L__BB33_6;
	bra.uni 	$L__BB33_5;
$L__BB33_6:
	shr.u64 	%rd155, %rd319, 3;
	setp.gt.u64 	%p20, %rd155, %rd7;
	selp.b64 	%rd17, %rd320, 0, %p20;
	setp.lt.u64 	%p1, %rd18, %rd322;
	add.s64 	%rd160, %rd322, %rd3;
	selp.b64 	%rd322, 1024, %rd160, %p1;
	cvt.u32.u64 	%r24, %rd18;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r20, %r25, %r8;
	add.s32 	%r21, %r20, 4;
	ld.u32 	%r22, [%rd17];
	ld.u32 	%r23, [%rd17+4];
	// begin inline asm
	st.shared.f32 [%r20], %r22;
st.shared.f32 [%r21], %r23;
	// end inline asm
	add.s64 	%rd321, %rd321, -1;
	add.s64 	%rd320, %rd320, %rd318;
	sub.s64 	%rd319, %rd319, %rd318;
	setp.ne.s64 	%p22, %rd321, 0;
	@%p22 bra 	$L__BB33_4;
$L__BB33_7:
	ld.global.nc.u64 	%rd23, [%rd1];
	ld.global.nc.u64 	%rd24, [%rd1+8];
	bar.sync 	0;
	setp.lt.u32 	%p23, %r16, 512;
	@%p23 bra 	$L__BB33_8;
	bra.uni 	$L__BB33_13;
$L__BB33_8:
	add.s32 	%r29, %r12, %r2;
	add.s32 	%r30, %r29, 4;
	// begin inline asm
	ld.shared.f32 %r27, [%r29];
ld.shared.f32 %r28, [%r30];
	// end inline asm
	setp.ne.s64 	%p24, %rd24, 0;
	@%p24 bra 	$L__BB33_12;
	bra.uni 	$L__BB33_9;
$L__BB33_12:
	shl.b32 	%r4, %r16, 4;
	mov.b32 	%f2, %r28;
	mov.b32 	%f1, %r27;
	ld.u32 	%r34, [%rd23];
	ld.u32 	%r37, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r32, %r27, %r34;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r35, %r28, %r37;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r73, %r32, %r35;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r41, %r27, %r37;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r44, %r28, %r34;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r76, %r41, %r44;
	// end inline asm
	add.s32 	%r92, %r8, %r4;
	add.s32 	%r88, %r92, 8;
	add.s32 	%r89, %r92, 12;
	// begin inline asm
	ld.shared.f32 %r55, [%r88];
ld.shared.f32 %r58, [%r89];
	// end inline asm
	ld.u32 	%r68, [%rd23];
	ld.u32 	%r65, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r54, %r55, %r68;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r57, %r58, %r65;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r74, %r54, %r57;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r63, %r55, %r65;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r66, %r58, %r68;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r77, %r63, %r66;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r72, %r73, %r74;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r75, %r76, %r77;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r29], %r72;
st.shared.f32 [%r30], %r75;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r82, %r73, %r74;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r85, %r76, %r77;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r88], %r82;
st.shared.f32 [%r89], %r85;
	// end inline asm
	add.s64 	%rd28, %rd76, %rd3;
	setp.lt.u64 	%p25, %rd28, 512;
	@%p25 bra 	$L__BB33_10;
	bra.uni 	$L__BB33_13;
$L__BB33_10:
	cvt.u32.u64 	%r157, %rd28;
	shl.b32 	%r158, %r157, 4;
	add.s32 	%r95, %r8, %r158;
	add.s32 	%r96, %r95, 4;
	// begin inline asm
	ld.shared.f32 %r98, [%r95];
ld.shared.f32 %r101, [%r96];
	// end inline asm
	ld.u32 	%r111, [%rd23];
	ld.u32 	%r108, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r97, %r98, %r111;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r100, %r101, %r108;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r138, %r97, %r100;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r106, %r98, %r108;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r109, %r101, %r111;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r141, %r106, %r109;
	// end inline asm
	add.s32 	%r153, %r95, 8;
	add.s32 	%r154, %r95, 12;
	// begin inline asm
	ld.shared.f32 %r120, [%r153];
ld.shared.f32 %r123, [%r154];
	// end inline asm
	ld.u32 	%r133, [%rd23];
	ld.u32 	%r130, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r119, %r120, %r133;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r122, %r123, %r130;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r139, %r119, %r122;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r128, %r120, %r130;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r131, %r123, %r133;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r142, %r128, %r131;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r137, %r138, %r139;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r140, %r141, %r142;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r95], %r137;
st.shared.f32 [%r96], %r140;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r147, %r138, %r139;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r150, %r141, %r142;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r153], %r147;
st.shared.f32 [%r154], %r150;
	// end inline asm
	add.s64 	%rd323, %rd28, %rd3;
	setp.gt.u64 	%p26, %rd323, 511;
	@%p26 bra 	$L__BB33_13;
$L__BB33_11:
	add.s64 	%rd161, %rd323, 1;
	cvt.u32.u64 	%r223, %rd323;
	shl.b32 	%r224, %r223, 4;
	add.s32 	%r161, %r224, %r8;
	add.s32 	%r162, %r161, 4;
	// begin inline asm
	ld.shared.f32 %r164, [%r161];
ld.shared.f32 %r167, [%r162];
	// end inline asm
	ld.u32 	%r177, [%rd23];
	ld.u32 	%r174, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r163, %r164, %r177;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r166, %r167, %r174;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r204, %r163, %r166;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r172, %r164, %r174;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r175, %r167, %r177;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r207, %r172, %r175;
	// end inline asm
	add.s32 	%r219, %r161, 8;
	add.s32 	%r220, %r161, 12;
	// begin inline asm
	ld.shared.f32 %r186, [%r219];
ld.shared.f32 %r189, [%r220];
	// end inline asm
	ld.u32 	%r199, [%rd23];
	ld.u32 	%r196, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r185, %r186, %r199;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r188, %r189, %r196;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r205, %r185, %r188;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r194, %r186, %r196;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r197, %r189, %r199;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r208, %r194, %r197;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r203, %r204, %r205;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r206, %r207, %r208;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r161], %r203;
st.shared.f32 [%r162], %r206;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r213, %r204, %r205;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r216, %r207, %r208;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r219], %r213;
st.shared.f32 [%r220], %r216;
	// end inline asm
	add.s64 	%rd162, %rd161, %rd7;
	setp.lt.u64 	%p27, %rd162, %rd161;
	add.s64 	%rd323, %rd323, %rd3;
	setp.gt.u64 	%p28, %rd323, 511;
	or.pred  	%p29, %p27, %p28;
	@%p29 bra 	$L__BB33_13;
	bra.uni 	$L__BB33_11;
$L__BB33_13:
	ld.global.nc.u64 	%rd164, [%rd1+32];
	ld.global.nc.u64 	%rd165, [%rd1+40];
	and.b64  	%rd166, %rd165, -1024;
	setp.lt.u64 	%p30, %rd5, %rd166;
	sub.s64 	%rd168, %rd165, %rd5;
	setp.gt.u64 	%p31, %rd168, 1023;
	and.pred  	%p32, %p30, %p31;
	@%p32 bra 	$L__BB33_15;
	bra.uni 	$L__BB33_14;
$L__BB33_15:
	setp.gt.u32 	%p33, %r16, 511;
	bar.sync 	0;
	mov.u64 	%rd328, 0;
	mov.u64 	%rd74, %rd328;
	@%p33 bra 	$L__BB33_19;
	add.s64 	%rd324, %rd76, %rd3;
	setp.gt.u64 	%p34, %rd324, 511;
	mov.u64 	%rd74, 1;
	@%p34 bra 	$L__BB33_19;
	mov.u64 	%rd74, 1;
$L__BB33_18:
	add.s64 	%rd174, %rd324, 1;
	add.s64 	%rd74, %rd74, 1;
	add.s64 	%rd175, %rd174, %rd7;
	setp.lt.u64 	%p35, %rd175, %rd174;
	add.s64 	%rd324, %rd324, %rd3;
	setp.gt.u64 	%p36, %rd324, 511;
	or.pred  	%p37, %p35, %p36;
	@!%p37 bra 	$L__BB33_18;
	bra.uni 	$L__BB33_19;
$L__BB33_19:
	setp.eq.s64 	%p90, %rd74, 0;
	mov.u64 	%rd327, %rd328;
	@%p90 bra 	$L__BB33_21;
	xor.b64  	%rd38, %rd76, 1023;
	cvt.u32.u64 	%r226, %rd38;
	div.u32 	%r228, %r226, %r253;
	cvt.u64.u32 	%rd178, %r228;
	add.s64 	%rd179, %rd178, 1;
	min.u64 	%rd327, %rd179, %rd74;
$L__BB33_21:
	add.s64 	%rd29, %rd164, %rd147;
	@%p33 bra 	$L__BB33_23;
	mov.u64 	%rd181, 511;
	sub.s64 	%rd41, %rd181, %rd76;
	cvt.u32.u64 	%r230, %rd41;
	div.u32 	%r232, %r230, %r253;
	cvt.u64.u32 	%rd182, %r232;
	add.s64 	%rd328, %rd182, 1;
$L__BB33_23:
	selp.b64 	%rd361, %rd29, 0, %p32;
	add.s64 	%rd31, %rd29, 8192;
	min.u64 	%rd44, %rd327, %rd328;
	setp.eq.s64 	%p39, %rd44, 0;
	@%p39 bra 	$L__BB33_38;
	setp.eq.s64 	%p40, %rd74, 0;
	mov.u64 	%rd334, 0;
	mov.u64 	%rd339, %rd361;
	mov.u64 	%rd340, %rd76;
	mov.u64 	%rd332, %rd334;
	@%p40 bra 	$L__BB33_28;
	add.s64 	%rd334, %rd74, -1;
	setp.eq.s32 	%p41, %r16, 0;
	@%p41 bra 	$L__BB33_27;
	add.s64 	%rd332, %rd361, %rd149;
	add.s64 	%rd339, %rd332, 8;
	mov.u64 	%rd340, 0;
	bra.uni 	$L__BB33_28;
$L__BB33_27:
	add.s64 	%rd339, %rd361, 8;
	mov.u64 	%rd340, 0;
	mov.u64 	%rd332, %rd361;
$L__BB33_28:
	@%p33 bra 	$L__BB33_36;
	add.s32 	%r7, %r8, 8;
	shl.b32 	%r240, %r16, 4;
	add.s32 	%r237, %r7, %r240;
	add.s32 	%r238, %r237, 4;
	// begin inline asm
	ld.shared.f32 %r235, [%r237];
ld.shared.f32 %r236, [%r238];
	// end inline asm
	st.u32 	[%rd332], %r235;
	st.u32 	[%rd332+4], %r236;
	setp.eq.s64 	%p43, %rd44, 1;
	@%p43 bra 	$L__BB33_38;
	mov.pred 	%p88, 0;
	add.s64 	%rd337, %rd76, 1;
	add.s64 	%rd333, %rd44, -1;
	mov.u64 	%rd189, 0;
$L__BB33_31:
	setp.eq.s64 	%p44, %rd334, 0;
	mov.u64 	%rd338, %rd189;
	mov.u64 	%rd341, %rd189;
	@%p44 bra 	$L__BB33_35;
	add.s64 	%rd338, %rd334, -1;
	selp.b64 	%rd61, 0, %rd7, %p90;
	setp.ne.s64 	%p45, %rd340, 0;
	sub.s64 	%rd316, %rd31, %rd339;
	@%p45 bra 	$L__BB33_34;
	shr.u64 	%rd199, %rd316, 3;
	setp.gt.u64 	%p49, %rd199, %rd61;
	shl.b64 	%rd200, %rd61, 3;
	add.s64 	%rd201, %rd339, %rd200;
	add.s64 	%rd202, %rd201, 8;
	selp.b64 	%rd339, %rd202, %rd31, %p49;
	selp.b64 	%rd341, %rd201, 0, %p49;
	mov.u64 	%rd340, 0;
	mov.pred 	%p90, %p88;
	bra.uni 	$L__BB33_35;
$L__BB33_34:
	add.s64 	%rd191, %rd61, %rd340;
	shr.u64 	%rd193, %rd316, 3;
	setp.gt.u64 	%p47, %rd193, %rd191;
	shl.b64 	%rd194, %rd191, 3;
	add.s64 	%rd195, %rd339, %rd194;
	add.s64 	%rd196, %rd195, 8;
	selp.b64 	%rd339, %rd196, %rd31, %p47;
	selp.b64 	%rd341, %rd195, 0, %p47;
	mov.u64 	%rd340, 0;
	mov.pred 	%p90, %p88;
$L__BB33_35:
	add.s64 	%rd70, %rd337, %rd7;
	shl.b64 	%rd71, %rd70, 1;
	setp.lt.u64 	%p50, %rd71, 1024;
	@%p50 bra 	$L__BB33_37;
	bra.uni 	$L__BB33_36;
$L__BB33_37:
	setp.lt.u64 	%p5, %rd70, %rd337;
	setp.gt.u64 	%p51, %rd70, 511;
	add.s64 	%rd207, %rd70, 1;
	selp.b64 	%rd208, 512, %rd207, %p51;
	selp.b64 	%rd337, 512, %rd208, %p5;
	cvt.u32.u64 	%r245, %rd71;
	shl.b32 	%r246, %r245, 3;
	add.s32 	%r243, %r246, %r7;
	add.s32 	%r244, %r243, 4;
	// begin inline asm
	ld.shared.f32 %r241, [%r243];
ld.shared.f32 %r242, [%r244];
	// end inline asm
	st.u32 	[%rd341], %r241;
	st.u32 	[%rd341+4], %r242;
	add.s64 	%rd333, %rd333, -1;
	setp.ne.s64 	%p52, %rd333, 0;
	mov.u64 	%rd334, %rd338;
	@%p52 bra 	$L__BB33_31;
$L__BB33_38:
	mov.u64 	%rd368, 0;
	mov.pred 	%p91, -1;
	mov.u64 	%rd369, %rd76;
	mov.pred 	%p94, %p91;
$L__BB33_39:
	setp.ne.s64 	%p54, %rd74, 0;
	@%p54 bra 	$L__BB33_43;
	bra.uni 	$L__BB33_40;
$L__BB33_43:
	@%p91 bra 	$L__BB33_46;
	bra.uni 	$L__BB33_44;
$L__BB33_46:
	setp.ne.s64 	%p55, %rd76, 0;
	@%p55 bra 	$L__BB33_48;
	bra.uni 	$L__BB33_47;
$L__BB33_48:
	sub.s64 	%rd210, %rd31, %rd361;
	shr.u64 	%rd211, %rd210, 3;
	setp.gt.u64 	%p56, %rd211, %rd76;
	shl.b64 	%rd212, %rd76, 3;
	add.s64 	%rd213, %rd361, %rd212;
	add.s64 	%rd214, %rd213, 8;
	selp.b64 	%rd361, %rd214, %rd31, %p56;
	bra.uni 	$L__BB33_49;
$L__BB33_40:
	selp.b64 	%rd79, 0, %rd7, %p91;
	setp.ne.s64 	%p77, %rd76, 0;
	@%p77 bra 	$L__BB33_42;
	bra.uni 	$L__BB33_41;
$L__BB33_42:
	add.s64 	%rd289, %rd79, %rd76;
	sub.s64 	%rd290, %rd31, %rd361;
	shr.u64 	%rd291, %rd290, 3;
	setp.gt.u64 	%p78, %rd291, %rd289;
	shl.b64 	%rd292, %rd289, 3;
	add.s64 	%rd293, %rd361, %rd292;
	add.s64 	%rd294, %rd293, 8;
	selp.b64 	%rd366, %rd294, %rd31, %p78;
	selp.b64 	%rd367, %rd293, 0, %p78;
	bra.uni 	$L__BB33_71;
$L__BB33_41:
	sub.s64 	%rd295, %rd31, %rd361;
	shr.u64 	%rd296, %rd295, 3;
	setp.gt.u64 	%p79, %rd296, %rd79;
	shl.b64 	%rd297, %rd79, 3;
	add.s64 	%rd298, %rd361, %rd297;
	add.s64 	%rd299, %rd298, 8;
	selp.b64 	%rd366, %rd299, %rd31, %p79;
	selp.b64 	%rd367, %rd298, 0, %p79;
	bra.uni 	$L__BB33_71;
$L__BB33_47:
	setp.eq.s64 	%p57, %rd361, %rd31;
	selp.b64 	%rd215, 0, 8, %p57;
	add.s64 	%rd361, %rd361, %rd215;
$L__BB33_49:
	add.s64 	%rd74, %rd74, -1;
	mov.u64 	%rd76, 0;
$L__BB33_44:
	setp.eq.s64 	%p58, %rd74, -1;
	@%p58 bra 	$L__BB33_50;
	add.s64 	%rd353, %rd74, 1;
	bra.uni 	$L__BB33_53;
$L__BB33_50:
	setp.ne.s64 	%p59, %rd76, 0;
	sub.s64 	%rd317, %rd31, %rd361;
	@%p59 bra 	$L__BB33_52;
	bra.uni 	$L__BB33_51;
$L__BB33_52:
	add.s64 	%rd219, %rd76, %rd7;
	shr.u64 	%rd221, %rd317, 3;
	setp.gt.u64 	%p60, %rd221, %rd219;
	add.s64 	%rd222, %rd76, %rd3;
	shl.b64 	%rd223, %rd222, 3;
	add.s64 	%rd224, %rd361, %rd223;
	selp.b64 	%rd361, %rd224, %rd31, %p60;
	mov.u64 	%rd353, -1;
	mov.u64 	%rd76, 0;
	bra.uni 	$L__BB33_53;
$L__BB33_51:
	shr.u64 	%rd228, %rd317, 3;
	setp.gt.u64 	%p61, %rd228, %rd7;
	add.s64 	%rd230, %rd361, %rd318;
	selp.b64 	%rd361, %rd230, %rd31, %p61;
	mov.u64 	%rd353, -1;
	mov.u64 	%rd76, 0;
$L__BB33_53:
	mul.lo.s64 	%rd363, %rd353, %rd3;
	mul.hi.u64 	%rd231, %rd353, %rd3;
	setp.eq.s64 	%p62, %rd231, 0;
	@%p62 bra 	$L__BB33_65;
	mov.u64 	%rd232, -1;
	div.u64 	%rd233, %rd232, %rd353;
	div.u64 	%rd234, %rd232, %rd3;
	mul.lo.s64 	%rd235, %rd233, %rd353;
	mul.lo.s64 	%rd236, %rd234, %rd3;
	max.u64 	%rd98, %rd235, %rd236;
	add.s64 	%rd355, %rd98, -1;
	setp.eq.s64 	%p64, %rd76, 0;
	@%p64 bra 	$L__BB33_59;
	add.s64 	%rd239, %rd76, %rd355;
	setp.lt.u64 	%p65, %rd239, %rd76;
	@%p65 bra 	$L__BB33_57;
	bra.uni 	$L__BB33_56;
$L__BB33_57:
	add.s64 	%rd240, %rd76, -1;
	sub.s64 	%rd241, %rd31, %rd361;
	shr.u64 	%rd242, %rd241, 3;
	setp.le.u64 	%p66, %rd242, %rd240;
	shl.b64 	%rd243, %rd76, 3;
	add.s64 	%rd354, %rd361, %rd243;
	mov.u64 	%rd361, %rd31;
	@%p66 bra 	$L__BB33_61;
	bra.uni 	$L__BB33_58;
$L__BB33_59:
	sub.s64 	%rd247, %rd31, %rd361;
	shr.u64 	%rd248, %rd247, 3;
	setp.gt.u64 	%p93, %rd248, %rd355;
	shl.b64 	%rd249, %rd98, 3;
	add.s64 	%rd250, %rd361, %rd249;
	add.s64 	%rd356, %rd250, -8;
	bra.uni 	$L__BB33_60;
$L__BB33_56:
	add.s64 	%rd355, %rd355, %rd76;
	mov.u64 	%rd354, %rd361;
$L__BB33_58:
	sub.s64 	%rd244, %rd31, %rd354;
	shr.u64 	%rd245, %rd244, 3;
	setp.gt.u64 	%p93, %rd245, %rd355;
	shl.b64 	%rd246, %rd355, 3;
	add.s64 	%rd356, %rd354, %rd246;
$L__BB33_60:
	add.s64 	%rd251, %rd356, 8;
	selp.b64 	%rd361, %rd251, %rd31, %p93;
$L__BB33_61:
	setp.gt.u64 	%p63, %rd235, %rd236;
	selp.b64 	%rd237, %rd233, 0, %p63;
	sub.s64 	%rd359, %rd3, %rd237;
	selp.b64 	%rd238, 0, %rd234, %p63;
	sub.s64 	%rd358, %rd353, %rd238;
	mul.lo.s64 	%rd363, %rd358, %rd359;
	mul.hi.u64 	%rd253, %rd358, %rd359;
	mov.u64 	%rd76, 0;
	setp.eq.s64 	%p67, %rd253, 0;
	@%p67 bra 	$L__BB33_65;
$L__BB33_62:
	setp.eq.s64 	%p68, %rd358, 0;
	@%p68 bra 	$L__BB33_78;
	setp.eq.s64 	%p69, %rd359, 0;
	@%p69 bra 	$L__BB33_79;
	div.u64 	%rd256, %rd232, %rd358;
	div.u64 	%rd257, %rd232, %rd359;
	mul.lo.s64 	%rd258, %rd256, %rd358;
	mul.lo.s64 	%rd259, %rd257, %rd359;
	setp.gt.u64 	%p70, %rd258, %rd259;
	selp.b64 	%rd260, %rd256, 0, %p70;
	sub.s64 	%rd359, %rd359, %rd260;
	selp.b64 	%rd261, 0, %rd257, %p70;
	sub.s64 	%rd358, %rd358, %rd261;
	max.u64 	%rd262, %rd258, %rd259;
	add.s64 	%rd263, %rd262, -1;
	shl.b64 	%rd264, %rd262, 3;
	add.s64 	%rd265, %rd361, %rd264;
	sub.s64 	%rd266, %rd31, %rd361;
	shr.u64 	%rd267, %rd266, 3;
	setp.gt.u64 	%p71, %rd267, %rd263;
	selp.b64 	%rd361, %rd265, %rd31, %p71;
	mul.lo.s64 	%rd363, %rd358, %rd359;
	mul.hi.u64 	%rd268, %rd358, %rd359;
	setp.ne.s64 	%p13, %rd268, 0;
	@%p13 bra 	$L__BB33_62;
$L__BB33_65:
	add.s64 	%rd365, %rd363, -1;
	setp.ne.s64 	%p72, %rd76, 0;
	@%p72 bra 	$L__BB33_67;
	bra.uni 	$L__BB33_66;
$L__BB33_67:
	add.s64 	%rd269, %rd76, %rd365;
	setp.lt.u64 	%p73, %rd269, %rd76;
	@%p73 bra 	$L__BB33_69;
	bra.uni 	$L__BB33_68;
$L__BB33_69:
	add.s64 	%rd271, %rd76, -1;
	sub.s64 	%rd272, %rd31, %rd361;
	shr.u64 	%rd273, %rd272, 3;
	setp.le.u64 	%p74, %rd273, %rd271;
	shl.b64 	%rd274, %rd76, 3;
	add.s64 	%rd361, %rd361, %rd274;
	mov.u64 	%rd367, 0;
	mov.u64 	%rd366, %rd31;
	@%p74 bra 	$L__BB33_71;
	bra.uni 	$L__BB33_70;
$L__BB33_66:
	sub.s64 	%rd280, %rd31, %rd361;
	shr.u64 	%rd281, %rd280, 3;
	setp.gt.u64 	%p76, %rd281, %rd365;
	shl.b64 	%rd282, %rd363, 3;
	add.s64 	%rd283, %rd361, %rd282;
	add.s64 	%rd284, %rd283, -8;
	selp.b64 	%rd366, %rd283, %rd31, %p76;
	selp.b64 	%rd367, %rd284, 0, %p76;
	bra.uni 	$L__BB33_71;
$L__BB33_68:
	add.s64 	%rd365, %rd365, %rd76;
$L__BB33_70:
	sub.s64 	%rd275, %rd31, %rd361;
	shr.u64 	%rd276, %rd275, 3;
	setp.gt.u64 	%p75, %rd276, %rd365;
	shl.b64 	%rd277, %rd365, 3;
	add.s64 	%rd278, %rd361, %rd277;
	add.s64 	%rd279, %rd278, 8;
	selp.b64 	%rd366, %rd279, %rd31, %p75;
	selp.b64 	%rd367, %rd278, 0, %p75;
$L__BB33_71:
	setp.eq.s64 	%p80, %rd367, 0;
	mov.u64 	%rd371, 0;
	@%p80 bra 	$L__BB33_73;
	selp.b64 	%rd303, 0, %rd7, %p94;
	add.s64 	%rd305, %rd303, %rd369;
	add.s64 	%rd370, %rd305, %rd368;
	setp.lt.u64 	%p82, %rd370, %rd368;
	setp.gt.u64 	%p83, %rd370, 511;
	or.pred  	%p84, %p82, %p83;
	add.s64 	%rd306, %rd370, 1;
	selp.b64 	%rd368, 512, %rd306, %p84;
	selp.b64 	%rd371, 0, %rd367, %p84;
	mov.pred 	%p94, 0;
	mov.u64 	%rd369, 0;
$L__BB33_73:
	setp.eq.s64 	%p85, %rd371, 0;
	@%p85 bra 	$L__BB33_77;
	shl.b64 	%rd141, %rd370, 1;
	setp.lt.u64 	%p86, %rd141, 1024;
	@%p86 bra 	$L__BB33_76;
	bra.uni 	$L__BB33_75;
$L__BB33_76:
	cvt.u32.u64 	%r251, %rd141;
	shl.b32 	%r252, %r251, 3;
	add.s32 	%r249, %r252, %r8;
	add.s32 	%r250, %r249, 4;
	// begin inline asm
	ld.shared.f32 %r247, [%r249];
ld.shared.f32 %r248, [%r250];
	// end inline asm
	st.u32 	[%rd371], %r247;
	st.u32 	[%rd371+4], %r248;
	mov.pred 	%p91, 0;
	mov.u64 	%rd74, 0;
	mov.u64 	%rd361, %rd366;
	mov.u64 	%rd76, %rd74;
	bra.uni 	$L__BB33_39;
$L__BB33_77:
	ret;
$L__BB33_78:
	mov.u64 	%rd287, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd288, %rd287;
	{ // callseq 184, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd288;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 184
$L__BB33_79:
	mov.u64 	%rd285, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd286, %rd285;
	{ // callseq 183, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd286;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 183
$L__BB33_75:
	mov.u64 	%rd307, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd308, %rd307;
	mov.u64 	%rd309, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd310, %rd309;
	{ // callseq 185, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd308;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd310;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 185
$L__BB33_5:
	mov.u64 	%rd156, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd157, %rd156;
	mov.u64 	%rd158, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd159, %rd158;
	{ // callseq 180, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd157;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd159;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 180
$L__BB33_36:
	mov.u64 	%rd203, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd204, %rd203;
	mov.u64 	%rd205, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd206, %rd205;
	{ // callseq 182, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd204;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd206;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 182
$L__BB33_1:
	mov.u64 	%rd314, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_12;
	cvta.global.u64 	%rd315, %rd314;
	{ // callseq 187, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd315;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 187
$L__BB33_14:
	mov.u64 	%rd169, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_13;
	cvta.global.u64 	%rd170, %rd169;
	{ // callseq 181, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd170;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 181
$L__BB33_9:
	mov.u64 	%rd312, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_16;
	cvta.global.u64 	%rd313, %rd312;
	{ // callseq 186, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd313;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 186

}
	// .globl	daubechies_first_forward_2048_kernel
.visible .entry daubechies_first_forward_2048_kernel(
	.param .u64 daubechies_first_forward_2048_kernel_param_0
)
{
	.reg .pred 	%p<90>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<250>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<361>;

	ld.param.u64 	%rd135, [daubechies_first_forward_2048_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd135;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[16384];
    mov.u32 %r8, nonphysical;
	// end inline asm
	mov.u32 	%r11, %ctaid.x;
	ld.global.nc.u64 	%rd136, [%rd1+24];
	and.b64  	%rd137, %rd136, -2048;
	mul.wide.u32 	%rd5, %r11, 2048;
	setp.ge.u64 	%p16, %rd5, %rd137;
	sub.s64 	%rd138, %rd136, %rd5;
	setp.lt.u64 	%p17, %rd138, 2048;
	or.pred  	%p18, %p16, %p17;
	@!%p18 bra 	$L__BB34_2;
	bra.uni 	$L__BB34_1;
$L__BB34_2:
	mov.u32 	%r9, %tid.x;
	cvt.u64.u32 	%rd69, %r9;
	mov.u32 	%r10, %ntid.x;
	cvt.u64.u32 	%rd3, %r10;
	cvt.u64.u32 	%rd4, %r11;
	cvt.u32.u64 	%r16, %rd69;
	ld.global.nc.u64 	%rd140, [%rd1+16];
	shl.b64 	%rd141, %rd5, 3;
	add.s64 	%rd142, %rd140, %rd141;
	add.s64 	%rd6, %rd3, -1;
	cvt.u16.u64 	%rs1, %rd69;
	xor.b16  	%rs2, %rs1, 2047;
	shl.b64 	%rd143, %rd69, 3;
	add.s64 	%rd144, %rd142, %rd143;
	shl.b32 	%r2, %r16, 3;
	add.s32 	%r12, %r8, %r2;
	add.s32 	%r13, %r12, 4;
	ld.u32 	%r14, [%rd144];
	ld.u32 	%r15, [%rd144+4];
	// begin inline asm
	st.shared.f32 [%r12], %r14;
st.shared.f32 [%r13], %r15;
	// end inline asm
	add.s64 	%rd326, %rd69, 1;
	cvt.u32.u16 	%r17, %rs2;
	cvt.u32.u64 	%r18, %rd3;
	div.u32 	%r19, %r17, %r18;
	cvt.u64.u32 	%rd311, %r19;
	shl.b64 	%rd145, %rd4, 14;
	shl.b64 	%rd9, %rd3, 3;
	add.s64 	%rd146, %rd145, %rd9;
	add.s64 	%rd147, %rd146, %rd143;
	add.s64 	%rd310, %rd140, %rd147;
	xor.b64  	%rd309, %rd143, 16376;
	mov.u64 	%rd312, %rd326;
$L__BB34_3:
	add.s64 	%rd17, %rd312, %rd6;
	setp.lt.u64 	%p20, %rd17, 2048;
	@%p20 bra 	$L__BB34_5;
	bra.uni 	$L__BB34_4;
$L__BB34_5:
	shr.u64 	%rd148, %rd309, 3;
	setp.gt.u64 	%p19, %rd148, %rd6;
	selp.b64 	%rd16, %rd310, 0, %p19;
	setp.lt.u64 	%p1, %rd17, %rd312;
	add.s64 	%rd153, %rd312, %rd3;
	selp.b64 	%rd312, 2048, %rd153, %p1;
	cvt.u32.u64 	%r24, %rd17;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r20, %r25, %r8;
	add.s32 	%r21, %r20, 4;
	ld.u32 	%r22, [%rd16];
	ld.u32 	%r23, [%rd16+4];
	// begin inline asm
	st.shared.f32 [%r20], %r22;
st.shared.f32 [%r21], %r23;
	// end inline asm
	add.s64 	%rd311, %rd311, -1;
	add.s64 	%rd310, %rd310, %rd9;
	sub.s64 	%rd309, %rd309, %rd9;
	setp.ne.s64 	%p21, %rd311, 0;
	@%p21 bra 	$L__BB34_3;
	ld.global.nc.u64 	%rd22, [%rd1];
	ld.global.nc.u64 	%rd154, [%rd1+8];
	bar.sync 	0;
	add.s32 	%r28, %r12, %r2;
	add.s32 	%r29, %r28, 4;
	// begin inline asm
	ld.shared.f32 %r26, [%r28];
ld.shared.f32 %r27, [%r29];
	// end inline asm
	setp.eq.s64 	%p22, %rd154, 0;
	@%p22 bra 	$L__BB34_7;
	shl.b32 	%r4, %r16, 4;
	mov.b32 	%f2, %r27;
	mov.b32 	%f1, %r26;
	ld.u32 	%r33, [%rd22];
	ld.u32 	%r36, [%rd22+4];
	// begin inline asm
	mul.rn.ftz.f32 %r31, %r26, %r33;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r34, %r27, %r36;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r72, %r31, %r34;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r40, %r26, %r36;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r43, %r27, %r33;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r75, %r40, %r43;
	// end inline asm
	add.s32 	%r91, %r8, %r4;
	add.s32 	%r87, %r91, 8;
	add.s32 	%r88, %r91, 12;
	// begin inline asm
	ld.shared.f32 %r54, [%r87];
ld.shared.f32 %r57, [%r88];
	// end inline asm
	ld.u32 	%r67, [%rd22];
	ld.u32 	%r64, [%rd22+4];
	// begin inline asm
	mul.rn.ftz.f32 %r53, %r54, %r67;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r56, %r57, %r64;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r73, %r53, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r62, %r54, %r64;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r65, %r57, %r67;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r76, %r62, %r65;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r71, %r72, %r73;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r74, %r75, %r76;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r28], %r71;
st.shared.f32 [%r29], %r74;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r81, %r72, %r73;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r84, %r75, %r76;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r87], %r81;
st.shared.f32 [%r88], %r84;
	// end inline asm
	add.s64 	%rd314, %rd69, %rd3;
	setp.lt.u64 	%p23, %rd314, 1024;
	@%p23 bra 	$L__BB34_8;
	bra.uni 	$L__BB34_11;
$L__BB34_8:
	cvt.u32.u64 	%r156, %rd314;
	shl.b32 	%r157, %r156, 4;
	add.s32 	%r94, %r8, %r157;
	add.s32 	%r95, %r94, 4;
	// begin inline asm
	ld.shared.f32 %r97, [%r94];
ld.shared.f32 %r100, [%r95];
	// end inline asm
	ld.u32 	%r110, [%rd22];
	ld.u32 	%r107, [%rd22+4];
	// begin inline asm
	mul.rn.ftz.f32 %r96, %r97, %r110;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r99, %r100, %r107;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r137, %r96, %r99;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r105, %r97, %r107;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r108, %r100, %r110;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r140, %r105, %r108;
	// end inline asm
	add.s32 	%r152, %r94, 8;
	add.s32 	%r153, %r94, 12;
	// begin inline asm
	ld.shared.f32 %r119, [%r152];
ld.shared.f32 %r122, [%r153];
	// end inline asm
	ld.u32 	%r132, [%rd22];
	ld.u32 	%r129, [%rd22+4];
	// begin inline asm
	mul.rn.ftz.f32 %r118, %r119, %r132;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r121, %r122, %r129;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r138, %r118, %r121;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r127, %r119, %r129;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r130, %r122, %r132;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r141, %r127, %r130;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r136, %r137, %r138;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r139, %r140, %r141;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r94], %r136;
st.shared.f32 [%r95], %r139;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r146, %r137, %r138;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r149, %r140, %r141;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r152], %r146;
st.shared.f32 [%r153], %r149;
	// end inline asm
	add.s64 	%rd313, %rd314, %rd3;
	setp.gt.u64 	%p24, %rd313, 1023;
	@%p24 bra 	$L__BB34_11;
$L__BB34_9:
	add.s64 	%rd155, %rd313, 1;
	cvt.u32.u64 	%r222, %rd313;
	shl.b32 	%r223, %r222, 4;
	add.s32 	%r160, %r223, %r8;
	add.s32 	%r161, %r160, 4;
	// begin inline asm
	ld.shared.f32 %r163, [%r160];
ld.shared.f32 %r166, [%r161];
	// end inline asm
	ld.u32 	%r176, [%rd22];
	ld.u32 	%r173, [%rd22+4];
	// begin inline asm
	mul.rn.ftz.f32 %r162, %r163, %r176;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r165, %r166, %r173;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r203, %r162, %r165;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r171, %r163, %r173;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r174, %r166, %r176;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r206, %r171, %r174;
	// end inline asm
	add.s32 	%r218, %r160, 8;
	add.s32 	%r219, %r160, 12;
	// begin inline asm
	ld.shared.f32 %r185, [%r218];
ld.shared.f32 %r188, [%r219];
	// end inline asm
	ld.u32 	%r198, [%rd22];
	ld.u32 	%r195, [%rd22+4];
	// begin inline asm
	mul.rn.ftz.f32 %r184, %r185, %r198;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r187, %r188, %r195;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r204, %r184, %r187;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r193, %r185, %r195;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r196, %r188, %r198;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r207, %r193, %r196;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r202, %r203, %r204;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r205, %r206, %r207;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r160], %r202;
st.shared.f32 [%r161], %r205;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r212, %r203, %r204;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r215, %r206, %r207;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r218], %r212;
st.shared.f32 [%r219], %r215;
	// end inline asm
	add.s64 	%rd156, %rd155, %rd6;
	setp.lt.u64 	%p25, %rd156, %rd155;
	add.s64 	%rd313, %rd313, %rd3;
	setp.gt.u64 	%p26, %rd313, 1023;
	or.pred  	%p27, %p25, %p26;
	@%p27 bra 	$L__BB34_11;
	bra.uni 	$L__BB34_9;
$L__BB34_11:
	ld.global.nc.u64 	%rd158, [%rd1+32];
	ld.global.nc.u64 	%rd159, [%rd1+40];
	and.b64  	%rd160, %rd159, -2048;
	setp.lt.u64 	%p28, %rd5, %rd160;
	sub.s64 	%rd162, %rd159, %rd5;
	setp.gt.u64 	%p29, %rd162, 2047;
	and.pred  	%p30, %p28, %p29;
	@%p30 bra 	$L__BB34_13;
	bra.uni 	$L__BB34_12;
$L__BB34_13:
	setp.gt.u64 	%p31, %rd314, 1023;
	bar.sync 	0;
	mov.u64 	%rd67, 1;
	@%p31 bra 	$L__BB34_16;
	mov.u64 	%rd67, 1;
$L__BB34_15:
	add.s64 	%rd167, %rd314, 1;
	add.s64 	%rd67, %rd67, 1;
	add.s64 	%rd168, %rd167, %rd6;
	setp.lt.u64 	%p32, %rd168, %rd167;
	add.s64 	%rd314, %rd314, %rd3;
	setp.gt.u64 	%p33, %rd314, 1023;
	or.pred  	%p34, %p32, %p33;
	@!%p34 bra 	$L__BB34_15;
	bra.uni 	$L__BB34_16;
$L__BB34_16:
	add.s64 	%rd27, %rd158, %rd141;
	setp.eq.s64 	%p85, %rd67, 0;
	mov.u64 	%rd317, 0;
	@%p85 bra 	$L__BB34_18;
	xor.b64  	%rd35, %rd69, 2047;
	cvt.u32.u64 	%r224, %rd35;
	div.u32 	%r226, %r224, %r18;
	cvt.u64.u32 	%rd171, %r226;
	add.s64 	%rd172, %rd171, 1;
	min.u64 	%rd317, %rd172, %rd67;
$L__BB34_18:
	selp.b64 	%rd350, %rd27, 0, %p30;
	add.s64 	%rd29, %rd27, 16384;
	setp.eq.s64 	%p35, %rd317, 0;
	@%p35 bra 	$L__BB34_32;
	xor.b32  	%r228, %r16, 1023;
	div.u32 	%r230, %r228, %r18;
	cvt.u64.u32 	%rd173, %r230;
	add.s64 	%rd174, %rd173, 1;
	min.u64 	%rd38, %rd317, %rd174;
	setp.eq.s64 	%p36, %rd67, 0;
	add.s32 	%r7, %r8, 8;
	mov.u64 	%rd323, 0;
	mov.u64 	%rd328, %rd350;
	mov.u64 	%rd329, %rd69;
	mov.u64 	%rd321, %rd323;
	@%p36 bra 	$L__BB34_23;
	add.s64 	%rd323, %rd67, -1;
	setp.eq.s32 	%p37, %r16, 0;
	@%p37 bra 	$L__BB34_22;
	add.s64 	%rd321, %rd350, %rd143;
	add.s64 	%rd328, %rd321, 8;
	mov.u64 	%rd329, 0;
	bra.uni 	$L__BB34_23;
$L__BB34_22:
	add.s64 	%rd328, %rd350, 8;
	mov.u64 	%rd329, 0;
	mov.u64 	%rd321, %rd350;
$L__BB34_23:
	add.s32 	%r234, %r7, %r4;
	add.s32 	%r235, %r234, 4;
	// begin inline asm
	ld.shared.f32 %r232, [%r234];
ld.shared.f32 %r233, [%r235];
	// end inline asm
	st.u32 	[%rd321], %r232;
	st.u32 	[%rd321+4], %r233;
	setp.eq.s64 	%p38, %rd38, 1;
	@%p38 bra 	$L__BB34_32;
	mov.pred 	%p83, 0;
	add.s64 	%rd322, %rd38, -1;
$L__BB34_25:
	setp.eq.s64 	%p39, %rd323, 0;
	mov.u64 	%rd327, 0;
	mov.u64 	%rd330, %rd327;
	@%p39 bra 	$L__BB34_29;
	add.s64 	%rd327, %rd323, -1;
	selp.b64 	%rd54, 0, %rd6, %p85;
	setp.ne.s64 	%p40, %rd329, 0;
	@%p40 bra 	$L__BB34_28;
	sub.s64 	%rd190, %rd29, %rd328;
	shr.u64 	%rd191, %rd190, 3;
	setp.gt.u64 	%p44, %rd191, %rd54;
	shl.b64 	%rd192, %rd54, 3;
	add.s64 	%rd193, %rd328, %rd192;
	add.s64 	%rd194, %rd193, 8;
	selp.b64 	%rd328, %rd194, %rd29, %p44;
	selp.b64 	%rd330, %rd193, 0, %p44;
	mov.u64 	%rd329, 0;
	mov.pred 	%p85, %p83;
	bra.uni 	$L__BB34_29;
$L__BB34_28:
	add.s64 	%rd183, %rd54, %rd329;
	sub.s64 	%rd184, %rd29, %rd328;
	shr.u64 	%rd185, %rd184, 3;
	setp.gt.u64 	%p42, %rd185, %rd183;
	shl.b64 	%rd186, %rd183, 3;
	add.s64 	%rd187, %rd328, %rd186;
	add.s64 	%rd188, %rd187, 8;
	selp.b64 	%rd328, %rd188, %rd29, %p42;
	selp.b64 	%rd330, %rd187, 0, %p42;
	mov.u64 	%rd329, 0;
	mov.pred 	%p85, %p83;
$L__BB34_29:
	add.s64 	%rd63, %rd326, %rd6;
	shl.b64 	%rd64, %rd63, 1;
	setp.lt.u64 	%p45, %rd64, 2048;
	@%p45 bra 	$L__BB34_31;
	bra.uni 	$L__BB34_30;
$L__BB34_31:
	setp.lt.u64 	%p5, %rd63, %rd326;
	setp.gt.u64 	%p46, %rd63, 1023;
	add.s64 	%rd199, %rd63, 1;
	selp.b64 	%rd200, 1024, %rd199, %p46;
	selp.b64 	%rd326, 1024, %rd200, %p5;
	cvt.u32.u64 	%r240, %rd64;
	shl.b32 	%r241, %r240, 3;
	add.s32 	%r238, %r241, %r7;
	add.s32 	%r239, %r238, 4;
	// begin inline asm
	ld.shared.f32 %r236, [%r238];
ld.shared.f32 %r237, [%r239];
	// end inline asm
	st.u32 	[%rd330], %r236;
	st.u32 	[%rd330+4], %r237;
	add.s64 	%rd322, %rd322, -1;
	setp.ne.s64 	%p47, %rd322, 0;
	mov.u64 	%rd323, %rd327;
	@%p47 bra 	$L__BB34_25;
$L__BB34_32:
	mov.u64 	%rd357, 0;
	mov.pred 	%p86, -1;
	mov.u64 	%rd358, %rd69;
	mov.pred 	%p89, %p86;
$L__BB34_33:
	setp.ne.s64 	%p49, %rd67, 0;
	@%p49 bra 	$L__BB34_37;
	bra.uni 	$L__BB34_34;
$L__BB34_37:
	@%p86 bra 	$L__BB34_40;
	bra.uni 	$L__BB34_38;
$L__BB34_40:
	setp.ne.s64 	%p50, %rd69, 0;
	@%p50 bra 	$L__BB34_42;
	bra.uni 	$L__BB34_41;
$L__BB34_42:
	sub.s64 	%rd202, %rd29, %rd350;
	shr.u64 	%rd203, %rd202, 3;
	setp.gt.u64 	%p51, %rd203, %rd69;
	shl.b64 	%rd204, %rd69, 3;
	add.s64 	%rd205, %rd350, %rd204;
	add.s64 	%rd206, %rd205, 8;
	selp.b64 	%rd350, %rd206, %rd29, %p51;
	bra.uni 	$L__BB34_43;
$L__BB34_34:
	selp.b64 	%rd72, 0, %rd6, %p86;
	setp.ne.s64 	%p72, %rd69, 0;
	@%p72 bra 	$L__BB34_36;
	bra.uni 	$L__BB34_35;
$L__BB34_36:
	add.s64 	%rd281, %rd72, %rd69;
	sub.s64 	%rd282, %rd29, %rd350;
	shr.u64 	%rd283, %rd282, 3;
	setp.gt.u64 	%p73, %rd283, %rd281;
	shl.b64 	%rd284, %rd281, 3;
	add.s64 	%rd285, %rd350, %rd284;
	add.s64 	%rd286, %rd285, 8;
	selp.b64 	%rd355, %rd286, %rd29, %p73;
	selp.b64 	%rd356, %rd285, 0, %p73;
	bra.uni 	$L__BB34_65;
$L__BB34_35:
	sub.s64 	%rd287, %rd29, %rd350;
	shr.u64 	%rd288, %rd287, 3;
	setp.gt.u64 	%p74, %rd288, %rd72;
	shl.b64 	%rd289, %rd72, 3;
	add.s64 	%rd290, %rd350, %rd289;
	add.s64 	%rd291, %rd290, 8;
	selp.b64 	%rd355, %rd291, %rd29, %p74;
	selp.b64 	%rd356, %rd290, 0, %p74;
	bra.uni 	$L__BB34_65;
$L__BB34_41:
	setp.eq.s64 	%p52, %rd350, %rd29;
	selp.b64 	%rd207, 0, 8, %p52;
	add.s64 	%rd350, %rd350, %rd207;
$L__BB34_43:
	add.s64 	%rd67, %rd67, -1;
	mov.u64 	%rd69, 0;
$L__BB34_38:
	setp.eq.s64 	%p53, %rd67, -1;
	@%p53 bra 	$L__BB34_44;
	add.s64 	%rd342, %rd67, 1;
	bra.uni 	$L__BB34_47;
$L__BB34_44:
	setp.ne.s64 	%p54, %rd69, 0;
	sub.s64 	%rd308, %rd29, %rd350;
	@%p54 bra 	$L__BB34_46;
	bra.uni 	$L__BB34_45;
$L__BB34_46:
	add.s64 	%rd211, %rd69, %rd6;
	shr.u64 	%rd213, %rd308, 3;
	setp.gt.u64 	%p55, %rd213, %rd211;
	add.s64 	%rd214, %rd69, %rd3;
	shl.b64 	%rd215, %rd214, 3;
	add.s64 	%rd216, %rd350, %rd215;
	selp.b64 	%rd350, %rd216, %rd29, %p55;
	mov.u64 	%rd342, -1;
	mov.u64 	%rd69, 0;
	bra.uni 	$L__BB34_47;
$L__BB34_45:
	shr.u64 	%rd220, %rd308, 3;
	setp.gt.u64 	%p56, %rd220, %rd6;
	add.s64 	%rd222, %rd350, %rd9;
	selp.b64 	%rd350, %rd222, %rd29, %p56;
	mov.u64 	%rd342, -1;
	mov.u64 	%rd69, 0;
$L__BB34_47:
	mul.lo.s64 	%rd352, %rd342, %rd3;
	mul.hi.u64 	%rd223, %rd342, %rd3;
	setp.eq.s64 	%p57, %rd223, 0;
	@%p57 bra 	$L__BB34_59;
	mov.u64 	%rd224, -1;
	div.u64 	%rd225, %rd224, %rd342;
	div.u64 	%rd226, %rd224, %rd3;
	mul.lo.s64 	%rd227, %rd225, %rd342;
	mul.lo.s64 	%rd228, %rd226, %rd3;
	max.u64 	%rd91, %rd227, %rd228;
	add.s64 	%rd344, %rd91, -1;
	setp.eq.s64 	%p59, %rd69, 0;
	@%p59 bra 	$L__BB34_53;
	add.s64 	%rd231, %rd69, %rd344;
	setp.lt.u64 	%p60, %rd231, %rd69;
	@%p60 bra 	$L__BB34_51;
	bra.uni 	$L__BB34_50;
$L__BB34_51:
	add.s64 	%rd232, %rd69, -1;
	sub.s64 	%rd233, %rd29, %rd350;
	shr.u64 	%rd234, %rd233, 3;
	setp.le.u64 	%p61, %rd234, %rd232;
	shl.b64 	%rd235, %rd69, 3;
	add.s64 	%rd343, %rd350, %rd235;
	mov.u64 	%rd350, %rd29;
	@%p61 bra 	$L__BB34_55;
	bra.uni 	$L__BB34_52;
$L__BB34_53:
	sub.s64 	%rd239, %rd29, %rd350;
	shr.u64 	%rd240, %rd239, 3;
	setp.gt.u64 	%p88, %rd240, %rd344;
	shl.b64 	%rd241, %rd91, 3;
	add.s64 	%rd242, %rd350, %rd241;
	add.s64 	%rd345, %rd242, -8;
	bra.uni 	$L__BB34_54;
$L__BB34_50:
	add.s64 	%rd344, %rd344, %rd69;
	mov.u64 	%rd343, %rd350;
$L__BB34_52:
	sub.s64 	%rd236, %rd29, %rd343;
	shr.u64 	%rd237, %rd236, 3;
	setp.gt.u64 	%p88, %rd237, %rd344;
	shl.b64 	%rd238, %rd344, 3;
	add.s64 	%rd345, %rd343, %rd238;
$L__BB34_54:
	add.s64 	%rd243, %rd345, 8;
	selp.b64 	%rd350, %rd243, %rd29, %p88;
$L__BB34_55:
	setp.gt.u64 	%p58, %rd227, %rd228;
	selp.b64 	%rd229, %rd225, 0, %p58;
	sub.s64 	%rd348, %rd3, %rd229;
	selp.b64 	%rd230, 0, %rd226, %p58;
	sub.s64 	%rd347, %rd342, %rd230;
	mul.lo.s64 	%rd352, %rd347, %rd348;
	mul.hi.u64 	%rd245, %rd347, %rd348;
	mov.u64 	%rd69, 0;
	setp.eq.s64 	%p62, %rd245, 0;
	@%p62 bra 	$L__BB34_59;
$L__BB34_56:
	setp.eq.s64 	%p63, %rd347, 0;
	@%p63 bra 	$L__BB34_72;
	setp.eq.s64 	%p64, %rd348, 0;
	@%p64 bra 	$L__BB34_73;
	div.u64 	%rd248, %rd224, %rd347;
	div.u64 	%rd249, %rd224, %rd348;
	mul.lo.s64 	%rd250, %rd248, %rd347;
	mul.lo.s64 	%rd251, %rd249, %rd348;
	setp.gt.u64 	%p65, %rd250, %rd251;
	selp.b64 	%rd252, %rd248, 0, %p65;
	sub.s64 	%rd348, %rd348, %rd252;
	selp.b64 	%rd253, 0, %rd249, %p65;
	sub.s64 	%rd347, %rd347, %rd253;
	max.u64 	%rd254, %rd250, %rd251;
	add.s64 	%rd255, %rd254, -1;
	shl.b64 	%rd256, %rd254, 3;
	add.s64 	%rd257, %rd350, %rd256;
	sub.s64 	%rd258, %rd29, %rd350;
	shr.u64 	%rd259, %rd258, 3;
	setp.gt.u64 	%p66, %rd259, %rd255;
	selp.b64 	%rd350, %rd257, %rd29, %p66;
	mul.lo.s64 	%rd352, %rd347, %rd348;
	mul.hi.u64 	%rd260, %rd347, %rd348;
	setp.ne.s64 	%p13, %rd260, 0;
	@%p13 bra 	$L__BB34_56;
$L__BB34_59:
	add.s64 	%rd354, %rd352, -1;
	setp.ne.s64 	%p67, %rd69, 0;
	@%p67 bra 	$L__BB34_61;
	bra.uni 	$L__BB34_60;
$L__BB34_61:
	add.s64 	%rd261, %rd69, %rd354;
	setp.lt.u64 	%p68, %rd261, %rd69;
	@%p68 bra 	$L__BB34_63;
	bra.uni 	$L__BB34_62;
$L__BB34_63:
	add.s64 	%rd263, %rd69, -1;
	sub.s64 	%rd264, %rd29, %rd350;
	shr.u64 	%rd265, %rd264, 3;
	setp.le.u64 	%p69, %rd265, %rd263;
	shl.b64 	%rd266, %rd69, 3;
	add.s64 	%rd350, %rd350, %rd266;
	mov.u64 	%rd356, 0;
	mov.u64 	%rd355, %rd29;
	@%p69 bra 	$L__BB34_65;
	bra.uni 	$L__BB34_64;
$L__BB34_60:
	sub.s64 	%rd272, %rd29, %rd350;
	shr.u64 	%rd273, %rd272, 3;
	setp.gt.u64 	%p71, %rd273, %rd354;
	shl.b64 	%rd274, %rd352, 3;
	add.s64 	%rd275, %rd350, %rd274;
	add.s64 	%rd276, %rd275, -8;
	selp.b64 	%rd355, %rd275, %rd29, %p71;
	selp.b64 	%rd356, %rd276, 0, %p71;
	bra.uni 	$L__BB34_65;
$L__BB34_62:
	add.s64 	%rd354, %rd354, %rd69;
$L__BB34_64:
	sub.s64 	%rd267, %rd29, %rd350;
	shr.u64 	%rd268, %rd267, 3;
	setp.gt.u64 	%p70, %rd268, %rd354;
	shl.b64 	%rd269, %rd354, 3;
	add.s64 	%rd270, %rd350, %rd269;
	add.s64 	%rd271, %rd270, 8;
	selp.b64 	%rd355, %rd271, %rd29, %p70;
	selp.b64 	%rd356, %rd270, 0, %p70;
$L__BB34_65:
	setp.eq.s64 	%p75, %rd356, 0;
	mov.u64 	%rd360, 0;
	@%p75 bra 	$L__BB34_67;
	selp.b64 	%rd295, 0, %rd6, %p89;
	add.s64 	%rd297, %rd295, %rd358;
	add.s64 	%rd359, %rd297, %rd357;
	setp.lt.u64 	%p77, %rd359, %rd357;
	setp.gt.u64 	%p78, %rd359, 1023;
	or.pred  	%p79, %p77, %p78;
	add.s64 	%rd298, %rd359, 1;
	selp.b64 	%rd357, 1024, %rd298, %p79;
	selp.b64 	%rd360, 0, %rd356, %p79;
	mov.pred 	%p89, 0;
	mov.u64 	%rd358, 0;
$L__BB34_67:
	setp.eq.s64 	%p80, %rd360, 0;
	@%p80 bra 	$L__BB34_71;
	shl.b64 	%rd134, %rd359, 1;
	setp.lt.u64 	%p81, %rd134, 2048;
	@%p81 bra 	$L__BB34_70;
	bra.uni 	$L__BB34_69;
$L__BB34_70:
	cvt.u32.u64 	%r246, %rd134;
	shl.b32 	%r247, %r246, 3;
	add.s32 	%r244, %r247, %r8;
	add.s32 	%r245, %r244, 4;
	// begin inline asm
	ld.shared.f32 %r242, [%r244];
ld.shared.f32 %r243, [%r245];
	// end inline asm
	st.u32 	[%rd360], %r242;
	st.u32 	[%rd360+4], %r243;
	mov.pred 	%p86, 0;
	mov.u64 	%rd67, 0;
	mov.u64 	%rd350, %rd355;
	mov.u64 	%rd69, %rd67;
	bra.uni 	$L__BB34_33;
$L__BB34_71:
	ret;
$L__BB34_72:
	mov.u64 	%rd279, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd280, %rd279;
	{ // callseq 192, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd280;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 192
$L__BB34_73:
	mov.u64 	%rd277, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd278, %rd277;
	{ // callseq 191, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd278;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 191
$L__BB34_4:
	mov.u64 	%rd149, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd150, %rd149;
	mov.u64 	%rd151, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd152, %rd151;
	{ // callseq 188, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd150;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd152;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 188
$L__BB34_69:
	mov.u64 	%rd299, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd300, %rd299;
	mov.u64 	%rd301, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd302, %rd301;
	{ // callseq 193, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd300;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd302;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 193
$L__BB34_30:
	mov.u64 	%rd195, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd196, %rd195;
	mov.u64 	%rd197, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd198, %rd197;
	{ // callseq 190, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd196;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd198;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 190
$L__BB34_1:
	mov.u64 	%rd306, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_12;
	cvta.global.u64 	%rd307, %rd306;
	{ // callseq 195, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd307;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 195
$L__BB34_7:
	mov.u64 	%rd304, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_16;
	cvta.global.u64 	%rd305, %rd304;
	{ // callseq 194, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd305;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 194
$L__BB34_12:
	mov.u64 	%rd163, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_13;
	cvta.global.u64 	%rd164, %rd163;
	{ // callseq 189, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd164;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 189

}
	// .globl	daubechies_first_forward_4096_kernel
.visible .entry daubechies_first_forward_4096_kernel(
	.param .u64 daubechies_first_forward_4096_kernel_param_0
)
{
	.reg .pred 	%p<88>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<254>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<361>;

	ld.param.u64 	%rd134, [daubechies_first_forward_4096_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd134;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[32768];
    mov.u32 %r8, nonphysical;
	// end inline asm
	mov.u32 	%r11, %ctaid.x;
	ld.global.nc.u64 	%rd135, [%rd1+24];
	and.b64  	%rd136, %rd135, -4096;
	mul.wide.u32 	%rd5, %r11, 4096;
	setp.ge.u64 	%p16, %rd5, %rd136;
	sub.s64 	%rd137, %rd135, %rd5;
	setp.lt.u64 	%p17, %rd137, 4096;
	or.pred  	%p18, %p16, %p17;
	@!%p18 bra 	$L__BB35_2;
	bra.uni 	$L__BB35_1;
$L__BB35_2:
	mov.u32 	%r9, %tid.x;
	cvt.u64.u32 	%rd68, %r9;
	mov.u32 	%r10, %ntid.x;
	cvt.u64.u32 	%rd3, %r10;
	cvt.u64.u32 	%rd4, %r11;
	cvt.u32.u64 	%r16, %rd68;
	ld.global.nc.u64 	%rd139, [%rd1+16];
	shl.b64 	%rd140, %rd5, 3;
	add.s64 	%rd141, %rd139, %rd140;
	add.s64 	%rd6, %rd3, -1;
	cvt.u16.u64 	%rs1, %rd68;
	xor.b16  	%rs2, %rs1, 4095;
	shl.b64 	%rd142, %rd68, 3;
	add.s64 	%rd143, %rd141, %rd142;
	add.s64 	%rd326, %rd68, 1;
	shl.b32 	%r2, %r16, 3;
	add.s32 	%r12, %r8, %r2;
	add.s32 	%r13, %r12, 4;
	ld.u32 	%r14, [%rd143];
	ld.u32 	%r15, [%rd143+4];
	// begin inline asm
	st.shared.f32 [%r12], %r14;
st.shared.f32 [%r13], %r15;
	// end inline asm
	cvt.u32.u16 	%r17, %rs2;
	cvt.u32.u64 	%r18, %rd3;
	div.u32 	%r19, %r17, %r18;
	cvt.u64.u32 	%rd312, %r19;
	shl.b64 	%rd144, %rd4, 15;
	shl.b64 	%rd9, %rd3, 3;
	add.s64 	%rd145, %rd144, %rd9;
	add.s64 	%rd146, %rd145, %rd142;
	add.s64 	%rd311, %rd139, %rd146;
	xor.b64  	%rd310, %rd142, 32760;
	mov.u64 	%rd313, %rd326;
$L__BB35_3:
	add.s64 	%rd17, %rd313, %rd6;
	setp.lt.u64 	%p20, %rd17, 4096;
	@%p20 bra 	$L__BB35_5;
	bra.uni 	$L__BB35_4;
$L__BB35_5:
	shr.u64 	%rd147, %rd310, 3;
	setp.gt.u64 	%p19, %rd147, %rd6;
	selp.b64 	%rd16, %rd311, 0, %p19;
	setp.lt.u64 	%p1, %rd17, %rd313;
	add.s64 	%rd152, %rd313, %rd3;
	selp.b64 	%rd313, 4096, %rd152, %p1;
	cvt.u32.u64 	%r24, %rd17;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r20, %r25, %r8;
	add.s32 	%r21, %r20, 4;
	ld.u32 	%r22, [%rd16];
	ld.u32 	%r23, [%rd16+4];
	// begin inline asm
	st.shared.f32 [%r20], %r22;
st.shared.f32 [%r21], %r23;
	// end inline asm
	add.s64 	%rd312, %rd312, -1;
	add.s64 	%rd311, %rd311, %rd9;
	sub.s64 	%rd310, %rd310, %rd9;
	setp.ne.s64 	%p21, %rd312, 0;
	@%p21 bra 	$L__BB35_3;
	ld.global.nc.u64 	%rd22, [%rd1];
	ld.global.nc.u64 	%rd153, [%rd1+8];
	bar.sync 	0;
	add.s32 	%r28, %r12, %r2;
	add.s32 	%r29, %r28, 4;
	// begin inline asm
	ld.shared.f32 %r26, [%r28];
ld.shared.f32 %r27, [%r29];
	// end inline asm
	setp.eq.s64 	%p22, %rd153, 0;
	@%p22 bra 	$L__BB35_7;
	shl.b32 	%r4, %r16, 4;
	mov.b32 	%f2, %r27;
	mov.b32 	%f1, %r26;
	ld.u32 	%r33, [%rd22];
	ld.u32 	%r36, [%rd22+4];
	// begin inline asm
	mul.rn.ftz.f32 %r31, %r26, %r33;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r34, %r27, %r36;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r72, %r31, %r34;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r40, %r26, %r36;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r43, %r27, %r33;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r75, %r40, %r43;
	// end inline asm
	add.s32 	%r155, %r8, %r4;
	add.s32 	%r87, %r155, 8;
	add.s32 	%r88, %r155, 12;
	// begin inline asm
	ld.shared.f32 %r54, [%r87];
ld.shared.f32 %r57, [%r88];
	// end inline asm
	ld.u32 	%r67, [%rd22];
	ld.u32 	%r64, [%rd22+4];
	// begin inline asm
	mul.rn.ftz.f32 %r53, %r54, %r67;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r56, %r57, %r64;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r73, %r53, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r62, %r54, %r64;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r65, %r57, %r67;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r76, %r62, %r65;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r71, %r72, %r73;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r74, %r75, %r76;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r28], %r71;
st.shared.f32 [%r29], %r74;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r81, %r72, %r73;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r84, %r75, %r76;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r87], %r81;
st.shared.f32 [%r88], %r84;
	// end inline asm
	add.s64 	%rd315, %rd68, %rd3;
	cvt.u32.u64 	%r156, %rd315;
	shl.b32 	%r157, %r156, 4;
	add.s32 	%r93, %r8, %r157;
	add.s32 	%r94, %r93, 4;
	// begin inline asm
	ld.shared.f32 %r96, [%r93];
ld.shared.f32 %r99, [%r94];
	// end inline asm
	ld.u32 	%r109, [%rd22];
	ld.u32 	%r106, [%rd22+4];
	// begin inline asm
	mul.rn.ftz.f32 %r95, %r96, %r109;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r98, %r99, %r106;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r136, %r95, %r98;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r104, %r96, %r106;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r107, %r99, %r109;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r139, %r104, %r107;
	// end inline asm
	add.s32 	%r151, %r93, 8;
	add.s32 	%r152, %r93, 12;
	// begin inline asm
	ld.shared.f32 %r118, [%r151];
ld.shared.f32 %r121, [%r152];
	// end inline asm
	ld.u32 	%r131, [%rd22];
	ld.u32 	%r128, [%rd22+4];
	// begin inline asm
	mul.rn.ftz.f32 %r117, %r118, %r131;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r120, %r121, %r128;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r137, %r117, %r120;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r126, %r118, %r128;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r129, %r121, %r131;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r140, %r126, %r129;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r135, %r136, %r137;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r138, %r139, %r140;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r93], %r135;
st.shared.f32 [%r94], %r138;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r145, %r136, %r137;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r148, %r139, %r140;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r151], %r145;
st.shared.f32 [%r152], %r148;
	// end inline asm
	add.s64 	%rd314, %rd315, %rd3;
	setp.gt.u64 	%p23, %rd314, 2047;
	@%p23 bra 	$L__BB35_9;
$L__BB35_69:
	add.s64 	%rd154, %rd314, 1;
	cvt.u32.u64 	%r222, %rd314;
	shl.b32 	%r223, %r222, 4;
	add.s32 	%r160, %r223, %r8;
	add.s32 	%r161, %r160, 4;
	// begin inline asm
	ld.shared.f32 %r163, [%r160];
ld.shared.f32 %r166, [%r161];
	// end inline asm
	ld.u32 	%r176, [%rd22];
	ld.u32 	%r173, [%rd22+4];
	// begin inline asm
	mul.rn.ftz.f32 %r162, %r163, %r176;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r165, %r166, %r173;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r203, %r162, %r165;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r171, %r163, %r173;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r174, %r166, %r176;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r206, %r171, %r174;
	// end inline asm
	add.s32 	%r218, %r160, 8;
	add.s32 	%r219, %r160, 12;
	// begin inline asm
	ld.shared.f32 %r185, [%r218];
ld.shared.f32 %r188, [%r219];
	// end inline asm
	ld.u32 	%r198, [%rd22];
	ld.u32 	%r195, [%rd22+4];
	// begin inline asm
	mul.rn.ftz.f32 %r184, %r185, %r198;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r187, %r188, %r195;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r204, %r184, %r187;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r193, %r185, %r195;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r196, %r188, %r198;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r207, %r193, %r196;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r202, %r203, %r204;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r205, %r206, %r207;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r160], %r202;
st.shared.f32 [%r161], %r205;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r212, %r203, %r204;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r215, %r206, %r207;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r218], %r212;
st.shared.f32 [%r219], %r215;
	// end inline asm
	add.s64 	%rd155, %rd154, %rd6;
	setp.lt.u64 	%p24, %rd155, %rd154;
	add.s64 	%rd314, %rd314, %rd3;
	setp.gt.u64 	%p25, %rd314, 2047;
	or.pred  	%p26, %p24, %p25;
	@%p26 bra 	$L__BB35_9;
	bra.uni 	$L__BB35_69;
$L__BB35_9:
	ld.global.nc.u64 	%rd157, [%rd1+32];
	ld.global.nc.u64 	%rd158, [%rd1+40];
	and.b64  	%rd159, %rd158, -4096;
	setp.lt.u64 	%p27, %rd5, %rd159;
	sub.s64 	%rd161, %rd158, %rd5;
	setp.gt.u64 	%p28, %rd161, 4095;
	and.pred  	%p29, %p27, %p28;
	@%p29 bra 	$L__BB35_11;
	bra.uni 	$L__BB35_10;
$L__BB35_11:
	add.s64 	%rd27, %rd157, %rd140;
	selp.b64 	%rd350, %rd27, 0, %p29;
	bar.sync 	0;
	mov.u64 	%rd66, 1;
$L__BB35_12:
	mov.u64 	%rd30, %rd66;
	add.s64 	%rd165, %rd315, 1;
	add.s64 	%rd66, %rd30, 1;
	add.s64 	%rd166, %rd165, %rd6;
	setp.lt.u64 	%p30, %rd166, %rd165;
	add.s64 	%rd315, %rd315, %rd3;
	setp.gt.u64 	%p31, %rd315, 2047;
	or.pred  	%p32, %p30, %p31;
	@!%p32 bra 	$L__BB35_12;
	bra.uni 	$L__BB35_13;
$L__BB35_13:
	setp.eq.s64 	%p83, %rd66, 0;
	xor.b64  	%rd34, %rd68, 4095;
	mov.u64 	%rd317, 0;
	@%p83 bra 	$L__BB35_15;
	cvt.u32.u64 	%r224, %rd34;
	div.u32 	%r226, %r224, %r18;
	cvt.u64.u32 	%rd169, %r226;
	add.s64 	%rd170, %rd169, 1;
	min.u64 	%rd317, %rd170, %rd66;
$L__BB35_15:
	add.s64 	%rd33, %rd27, 32768;
	setp.eq.s64 	%p33, %rd317, 0;
	@%p33 bra 	$L__BB35_29;
	xor.b32  	%r228, %r16, 2047;
	div.u32 	%r230, %r228, %r18;
	cvt.u64.u32 	%rd171, %r230;
	add.s64 	%rd37, %rd171, 1;
	min.u64 	%rd38, %rd317, %rd37;
	setp.eq.s64 	%p34, %rd66, 0;
	add.s32 	%r7, %r8, 8;
	mov.u64 	%rd318, 0;
	mov.u64 	%rd328, %rd350;
	mov.u64 	%rd329, %rd68;
	mov.u64 	%rd321, %rd318;
	@%p34 bra 	$L__BB35_20;
	setp.eq.s32 	%p35, %r16, 0;
	@%p35 bra 	$L__BB35_19;
	add.s64 	%rd321, %rd350, %rd142;
	add.s64 	%rd328, %rd321, 8;
	mov.u64 	%rd329, 0;
	mov.u64 	%rd318, %rd30;
	bra.uni 	$L__BB35_20;
$L__BB35_19:
	add.s64 	%rd328, %rd350, 8;
	mov.u64 	%rd329, 0;
	mov.u64 	%rd318, %rd30;
	mov.u64 	%rd321, %rd350;
$L__BB35_20:
	add.s32 	%r234, %r7, %r4;
	add.s32 	%r235, %r234, 4;
	// begin inline asm
	ld.shared.f32 %r232, [%r234];
ld.shared.f32 %r233, [%r235];
	// end inline asm
	st.u32 	[%rd321], %r232;
	st.u32 	[%rd321+4], %r233;
	setp.eq.s64 	%p36, %rd38, 1;
	@%p36 bra 	$L__BB35_29;
	mov.pred 	%p81, 0;
	cvt.u32.u64 	%r236, %rd34;
	div.u32 	%r238, %r236, %r18;
	cvt.u64.u32 	%rd177, %r238;
	add.s64 	%rd178, %rd177, 1;
	min.u64 	%rd179, %rd178, %rd66;
	min.u64 	%rd180, %rd179, %rd37;
	add.s64 	%rd322, %rd180, -1;
$L__BB35_22:
	setp.eq.s64 	%p37, %rd318, 0;
	mov.u64 	%rd327, 0;
	mov.u64 	%rd330, %rd327;
	@%p37 bra 	$L__BB35_26;
	add.s64 	%rd327, %rd318, -1;
	selp.b64 	%rd53, 0, %rd6, %p83;
	setp.ne.s64 	%p38, %rd329, 0;
	@%p38 bra 	$L__BB35_25;
	sub.s64 	%rd191, %rd33, %rd328;
	shr.u64 	%rd192, %rd191, 3;
	setp.gt.u64 	%p42, %rd192, %rd53;
	shl.b64 	%rd193, %rd53, 3;
	add.s64 	%rd194, %rd328, %rd193;
	add.s64 	%rd195, %rd194, 8;
	selp.b64 	%rd328, %rd195, %rd33, %p42;
	selp.b64 	%rd330, %rd194, 0, %p42;
	mov.u64 	%rd329, 0;
	mov.pred 	%p83, %p81;
	bra.uni 	$L__BB35_26;
$L__BB35_25:
	add.s64 	%rd184, %rd53, %rd329;
	sub.s64 	%rd185, %rd33, %rd328;
	shr.u64 	%rd186, %rd185, 3;
	setp.gt.u64 	%p40, %rd186, %rd184;
	shl.b64 	%rd187, %rd184, 3;
	add.s64 	%rd188, %rd328, %rd187;
	add.s64 	%rd189, %rd188, 8;
	selp.b64 	%rd328, %rd189, %rd33, %p40;
	selp.b64 	%rd330, %rd188, 0, %p40;
	mov.u64 	%rd329, 0;
	mov.pred 	%p83, %p81;
$L__BB35_26:
	add.s64 	%rd62, %rd326, %rd6;
	shl.b64 	%rd63, %rd62, 1;
	setp.lt.u64 	%p43, %rd63, 4096;
	@%p43 bra 	$L__BB35_28;
	bra.uni 	$L__BB35_27;
$L__BB35_28:
	setp.lt.u64 	%p5, %rd62, %rd326;
	setp.gt.u64 	%p44, %rd62, 2047;
	add.s64 	%rd200, %rd62, 1;
	selp.b64 	%rd201, 2048, %rd200, %p44;
	selp.b64 	%rd326, 2048, %rd201, %p5;
	cvt.u32.u64 	%r244, %rd63;
	shl.b32 	%r245, %r244, 3;
	add.s32 	%r242, %r245, %r7;
	add.s32 	%r243, %r242, 4;
	// begin inline asm
	ld.shared.f32 %r240, [%r242];
ld.shared.f32 %r241, [%r243];
	// end inline asm
	st.u32 	[%rd330], %r240;
	st.u32 	[%rd330+4], %r241;
	add.s64 	%rd322, %rd322, -1;
	setp.ne.s64 	%p45, %rd322, 0;
	mov.u64 	%rd318, %rd327;
	@%p45 bra 	$L__BB35_22;
$L__BB35_29:
	mov.u64 	%rd357, 0;
	mov.pred 	%p84, -1;
	mov.u64 	%rd358, %rd68;
	mov.pred 	%p87, %p84;
$L__BB35_30:
	setp.ne.s64 	%p47, %rd66, 0;
	@%p47 bra 	$L__BB35_34;
	bra.uni 	$L__BB35_31;
$L__BB35_34:
	@%p84 bra 	$L__BB35_37;
	bra.uni 	$L__BB35_35;
$L__BB35_37:
	setp.ne.s64 	%p48, %rd68, 0;
	@%p48 bra 	$L__BB35_39;
	bra.uni 	$L__BB35_38;
$L__BB35_39:
	sub.s64 	%rd203, %rd33, %rd350;
	shr.u64 	%rd204, %rd203, 3;
	setp.gt.u64 	%p49, %rd204, %rd68;
	shl.b64 	%rd205, %rd68, 3;
	add.s64 	%rd206, %rd350, %rd205;
	add.s64 	%rd207, %rd206, 8;
	selp.b64 	%rd350, %rd207, %rd33, %p49;
	bra.uni 	$L__BB35_40;
$L__BB35_31:
	selp.b64 	%rd71, 0, %rd6, %p84;
	setp.ne.s64 	%p70, %rd68, 0;
	@%p70 bra 	$L__BB35_33;
	bra.uni 	$L__BB35_32;
$L__BB35_33:
	add.s64 	%rd282, %rd71, %rd68;
	sub.s64 	%rd283, %rd33, %rd350;
	shr.u64 	%rd284, %rd283, 3;
	setp.gt.u64 	%p71, %rd284, %rd282;
	shl.b64 	%rd285, %rd282, 3;
	add.s64 	%rd286, %rd350, %rd285;
	add.s64 	%rd287, %rd286, 8;
	selp.b64 	%rd355, %rd287, %rd33, %p71;
	selp.b64 	%rd356, %rd286, 0, %p71;
	bra.uni 	$L__BB35_62;
$L__BB35_32:
	sub.s64 	%rd288, %rd33, %rd350;
	shr.u64 	%rd289, %rd288, 3;
	setp.gt.u64 	%p72, %rd289, %rd71;
	shl.b64 	%rd290, %rd71, 3;
	add.s64 	%rd291, %rd350, %rd290;
	add.s64 	%rd292, %rd291, 8;
	selp.b64 	%rd355, %rd292, %rd33, %p72;
	selp.b64 	%rd356, %rd291, 0, %p72;
	bra.uni 	$L__BB35_62;
$L__BB35_38:
	setp.eq.s64 	%p50, %rd350, %rd33;
	selp.b64 	%rd208, 0, 8, %p50;
	add.s64 	%rd350, %rd350, %rd208;
$L__BB35_40:
	add.s64 	%rd66, %rd66, -1;
	mov.u64 	%rd68, 0;
$L__BB35_35:
	setp.eq.s64 	%p51, %rd66, -1;
	@%p51 bra 	$L__BB35_41;
	add.s64 	%rd342, %rd66, 1;
	bra.uni 	$L__BB35_44;
$L__BB35_41:
	setp.ne.s64 	%p52, %rd68, 0;
	sub.s64 	%rd309, %rd33, %rd350;
	@%p52 bra 	$L__BB35_43;
	bra.uni 	$L__BB35_42;
$L__BB35_43:
	add.s64 	%rd212, %rd68, %rd6;
	shr.u64 	%rd214, %rd309, 3;
	setp.gt.u64 	%p53, %rd214, %rd212;
	add.s64 	%rd215, %rd68, %rd3;
	shl.b64 	%rd216, %rd215, 3;
	add.s64 	%rd217, %rd350, %rd216;
	selp.b64 	%rd350, %rd217, %rd33, %p53;
	mov.u64 	%rd342, -1;
	mov.u64 	%rd68, 0;
	bra.uni 	$L__BB35_44;
$L__BB35_42:
	shr.u64 	%rd221, %rd309, 3;
	setp.gt.u64 	%p54, %rd221, %rd6;
	add.s64 	%rd223, %rd350, %rd9;
	selp.b64 	%rd350, %rd223, %rd33, %p54;
	mov.u64 	%rd342, -1;
	mov.u64 	%rd68, 0;
$L__BB35_44:
	mul.lo.s64 	%rd352, %rd342, %rd3;
	mul.hi.u64 	%rd224, %rd342, %rd3;
	setp.eq.s64 	%p55, %rd224, 0;
	@%p55 bra 	$L__BB35_56;
	mov.u64 	%rd225, -1;
	div.u64 	%rd226, %rd225, %rd342;
	div.u64 	%rd227, %rd225, %rd3;
	mul.lo.s64 	%rd228, %rd226, %rd342;
	mul.lo.s64 	%rd229, %rd227, %rd3;
	max.u64 	%rd90, %rd228, %rd229;
	add.s64 	%rd344, %rd90, -1;
	setp.eq.s64 	%p57, %rd68, 0;
	@%p57 bra 	$L__BB35_50;
	add.s64 	%rd232, %rd68, %rd344;
	setp.lt.u64 	%p58, %rd232, %rd68;
	@%p58 bra 	$L__BB35_48;
	bra.uni 	$L__BB35_47;
$L__BB35_48:
	add.s64 	%rd233, %rd68, -1;
	sub.s64 	%rd234, %rd33, %rd350;
	shr.u64 	%rd235, %rd234, 3;
	setp.le.u64 	%p59, %rd235, %rd233;
	shl.b64 	%rd236, %rd68, 3;
	add.s64 	%rd343, %rd350, %rd236;
	mov.u64 	%rd350, %rd33;
	@%p59 bra 	$L__BB35_52;
	bra.uni 	$L__BB35_49;
$L__BB35_50:
	sub.s64 	%rd240, %rd33, %rd350;
	shr.u64 	%rd241, %rd240, 3;
	setp.gt.u64 	%p86, %rd241, %rd344;
	shl.b64 	%rd242, %rd90, 3;
	add.s64 	%rd243, %rd350, %rd242;
	add.s64 	%rd345, %rd243, -8;
	bra.uni 	$L__BB35_51;
$L__BB35_47:
	add.s64 	%rd344, %rd344, %rd68;
	mov.u64 	%rd343, %rd350;
$L__BB35_49:
	sub.s64 	%rd237, %rd33, %rd343;
	shr.u64 	%rd238, %rd237, 3;
	setp.gt.u64 	%p86, %rd238, %rd344;
	shl.b64 	%rd239, %rd344, 3;
	add.s64 	%rd345, %rd343, %rd239;
$L__BB35_51:
	add.s64 	%rd244, %rd345, 8;
	selp.b64 	%rd350, %rd244, %rd33, %p86;
$L__BB35_52:
	setp.gt.u64 	%p56, %rd228, %rd229;
	selp.b64 	%rd230, %rd226, 0, %p56;
	sub.s64 	%rd348, %rd3, %rd230;
	selp.b64 	%rd231, 0, %rd227, %p56;
	sub.s64 	%rd347, %rd342, %rd231;
	mul.lo.s64 	%rd352, %rd347, %rd348;
	mul.hi.u64 	%rd246, %rd347, %rd348;
	mov.u64 	%rd68, 0;
	setp.eq.s64 	%p60, %rd246, 0;
	@%p60 bra 	$L__BB35_56;
$L__BB35_53:
	setp.eq.s64 	%p61, %rd347, 0;
	@%p61 bra 	$L__BB35_70;
	setp.eq.s64 	%p62, %rd348, 0;
	@%p62 bra 	$L__BB35_71;
	div.u64 	%rd249, %rd225, %rd347;
	div.u64 	%rd250, %rd225, %rd348;
	mul.lo.s64 	%rd251, %rd249, %rd347;
	mul.lo.s64 	%rd252, %rd250, %rd348;
	setp.gt.u64 	%p63, %rd251, %rd252;
	selp.b64 	%rd253, %rd249, 0, %p63;
	sub.s64 	%rd348, %rd348, %rd253;
	selp.b64 	%rd254, 0, %rd250, %p63;
	sub.s64 	%rd347, %rd347, %rd254;
	max.u64 	%rd255, %rd251, %rd252;
	add.s64 	%rd256, %rd255, -1;
	shl.b64 	%rd257, %rd255, 3;
	add.s64 	%rd258, %rd350, %rd257;
	sub.s64 	%rd259, %rd33, %rd350;
	shr.u64 	%rd260, %rd259, 3;
	setp.gt.u64 	%p64, %rd260, %rd256;
	selp.b64 	%rd350, %rd258, %rd33, %p64;
	mul.lo.s64 	%rd352, %rd347, %rd348;
	mul.hi.u64 	%rd261, %rd347, %rd348;
	setp.ne.s64 	%p13, %rd261, 0;
	@%p13 bra 	$L__BB35_53;
$L__BB35_56:
	add.s64 	%rd354, %rd352, -1;
	setp.ne.s64 	%p65, %rd68, 0;
	@%p65 bra 	$L__BB35_58;
	bra.uni 	$L__BB35_57;
$L__BB35_58:
	add.s64 	%rd262, %rd68, %rd354;
	setp.lt.u64 	%p66, %rd262, %rd68;
	@%p66 bra 	$L__BB35_60;
	bra.uni 	$L__BB35_59;
$L__BB35_60:
	add.s64 	%rd264, %rd68, -1;
	sub.s64 	%rd265, %rd33, %rd350;
	shr.u64 	%rd266, %rd265, 3;
	setp.le.u64 	%p67, %rd266, %rd264;
	shl.b64 	%rd267, %rd68, 3;
	add.s64 	%rd350, %rd350, %rd267;
	mov.u64 	%rd356, 0;
	mov.u64 	%rd355, %rd33;
	@%p67 bra 	$L__BB35_62;
	bra.uni 	$L__BB35_61;
$L__BB35_57:
	sub.s64 	%rd273, %rd33, %rd350;
	shr.u64 	%rd274, %rd273, 3;
	setp.gt.u64 	%p69, %rd274, %rd354;
	shl.b64 	%rd275, %rd352, 3;
	add.s64 	%rd276, %rd350, %rd275;
	add.s64 	%rd277, %rd276, -8;
	selp.b64 	%rd355, %rd276, %rd33, %p69;
	selp.b64 	%rd356, %rd277, 0, %p69;
	bra.uni 	$L__BB35_62;
$L__BB35_59:
	add.s64 	%rd354, %rd354, %rd68;
$L__BB35_61:
	sub.s64 	%rd268, %rd33, %rd350;
	shr.u64 	%rd269, %rd268, 3;
	setp.gt.u64 	%p68, %rd269, %rd354;
	shl.b64 	%rd270, %rd354, 3;
	add.s64 	%rd271, %rd350, %rd270;
	add.s64 	%rd272, %rd271, 8;
	selp.b64 	%rd355, %rd272, %rd33, %p68;
	selp.b64 	%rd356, %rd271, 0, %p68;
$L__BB35_62:
	setp.eq.s64 	%p73, %rd356, 0;
	mov.u64 	%rd360, 0;
	@%p73 bra 	$L__BB35_64;
	selp.b64 	%rd296, 0, %rd6, %p87;
	add.s64 	%rd298, %rd296, %rd358;
	add.s64 	%rd359, %rd298, %rd357;
	setp.lt.u64 	%p75, %rd359, %rd357;
	setp.gt.u64 	%p76, %rd359, 2047;
	or.pred  	%p77, %p75, %p76;
	add.s64 	%rd299, %rd359, 1;
	selp.b64 	%rd357, 2048, %rd299, %p77;
	selp.b64 	%rd360, 0, %rd356, %p77;
	mov.pred 	%p87, 0;
	mov.u64 	%rd358, 0;
$L__BB35_64:
	setp.eq.s64 	%p78, %rd360, 0;
	@%p78 bra 	$L__BB35_68;
	shl.b64 	%rd133, %rd359, 1;
	setp.lt.u64 	%p79, %rd133, 4096;
	@%p79 bra 	$L__BB35_67;
	bra.uni 	$L__BB35_66;
$L__BB35_67:
	cvt.u32.u64 	%r250, %rd133;
	shl.b32 	%r251, %r250, 3;
	add.s32 	%r248, %r251, %r8;
	add.s32 	%r249, %r248, 4;
	// begin inline asm
	ld.shared.f32 %r246, [%r248];
ld.shared.f32 %r247, [%r249];
	// end inline asm
	st.u32 	[%rd360], %r246;
	st.u32 	[%rd360+4], %r247;
	mov.pred 	%p84, 0;
	mov.u64 	%rd66, 0;
	mov.u64 	%rd350, %rd355;
	mov.u64 	%rd68, %rd66;
	bra.uni 	$L__BB35_30;
$L__BB35_68:
	ret;
$L__BB35_70:
	mov.u64 	%rd280, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_1;
	cvta.global.u64 	%rd281, %rd280;
	{ // callseq 200, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd281;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 200
$L__BB35_71:
	mov.u64 	%rd278, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_2;
	cvta.global.u64 	%rd279, %rd278;
	{ // callseq 199, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd279;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 199
$L__BB35_4:
	mov.u64 	%rd148, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd149, %rd148;
	mov.u64 	%rd150, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd151, %rd150;
	{ // callseq 196, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd149;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd151;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 196
$L__BB35_66:
	mov.u64 	%rd300, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd301, %rd300;
	mov.u64 	%rd302, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd303, %rd302;
	{ // callseq 201, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd301;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd303;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 201
$L__BB35_27:
	mov.u64 	%rd196, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd197, %rd196;
	mov.u64 	%rd198, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd199, %rd198;
	{ // callseq 198, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd197;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd199;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 198
$L__BB35_1:
	mov.u64 	%rd307, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_12;
	cvta.global.u64 	%rd308, %rd307;
	{ // callseq 203, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd308;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 203
$L__BB35_7:
	mov.u64 	%rd305, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_16;
	cvta.global.u64 	%rd306, %rd305;
	{ // callseq 202, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd306;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 202
$L__BB35_10:
	mov.u64 	%rd162, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_13;
	cvta.global.u64 	%rd163, %rd162;
	{ // callseq 197, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd163;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 197

}
	// .globl	daubechies_first_backward_128_kernel
.visible .entry daubechies_first_backward_128_kernel(
	.param .u64 daubechies_first_backward_128_kernel_param_0
)
{
	.reg .pred 	%p<39>;
	.reg .b32 	%r<251>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<160>;

	ld.param.u64 	%rd74, [daubechies_first_backward_128_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd74;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[1024];
    mov.u32 %r5, nonphysical;
	// end inline asm
	mov.u32 	%r8, %ctaid.x;
	ld.global.nc.u64 	%rd5, [%rd1+16];
	ld.global.nc.u64 	%rd75, [%rd1+24];
	and.b64  	%rd76, %rd75, -128;
	mul.wide.u32 	%rd6, %r8, 128;
	setp.ge.u64 	%p3, %rd6, %rd76;
	sub.s64 	%rd78, %rd75, %rd6;
	setp.lt.u64 	%p4, %rd78, 128;
	or.pred  	%p5, %p3, %p4;
	@!%p5 bra 	$L__BB36_2;
	bra.uni 	$L__BB36_1;
$L__BB36_2:
	mov.u32 	%r6, %tid.x;
	cvt.u64.u32 	%rd2, %r6;
	mov.u32 	%r7, %ntid.x;
	cvt.u64.u32 	%rd3, %r7;
	cvt.u32.u64 	%r9, %rd2;
	max.u64 	%rd81, %rd2, 128;
	setp.gt.u32 	%p6, %r9, 127;
	not.b64 	%rd82, %rd2;
	add.s64 	%rd9, %rd82, %rd81;
	mov.u64 	%rd146, 0;
	and.b64  	%rd142, %rd9, -4294967296;
	mov.u64 	%rd144, %rd146;
	@%p6 bra 	$L__BB36_7;
	setp.ne.s64 	%p7, %rd142, 0;
	@%p7 bra 	$L__BB36_5;
	bra.uni 	$L__BB36_4;
$L__BB36_5:
	div.u64 	%rd143, %rd9, %rd3;
	bra.uni 	$L__BB36_6;
$L__BB36_4:
	cvt.u32.u64 	%r10, %rd3;
	cvt.u32.u64 	%r11, %rd9;
	div.u32 	%r12, %r11, %r10;
	cvt.u64.u32 	%rd143, %r12;
$L__BB36_6:
	add.s64 	%rd144, %rd143, 1;
$L__BB36_7:
	@%p6 bra 	$L__BB36_12;
	setp.ne.s64 	%p9, %rd142, 0;
	@%p9 bra 	$L__BB36_10;
	bra.uni 	$L__BB36_9;
$L__BB36_10:
	div.u64 	%rd145, %rd9, %rd3;
	bra.uni 	$L__BB36_11;
$L__BB36_9:
	cvt.u32.u64 	%r14, %rd3;
	cvt.u32.u64 	%r15, %rd9;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd145, %r16;
$L__BB36_11:
	add.s64 	%rd146, %rd145, 1;
$L__BB36_12:
	shl.b64 	%rd77, %rd6, 3;
	cvt.u64.u32 	%rd4, %r8;
	add.s64 	%rd8, %rd3, -1;
	min.u64 	%rd20, %rd144, %rd146;
	setp.eq.s64 	%p10, %rd20, 0;
	shl.b64 	%rd139, %rd2, 3;
	@%p10 bra 	$L__BB36_18;
	add.s64 	%rd7, %rd5, %rd77;
	add.s64 	%rd87, %rd7, %rd139;
	shl.b32 	%r22, %r9, 3;
	add.s32 	%r17, %r5, %r22;
	add.s32 	%r18, %r17, 4;
	ld.u32 	%r19, [%rd87];
	ld.u32 	%r20, [%rd87+4];
	// begin inline asm
	st.shared.f32 [%r17], %r19;
st.shared.f32 [%r18], %r20;
	// end inline asm
	setp.eq.s64 	%p11, %rd20, 1;
	@%p11 bra 	$L__BB36_18;
	add.s64 	%rd150, %rd2, 1;
	add.s64 	%rd149, %rd20, -1;
	shl.b64 	%rd88, %rd4, 10;
	shl.b64 	%rd23, %rd3, 3;
	add.s64 	%rd89, %rd88, %rd23;
	add.s64 	%rd91, %rd89, %rd139;
	add.s64 	%rd148, %rd5, %rd91;
	mov.u64 	%rd92, 1016;
	sub.s64 	%rd147, %rd92, %rd139;
$L__BB36_15:
	add.s64 	%rd31, %rd150, %rd8;
	setp.lt.u64 	%p13, %rd31, 128;
	@%p13 bra 	$L__BB36_17;
	bra.uni 	$L__BB36_16;
$L__BB36_17:
	shr.u64 	%rd93, %rd147, 3;
	setp.gt.u64 	%p12, %rd93, %rd8;
	selp.b64 	%rd30, %rd148, 0, %p12;
	setp.lt.u64 	%p1, %rd31, %rd150;
	add.s64 	%rd98, %rd150, %rd3;
	selp.b64 	%rd150, 128, %rd98, %p1;
	cvt.u32.u64 	%r27, %rd31;
	shl.b32 	%r28, %r27, 3;
	add.s32 	%r23, %r28, %r5;
	add.s32 	%r24, %r23, 4;
	ld.u32 	%r25, [%rd30];
	ld.u32 	%r26, [%rd30+4];
	// begin inline asm
	st.shared.f32 [%r23], %r25;
st.shared.f32 [%r24], %r26;
	// end inline asm
	add.s64 	%rd149, %rd149, -1;
	add.s64 	%rd148, %rd148, %rd23;
	sub.s64 	%rd147, %rd147, %rd23;
	setp.ne.s64 	%p14, %rd149, 0;
	@%p14 bra 	$L__BB36_15;
$L__BB36_18:
	ld.global.nc.u64 	%rd36, [%rd1];
	ld.global.nc.u64 	%rd37, [%rd1+8];
	bar.sync 	0;
	setp.gt.u32 	%p15, %r9, 63;
	@%p15 bra 	$L__BB36_24;
	shl.b32 	%r2, %r9, 3;
	add.s32 	%r81, %r5, %r2;
	add.s32 	%r82, %r81, 4;
	// begin inline asm
	ld.shared.f32 %r36, [%r81];
ld.shared.f32 %r39, [%r82];
	// end inline asm
	setp.eq.s64 	%p16, %rd37, 0;
	@%p16 bra 	$L__BB36_20;
	mov.b32 	%f2, %r39;
	mov.b32 	%f1, %r36;
	ld.u32 	%r37, [%rd36];
	ld.u32 	%r40, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r35, %r36, %r37;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r38, %r39, %r40;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r76, %r35, %r38;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r44, %r36, %r40;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r47, %r39, %r37;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r79, %r44, %r47;
	// end inline asm
	add.s32 	%r91, %r81, 512;
	add.s32 	%r92, %r81, 516;
	// begin inline asm
	ld.shared.f32 %r58, [%r91];
ld.shared.f32 %r61, [%r92];
	// end inline asm
	ld.u32 	%r71, [%rd36];
	ld.u32 	%r68, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r57, %r58, %r71;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r60, %r61, %r68;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r77, %r57, %r60;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r66, %r58, %r68;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r69, %r61, %r71;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r66, %r69;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r75, %r76, %r77;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r78, %r79, %r80;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r81], %r75;
st.shared.f32 [%r82], %r78;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r85, %r76, %r77;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r88, %r79, %r80;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r91], %r85;
st.shared.f32 [%r92], %r88;
	// end inline asm
	add.s64 	%rd41, %rd2, %rd3;
	setp.gt.u64 	%p17, %rd41, 63;
	@%p17 bra 	$L__BB36_24;
	cvt.u32.u64 	%r160, %rd41;
	shl.b32 	%r161, %r160, 3;
	add.s32 	%r98, %r5, %r161;
	add.s32 	%r99, %r98, 4;
	// begin inline asm
	ld.shared.f32 %r101, [%r98];
ld.shared.f32 %r104, [%r99];
	// end inline asm
	ld.u32 	%r114, [%rd36];
	ld.u32 	%r111, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r100, %r101, %r114;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r103, %r104, %r111;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r100, %r103;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r109, %r101, %r111;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r112, %r104, %r114;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r144, %r109, %r112;
	// end inline asm
	add.s32 	%r156, %r98, 512;
	add.s32 	%r157, %r98, 516;
	// begin inline asm
	ld.shared.f32 %r123, [%r156];
ld.shared.f32 %r126, [%r157];
	// end inline asm
	ld.u32 	%r136, [%rd36];
	ld.u32 	%r133, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r122, %r123, %r136;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r125, %r126, %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r142, %r122, %r125;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r131, %r123, %r133;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r134, %r126, %r136;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r145, %r131, %r134;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r140, %r141, %r142;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r143, %r144, %r145;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r98], %r140;
st.shared.f32 [%r99], %r143;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r150, %r141, %r142;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r153, %r144, %r145;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r156], %r150;
st.shared.f32 [%r157], %r153;
	// end inline asm
	add.s64 	%rd151, %rd41, %rd3;
	setp.gt.u64 	%p18, %rd151, 63;
	@%p18 bra 	$L__BB36_24;
$L__BB36_22:
	add.s64 	%rd99, %rd151, 1;
	cvt.u32.u64 	%r226, %rd151;
	shl.b32 	%r227, %r226, 3;
	add.s32 	%r164, %r227, %r5;
	add.s32 	%r165, %r164, 4;
	// begin inline asm
	ld.shared.f32 %r167, [%r164];
ld.shared.f32 %r170, [%r165];
	// end inline asm
	ld.u32 	%r180, [%rd36];
	ld.u32 	%r177, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r166, %r167, %r180;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r169, %r170, %r177;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r207, %r166, %r169;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r175, %r167, %r177;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r178, %r170, %r180;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r210, %r175, %r178;
	// end inline asm
	add.s32 	%r186, %r164, 512;
	add.s32 	%r223, %r164, 516;
	// begin inline asm
	ld.shared.f32 %r189, [%r186];
ld.shared.f32 %r192, [%r223];
	// end inline asm
	ld.u32 	%r202, [%rd36];
	ld.u32 	%r199, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r188, %r189, %r202;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r191, %r192, %r199;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r208, %r188, %r191;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r197, %r189, %r199;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r200, %r192, %r202;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r211, %r197, %r200;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r206, %r207, %r208;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r209, %r210, %r211;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r164], %r206;
st.shared.f32 [%r165], %r209;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r216, %r207, %r208;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r219, %r210, %r211;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r186], %r216;
st.shared.f32 [%r223], %r219;
	// end inline asm
	add.s64 	%rd100, %rd99, %rd8;
	setp.lt.u64 	%p19, %rd100, %rd99;
	add.s64 	%rd151, %rd151, %rd3;
	setp.gt.u64 	%p20, %rd151, 63;
	or.pred  	%p21, %p19, %p20;
	@%p21 bra 	$L__BB36_24;
	bra.uni 	$L__BB36_22;
$L__BB36_24:
	ld.global.nc.u64 	%rd42, [%rd1+32];
	ld.global.nc.u64 	%rd104, [%rd1+40];
	and.b64  	%rd105, %rd104, -128;
	setp.lt.u64 	%p22, %rd6, %rd105;
	sub.s64 	%rd107, %rd104, %rd6;
	setp.gt.u64 	%p23, %rd107, 127;
	and.pred  	%p24, %p22, %p23;
	@%p24 bra 	$L__BB36_26;
	bra.uni 	$L__BB36_25;
$L__BB36_26:
	bar.sync 	0;
	mov.u64 	%rd155, 0;
	mov.u64 	%rd153, %rd155;
	@%p6 bra 	$L__BB36_31;
	setp.ne.s64 	%p26, %rd142, 0;
	@%p26 bra 	$L__BB36_29;
	bra.uni 	$L__BB36_28;
$L__BB36_29:
	div.u64 	%rd152, %rd9, %rd3;
	bra.uni 	$L__BB36_30;
$L__BB36_28:
	cvt.u32.u64 	%r229, %rd3;
	cvt.u32.u64 	%r230, %rd9;
	div.u32 	%r231, %r230, %r229;
	cvt.u64.u32 	%rd152, %r231;
$L__BB36_30:
	add.s64 	%rd153, %rd152, 1;
$L__BB36_31:
	@%p6 bra 	$L__BB36_36;
	setp.ne.s64 	%p28, %rd142, 0;
	@%p28 bra 	$L__BB36_34;
	bra.uni 	$L__BB36_33;
$L__BB36_34:
	div.u64 	%rd154, %rd9, %rd3;
	bra.uni 	$L__BB36_35;
$L__BB36_33:
	cvt.u32.u64 	%r233, %rd3;
	cvt.u32.u64 	%r234, %rd9;
	div.u32 	%r235, %r234, %r233;
	cvt.u64.u32 	%rd154, %r235;
$L__BB36_35:
	add.s64 	%rd155, %rd154, 1;
$L__BB36_36:
	min.u64 	%rd55, %rd153, %rd155;
	setp.eq.s64 	%p29, %rd55, 0;
	@%p29 bra 	$L__BB36_43;
	shr.u64 	%rd112, %rd2, 1;
	shl.b64 	%rd113, %rd2, 6;
	and.b64  	%rd114, %rd113, 64;
	add.s64 	%rd56, %rd114, %rd112;
	setp.gt.u64 	%p30, %rd56, 127;
	@%p30 bra 	$L__BB36_41;
	add.s64 	%rd43, %rd42, %rd77;
	add.s64 	%rd57, %rd43, %rd139;
	cvt.u32.u64 	%r240, %rd56;
	shl.b32 	%r241, %r240, 3;
	add.s32 	%r238, %r5, %r241;
	add.s32 	%r239, %r238, 4;
	// begin inline asm
	ld.shared.f32 %r236, [%r238];
ld.shared.f32 %r237, [%r239];
	// end inline asm
	st.u32 	[%rd57], %r236;
	st.u32 	[%rd57+4], %r237;
	setp.eq.s64 	%p31, %rd55, 1;
	@%p31 bra 	$L__BB36_43;
	add.s64 	%rd44, %rd43, 1024;
	setp.lt.u32 	%p32, %r9, 128;
	setp.eq.s32 	%p33, %r9, 0;
	add.s64 	%rd116, %rd43, 8;
	add.s64 	%rd117, %rd57, 8;
	selp.b64 	%rd118, %rd117, %rd44, %p32;
	selp.b64 	%rd119, %rd116, %rd118, %p33;
	add.s64 	%rd120, %rd2, 1;
	selp.b64 	%rd159, 128, %rd120, %p6;
	shl.b64 	%rd59, %rd3, 3;
	add.s64 	%rd121, %rd59, %rd119;
	add.s64 	%rd158, %rd121, -8;
	shl.b64 	%rd122, %rd4, 10;
	add.s64 	%rd123, %rd42, %rd122;
	sub.s64 	%rd124, %rd123, %rd119;
	add.s64 	%rd157, %rd124, 1024;
	add.s64 	%rd156, %rd55, -1;
$L__BB36_40:
	add.s64 	%rd68, %rd159, %rd8;
	shr.u64 	%rd126, %rd68, 1;
	shl.b64 	%rd127, %rd68, 6;
	and.b64  	%rd128, %rd127, 64;
	add.s64 	%rd69, %rd128, %rd126;
	setp.lt.u64 	%p36, %rd69, 128;
	@%p36 bra 	$L__BB36_42;
	bra.uni 	$L__BB36_41;
$L__BB36_42:
	shr.u64 	%rd125, %rd157, 3;
	setp.gt.u64 	%p35, %rd125, %rd8;
	selp.b64 	%rd67, %rd158, 0, %p35;
	setp.lt.u64 	%p2, %rd68, %rd159;
	setp.gt.u64 	%p37, %rd68, 127;
	add.s64 	%rd133, %rd68, 1;
	selp.b64 	%rd134, 128, %rd133, %p37;
	selp.b64 	%rd159, 128, %rd134, %p2;
	cvt.u32.u64 	%r247, %rd69;
	shl.b32 	%r248, %r247, 3;
	add.s32 	%r245, %r248, %r5;
	add.s32 	%r246, %r245, 4;
	// begin inline asm
	ld.shared.f32 %r243, [%r245];
ld.shared.f32 %r244, [%r246];
	// end inline asm
	st.u32 	[%rd67], %r243;
	st.u32 	[%rd67+4], %r244;
	add.s64 	%rd158, %rd158, %rd59;
	sub.s64 	%rd157, %rd157, %rd59;
	add.s64 	%rd156, %rd156, -1;
	setp.ne.s64 	%p38, %rd156, 0;
	@%p38 bra 	$L__BB36_40;
$L__BB36_43:
	ret;
$L__BB36_41:
	mov.u64 	%rd129, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd130, %rd129;
	mov.u64 	%rd131, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd132, %rd131;
	{ // callseq 206, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd130;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd132;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 206
$L__BB36_16:
	mov.u64 	%rd94, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd95, %rd94;
	mov.u64 	%rd96, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd97, %rd96;
	{ // callseq 204, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd95;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd97;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 204
$L__BB36_1:
	mov.u64 	%rd137, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_14;
	cvta.global.u64 	%rd138, %rd137;
	{ // callseq 208, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd138;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 208
$L__BB36_25:
	mov.u64 	%rd135, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_15;
	cvta.global.u64 	%rd136, %rd135;
	{ // callseq 207, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd136;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 207
$L__BB36_20:
	mov.u64 	%rd102, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_17;
	cvta.global.u64 	%rd103, %rd102;
	{ // callseq 205, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd103;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 205

}
	// .globl	daubechies_first_backward_256_kernel
.visible .entry daubechies_first_backward_256_kernel(
	.param .u64 daubechies_first_backward_256_kernel_param_0
)
{
	.reg .pred 	%p<39>;
	.reg .b32 	%r<251>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<160>;

	ld.param.u64 	%rd74, [daubechies_first_backward_256_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd74;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[2048];
    mov.u32 %r5, nonphysical;
	// end inline asm
	mov.u32 	%r8, %ctaid.x;
	ld.global.nc.u64 	%rd5, [%rd1+16];
	ld.global.nc.u64 	%rd75, [%rd1+24];
	and.b64  	%rd76, %rd75, -256;
	mul.wide.u32 	%rd6, %r8, 256;
	setp.ge.u64 	%p3, %rd6, %rd76;
	sub.s64 	%rd78, %rd75, %rd6;
	setp.lt.u64 	%p4, %rd78, 256;
	or.pred  	%p5, %p3, %p4;
	@!%p5 bra 	$L__BB37_2;
	bra.uni 	$L__BB37_1;
$L__BB37_2:
	mov.u32 	%r6, %tid.x;
	cvt.u64.u32 	%rd2, %r6;
	mov.u32 	%r7, %ntid.x;
	cvt.u64.u32 	%rd3, %r7;
	cvt.u32.u64 	%r9, %rd2;
	max.u64 	%rd81, %rd2, 256;
	setp.gt.u32 	%p6, %r9, 255;
	not.b64 	%rd82, %rd2;
	add.s64 	%rd9, %rd82, %rd81;
	mov.u64 	%rd146, 0;
	and.b64  	%rd142, %rd9, -4294967296;
	mov.u64 	%rd144, %rd146;
	@%p6 bra 	$L__BB37_7;
	setp.ne.s64 	%p7, %rd142, 0;
	@%p7 bra 	$L__BB37_5;
	bra.uni 	$L__BB37_4;
$L__BB37_5:
	div.u64 	%rd143, %rd9, %rd3;
	bra.uni 	$L__BB37_6;
$L__BB37_4:
	cvt.u32.u64 	%r10, %rd3;
	cvt.u32.u64 	%r11, %rd9;
	div.u32 	%r12, %r11, %r10;
	cvt.u64.u32 	%rd143, %r12;
$L__BB37_6:
	add.s64 	%rd144, %rd143, 1;
$L__BB37_7:
	@%p6 bra 	$L__BB37_12;
	setp.ne.s64 	%p9, %rd142, 0;
	@%p9 bra 	$L__BB37_10;
	bra.uni 	$L__BB37_9;
$L__BB37_10:
	div.u64 	%rd145, %rd9, %rd3;
	bra.uni 	$L__BB37_11;
$L__BB37_9:
	cvt.u32.u64 	%r14, %rd3;
	cvt.u32.u64 	%r15, %rd9;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd145, %r16;
$L__BB37_11:
	add.s64 	%rd146, %rd145, 1;
$L__BB37_12:
	shl.b64 	%rd77, %rd6, 3;
	cvt.u64.u32 	%rd4, %r8;
	add.s64 	%rd8, %rd3, -1;
	min.u64 	%rd20, %rd144, %rd146;
	setp.eq.s64 	%p10, %rd20, 0;
	shl.b64 	%rd139, %rd2, 3;
	@%p10 bra 	$L__BB37_18;
	add.s64 	%rd7, %rd5, %rd77;
	add.s64 	%rd87, %rd7, %rd139;
	shl.b32 	%r22, %r9, 3;
	add.s32 	%r17, %r5, %r22;
	add.s32 	%r18, %r17, 4;
	ld.u32 	%r19, [%rd87];
	ld.u32 	%r20, [%rd87+4];
	// begin inline asm
	st.shared.f32 [%r17], %r19;
st.shared.f32 [%r18], %r20;
	// end inline asm
	setp.eq.s64 	%p11, %rd20, 1;
	@%p11 bra 	$L__BB37_18;
	add.s64 	%rd150, %rd2, 1;
	add.s64 	%rd149, %rd20, -1;
	shl.b64 	%rd88, %rd4, 11;
	shl.b64 	%rd23, %rd3, 3;
	add.s64 	%rd89, %rd88, %rd23;
	add.s64 	%rd91, %rd89, %rd139;
	add.s64 	%rd148, %rd5, %rd91;
	mov.u64 	%rd92, 2040;
	sub.s64 	%rd147, %rd92, %rd139;
$L__BB37_15:
	add.s64 	%rd31, %rd150, %rd8;
	setp.lt.u64 	%p13, %rd31, 256;
	@%p13 bra 	$L__BB37_17;
	bra.uni 	$L__BB37_16;
$L__BB37_17:
	shr.u64 	%rd93, %rd147, 3;
	setp.gt.u64 	%p12, %rd93, %rd8;
	selp.b64 	%rd30, %rd148, 0, %p12;
	setp.lt.u64 	%p1, %rd31, %rd150;
	add.s64 	%rd98, %rd150, %rd3;
	selp.b64 	%rd150, 256, %rd98, %p1;
	cvt.u32.u64 	%r27, %rd31;
	shl.b32 	%r28, %r27, 3;
	add.s32 	%r23, %r28, %r5;
	add.s32 	%r24, %r23, 4;
	ld.u32 	%r25, [%rd30];
	ld.u32 	%r26, [%rd30+4];
	// begin inline asm
	st.shared.f32 [%r23], %r25;
st.shared.f32 [%r24], %r26;
	// end inline asm
	add.s64 	%rd149, %rd149, -1;
	add.s64 	%rd148, %rd148, %rd23;
	sub.s64 	%rd147, %rd147, %rd23;
	setp.ne.s64 	%p14, %rd149, 0;
	@%p14 bra 	$L__BB37_15;
$L__BB37_18:
	ld.global.nc.u64 	%rd36, [%rd1];
	ld.global.nc.u64 	%rd37, [%rd1+8];
	bar.sync 	0;
	setp.gt.u32 	%p15, %r9, 127;
	@%p15 bra 	$L__BB37_24;
	shl.b32 	%r2, %r9, 3;
	add.s32 	%r81, %r5, %r2;
	add.s32 	%r82, %r81, 4;
	// begin inline asm
	ld.shared.f32 %r36, [%r81];
ld.shared.f32 %r39, [%r82];
	// end inline asm
	setp.eq.s64 	%p16, %rd37, 0;
	@%p16 bra 	$L__BB37_20;
	mov.b32 	%f2, %r39;
	mov.b32 	%f1, %r36;
	ld.u32 	%r37, [%rd36];
	ld.u32 	%r40, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r35, %r36, %r37;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r38, %r39, %r40;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r76, %r35, %r38;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r44, %r36, %r40;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r47, %r39, %r37;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r79, %r44, %r47;
	// end inline asm
	add.s32 	%r91, %r81, 1024;
	add.s32 	%r92, %r81, 1028;
	// begin inline asm
	ld.shared.f32 %r58, [%r91];
ld.shared.f32 %r61, [%r92];
	// end inline asm
	ld.u32 	%r71, [%rd36];
	ld.u32 	%r68, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r57, %r58, %r71;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r60, %r61, %r68;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r77, %r57, %r60;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r66, %r58, %r68;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r69, %r61, %r71;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r66, %r69;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r75, %r76, %r77;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r78, %r79, %r80;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r81], %r75;
st.shared.f32 [%r82], %r78;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r85, %r76, %r77;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r88, %r79, %r80;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r91], %r85;
st.shared.f32 [%r92], %r88;
	// end inline asm
	add.s64 	%rd41, %rd2, %rd3;
	setp.gt.u64 	%p17, %rd41, 127;
	@%p17 bra 	$L__BB37_24;
	cvt.u32.u64 	%r160, %rd41;
	shl.b32 	%r161, %r160, 3;
	add.s32 	%r98, %r5, %r161;
	add.s32 	%r99, %r98, 4;
	// begin inline asm
	ld.shared.f32 %r101, [%r98];
ld.shared.f32 %r104, [%r99];
	// end inline asm
	ld.u32 	%r114, [%rd36];
	ld.u32 	%r111, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r100, %r101, %r114;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r103, %r104, %r111;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r100, %r103;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r109, %r101, %r111;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r112, %r104, %r114;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r144, %r109, %r112;
	// end inline asm
	add.s32 	%r156, %r98, 1024;
	add.s32 	%r157, %r98, 1028;
	// begin inline asm
	ld.shared.f32 %r123, [%r156];
ld.shared.f32 %r126, [%r157];
	// end inline asm
	ld.u32 	%r136, [%rd36];
	ld.u32 	%r133, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r122, %r123, %r136;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r125, %r126, %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r142, %r122, %r125;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r131, %r123, %r133;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r134, %r126, %r136;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r145, %r131, %r134;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r140, %r141, %r142;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r143, %r144, %r145;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r98], %r140;
st.shared.f32 [%r99], %r143;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r150, %r141, %r142;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r153, %r144, %r145;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r156], %r150;
st.shared.f32 [%r157], %r153;
	// end inline asm
	add.s64 	%rd151, %rd41, %rd3;
	setp.gt.u64 	%p18, %rd151, 127;
	@%p18 bra 	$L__BB37_24;
$L__BB37_22:
	add.s64 	%rd99, %rd151, 1;
	cvt.u32.u64 	%r226, %rd151;
	shl.b32 	%r227, %r226, 3;
	add.s32 	%r164, %r227, %r5;
	add.s32 	%r165, %r164, 4;
	// begin inline asm
	ld.shared.f32 %r167, [%r164];
ld.shared.f32 %r170, [%r165];
	// end inline asm
	ld.u32 	%r180, [%rd36];
	ld.u32 	%r177, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r166, %r167, %r180;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r169, %r170, %r177;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r207, %r166, %r169;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r175, %r167, %r177;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r178, %r170, %r180;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r210, %r175, %r178;
	// end inline asm
	add.s32 	%r186, %r164, 1024;
	add.s32 	%r223, %r164, 1028;
	// begin inline asm
	ld.shared.f32 %r189, [%r186];
ld.shared.f32 %r192, [%r223];
	// end inline asm
	ld.u32 	%r202, [%rd36];
	ld.u32 	%r199, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r188, %r189, %r202;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r191, %r192, %r199;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r208, %r188, %r191;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r197, %r189, %r199;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r200, %r192, %r202;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r211, %r197, %r200;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r206, %r207, %r208;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r209, %r210, %r211;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r164], %r206;
st.shared.f32 [%r165], %r209;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r216, %r207, %r208;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r219, %r210, %r211;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r186], %r216;
st.shared.f32 [%r223], %r219;
	// end inline asm
	add.s64 	%rd100, %rd99, %rd8;
	setp.lt.u64 	%p19, %rd100, %rd99;
	add.s64 	%rd151, %rd151, %rd3;
	setp.gt.u64 	%p20, %rd151, 127;
	or.pred  	%p21, %p19, %p20;
	@%p21 bra 	$L__BB37_24;
	bra.uni 	$L__BB37_22;
$L__BB37_24:
	ld.global.nc.u64 	%rd42, [%rd1+32];
	ld.global.nc.u64 	%rd104, [%rd1+40];
	and.b64  	%rd105, %rd104, -256;
	setp.lt.u64 	%p22, %rd6, %rd105;
	sub.s64 	%rd107, %rd104, %rd6;
	setp.gt.u64 	%p23, %rd107, 255;
	and.pred  	%p24, %p22, %p23;
	@%p24 bra 	$L__BB37_26;
	bra.uni 	$L__BB37_25;
$L__BB37_26:
	bar.sync 	0;
	mov.u64 	%rd155, 0;
	mov.u64 	%rd153, %rd155;
	@%p6 bra 	$L__BB37_31;
	setp.ne.s64 	%p26, %rd142, 0;
	@%p26 bra 	$L__BB37_29;
	bra.uni 	$L__BB37_28;
$L__BB37_29:
	div.u64 	%rd152, %rd9, %rd3;
	bra.uni 	$L__BB37_30;
$L__BB37_28:
	cvt.u32.u64 	%r229, %rd3;
	cvt.u32.u64 	%r230, %rd9;
	div.u32 	%r231, %r230, %r229;
	cvt.u64.u32 	%rd152, %r231;
$L__BB37_30:
	add.s64 	%rd153, %rd152, 1;
$L__BB37_31:
	@%p6 bra 	$L__BB37_36;
	setp.ne.s64 	%p28, %rd142, 0;
	@%p28 bra 	$L__BB37_34;
	bra.uni 	$L__BB37_33;
$L__BB37_34:
	div.u64 	%rd154, %rd9, %rd3;
	bra.uni 	$L__BB37_35;
$L__BB37_33:
	cvt.u32.u64 	%r233, %rd3;
	cvt.u32.u64 	%r234, %rd9;
	div.u32 	%r235, %r234, %r233;
	cvt.u64.u32 	%rd154, %r235;
$L__BB37_35:
	add.s64 	%rd155, %rd154, 1;
$L__BB37_36:
	min.u64 	%rd55, %rd153, %rd155;
	setp.eq.s64 	%p29, %rd55, 0;
	@%p29 bra 	$L__BB37_43;
	shr.u64 	%rd112, %rd2, 1;
	shl.b64 	%rd113, %rd2, 7;
	and.b64  	%rd114, %rd113, 128;
	add.s64 	%rd56, %rd114, %rd112;
	setp.gt.u64 	%p30, %rd56, 255;
	@%p30 bra 	$L__BB37_41;
	add.s64 	%rd43, %rd42, %rd77;
	add.s64 	%rd57, %rd43, %rd139;
	cvt.u32.u64 	%r240, %rd56;
	shl.b32 	%r241, %r240, 3;
	add.s32 	%r238, %r5, %r241;
	add.s32 	%r239, %r238, 4;
	// begin inline asm
	ld.shared.f32 %r236, [%r238];
ld.shared.f32 %r237, [%r239];
	// end inline asm
	st.u32 	[%rd57], %r236;
	st.u32 	[%rd57+4], %r237;
	setp.eq.s64 	%p31, %rd55, 1;
	@%p31 bra 	$L__BB37_43;
	add.s64 	%rd44, %rd43, 2048;
	setp.lt.u32 	%p32, %r9, 256;
	setp.eq.s32 	%p33, %r9, 0;
	add.s64 	%rd116, %rd43, 8;
	add.s64 	%rd117, %rd57, 8;
	selp.b64 	%rd118, %rd117, %rd44, %p32;
	selp.b64 	%rd119, %rd116, %rd118, %p33;
	add.s64 	%rd120, %rd2, 1;
	selp.b64 	%rd159, 256, %rd120, %p6;
	shl.b64 	%rd59, %rd3, 3;
	add.s64 	%rd121, %rd59, %rd119;
	add.s64 	%rd158, %rd121, -8;
	shl.b64 	%rd122, %rd4, 11;
	add.s64 	%rd123, %rd42, %rd122;
	sub.s64 	%rd124, %rd123, %rd119;
	add.s64 	%rd157, %rd124, 2048;
	add.s64 	%rd156, %rd55, -1;
$L__BB37_40:
	add.s64 	%rd68, %rd159, %rd8;
	shr.u64 	%rd126, %rd68, 1;
	shl.b64 	%rd127, %rd68, 7;
	and.b64  	%rd128, %rd127, 128;
	add.s64 	%rd69, %rd128, %rd126;
	setp.lt.u64 	%p36, %rd69, 256;
	@%p36 bra 	$L__BB37_42;
	bra.uni 	$L__BB37_41;
$L__BB37_42:
	shr.u64 	%rd125, %rd157, 3;
	setp.gt.u64 	%p35, %rd125, %rd8;
	selp.b64 	%rd67, %rd158, 0, %p35;
	setp.lt.u64 	%p2, %rd68, %rd159;
	setp.gt.u64 	%p37, %rd68, 255;
	add.s64 	%rd133, %rd68, 1;
	selp.b64 	%rd134, 256, %rd133, %p37;
	selp.b64 	%rd159, 256, %rd134, %p2;
	cvt.u32.u64 	%r247, %rd69;
	shl.b32 	%r248, %r247, 3;
	add.s32 	%r245, %r248, %r5;
	add.s32 	%r246, %r245, 4;
	// begin inline asm
	ld.shared.f32 %r243, [%r245];
ld.shared.f32 %r244, [%r246];
	// end inline asm
	st.u32 	[%rd67], %r243;
	st.u32 	[%rd67+4], %r244;
	add.s64 	%rd158, %rd158, %rd59;
	sub.s64 	%rd157, %rd157, %rd59;
	add.s64 	%rd156, %rd156, -1;
	setp.ne.s64 	%p38, %rd156, 0;
	@%p38 bra 	$L__BB37_40;
$L__BB37_43:
	ret;
$L__BB37_41:
	mov.u64 	%rd129, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd130, %rd129;
	mov.u64 	%rd131, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd132, %rd131;
	{ // callseq 211, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd130;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd132;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 211
$L__BB37_16:
	mov.u64 	%rd94, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd95, %rd94;
	mov.u64 	%rd96, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd97, %rd96;
	{ // callseq 209, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd95;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd97;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 209
$L__BB37_1:
	mov.u64 	%rd137, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_14;
	cvta.global.u64 	%rd138, %rd137;
	{ // callseq 213, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd138;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 213
$L__BB37_25:
	mov.u64 	%rd135, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_15;
	cvta.global.u64 	%rd136, %rd135;
	{ // callseq 212, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd136;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 212
$L__BB37_20:
	mov.u64 	%rd102, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_17;
	cvta.global.u64 	%rd103, %rd102;
	{ // callseq 210, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd103;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 210

}
	// .globl	daubechies_first_backward_512_kernel
.visible .entry daubechies_first_backward_512_kernel(
	.param .u64 daubechies_first_backward_512_kernel_param_0
)
{
	.reg .pred 	%p<39>;
	.reg .b32 	%r<251>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<160>;

	ld.param.u64 	%rd74, [daubechies_first_backward_512_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd74;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[4096];
    mov.u32 %r5, nonphysical;
	// end inline asm
	mov.u32 	%r8, %ctaid.x;
	ld.global.nc.u64 	%rd5, [%rd1+16];
	ld.global.nc.u64 	%rd75, [%rd1+24];
	and.b64  	%rd76, %rd75, -512;
	mul.wide.u32 	%rd6, %r8, 512;
	setp.ge.u64 	%p3, %rd6, %rd76;
	sub.s64 	%rd78, %rd75, %rd6;
	setp.lt.u64 	%p4, %rd78, 512;
	or.pred  	%p5, %p3, %p4;
	@!%p5 bra 	$L__BB38_2;
	bra.uni 	$L__BB38_1;
$L__BB38_2:
	mov.u32 	%r6, %tid.x;
	cvt.u64.u32 	%rd2, %r6;
	mov.u32 	%r7, %ntid.x;
	cvt.u64.u32 	%rd3, %r7;
	cvt.u32.u64 	%r9, %rd2;
	max.u64 	%rd81, %rd2, 512;
	setp.gt.u32 	%p6, %r9, 511;
	not.b64 	%rd82, %rd2;
	add.s64 	%rd9, %rd82, %rd81;
	mov.u64 	%rd146, 0;
	and.b64  	%rd142, %rd9, -4294967296;
	mov.u64 	%rd144, %rd146;
	@%p6 bra 	$L__BB38_7;
	setp.ne.s64 	%p7, %rd142, 0;
	@%p7 bra 	$L__BB38_5;
	bra.uni 	$L__BB38_4;
$L__BB38_5:
	div.u64 	%rd143, %rd9, %rd3;
	bra.uni 	$L__BB38_6;
$L__BB38_4:
	cvt.u32.u64 	%r10, %rd3;
	cvt.u32.u64 	%r11, %rd9;
	div.u32 	%r12, %r11, %r10;
	cvt.u64.u32 	%rd143, %r12;
$L__BB38_6:
	add.s64 	%rd144, %rd143, 1;
$L__BB38_7:
	@%p6 bra 	$L__BB38_12;
	setp.ne.s64 	%p9, %rd142, 0;
	@%p9 bra 	$L__BB38_10;
	bra.uni 	$L__BB38_9;
$L__BB38_10:
	div.u64 	%rd145, %rd9, %rd3;
	bra.uni 	$L__BB38_11;
$L__BB38_9:
	cvt.u32.u64 	%r14, %rd3;
	cvt.u32.u64 	%r15, %rd9;
	div.u32 	%r16, %r15, %r14;
	cvt.u64.u32 	%rd145, %r16;
$L__BB38_11:
	add.s64 	%rd146, %rd145, 1;
$L__BB38_12:
	shl.b64 	%rd77, %rd6, 3;
	cvt.u64.u32 	%rd4, %r8;
	add.s64 	%rd8, %rd3, -1;
	min.u64 	%rd20, %rd144, %rd146;
	setp.eq.s64 	%p10, %rd20, 0;
	shl.b64 	%rd139, %rd2, 3;
	@%p10 bra 	$L__BB38_18;
	add.s64 	%rd7, %rd5, %rd77;
	add.s64 	%rd87, %rd7, %rd139;
	shl.b32 	%r22, %r9, 3;
	add.s32 	%r17, %r5, %r22;
	add.s32 	%r18, %r17, 4;
	ld.u32 	%r19, [%rd87];
	ld.u32 	%r20, [%rd87+4];
	// begin inline asm
	st.shared.f32 [%r17], %r19;
st.shared.f32 [%r18], %r20;
	// end inline asm
	setp.eq.s64 	%p11, %rd20, 1;
	@%p11 bra 	$L__BB38_18;
	add.s64 	%rd150, %rd2, 1;
	add.s64 	%rd149, %rd20, -1;
	shl.b64 	%rd88, %rd4, 12;
	shl.b64 	%rd23, %rd3, 3;
	add.s64 	%rd89, %rd88, %rd23;
	add.s64 	%rd91, %rd89, %rd139;
	add.s64 	%rd148, %rd5, %rd91;
	mov.u64 	%rd92, 4088;
	sub.s64 	%rd147, %rd92, %rd139;
$L__BB38_15:
	add.s64 	%rd31, %rd150, %rd8;
	setp.lt.u64 	%p13, %rd31, 512;
	@%p13 bra 	$L__BB38_17;
	bra.uni 	$L__BB38_16;
$L__BB38_17:
	shr.u64 	%rd93, %rd147, 3;
	setp.gt.u64 	%p12, %rd93, %rd8;
	selp.b64 	%rd30, %rd148, 0, %p12;
	setp.lt.u64 	%p1, %rd31, %rd150;
	add.s64 	%rd98, %rd150, %rd3;
	selp.b64 	%rd150, 512, %rd98, %p1;
	cvt.u32.u64 	%r27, %rd31;
	shl.b32 	%r28, %r27, 3;
	add.s32 	%r23, %r28, %r5;
	add.s32 	%r24, %r23, 4;
	ld.u32 	%r25, [%rd30];
	ld.u32 	%r26, [%rd30+4];
	// begin inline asm
	st.shared.f32 [%r23], %r25;
st.shared.f32 [%r24], %r26;
	// end inline asm
	add.s64 	%rd149, %rd149, -1;
	add.s64 	%rd148, %rd148, %rd23;
	sub.s64 	%rd147, %rd147, %rd23;
	setp.ne.s64 	%p14, %rd149, 0;
	@%p14 bra 	$L__BB38_15;
$L__BB38_18:
	ld.global.nc.u64 	%rd36, [%rd1];
	ld.global.nc.u64 	%rd37, [%rd1+8];
	bar.sync 	0;
	setp.gt.u32 	%p15, %r9, 255;
	@%p15 bra 	$L__BB38_24;
	shl.b32 	%r2, %r9, 3;
	add.s32 	%r81, %r5, %r2;
	add.s32 	%r82, %r81, 4;
	// begin inline asm
	ld.shared.f32 %r36, [%r81];
ld.shared.f32 %r39, [%r82];
	// end inline asm
	setp.eq.s64 	%p16, %rd37, 0;
	@%p16 bra 	$L__BB38_20;
	mov.b32 	%f2, %r39;
	mov.b32 	%f1, %r36;
	ld.u32 	%r37, [%rd36];
	ld.u32 	%r40, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r35, %r36, %r37;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r38, %r39, %r40;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r76, %r35, %r38;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r44, %r36, %r40;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r47, %r39, %r37;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r79, %r44, %r47;
	// end inline asm
	add.s32 	%r91, %r81, 2048;
	add.s32 	%r92, %r81, 2052;
	// begin inline asm
	ld.shared.f32 %r58, [%r91];
ld.shared.f32 %r61, [%r92];
	// end inline asm
	ld.u32 	%r71, [%rd36];
	ld.u32 	%r68, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r57, %r58, %r71;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r60, %r61, %r68;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r77, %r57, %r60;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r66, %r58, %r68;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r69, %r61, %r71;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r66, %r69;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r75, %r76, %r77;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r78, %r79, %r80;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r81], %r75;
st.shared.f32 [%r82], %r78;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r85, %r76, %r77;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r88, %r79, %r80;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r91], %r85;
st.shared.f32 [%r92], %r88;
	// end inline asm
	add.s64 	%rd41, %rd2, %rd3;
	setp.gt.u64 	%p17, %rd41, 255;
	@%p17 bra 	$L__BB38_24;
	cvt.u32.u64 	%r160, %rd41;
	shl.b32 	%r161, %r160, 3;
	add.s32 	%r98, %r5, %r161;
	add.s32 	%r99, %r98, 4;
	// begin inline asm
	ld.shared.f32 %r101, [%r98];
ld.shared.f32 %r104, [%r99];
	// end inline asm
	ld.u32 	%r114, [%rd36];
	ld.u32 	%r111, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r100, %r101, %r114;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r103, %r104, %r111;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r100, %r103;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r109, %r101, %r111;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r112, %r104, %r114;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r144, %r109, %r112;
	// end inline asm
	add.s32 	%r156, %r98, 2048;
	add.s32 	%r157, %r98, 2052;
	// begin inline asm
	ld.shared.f32 %r123, [%r156];
ld.shared.f32 %r126, [%r157];
	// end inline asm
	ld.u32 	%r136, [%rd36];
	ld.u32 	%r133, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r122, %r123, %r136;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r125, %r126, %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r142, %r122, %r125;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r131, %r123, %r133;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r134, %r126, %r136;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r145, %r131, %r134;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r140, %r141, %r142;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r143, %r144, %r145;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r98], %r140;
st.shared.f32 [%r99], %r143;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r150, %r141, %r142;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r153, %r144, %r145;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r156], %r150;
st.shared.f32 [%r157], %r153;
	// end inline asm
	add.s64 	%rd151, %rd41, %rd3;
	setp.gt.u64 	%p18, %rd151, 255;
	@%p18 bra 	$L__BB38_24;
$L__BB38_22:
	add.s64 	%rd99, %rd151, 1;
	cvt.u32.u64 	%r226, %rd151;
	shl.b32 	%r227, %r226, 3;
	add.s32 	%r164, %r227, %r5;
	add.s32 	%r165, %r164, 4;
	// begin inline asm
	ld.shared.f32 %r167, [%r164];
ld.shared.f32 %r170, [%r165];
	// end inline asm
	ld.u32 	%r180, [%rd36];
	ld.u32 	%r177, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r166, %r167, %r180;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r169, %r170, %r177;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r207, %r166, %r169;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r175, %r167, %r177;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r178, %r170, %r180;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r210, %r175, %r178;
	// end inline asm
	add.s32 	%r186, %r164, 2048;
	add.s32 	%r223, %r164, 2052;
	// begin inline asm
	ld.shared.f32 %r189, [%r186];
ld.shared.f32 %r192, [%r223];
	// end inline asm
	ld.u32 	%r202, [%rd36];
	ld.u32 	%r199, [%rd36+4];
	// begin inline asm
	mul.rn.ftz.f32 %r188, %r189, %r202;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r191, %r192, %r199;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r208, %r188, %r191;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r197, %r189, %r199;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r200, %r192, %r202;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r211, %r197, %r200;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r206, %r207, %r208;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r209, %r210, %r211;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r164], %r206;
st.shared.f32 [%r165], %r209;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r216, %r207, %r208;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r219, %r210, %r211;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r186], %r216;
st.shared.f32 [%r223], %r219;
	// end inline asm
	add.s64 	%rd100, %rd99, %rd8;
	setp.lt.u64 	%p19, %rd100, %rd99;
	add.s64 	%rd151, %rd151, %rd3;
	setp.gt.u64 	%p20, %rd151, 255;
	or.pred  	%p21, %p19, %p20;
	@%p21 bra 	$L__BB38_24;
	bra.uni 	$L__BB38_22;
$L__BB38_24:
	ld.global.nc.u64 	%rd42, [%rd1+32];
	ld.global.nc.u64 	%rd104, [%rd1+40];
	and.b64  	%rd105, %rd104, -512;
	setp.lt.u64 	%p22, %rd6, %rd105;
	sub.s64 	%rd107, %rd104, %rd6;
	setp.gt.u64 	%p23, %rd107, 511;
	and.pred  	%p24, %p22, %p23;
	@%p24 bra 	$L__BB38_26;
	bra.uni 	$L__BB38_25;
$L__BB38_26:
	bar.sync 	0;
	mov.u64 	%rd155, 0;
	mov.u64 	%rd153, %rd155;
	@%p6 bra 	$L__BB38_31;
	setp.ne.s64 	%p26, %rd142, 0;
	@%p26 bra 	$L__BB38_29;
	bra.uni 	$L__BB38_28;
$L__BB38_29:
	div.u64 	%rd152, %rd9, %rd3;
	bra.uni 	$L__BB38_30;
$L__BB38_28:
	cvt.u32.u64 	%r229, %rd3;
	cvt.u32.u64 	%r230, %rd9;
	div.u32 	%r231, %r230, %r229;
	cvt.u64.u32 	%rd152, %r231;
$L__BB38_30:
	add.s64 	%rd153, %rd152, 1;
$L__BB38_31:
	@%p6 bra 	$L__BB38_36;
	setp.ne.s64 	%p28, %rd142, 0;
	@%p28 bra 	$L__BB38_34;
	bra.uni 	$L__BB38_33;
$L__BB38_34:
	div.u64 	%rd154, %rd9, %rd3;
	bra.uni 	$L__BB38_35;
$L__BB38_33:
	cvt.u32.u64 	%r233, %rd3;
	cvt.u32.u64 	%r234, %rd9;
	div.u32 	%r235, %r234, %r233;
	cvt.u64.u32 	%rd154, %r235;
$L__BB38_35:
	add.s64 	%rd155, %rd154, 1;
$L__BB38_36:
	min.u64 	%rd55, %rd153, %rd155;
	setp.eq.s64 	%p29, %rd55, 0;
	@%p29 bra 	$L__BB38_43;
	shr.u64 	%rd112, %rd2, 1;
	shl.b64 	%rd113, %rd2, 8;
	and.b64  	%rd114, %rd113, 256;
	add.s64 	%rd56, %rd114, %rd112;
	setp.gt.u64 	%p30, %rd56, 511;
	@%p30 bra 	$L__BB38_41;
	add.s64 	%rd43, %rd42, %rd77;
	add.s64 	%rd57, %rd43, %rd139;
	cvt.u32.u64 	%r240, %rd56;
	shl.b32 	%r241, %r240, 3;
	add.s32 	%r238, %r5, %r241;
	add.s32 	%r239, %r238, 4;
	// begin inline asm
	ld.shared.f32 %r236, [%r238];
ld.shared.f32 %r237, [%r239];
	// end inline asm
	st.u32 	[%rd57], %r236;
	st.u32 	[%rd57+4], %r237;
	setp.eq.s64 	%p31, %rd55, 1;
	@%p31 bra 	$L__BB38_43;
	add.s64 	%rd44, %rd43, 4096;
	setp.lt.u32 	%p32, %r9, 512;
	setp.eq.s32 	%p33, %r9, 0;
	add.s64 	%rd116, %rd43, 8;
	add.s64 	%rd117, %rd57, 8;
	selp.b64 	%rd118, %rd117, %rd44, %p32;
	selp.b64 	%rd119, %rd116, %rd118, %p33;
	add.s64 	%rd120, %rd2, 1;
	selp.b64 	%rd159, 512, %rd120, %p6;
	shl.b64 	%rd59, %rd3, 3;
	add.s64 	%rd121, %rd59, %rd119;
	add.s64 	%rd158, %rd121, -8;
	shl.b64 	%rd122, %rd4, 12;
	add.s64 	%rd123, %rd42, %rd122;
	sub.s64 	%rd124, %rd123, %rd119;
	add.s64 	%rd157, %rd124, 4096;
	add.s64 	%rd156, %rd55, -1;
$L__BB38_40:
	add.s64 	%rd68, %rd159, %rd8;
	shr.u64 	%rd126, %rd68, 1;
	shl.b64 	%rd127, %rd68, 8;
	and.b64  	%rd128, %rd127, 256;
	add.s64 	%rd69, %rd128, %rd126;
	setp.lt.u64 	%p36, %rd69, 512;
	@%p36 bra 	$L__BB38_42;
	bra.uni 	$L__BB38_41;
$L__BB38_42:
	shr.u64 	%rd125, %rd157, 3;
	setp.gt.u64 	%p35, %rd125, %rd8;
	selp.b64 	%rd67, %rd158, 0, %p35;
	setp.lt.u64 	%p2, %rd68, %rd159;
	setp.gt.u64 	%p37, %rd68, 511;
	add.s64 	%rd133, %rd68, 1;
	selp.b64 	%rd134, 512, %rd133, %p37;
	selp.b64 	%rd159, 512, %rd134, %p2;
	cvt.u32.u64 	%r247, %rd69;
	shl.b32 	%r248, %r247, 3;
	add.s32 	%r245, %r248, %r5;
	add.s32 	%r246, %r245, 4;
	// begin inline asm
	ld.shared.f32 %r243, [%r245];
ld.shared.f32 %r244, [%r246];
	// end inline asm
	st.u32 	[%rd67], %r243;
	st.u32 	[%rd67+4], %r244;
	add.s64 	%rd158, %rd158, %rd59;
	sub.s64 	%rd157, %rd157, %rd59;
	add.s64 	%rd156, %rd156, -1;
	setp.ne.s64 	%p38, %rd156, 0;
	@%p38 bra 	$L__BB38_40;
$L__BB38_43:
	ret;
$L__BB38_41:
	mov.u64 	%rd129, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd130, %rd129;
	mov.u64 	%rd131, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd132, %rd131;
	{ // callseq 216, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd130;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd132;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 216
$L__BB38_16:
	mov.u64 	%rd94, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd95, %rd94;
	mov.u64 	%rd96, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd97, %rd96;
	{ // callseq 214, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd95;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd97;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 214
$L__BB38_1:
	mov.u64 	%rd137, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_14;
	cvta.global.u64 	%rd138, %rd137;
	{ // callseq 218, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd138;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 218
$L__BB38_25:
	mov.u64 	%rd135, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_15;
	cvta.global.u64 	%rd136, %rd135;
	{ // callseq 217, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd136;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 217
$L__BB38_20:
	mov.u64 	%rd102, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_17;
	cvta.global.u64 	%rd103, %rd102;
	{ // callseq 215, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd103;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 215

}
	// .globl	daubechies_first_backward_1024_kernel
.visible .entry daubechies_first_backward_1024_kernel(
	.param .u64 daubechies_first_backward_1024_kernel_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b16 	%rs<4>;
	.reg .b32 	%r<243>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<105>;

	ld.param.u64 	%rd46, [daubechies_first_backward_1024_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd46;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[8192];
    mov.u32 %r5, nonphysical;
	// end inline asm
	mov.u32 	%r8, %ctaid.x;
	ld.global.nc.u64 	%rd47, [%rd1+24];
	and.b64  	%rd48, %rd47, -1024;
	mul.wide.u32 	%rd5, %r8, 1024;
	setp.ge.u64 	%p3, %rd5, %rd48;
	sub.s64 	%rd49, %rd47, %rd5;
	setp.lt.u64 	%p4, %rd49, 1024;
	or.pred  	%p5, %p3, %p4;
	@!%p5 bra 	$L__BB39_2;
	bra.uni 	$L__BB39_1;
$L__BB39_2:
	mov.u32 	%r6, %tid.x;
	cvt.u64.u32 	%rd2, %r6;
	mov.u32 	%r7, %ntid.x;
	cvt.u64.u32 	%rd3, %r7;
	cvt.u64.u32 	%rd4, %r8;
	cvt.u32.u64 	%r13, %rd2;
	ld.global.nc.u64 	%rd6, [%rd1+16];
	shl.b64 	%rd51, %rd5, 3;
	add.s64 	%rd52, %rd6, %rd51;
	add.s64 	%rd7, %rd3, -1;
	cvt.u16.u64 	%rs2, %rd2;
	xor.b16  	%rs1, %rs2, 1023;
	cvt.u16.u64 	%rs3, %rd3;
	shl.b64 	%rd53, %rd2, 3;
	add.s64 	%rd54, %rd52, %rd53;
	shl.b32 	%r2, %r13, 3;
	add.s32 	%r9, %r5, %r2;
	add.s32 	%r10, %r9, 4;
	ld.u32 	%r11, [%rd54];
	ld.u32 	%r12, [%rd54+4];
	// begin inline asm
	st.shared.f32 [%r9], %r11;
st.shared.f32 [%r10], %r12;
	// end inline asm
	setp.lt.u16 	%p6, %rs1, %rs3;
	cvt.u32.u64 	%r240, %rd3;
	@%p6 bra 	$L__BB39_7;
	add.s64 	%rd99, %rd2, 1;
	cvt.u32.u16 	%r14, %rs1;
	div.u32 	%r16, %r14, %r240;
	cvt.u64.u32 	%rd98, %r16;
	shl.b64 	%rd55, %rd4, 13;
	shl.b64 	%rd10, %rd3, 3;
	add.s64 	%rd56, %rd55, %rd10;
	add.s64 	%rd58, %rd56, %rd53;
	add.s64 	%rd97, %rd6, %rd58;
	xor.b64  	%rd96, %rd53, 8184;
$L__BB39_4:
	add.s64 	%rd18, %rd99, %rd7;
	setp.lt.u64 	%p8, %rd18, 1024;
	@%p8 bra 	$L__BB39_6;
	bra.uni 	$L__BB39_5;
$L__BB39_6:
	shr.u64 	%rd59, %rd96, 3;
	setp.gt.u64 	%p7, %rd59, %rd7;
	selp.b64 	%rd17, %rd97, 0, %p7;
	setp.lt.u64 	%p1, %rd18, %rd99;
	add.s64 	%rd64, %rd99, %rd3;
	selp.b64 	%rd99, 1024, %rd64, %p1;
	cvt.u32.u64 	%r21, %rd18;
	shl.b32 	%r22, %r21, 3;
	add.s32 	%r17, %r22, %r5;
	add.s32 	%r18, %r17, 4;
	ld.u32 	%r19, [%rd17];
	ld.u32 	%r20, [%rd17+4];
	// begin inline asm
	st.shared.f32 [%r17], %r19;
st.shared.f32 [%r18], %r20;
	// end inline asm
	add.s64 	%rd98, %rd98, -1;
	add.s64 	%rd97, %rd97, %rd10;
	sub.s64 	%rd96, %rd96, %rd10;
	setp.ne.s64 	%p9, %rd98, 0;
	@%p9 bra 	$L__BB39_4;
$L__BB39_7:
	ld.global.nc.u64 	%rd23, [%rd1];
	ld.global.nc.u64 	%rd24, [%rd1+8];
	bar.sync 	0;
	setp.gt.u32 	%p10, %r13, 511;
	@%p10 bra 	$L__BB39_13;
	// begin inline asm
	ld.shared.f32 %r29, [%r9];
ld.shared.f32 %r32, [%r10];
	// end inline asm
	setp.eq.s64 	%p11, %rd24, 0;
	@%p11 bra 	$L__BB39_9;
	mov.b32 	%f2, %r32;
	mov.b32 	%f1, %r29;
	ld.u32 	%r30, [%rd23];
	ld.u32 	%r33, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r28, %r29, %r30;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r31, %r32, %r33;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r69, %r28, %r31;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r37, %r29, %r33;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r40, %r32, %r30;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r72, %r37, %r40;
	// end inline asm
	add.s32 	%r84, %r9, 4096;
	add.s32 	%r85, %r9, 4100;
	// begin inline asm
	ld.shared.f32 %r51, [%r84];
ld.shared.f32 %r54, [%r85];
	// end inline asm
	ld.u32 	%r64, [%rd23];
	ld.u32 	%r61, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r50, %r51, %r64;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r53, %r54, %r61;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r70, %r50, %r53;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r59, %r51, %r61;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r62, %r54, %r64;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r73, %r59, %r62;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r68, %r69, %r70;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r71, %r72, %r73;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r9], %r68;
st.shared.f32 [%r10], %r71;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r78, %r69, %r70;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r81, %r72, %r73;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r84], %r78;
st.shared.f32 [%r85], %r81;
	// end inline asm
	add.s64 	%rd28, %rd2, %rd3;
	setp.gt.u64 	%p12, %rd28, 511;
	@%p12 bra 	$L__BB39_13;
	cvt.u32.u64 	%r153, %rd28;
	shl.b32 	%r154, %r153, 3;
	add.s32 	%r91, %r5, %r154;
	add.s32 	%r92, %r91, 4;
	// begin inline asm
	ld.shared.f32 %r94, [%r91];
ld.shared.f32 %r97, [%r92];
	// end inline asm
	ld.u32 	%r107, [%rd23];
	ld.u32 	%r104, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r93, %r94, %r107;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r96, %r97, %r104;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r134, %r93, %r96;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r102, %r94, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r105, %r97, %r107;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r137, %r102, %r105;
	// end inline asm
	add.s32 	%r149, %r91, 4096;
	add.s32 	%r150, %r91, 4100;
	// begin inline asm
	ld.shared.f32 %r116, [%r149];
ld.shared.f32 %r119, [%r150];
	// end inline asm
	ld.u32 	%r129, [%rd23];
	ld.u32 	%r126, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r115, %r116, %r129;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r118, %r119, %r126;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r135, %r115, %r118;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r124, %r116, %r126;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r127, %r119, %r129;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r138, %r124, %r127;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r133, %r134, %r135;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r136, %r137, %r138;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r91], %r133;
st.shared.f32 [%r92], %r136;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r143, %r134, %r135;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r146, %r137, %r138;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r149], %r143;
st.shared.f32 [%r150], %r146;
	// end inline asm
	add.s64 	%rd100, %rd28, %rd3;
	setp.gt.u64 	%p13, %rd100, 511;
	@%p13 bra 	$L__BB39_13;
$L__BB39_11:
	add.s64 	%rd65, %rd100, 1;
	cvt.u32.u64 	%r219, %rd100;
	shl.b32 	%r220, %r219, 3;
	add.s32 	%r157, %r220, %r5;
	add.s32 	%r158, %r157, 4;
	// begin inline asm
	ld.shared.f32 %r160, [%r157];
ld.shared.f32 %r163, [%r158];
	// end inline asm
	ld.u32 	%r173, [%rd23];
	ld.u32 	%r170, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r159, %r160, %r173;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r162, %r163, %r170;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r200, %r159, %r162;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r168, %r160, %r170;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r171, %r163, %r173;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r203, %r168, %r171;
	// end inline asm
	add.s32 	%r179, %r157, 4096;
	add.s32 	%r216, %r157, 4100;
	// begin inline asm
	ld.shared.f32 %r182, [%r179];
ld.shared.f32 %r185, [%r216];
	// end inline asm
	ld.u32 	%r195, [%rd23];
	ld.u32 	%r192, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r181, %r182, %r195;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r184, %r185, %r192;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r201, %r181, %r184;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r190, %r182, %r192;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r193, %r185, %r195;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r204, %r190, %r193;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r199, %r200, %r201;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r202, %r203, %r204;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r157], %r199;
st.shared.f32 [%r158], %r202;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r209, %r200, %r201;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r212, %r203, %r204;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r179], %r209;
st.shared.f32 [%r216], %r212;
	// end inline asm
	add.s64 	%rd66, %rd65, %rd7;
	setp.lt.u64 	%p14, %rd66, %rd65;
	add.s64 	%rd100, %rd100, %rd3;
	setp.gt.u64 	%p15, %rd100, 511;
	or.pred  	%p16, %p14, %p15;
	@%p16 bra 	$L__BB39_13;
	bra.uni 	$L__BB39_11;
$L__BB39_13:
	ld.global.nc.u64 	%rd70, [%rd1+40];
	and.b64  	%rd71, %rd70, -1024;
	setp.lt.u64 	%p17, %rd5, %rd71;
	sub.s64 	%rd72, %rd70, %rd5;
	setp.gt.u64 	%p18, %rd72, 1023;
	and.pred  	%p19, %p17, %p18;
	@%p19 bra 	$L__BB39_15;
	bra.uni 	$L__BB39_14;
$L__BB39_15:
	ld.global.nc.u64 	%rd29, [%rd1+32];
	add.s64 	%rd74, %rd29, %rd51;
	bar.sync 	0;
	xor.b64  	%rd75, %rd2, 1023;
	add.s64 	%rd77, %rd74, %rd53;
	shl.b32 	%r229, %r13, 9;
	and.b32  	%r230, %r229, 512;
	shr.u32 	%r231, %r13, 1;
	or.b32  	%r232, %r230, %r231;
	shl.b32 	%r233, %r232, 3;
	add.s32 	%r223, %r5, %r233;
	add.s32 	%r224, %r223, 4;
	// begin inline asm
	ld.shared.f32 %r221, [%r223];
ld.shared.f32 %r222, [%r224];
	// end inline asm
	st.u32 	[%rd77], %r221;
	st.u32 	[%rd77+4], %r222;
	setp.lt.u64 	%p20, %rd75, %rd3;
	@%p20 bra 	$L__BB39_20;
	cvt.u32.u64 	%r226, %rd75;
	div.u32 	%r228, %r226, %r240;
	cvt.u64.u32 	%rd101, %r228;
	add.s64 	%rd104, %rd2, 1;
	shl.b64 	%rd78, %rd4, 13;
	shl.b64 	%rd32, %rd3, 3;
	add.s64 	%rd79, %rd78, %rd32;
	add.s64 	%rd81, %rd79, %rd53;
	add.s64 	%rd103, %rd29, %rd81;
	xor.b64  	%rd102, %rd53, 8184;
$L__BB39_17:
	add.s64 	%rd40, %rd104, %rd7;
	shr.u64 	%rd83, %rd40, 1;
	shl.b64 	%rd84, %rd40, 9;
	and.b64  	%rd85, %rd84, 512;
	add.s64 	%rd41, %rd85, %rd83;
	setp.lt.u64 	%p22, %rd41, 1024;
	@%p22 bra 	$L__BB39_19;
	bra.uni 	$L__BB39_18;
$L__BB39_19:
	shr.u64 	%rd82, %rd102, 3;
	setp.gt.u64 	%p21, %rd82, %rd7;
	selp.b64 	%rd39, %rd103, 0, %p21;
	setp.lt.u64 	%p2, %rd40, %rd104;
	setp.gt.u64 	%p23, %rd40, 1023;
	add.s64 	%rd90, %rd40, 1;
	selp.b64 	%rd91, 1024, %rd90, %p23;
	selp.b64 	%rd104, 1024, %rd91, %p2;
	cvt.u32.u64 	%r238, %rd41;
	shl.b32 	%r239, %r238, 3;
	add.s32 	%r236, %r239, %r5;
	add.s32 	%r237, %r236, 4;
	// begin inline asm
	ld.shared.f32 %r234, [%r236];
ld.shared.f32 %r235, [%r237];
	// end inline asm
	st.u32 	[%rd39], %r234;
	st.u32 	[%rd39+4], %r235;
	add.s64 	%rd103, %rd103, %rd32;
	sub.s64 	%rd102, %rd102, %rd32;
	add.s64 	%rd101, %rd101, -1;
	setp.ne.s64 	%p24, %rd101, 0;
	@%p24 bra 	$L__BB39_17;
$L__BB39_20:
	ret;
$L__BB39_5:
	mov.u64 	%rd60, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd61, %rd60;
	mov.u64 	%rd62, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd63, %rd62;
	{ // callseq 219, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd61;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd63;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 219
$L__BB39_18:
	mov.u64 	%rd86, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd87, %rd86;
	mov.u64 	%rd88, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd89, %rd88;
	{ // callseq 221, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd87;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd89;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 221
$L__BB39_1:
	mov.u64 	%rd94, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_14;
	cvta.global.u64 	%rd95, %rd94;
	{ // callseq 223, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd95;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 223
$L__BB39_14:
	mov.u64 	%rd92, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_15;
	cvta.global.u64 	%rd93, %rd92;
	{ // callseq 222, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd93;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 222
$L__BB39_9:
	mov.u64 	%rd68, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_17;
	cvta.global.u64 	%rd69, %rd68;
	{ // callseq 220, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd69;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 220

}
	// .globl	daubechies_first_backward_2048_kernel
.visible .entry daubechies_first_backward_2048_kernel(
	.param .u64 daubechies_first_backward_2048_kernel_param_0
)
{
	.reg .pred 	%p<22>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<241>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<96>;

	ld.param.u64 	%rd41, [daubechies_first_backward_2048_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd41;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[16384];
    mov.u32 %r5, nonphysical;
	// end inline asm
	mov.u32 	%r8, %ctaid.x;
	ld.global.nc.u64 	%rd42, [%rd1+24];
	and.b64  	%rd43, %rd42, -2048;
	mul.wide.u32 	%rd5, %r8, 2048;
	setp.ge.u64 	%p3, %rd5, %rd43;
	sub.s64 	%rd44, %rd42, %rd5;
	setp.lt.u64 	%p4, %rd44, 2048;
	or.pred  	%p5, %p3, %p4;
	@!%p5 bra 	$L__BB40_2;
	bra.uni 	$L__BB40_1;
$L__BB40_2:
	mov.u32 	%r6, %tid.x;
	cvt.u64.u32 	%rd2, %r6;
	mov.u32 	%r7, %ntid.x;
	cvt.u64.u32 	%rd3, %r7;
	cvt.u64.u32 	%rd4, %r8;
	cvt.u32.u64 	%r13, %rd2;
	ld.global.nc.u64 	%rd46, [%rd1+16];
	shl.b64 	%rd47, %rd5, 3;
	add.s64 	%rd48, %rd46, %rd47;
	add.s64 	%rd6, %rd3, -1;
	cvt.u16.u64 	%rs1, %rd2;
	xor.b16  	%rs2, %rs1, 2047;
	shl.b64 	%rd49, %rd2, 3;
	add.s64 	%rd50, %rd48, %rd49;
	shl.b32 	%r2, %r13, 3;
	add.s32 	%r3, %r5, %r2;
	add.s32 	%r4, %r3, 4;
	ld.u32 	%r11, [%rd50];
	ld.u32 	%r12, [%rd50+4];
	// begin inline asm
	st.shared.f32 [%r3], %r11;
st.shared.f32 [%r4], %r12;
	// end inline asm
	add.s64 	%rd95, %rd2, 1;
	cvt.u32.u16 	%r14, %rs2;
	cvt.u32.u64 	%r15, %rd3;
	div.u32 	%r16, %r14, %r15;
	cvt.u64.u32 	%rd89, %r16;
	shl.b64 	%rd51, %rd4, 14;
	shl.b64 	%rd9, %rd3, 3;
	add.s64 	%rd52, %rd51, %rd9;
	add.s64 	%rd10, %rd52, %rd49;
	add.s64 	%rd88, %rd46, %rd10;
	xor.b64  	%rd93, %rd49, 16376;
	mov.u64 	%rd87, %rd93;
	mov.u64 	%rd90, %rd95;
$L__BB40_3:
	add.s64 	%rd18, %rd90, %rd6;
	setp.lt.u64 	%p7, %rd18, 2048;
	@%p7 bra 	$L__BB40_5;
	bra.uni 	$L__BB40_4;
$L__BB40_5:
	shr.u64 	%rd53, %rd87, 3;
	setp.gt.u64 	%p6, %rd53, %rd6;
	selp.b64 	%rd17, %rd88, 0, %p6;
	setp.lt.u64 	%p1, %rd18, %rd90;
	add.s64 	%rd58, %rd90, %rd3;
	selp.b64 	%rd90, 2048, %rd58, %p1;
	cvt.u32.u64 	%r21, %rd18;
	shl.b32 	%r22, %r21, 3;
	add.s32 	%r17, %r22, %r5;
	add.s32 	%r18, %r17, 4;
	ld.u32 	%r19, [%rd17];
	ld.u32 	%r20, [%rd17+4];
	// begin inline asm
	st.shared.f32 [%r17], %r19;
st.shared.f32 [%r18], %r20;
	// end inline asm
	add.s64 	%rd89, %rd89, -1;
	add.s64 	%rd88, %rd88, %rd9;
	sub.s64 	%rd87, %rd87, %rd9;
	setp.ne.s64 	%p8, %rd89, 0;
	@%p8 bra 	$L__BB40_3;
	ld.global.nc.u64 	%rd23, [%rd1];
	ld.global.nc.u64 	%rd59, [%rd1+8];
	bar.sync 	0;
	// begin inline asm
	ld.shared.f32 %r23, [%r3];
ld.shared.f32 %r24, [%r4];
	// end inline asm
	setp.eq.s64 	%p9, %rd59, 0;
	@%p9 bra 	$L__BB40_17;
	mov.b32 	%f2, %r24;
	mov.b32 	%f1, %r23;
	ld.u32 	%r29, [%rd23];
	ld.u32 	%r32, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r27, %r23, %r29;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r30, %r24, %r32;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r68, %r27, %r30;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r36, %r23, %r32;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r39, %r24, %r29;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r71, %r36, %r39;
	// end inline asm
	add.s32 	%r83, %r3, 8192;
	add.s32 	%r84, %r3, 8196;
	// begin inline asm
	ld.shared.f32 %r50, [%r83];
ld.shared.f32 %r53, [%r84];
	// end inline asm
	ld.u32 	%r63, [%rd23];
	ld.u32 	%r60, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r50, %r63;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r52, %r53, %r60;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r69, %r49, %r52;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r58, %r50, %r60;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r61, %r53, %r63;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r72, %r58, %r61;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r67, %r68, %r69;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r70, %r71, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r3], %r67;
st.shared.f32 [%r4], %r70;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r77, %r68, %r69;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r80, %r71, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r83], %r77;
st.shared.f32 [%r84], %r80;
	// end inline asm
	add.s64 	%rd27, %rd2, %rd3;
	setp.gt.u64 	%p10, %rd27, 1023;
	@%p10 bra 	$L__BB40_10;
	cvt.u32.u64 	%r152, %rd27;
	shl.b32 	%r153, %r152, 3;
	add.s32 	%r90, %r5, %r153;
	add.s32 	%r91, %r90, 4;
	// begin inline asm
	ld.shared.f32 %r93, [%r90];
ld.shared.f32 %r96, [%r91];
	// end inline asm
	ld.u32 	%r106, [%rd23];
	ld.u32 	%r103, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r92, %r93, %r106;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r95, %r96, %r103;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r133, %r92, %r95;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r101, %r93, %r103;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r104, %r96, %r106;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r136, %r101, %r104;
	// end inline asm
	add.s32 	%r148, %r90, 8192;
	add.s32 	%r149, %r90, 8196;
	// begin inline asm
	ld.shared.f32 %r115, [%r148];
ld.shared.f32 %r118, [%r149];
	// end inline asm
	ld.u32 	%r128, [%rd23];
	ld.u32 	%r125, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r114, %r115, %r128;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r117, %r118, %r125;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r134, %r114, %r117;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r123, %r115, %r125;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r126, %r118, %r128;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r137, %r123, %r126;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r132, %r133, %r134;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r135, %r136, %r137;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r90], %r132;
st.shared.f32 [%r91], %r135;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r142, %r133, %r134;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r145, %r136, %r137;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r148], %r142;
st.shared.f32 [%r149], %r145;
	// end inline asm
	add.s64 	%rd91, %rd27, %rd3;
	setp.gt.u64 	%p11, %rd91, 1023;
	@%p11 bra 	$L__BB40_10;
$L__BB40_9:
	add.s64 	%rd60, %rd91, 1;
	cvt.u32.u64 	%r218, %rd91;
	shl.b32 	%r219, %r218, 3;
	add.s32 	%r156, %r219, %r5;
	add.s32 	%r157, %r156, 4;
	// begin inline asm
	ld.shared.f32 %r159, [%r156];
ld.shared.f32 %r162, [%r157];
	// end inline asm
	ld.u32 	%r172, [%rd23];
	ld.u32 	%r169, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r158, %r159, %r172;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r161, %r162, %r169;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r199, %r158, %r161;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r167, %r159, %r169;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r170, %r162, %r172;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r202, %r167, %r170;
	// end inline asm
	add.s32 	%r178, %r156, 8192;
	add.s32 	%r215, %r156, 8196;
	// begin inline asm
	ld.shared.f32 %r181, [%r178];
ld.shared.f32 %r184, [%r215];
	// end inline asm
	ld.u32 	%r194, [%rd23];
	ld.u32 	%r191, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r180, %r181, %r194;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r183, %r184, %r191;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r200, %r180, %r183;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r189, %r181, %r191;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r192, %r184, %r194;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r203, %r189, %r192;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r198, %r199, %r200;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r201, %r202, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r156], %r198;
st.shared.f32 [%r157], %r201;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r208, %r199, %r200;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r211, %r202, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r178], %r208;
st.shared.f32 [%r215], %r211;
	// end inline asm
	add.s64 	%rd61, %rd60, %rd6;
	setp.lt.u64 	%p12, %rd61, %rd60;
	add.s64 	%rd91, %rd91, %rd3;
	setp.gt.u64 	%p13, %rd91, 1023;
	or.pred  	%p14, %p12, %p13;
	@%p14 bra 	$L__BB40_10;
	bra.uni 	$L__BB40_9;
$L__BB40_10:
	ld.global.nc.u64 	%rd63, [%rd1+40];
	and.b64  	%rd64, %rd63, -2048;
	setp.lt.u64 	%p15, %rd5, %rd64;
	sub.s64 	%rd65, %rd63, %rd5;
	setp.gt.u64 	%p16, %rd65, 2047;
	and.pred  	%p17, %p15, %p16;
	@%p17 bra 	$L__BB40_12;
	bra.uni 	$L__BB40_11;
$L__BB40_12:
	ld.global.nc.u64 	%rd66, [%rd1+32];
	add.s64 	%rd68, %rd66, %rd47;
	bar.sync 	0;
	xor.b32  	%r225, %r13, 2047;
	div.u32 	%r227, %r225, %r15;
	cvt.u64.u32 	%rd92, %r227;
	add.s64 	%rd70, %rd68, %rd49;
	shl.b32 	%r228, %r13, 10;
	and.b32  	%r229, %r228, 1024;
	shr.u32 	%r230, %r13, 1;
	or.b32  	%r231, %r229, %r230;
	shl.b32 	%r232, %r231, 3;
	add.s32 	%r222, %r5, %r232;
	add.s32 	%r223, %r222, 4;
	// begin inline asm
	ld.shared.f32 %r220, [%r222];
ld.shared.f32 %r221, [%r223];
	// end inline asm
	st.u32 	[%rd70], %r220;
	st.u32 	[%rd70+4], %r221;
	add.s64 	%rd94, %rd66, %rd10;
$L__BB40_13:
	add.s64 	%rd35, %rd95, %rd6;
	shr.u64 	%rd72, %rd35, 1;
	shl.b64 	%rd73, %rd35, 10;
	and.b64  	%rd74, %rd73, 1024;
	add.s64 	%rd36, %rd74, %rd72;
	setp.lt.u64 	%p19, %rd36, 2048;
	@%p19 bra 	$L__BB40_15;
	bra.uni 	$L__BB40_14;
$L__BB40_15:
	shr.u64 	%rd71, %rd93, 3;
	setp.gt.u64 	%p18, %rd71, %rd6;
	selp.b64 	%rd34, %rd94, 0, %p18;
	setp.lt.u64 	%p2, %rd35, %rd95;
	setp.gt.u64 	%p20, %rd35, 2047;
	add.s64 	%rd79, %rd35, 1;
	selp.b64 	%rd80, 2048, %rd79, %p20;
	selp.b64 	%rd95, 2048, %rd80, %p2;
	cvt.u32.u64 	%r237, %rd36;
	shl.b32 	%r238, %r237, 3;
	add.s32 	%r235, %r238, %r5;
	add.s32 	%r236, %r235, 4;
	// begin inline asm
	ld.shared.f32 %r233, [%r235];
ld.shared.f32 %r234, [%r236];
	// end inline asm
	st.u32 	[%rd34], %r233;
	st.u32 	[%rd34+4], %r234;
	add.s64 	%rd94, %rd94, %rd9;
	sub.s64 	%rd93, %rd93, %rd9;
	add.s64 	%rd92, %rd92, -1;
	setp.ne.s64 	%p21, %rd92, 0;
	@%p21 bra 	$L__BB40_13;
	ret;
$L__BB40_4:
	mov.u64 	%rd54, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd55, %rd54;
	mov.u64 	%rd56, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd57, %rd56;
	{ // callseq 224, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd55;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd57;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 224
$L__BB40_14:
	mov.u64 	%rd75, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd76, %rd75;
	mov.u64 	%rd77, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd78, %rd77;
	{ // callseq 225, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd76;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd78;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 225
$L__BB40_1:
	mov.u64 	%rd85, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_14;
	cvta.global.u64 	%rd86, %rd85;
	{ // callseq 228, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd86;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 228
$L__BB40_17:
	mov.u64 	%rd83, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_17;
	cvta.global.u64 	%rd84, %rd83;
	{ // callseq 227, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd84;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 227
$L__BB40_11:
	mov.u64 	%rd81, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_15;
	cvta.global.u64 	%rd82, %rd81;
	{ // callseq 226, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd82;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 226

}
	// .globl	daubechies_first_backward_4096_kernel
.visible .entry daubechies_first_backward_4096_kernel(
	.param .u64 daubechies_first_backward_4096_kernel_param_0
)
{
	.reg .pred 	%p<21>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<241>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<96>;

	ld.param.u64 	%rd40, [daubechies_first_backward_4096_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd40;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[32768];
    mov.u32 %r5, nonphysical;
	// end inline asm
	mov.u32 	%r8, %ctaid.x;
	ld.global.nc.u64 	%rd41, [%rd1+24];
	and.b64  	%rd42, %rd41, -4096;
	mul.wide.u32 	%rd5, %r8, 4096;
	setp.ge.u64 	%p3, %rd5, %rd42;
	sub.s64 	%rd43, %rd41, %rd5;
	setp.lt.u64 	%p4, %rd43, 4096;
	or.pred  	%p5, %p3, %p4;
	@!%p5 bra 	$L__BB41_2;
	bra.uni 	$L__BB41_1;
$L__BB41_2:
	mov.u32 	%r6, %tid.x;
	cvt.u64.u32 	%rd2, %r6;
	mov.u32 	%r7, %ntid.x;
	cvt.u64.u32 	%rd3, %r7;
	cvt.u64.u32 	%rd4, %r8;
	cvt.u32.u64 	%r13, %rd2;
	ld.global.nc.u64 	%rd45, [%rd1+16];
	shl.b64 	%rd46, %rd5, 3;
	add.s64 	%rd47, %rd45, %rd46;
	add.s64 	%rd6, %rd3, -1;
	cvt.u16.u64 	%rs1, %rd2;
	xor.b16  	%rs2, %rs1, 4095;
	shl.b64 	%rd48, %rd2, 3;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd95, %rd2, 1;
	shl.b32 	%r2, %r13, 3;
	add.s32 	%r3, %r5, %r2;
	add.s32 	%r4, %r3, 4;
	ld.u32 	%r11, [%rd49];
	ld.u32 	%r12, [%rd49+4];
	// begin inline asm
	st.shared.f32 [%r3], %r11;
st.shared.f32 [%r4], %r12;
	// end inline asm
	cvt.u32.u16 	%r14, %rs2;
	cvt.u32.u64 	%r15, %rd3;
	div.u32 	%r16, %r14, %r15;
	cvt.u64.u32 	%rd89, %r16;
	shl.b64 	%rd50, %rd4, 15;
	shl.b64 	%rd9, %rd3, 3;
	add.s64 	%rd51, %rd50, %rd9;
	add.s64 	%rd10, %rd51, %rd48;
	add.s64 	%rd88, %rd45, %rd10;
	xor.b64  	%rd93, %rd48, 32760;
	mov.u64 	%rd87, %rd93;
	mov.u64 	%rd90, %rd95;
$L__BB41_3:
	add.s64 	%rd18, %rd90, %rd6;
	setp.lt.u64 	%p7, %rd18, 4096;
	@%p7 bra 	$L__BB41_5;
	bra.uni 	$L__BB41_4;
$L__BB41_5:
	shr.u64 	%rd52, %rd87, 3;
	setp.gt.u64 	%p6, %rd52, %rd6;
	selp.b64 	%rd17, %rd88, 0, %p6;
	setp.lt.u64 	%p1, %rd18, %rd90;
	add.s64 	%rd57, %rd90, %rd3;
	selp.b64 	%rd90, 4096, %rd57, %p1;
	cvt.u32.u64 	%r21, %rd18;
	shl.b32 	%r22, %r21, 3;
	add.s32 	%r17, %r22, %r5;
	add.s32 	%r18, %r17, 4;
	ld.u32 	%r19, [%rd17];
	ld.u32 	%r20, [%rd17+4];
	// begin inline asm
	st.shared.f32 [%r17], %r19;
st.shared.f32 [%r18], %r20;
	// end inline asm
	add.s64 	%rd89, %rd89, -1;
	add.s64 	%rd88, %rd88, %rd9;
	sub.s64 	%rd87, %rd87, %rd9;
	setp.ne.s64 	%p8, %rd89, 0;
	@%p8 bra 	$L__BB41_3;
	ld.global.nc.u64 	%rd23, [%rd1];
	ld.global.nc.u64 	%rd58, [%rd1+8];
	bar.sync 	0;
	// begin inline asm
	ld.shared.f32 %r23, [%r3];
ld.shared.f32 %r24, [%r4];
	// end inline asm
	setp.eq.s64 	%p9, %rd58, 0;
	@%p9 bra 	$L__BB41_16;
	mov.b32 	%f2, %r24;
	mov.b32 	%f1, %r23;
	ld.u32 	%r29, [%rd23];
	ld.u32 	%r32, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r27, %r23, %r29;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r30, %r24, %r32;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r68, %r27, %r30;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r36, %r23, %r32;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r39, %r24, %r29;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r71, %r36, %r39;
	// end inline asm
	add.s32 	%r83, %r3, 16384;
	add.s32 	%r84, %r3, 16388;
	// begin inline asm
	ld.shared.f32 %r50, [%r83];
ld.shared.f32 %r53, [%r84];
	// end inline asm
	ld.u32 	%r63, [%rd23];
	ld.u32 	%r60, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r50, %r63;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r52, %r53, %r60;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r69, %r49, %r52;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r58, %r50, %r60;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r61, %r53, %r63;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r72, %r58, %r61;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r67, %r68, %r69;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r70, %r71, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r3], %r67;
st.shared.f32 [%r4], %r70;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r77, %r68, %r69;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r80, %r71, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r83], %r77;
st.shared.f32 [%r84], %r80;
	// end inline asm
	add.s64 	%rd59, %rd2, %rd3;
	cvt.u32.u64 	%r152, %rd59;
	shl.b32 	%r153, %r152, 3;
	add.s32 	%r89, %r5, %r153;
	add.s32 	%r90, %r89, 4;
	// begin inline asm
	ld.shared.f32 %r92, [%r89];
ld.shared.f32 %r95, [%r90];
	// end inline asm
	ld.u32 	%r105, [%rd23];
	ld.u32 	%r102, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r91, %r92, %r105;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r94, %r95, %r102;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r132, %r91, %r94;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r100, %r92, %r102;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r103, %r95, %r105;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r135, %r100, %r103;
	// end inline asm
	add.s32 	%r147, %r89, 16384;
	add.s32 	%r148, %r89, 16388;
	// begin inline asm
	ld.shared.f32 %r114, [%r147];
ld.shared.f32 %r117, [%r148];
	// end inline asm
	ld.u32 	%r127, [%rd23];
	ld.u32 	%r124, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r113, %r114, %r127;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r116, %r117, %r124;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r133, %r113, %r116;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r122, %r114, %r124;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r125, %r117, %r127;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r136, %r122, %r125;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r131, %r132, %r133;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r134, %r135, %r136;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r89], %r131;
st.shared.f32 [%r90], %r134;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r132, %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r135, %r136;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r147], %r141;
st.shared.f32 [%r148], %r144;
	// end inline asm
	add.s64 	%rd91, %rd59, %rd3;
	setp.gt.u64 	%p10, %rd91, 2047;
	@%p10 bra 	$L__BB41_9;
$L__BB41_8:
	add.s64 	%rd60, %rd91, 1;
	cvt.u32.u64 	%r218, %rd91;
	shl.b32 	%r219, %r218, 3;
	add.s32 	%r156, %r219, %r5;
	add.s32 	%r157, %r156, 4;
	// begin inline asm
	ld.shared.f32 %r159, [%r156];
ld.shared.f32 %r162, [%r157];
	// end inline asm
	ld.u32 	%r172, [%rd23];
	ld.u32 	%r169, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r158, %r159, %r172;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r161, %r162, %r169;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r199, %r158, %r161;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r167, %r159, %r169;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r170, %r162, %r172;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r202, %r167, %r170;
	// end inline asm
	add.s32 	%r178, %r156, 16384;
	add.s32 	%r215, %r156, 16388;
	// begin inline asm
	ld.shared.f32 %r181, [%r178];
ld.shared.f32 %r184, [%r215];
	// end inline asm
	ld.u32 	%r194, [%rd23];
	ld.u32 	%r191, [%rd23+4];
	// begin inline asm
	mul.rn.ftz.f32 %r180, %r181, %r194;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r183, %r184, %r191;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r200, %r180, %r183;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r189, %r181, %r191;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r192, %r184, %r194;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r203, %r189, %r192;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r198, %r199, %r200;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r201, %r202, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r156], %r198;
st.shared.f32 [%r157], %r201;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r208, %r199, %r200;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r211, %r202, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r178], %r208;
st.shared.f32 [%r215], %r211;
	// end inline asm
	add.s64 	%rd61, %rd60, %rd6;
	setp.lt.u64 	%p11, %rd61, %rd60;
	add.s64 	%rd91, %rd91, %rd3;
	setp.gt.u64 	%p12, %rd91, 2047;
	or.pred  	%p13, %p11, %p12;
	@%p13 bra 	$L__BB41_9;
	bra.uni 	$L__BB41_8;
$L__BB41_9:
	ld.global.nc.u64 	%rd63, [%rd1+40];
	and.b64  	%rd64, %rd63, -4096;
	setp.lt.u64 	%p14, %rd5, %rd64;
	sub.s64 	%rd65, %rd63, %rd5;
	setp.gt.u64 	%p15, %rd65, 4095;
	and.pred  	%p16, %p14, %p15;
	@%p16 bra 	$L__BB41_11;
	bra.uni 	$L__BB41_10;
$L__BB41_11:
	ld.global.nc.u64 	%rd66, [%rd1+32];
	add.s64 	%rd68, %rd66, %rd46;
	bar.sync 	0;
	xor.b32  	%r225, %r13, 4095;
	div.u32 	%r227, %r225, %r15;
	cvt.u64.u32 	%rd92, %r227;
	add.s64 	%rd70, %rd68, %rd48;
	shl.b32 	%r228, %r13, 11;
	and.b32  	%r229, %r228, 2048;
	shr.u32 	%r230, %r13, 1;
	or.b32  	%r231, %r229, %r230;
	shl.b32 	%r232, %r231, 3;
	add.s32 	%r222, %r5, %r232;
	add.s32 	%r223, %r222, 4;
	// begin inline asm
	ld.shared.f32 %r220, [%r222];
ld.shared.f32 %r221, [%r223];
	// end inline asm
	st.u32 	[%rd70], %r220;
	st.u32 	[%rd70+4], %r221;
	add.s64 	%rd94, %rd66, %rd10;
$L__BB41_12:
	add.s64 	%rd34, %rd95, %rd6;
	shr.u64 	%rd72, %rd34, 1;
	shl.b64 	%rd73, %rd34, 11;
	and.b64  	%rd74, %rd73, 2048;
	add.s64 	%rd35, %rd74, %rd72;
	setp.lt.u64 	%p18, %rd35, 4096;
	@%p18 bra 	$L__BB41_14;
	bra.uni 	$L__BB41_13;
$L__BB41_14:
	shr.u64 	%rd71, %rd93, 3;
	setp.gt.u64 	%p17, %rd71, %rd6;
	selp.b64 	%rd33, %rd94, 0, %p17;
	setp.lt.u64 	%p2, %rd34, %rd95;
	setp.gt.u64 	%p19, %rd34, 4095;
	add.s64 	%rd79, %rd34, 1;
	selp.b64 	%rd80, 4096, %rd79, %p19;
	selp.b64 	%rd95, 4096, %rd80, %p2;
	cvt.u32.u64 	%r237, %rd35;
	shl.b32 	%r238, %r237, 3;
	add.s32 	%r235, %r238, %r5;
	add.s32 	%r236, %r235, 4;
	// begin inline asm
	ld.shared.f32 %r233, [%r235];
ld.shared.f32 %r234, [%r236];
	// end inline asm
	st.u32 	[%rd33], %r233;
	st.u32 	[%rd33+4], %r234;
	add.s64 	%rd94, %rd94, %rd9;
	sub.s64 	%rd93, %rd93, %rd9;
	add.s64 	%rd92, %rd92, -1;
	setp.ne.s64 	%p20, %rd92, 0;
	@%p20 bra 	$L__BB41_12;
	ret;
$L__BB41_4:
	mov.u64 	%rd53, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd54, %rd53;
	mov.u64 	%rd55, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_73;
	cvta.global.u64 	%rd56, %rd55;
	{ // callseq 229, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd54;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd56;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 229
$L__BB41_13:
	mov.u64 	%rd75, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_70;
	cvta.global.u64 	%rd76, %rd75;
	mov.u64 	%rd77, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_72;
	cvta.global.u64 	%rd78, %rd77;
	{ // callseq 230, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd76;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd78;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 230
$L__BB41_1:
	mov.u64 	%rd85, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_14;
	cvta.global.u64 	%rd86, %rd85;
	{ // callseq 233, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd86;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 233
$L__BB41_16:
	mov.u64 	%rd83, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_17;
	cvta.global.u64 	%rd84, %rd83;
	{ // callseq 232, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd84;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 232
$L__BB41_10:
	mov.u64 	%rd81, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_15;
	cvta.global.u64 	%rd82, %rd81;
	{ // callseq 231, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd82;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 231

}
	// .globl	_ZN81_$LT$nonphysical_ptx$$shared$$primitive$$F32$u20$as$u20$core$$ops$$arith$$Rem$GT$3rem17h47656ded776ad7efE
.visible .func  (.param .b32 func_retval0) _ZN81_$LT$nonphysical_ptx$$shared$$primitive$$F32$u20$as$u20$core$$ops$$arith$$Rem$GT$3rem17h47656ded776ad7efE(
	.param .b32 _ZN81_$LT$nonphysical_ptx$$shared$$primitive$$F32$u20$as$u20$core$$ops$$arith$$Rem$GT$3rem17h47656ded776ad7efE_param_0,
	.param .b32 _ZN81_$LT$nonphysical_ptx$$shared$$primitive$$F32$u20$as$u20$core$$ops$$arith$$Rem$GT$3rem17h47656ded776ad7efE_param_1
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_27;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 234, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 234

}
	// .globl	_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E
.visible .func  (.param .b32 func_retval0) _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E(
	.param .b32 _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E_param_0,
	.param .b32 _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<3>;

	ld.param.u32 	%r2, [_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E_param_1];
	setp.eq.s32 	%p1, %r2, 0;
	@%p1 bra 	$L__BB43_2;
	ld.param.u32 	%r1, [_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E_param_0];
	div.u32 	%r3, %r1, %r2;
	st.param.b32 	[func_retval0+0], %r3;
	ret;
$L__BB43_2:
	mov.u64 	%rd1, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_29;
	cvta.global.u64 	%rd2, %rd1;
	{ // callseq 235, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 235

}
	// .globl	_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E
.visible .func _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E(
	.param .b64 _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E_param_0,
	.param .b32 _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<4>;

	ld.param.u32 	%r1, [_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E_param_1];
	setp.eq.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB44_2;
	ld.param.u64 	%rd1, [_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E_param_0];
	ld.u32 	%r2, [%rd1];
	div.u32 	%r3, %r2, %r1;
	st.u32 	[%rd1], %r3;
	ret;
$L__BB44_2:
	mov.u64 	%rd2, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_30;
	cvta.global.u64 	%rd3, %rd2;
	{ // callseq 236, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd3;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 236

}
	// .globl	_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE
.visible .func  (.param .b64 func_retval0) _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE(
	.param .b64 _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE_param_0,
	.param .b64 _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd5, [_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE_param_1];
	setp.eq.s64 	%p1, %rd5, 0;
	@%p1 bra 	$L__BB45_5;
	ld.param.u64 	%rd4, [_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE_param_0];
	or.b64  	%rd6, %rd4, %rd5;
	and.b64  	%rd7, %rd6, -4294967296;
	setp.ne.s64 	%p2, %rd7, 0;
	@%p2 bra 	$L__BB45_3;
	bra.uni 	$L__BB45_2;
$L__BB45_3:
	div.u64 	%rd10, %rd4, %rd5;
	bra.uni 	$L__BB45_4;
$L__BB45_2:
	cvt.u32.u64 	%r1, %rd5;
	cvt.u32.u64 	%r2, %rd4;
	div.u32 	%r3, %r2, %r1;
	cvt.u64.u32 	%rd10, %r3;
$L__BB45_4:
	st.param.b64 	[func_retval0+0], %rd10;
	ret;
$L__BB45_5:
	mov.u64 	%rd8, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_31;
	cvta.global.u64 	%rd9, %rd8;
	{ // callseq 237, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd9;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 237

}
	// .globl	_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE
.visible .func _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE(
	.param .b64 _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE_param_0,
	.param .b64 _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<12>;

	ld.param.u64 	%rd6, [_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE_param_1];
	setp.eq.s64 	%p1, %rd6, 0;
	@%p1 bra 	$L__BB46_5;
	ld.param.u64 	%rd5, [_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE_param_0];
	ld.u64 	%rd1, [%rd5];
	or.b64  	%rd7, %rd1, %rd6;
	and.b64  	%rd8, %rd7, -4294967296;
	setp.ne.s64 	%p2, %rd8, 0;
	@%p2 bra 	$L__BB46_3;
	bra.uni 	$L__BB46_2;
$L__BB46_3:
	div.u64 	%rd11, %rd1, %rd6;
	bra.uni 	$L__BB46_4;
$L__BB46_2:
	cvt.u32.u64 	%r1, %rd6;
	cvt.u32.u64 	%r2, %rd1;
	div.u32 	%r3, %r2, %r1;
	cvt.u64.u32 	%rd11, %r3;
$L__BB46_4:
	st.u64 	[%rd5], %rd11;
	ret;
$L__BB46_5:
	mov.u64 	%rd9, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_32;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 238, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd10;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 238

}
	// .globl	vector_sum_f32
.visible .entry vector_sum_f32(
	.param .u64 vector_sum_f32_param_0
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<107>;
	.reg .f32 	%f<16>;
	.reg .b64 	%rd<54>;

	ld.param.u64 	%rd23, [vector_sum_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd23;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	mov.u32 	%r6, %tid.x;
	cvt.u64.u32 	%rd4, %r6;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	shl.b64 	%rd24, %rd3, 2;
	add.s64 	%rd5, %rd2, %rd24;
	mul.wide.u32 	%rd25, %r7, %r1;
	add.s64 	%rd6, %rd25, %rd4;
	// begin inline asm
	.shared .align 4 .b8 nonphysical[128];
    mov.u32 %r5, nonphysical;
	// end inline asm
	setp.eq.s64 	%p1, %rd6, 0;
	@%p1 bra 	$L__BB47_1;
	setp.gt.u64 	%p2, %rd3, %rd6;
	shl.b64 	%rd26, %rd6, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd52, %rd28, %rd5, %p2;
	selp.b64 	%rd53, %rd27, 0, %p2;
	bra.uni 	$L__BB47_6;
$L__BB47_1:
	setp.eq.s64 	%p3, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p3;
	add.s64 	%rd52, %rd2, %rd29;
	selp.b64 	%rd53, 0, %rd2, %p3;
$L__BB47_6:
	setp.eq.s64 	%p4, %rd53, 0;
	mov.f32 	%f13, 0f00000000;
	@%p4 bra 	$L__BB47_8;
	mov.u32 	%r8, %nctaid.x;
	mul.wide.u32 	%rd7, %r1, %r8;
	add.s64 	%rd8, %rd7, -1;
	ld.u32 	%r11, [%rd53];
	mov.b32 	%r10, 0;
	// begin inline asm
	add.rn.ftz.f32 %r9, %r10, %r11;
	// end inline asm
	mov.b32 	%f13, %r9;
	sub.s64 	%rd30, %rd5, %rd52;
	shr.u64 	%rd31, %rd30, 2;
	setp.gt.u64 	%p5, %rd31, %rd8;
	@%p5 bra 	$L__BB47_2;
	bra.uni 	$L__BB47_8;
$L__BB47_2:
	shl.b64 	%rd32, %rd7, 2;
	add.s64 	%rd33, %rd52, %rd32;
	add.s64 	%rd34, %rd33, -4;
	ld.u32 	%r14, [%rd33+-4];
	// begin inline asm
	add.rn.ftz.f32 %r106, %r9, %r14;
	// end inline asm
	mov.b32 	%f13, %r106;
	sub.s64 	%rd35, %rd5, %rd34;
	add.s64 	%rd36, %rd35, -4;
	shr.u64 	%rd37, %rd36, 2;
	setp.le.u64 	%p6, %rd37, %rd8;
	@%p6 bra 	$L__BB47_8;
	shl.b64 	%rd40, %rd7, 3;
	sub.s64 	%rd41, %rd5, %rd40;
	sub.s64 	%rd51, %rd41, %rd52;
	add.s64 	%rd42, %rd40, %rd52;
	add.s64 	%rd50, %rd42, -4;
$L__BB47_4:
	ld.u32 	%r17, [%rd50];
	// begin inline asm
	add.rn.ftz.f32 %r15, %r106, %r17;
	// end inline asm
	mov.b32 	%f13, %r15;
	shr.u64 	%rd43, %rd51, 2;
	setp.gt.u64 	%p7, %rd43, %rd8;
	sub.s64 	%rd51, %rd51, %rd32;
	add.s64 	%rd50, %rd50, %rd32;
	mov.u32 	%r106, %r15;
	@%p7 bra 	$L__BB47_4;
$L__BB47_8:
	cvt.u32.u64 	%r53, %rd4;
	mov.b32 	%r21, %f13;
	mov.b32 	%r19, 16;
	mov.b32 	%r67, 31;
	// begin inline asm
	shfl.sync.bfly.b32 %r18,%r21, %r19, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r28, %r21, %r18;
	// end inline asm
	mov.b32 	%r26, 8;
	// begin inline asm
	shfl.sync.bfly.b32 %r25,%r28, %r26, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r35, %r28, %r25;
	// end inline asm
	mov.b32 	%r33, 4;
	// begin inline asm
	shfl.sync.bfly.b32 %r32,%r35, %r33, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r42, %r35, %r32;
	// end inline asm
	mov.b32 	%r40, 2;
	// begin inline asm
	shfl.sync.bfly.b32 %r39,%r42, %r40, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r49, %r42, %r39;
	// end inline asm
	mov.b32 	%r47, 1;
	// begin inline asm
	shfl.sync.bfly.b32 %r46,%r49, %r47, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r104, %r49, %r46;
	// end inline asm
	and.b64  	%rd22, %rd4, 31;
	setp.ne.s64 	%p8, %rd22, 0;
	@%p8 bra 	$L__BB47_10;
	mov.b32 	%f6, %r104;
	shr.u32 	%r54, %r53, 3;
	and.b32  	%r55, %r54, 124;
	add.s32 	%r56, %r5, %r55;
	// begin inline asm
	st.shared.f32 [%r56], %r104;
	// end inline asm
$L__BB47_10:
	bar.sync 	0;
	shr.u32 	%r59, %r1, 5;
	setp.ge.u32 	%p9, %r53, %r59;
	mov.f32 	%f14, 0f00000000;
	@%p9 bra 	$L__BB47_12;
	cvt.u32.u64 	%r60, %rd22;
	shl.b32 	%r61, %r60, 2;
	add.s32 	%r63, %r5, %r61;
	// begin inline asm
	ld.shared.f32 %r62, [%r63];
	// end inline asm
	mov.b32 	%f14, %r62;
$L__BB47_12:
	setp.gt.u32 	%p10, %r53, 31;
	@%p10 bra 	$L__BB47_14;
	setp.eq.s64 	%p11, %rd22, 0;
	mov.b32 	%r68, %f14;
	// begin inline asm
	shfl.sync.bfly.b32 %r65,%r68, %r19, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r75, %r68, %r65;
	// end inline asm
	// begin inline asm
	shfl.sync.bfly.b32 %r72,%r75, %r26, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r82, %r75, %r72;
	// end inline asm
	// begin inline asm
	shfl.sync.bfly.b32 %r79,%r82, %r33, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r89, %r82, %r79;
	// end inline asm
	// begin inline asm
	shfl.sync.bfly.b32 %r86,%r89, %r40, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r96, %r89, %r86;
	// end inline asm
	// begin inline asm
	shfl.sync.bfly.b32 %r93,%r96, %r47, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r97, %r96, %r93;
	// end inline asm
	@%p11 bra 	$L__BB47_15;
	bra.uni 	$L__BB47_14;
$L__BB47_15:
	ld.global.nc.u64 	%rd44, [%rd1+24];
	setp.ne.s64 	%p12, %rd44, 0;
	@%p12 bra 	$L__BB47_17;
	bra.uni 	$L__BB47_16;
$L__BB47_17:
	mov.b32 	%f9, %r97;
	ld.global.nc.u64 	%rd45, [%rd1+16];
	// begin inline asm
	red.global.add.f32 [%rd45], %r97;
	// end inline asm
$L__BB47_14:
	ret;
$L__BB47_16:
	mov.u64 	%rd46, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_33;
	cvta.global.u64 	%rd47, %rd46;
	mov.u64 	%rd48, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_35;
	cvta.global.u64 	%rd49, %rd48;
	{ // callseq 239, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd47;
	.param .b64 param1;
	st.param.b64 	[param1+0], 40;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd49;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 239

}
	// .globl	vector_product_f32
.visible .entry vector_product_f32(
	.param .u64 vector_product_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_37;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 240, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 240

}
	// .globl	vector_greater_f32
.visible .entry vector_greater_f32(
	.param .u64 vector_greater_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_38;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 241, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 241

}
	// .globl	vector_lesser_f32
.visible .entry vector_lesser_f32(
	.param .u64 vector_lesser_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_39;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 242, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 242

}
	// .globl	vector_mean_f32
.visible .entry vector_mean_f32(
	.param .u64 vector_mean_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_40;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 243, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 243

}
	// .globl	vector_variance_f32
.visible .entry vector_variance_f32(
	.param .u64 vector_variance_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_41;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 244, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 244

}
	// .globl	vector_deviation_f32
.visible .entry vector_deviation_f32(
	.param .u64 vector_deviation_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_42;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 245, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 245

}
	// .globl	vector_add_f32
.visible .entry vector_add_f32(
	.param .u64 vector_add_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_add_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB54_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB54_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB54_4;
	bra.uni 	$L__BB54_3;
$L__BB54_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB54_5;
$L__BB54_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB54_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB54_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB54_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB54_9;
	bra.uni 	$L__BB54_8;
$L__BB54_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB54_10;
$L__BB54_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB54_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB54_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB54_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB54_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB54_15;
$L__BB54_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB54_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB54_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB54_18;
$L__BB54_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB54_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB54_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	add.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB54_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB54_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB54_28;
	bra.uni 	$L__BB54_22;
$L__BB54_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB54_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB54_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	add.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB54_31:
	st.f32 	[%rd53], %f17;
$L__BB54_32:
	ret;
$L__BB54_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB54_23;
$L__BB54_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB54_23;
	bra.uni 	$L__BB54_28;
$L__BB54_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB54_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	add.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB54_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB54_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	add.rn.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB54_27;
$L__BB54_33:
	mov.u64 	%rd99, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_43;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 246, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 246

}
	// .globl	vector_sub_f32
.visible .entry vector_sub_f32(
	.param .u64 vector_sub_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_sub_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB55_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB55_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB55_4;
	bra.uni 	$L__BB55_3;
$L__BB55_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB55_5;
$L__BB55_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB55_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB55_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB55_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB55_9;
	bra.uni 	$L__BB55_8;
$L__BB55_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB55_10;
$L__BB55_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB55_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB55_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB55_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB55_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB55_15;
$L__BB55_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB55_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB55_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB55_18;
$L__BB55_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB55_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB55_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	sub.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB55_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB55_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB55_28;
	bra.uni 	$L__BB55_22;
$L__BB55_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB55_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB55_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	sub.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB55_31:
	st.f32 	[%rd53], %f17;
$L__BB55_32:
	ret;
$L__BB55_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB55_23;
$L__BB55_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB55_23;
	bra.uni 	$L__BB55_28;
$L__BB55_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB55_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	sub.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB55_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB55_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	sub.rn.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB55_27;
$L__BB55_33:
	mov.u64 	%rd99, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_44;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 247, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 247

}
	// .globl	vector_mul_f32
.visible .entry vector_mul_f32(
	.param .u64 vector_mul_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_mul_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB56_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB56_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB56_4;
	bra.uni 	$L__BB56_3;
$L__BB56_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB56_5;
$L__BB56_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB56_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB56_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB56_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB56_9;
	bra.uni 	$L__BB56_8;
$L__BB56_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB56_10;
$L__BB56_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB56_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB56_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB56_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB56_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB56_15;
$L__BB56_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB56_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB56_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB56_18;
$L__BB56_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB56_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB56_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB56_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB56_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB56_28;
	bra.uni 	$L__BB56_22;
$L__BB56_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB56_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB56_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB56_31:
	st.f32 	[%rd53], %f17;
$L__BB56_32:
	ret;
$L__BB56_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB56_23;
$L__BB56_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB56_23;
	bra.uni 	$L__BB56_28;
$L__BB56_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB56_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB56_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB56_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB56_27;
$L__BB56_33:
	mov.u64 	%rd99, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_45;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 248, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 248

}
	// .globl	vector_div_f32
.visible .entry vector_div_f32(
	.param .u64 vector_div_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_div_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB57_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB57_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB57_4;
	bra.uni 	$L__BB57_3;
$L__BB57_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB57_5;
$L__BB57_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB57_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB57_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB57_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB57_9;
	bra.uni 	$L__BB57_8;
$L__BB57_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB57_10;
$L__BB57_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB57_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB57_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB57_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB57_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB57_15;
$L__BB57_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB57_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB57_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB57_18;
$L__BB57_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB57_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB57_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB57_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB57_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB57_28;
	bra.uni 	$L__BB57_22;
$L__BB57_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB57_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB57_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB57_31:
	st.f32 	[%rd53], %f17;
$L__BB57_32:
	ret;
$L__BB57_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB57_23;
$L__BB57_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB57_23;
	bra.uni 	$L__BB57_28;
$L__BB57_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB57_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB57_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB57_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB57_27;
$L__BB57_33:
	mov.u64 	%rd99, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_46;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 249, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 249

}
	// .globl	vector_neg_f32
.visible .entry vector_neg_f32(
	.param .u64 vector_neg_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_neg_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB58_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB58_3;
	bra.uni 	$L__BB58_2;
$L__BB58_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB58_4;
$L__BB58_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB58_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB58_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB58_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB58_8;
	bra.uni 	$L__BB58_7;
$L__BB58_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB58_9;
$L__BB58_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB58_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB58_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB58_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB58_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB58_14;
$L__BB58_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB58_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB58_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB58_17;
$L__BB58_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB58_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB58_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	neg.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB58_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB58_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB58_27;
	bra.uni 	$L__BB58_21;
$L__BB58_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB58_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB58_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	neg.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB58_30:
	st.f32 	[%rd52], %f16;
$L__BB58_31:
	ret;
$L__BB58_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB58_22;
$L__BB58_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB58_22;
	bra.uni 	$L__BB58_27;
$L__BB58_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB58_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	neg.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB58_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB58_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	neg.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB58_26;

}
	// .globl	vector_scale_f32
.visible .entry vector_scale_f32(
	.param .u64 vector_scale_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_scale_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB59_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB59_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB59_4;
	bra.uni 	$L__BB59_3;
$L__BB59_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB59_5;
$L__BB59_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB59_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB59_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB59_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB59_9;
	bra.uni 	$L__BB59_8;
$L__BB59_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB59_10;
$L__BB59_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB59_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB59_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB59_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB59_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB59_15;
$L__BB59_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB59_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB59_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB59_18;
$L__BB59_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB59_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB59_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB59_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB59_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB59_28;
	bra.uni 	$L__BB59_22;
$L__BB59_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB59_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB59_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB59_31:
	st.f32 	[%rd53], %f17;
$L__BB59_32:
	ret;
$L__BB59_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB59_23;
$L__BB59_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB59_23;
	bra.uni 	$L__BB59_28;
$L__BB59_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB59_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB59_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB59_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB59_27;
$L__BB59_33:
	mov.u64 	%rd99, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_47;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 250, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 250

}
	// .globl	vector_descale_f32
.visible .entry vector_descale_f32(
	.param .u64 vector_descale_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_descale_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB60_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB60_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB60_4;
	bra.uni 	$L__BB60_3;
$L__BB60_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB60_5;
$L__BB60_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB60_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB60_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB60_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB60_9;
	bra.uni 	$L__BB60_8;
$L__BB60_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB60_10;
$L__BB60_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB60_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB60_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB60_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB60_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB60_15;
$L__BB60_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB60_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB60_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB60_18;
$L__BB60_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB60_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB60_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB60_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB60_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB60_28;
	bra.uni 	$L__BB60_22;
$L__BB60_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB60_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB60_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB60_31:
	st.f32 	[%rd53], %f17;
$L__BB60_32:
	ret;
$L__BB60_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB60_23;
$L__BB60_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB60_23;
	bra.uni 	$L__BB60_28;
$L__BB60_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB60_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB60_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB60_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB60_27;
$L__BB60_33:
	mov.u64 	%rd99, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_48;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 251, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 251

}
	// .globl	vector_powf_f32
.visible .entry vector_powf_f32(
	.param .u64 vector_powf_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_powf_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB61_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB61_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB61_4;
	bra.uni 	$L__BB61_3;
$L__BB61_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB61_5;
$L__BB61_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB61_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB61_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB61_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB61_9;
	bra.uni 	$L__BB61_8;
$L__BB61_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB61_10;
$L__BB61_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB61_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB61_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB61_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB61_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB61_15;
$L__BB61_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB61_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB61_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB61_18;
$L__BB61_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB61_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB61_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	lg2.approx.ftz.f32 %r11, %r12;
    mul.rn.ftz.f32 %r11, %r11, %r13;
    ex2.approx.ftz.f32 %r11, %r11;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB61_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB61_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB61_28;
	bra.uni 	$L__BB61_22;
$L__BB61_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB61_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB61_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	lg2.approx.ftz.f32 %r20, %r21;
    mul.rn.ftz.f32 %r20, %r20, %r22;
    ex2.approx.ftz.f32 %r20, %r20;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB61_31:
	st.f32 	[%rd53], %f17;
$L__BB61_32:
	ret;
$L__BB61_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB61_23;
$L__BB61_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB61_23;
	bra.uni 	$L__BB61_28;
$L__BB61_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB61_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	lg2.approx.ftz.f32 %r14, %r15;
    mul.rn.ftz.f32 %r14, %r14, %r16;
    ex2.approx.ftz.f32 %r14, %r14;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB61_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB61_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	lg2.approx.ftz.f32 %r17, %r18;
    mul.rn.ftz.f32 %r17, %r17, %r16;
    ex2.approx.ftz.f32 %r17, %r17;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB61_27;
$L__BB61_33:
	mov.u64 	%rd99, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_49;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 252, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 252

}
	// .globl	vector_ln_f32
.visible .entry vector_ln_f32(
	.param .u64 vector_ln_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_ln_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB62_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB62_3;
	bra.uni 	$L__BB62_2;
$L__BB62_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB62_4;
$L__BB62_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB62_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB62_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB62_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB62_8;
	bra.uni 	$L__BB62_7;
$L__BB62_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB62_9;
$L__BB62_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB62_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB62_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB62_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB62_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB62_14;
$L__BB62_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB62_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB62_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB62_17;
$L__BB62_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB62_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB62_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	lg2.approx.ftz.f32 %r11, %r12;
    mul.rn.ftz.f32 %r11, %r11, 0f3F317218;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB62_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB62_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB62_27;
	bra.uni 	$L__BB62_21;
$L__BB62_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB62_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB62_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	lg2.approx.ftz.f32 %r17, %r18;
    mul.rn.ftz.f32 %r17, %r17, 0f3F317218;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB62_30:
	st.f32 	[%rd52], %f16;
$L__BB62_31:
	ret;
$L__BB62_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB62_22;
$L__BB62_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB62_22;
	bra.uni 	$L__BB62_27;
$L__BB62_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB62_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	lg2.approx.ftz.f32 %r13, %r14;
    mul.rn.ftz.f32 %r13, %r13, 0f3F317218;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB62_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB62_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	lg2.approx.ftz.f32 %r15, %r16;
    mul.rn.ftz.f32 %r15, %r15, 0f3F317218;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB62_26;

}
	// .globl	vector_log2_f32
.visible .entry vector_log2_f32(
	.param .u64 vector_log2_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_log2_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB63_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB63_3;
	bra.uni 	$L__BB63_2;
$L__BB63_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB63_4;
$L__BB63_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB63_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB63_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB63_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB63_8;
	bra.uni 	$L__BB63_7;
$L__BB63_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB63_9;
$L__BB63_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB63_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB63_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB63_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB63_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB63_14;
$L__BB63_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB63_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB63_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB63_17;
$L__BB63_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB63_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB63_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	lg2.approx.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB63_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB63_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB63_27;
	bra.uni 	$L__BB63_21;
$L__BB63_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB63_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB63_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	lg2.approx.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB63_30:
	st.f32 	[%rd52], %f16;
$L__BB63_31:
	ret;
$L__BB63_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB63_22;
$L__BB63_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB63_22;
	bra.uni 	$L__BB63_27;
$L__BB63_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB63_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	lg2.approx.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB63_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB63_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	lg2.approx.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB63_26;

}
	// .globl	vector_exp_f32
.visible .entry vector_exp_f32(
	.param .u64 vector_exp_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_exp_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB64_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB64_3;
	bra.uni 	$L__BB64_2;
$L__BB64_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB64_4;
$L__BB64_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB64_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB64_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB64_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB64_8;
	bra.uni 	$L__BB64_7;
$L__BB64_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB64_9;
$L__BB64_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB64_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB64_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB64_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB64_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB64_14;
$L__BB64_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB64_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB64_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB64_17;
$L__BB64_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB64_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB64_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r11, %r11;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB64_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB64_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB64_27;
	bra.uni 	$L__BB64_21;
$L__BB64_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB64_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB64_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r17, %r17;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB64_30:
	st.f32 	[%rd52], %f16;
$L__BB64_31:
	ret;
$L__BB64_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB64_22;
$L__BB64_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB64_22;
	bra.uni 	$L__BB64_27;
$L__BB64_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB64_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	mul.rn.ftz.f32 %r13, %r14, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r13, %r13;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB64_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB64_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	mul.rn.ftz.f32 %r15, %r16, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r15, %r15;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB64_26;

}
	// .globl	vector_exp2_f32
.visible .entry vector_exp2_f32(
	.param .u64 vector_exp2_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_exp2_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB65_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB65_3;
	bra.uni 	$L__BB65_2;
$L__BB65_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB65_4;
$L__BB65_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB65_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB65_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB65_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB65_8;
	bra.uni 	$L__BB65_7;
$L__BB65_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB65_9;
$L__BB65_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB65_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB65_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB65_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB65_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB65_14;
$L__BB65_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB65_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB65_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB65_17;
$L__BB65_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB65_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB65_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	ex2.approx.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB65_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB65_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB65_27;
	bra.uni 	$L__BB65_21;
$L__BB65_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB65_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB65_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	ex2.approx.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB65_30:
	st.f32 	[%rd52], %f16;
$L__BB65_31:
	ret;
$L__BB65_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB65_22;
$L__BB65_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB65_22;
	bra.uni 	$L__BB65_27;
$L__BB65_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB65_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	ex2.approx.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB65_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB65_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	ex2.approx.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB65_26;

}
	// .globl	vector_recip_f32
.visible .entry vector_recip_f32(
	.param .u64 vector_recip_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_recip_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB66_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB66_3;
	bra.uni 	$L__BB66_2;
$L__BB66_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB66_4;
$L__BB66_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB66_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB66_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB66_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB66_8;
	bra.uni 	$L__BB66_7;
$L__BB66_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB66_9;
$L__BB66_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB66_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB66_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB66_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB66_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB66_14;
$L__BB66_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB66_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB66_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB66_17;
$L__BB66_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB66_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB66_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	rcp.approx.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB66_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB66_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB66_27;
	bra.uni 	$L__BB66_21;
$L__BB66_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB66_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB66_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	rcp.approx.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB66_30:
	st.f32 	[%rd52], %f16;
$L__BB66_31:
	ret;
$L__BB66_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB66_22;
$L__BB66_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB66_22;
	bra.uni 	$L__BB66_27;
$L__BB66_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB66_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	rcp.approx.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB66_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB66_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	rcp.approx.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB66_26;

}
	// .globl	vector_sin_f32
.visible .entry vector_sin_f32(
	.param .u64 vector_sin_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_sin_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB67_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB67_3;
	bra.uni 	$L__BB67_2;
$L__BB67_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB67_4;
$L__BB67_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB67_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB67_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB67_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB67_8;
	bra.uni 	$L__BB67_7;
$L__BB67_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB67_9;
$L__BB67_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB67_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB67_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB67_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB67_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB67_14;
$L__BB67_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB67_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB67_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB67_17;
$L__BB67_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB67_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB67_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	sin.approx.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB67_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB67_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB67_27;
	bra.uni 	$L__BB67_21;
$L__BB67_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB67_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB67_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	sin.approx.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB67_30:
	st.f32 	[%rd52], %f16;
$L__BB67_31:
	ret;
$L__BB67_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB67_22;
$L__BB67_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB67_22;
	bra.uni 	$L__BB67_27;
$L__BB67_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB67_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	sin.approx.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB67_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB67_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	sin.approx.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB67_26;

}
	// .globl	vector_cos_f32
.visible .entry vector_cos_f32(
	.param .u64 vector_cos_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_cos_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB68_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB68_3;
	bra.uni 	$L__BB68_2;
$L__BB68_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB68_4;
$L__BB68_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB68_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB68_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB68_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB68_8;
	bra.uni 	$L__BB68_7;
$L__BB68_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB68_9;
$L__BB68_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB68_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB68_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB68_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB68_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB68_14;
$L__BB68_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB68_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB68_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB68_17;
$L__BB68_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB68_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB68_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	cos.approx.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB68_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB68_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB68_27;
	bra.uni 	$L__BB68_21;
$L__BB68_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB68_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB68_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	cos.approx.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB68_30:
	st.f32 	[%rd52], %f16;
$L__BB68_31:
	ret;
$L__BB68_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB68_22;
$L__BB68_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB68_22;
	bra.uni 	$L__BB68_27;
$L__BB68_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB68_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	cos.approx.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB68_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB68_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	cos.approx.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB68_26;

}
	// .globl	vector_tan_f32
.visible .entry vector_tan_f32(
	.param .u64 vector_tan_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_tan_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB69_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB69_3;
	bra.uni 	$L__BB69_2;
$L__BB69_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB69_4;
$L__BB69_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB69_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB69_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB69_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB69_8;
	bra.uni 	$L__BB69_7;
$L__BB69_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB69_9;
$L__BB69_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB69_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB69_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB69_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB69_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB69_14;
$L__BB69_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB69_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB69_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB69_17;
$L__BB69_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB69_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB69_19;
	ld.u32 	%r13, [%rd106];
	// begin inline asm
	sin.approx.ftz.f32 %r12, %r13;
    cos.approx.ftz.f32 %r11, %r13;
    div.approx.ftz.f32 %r12, %r12, %r11;
	// end inline asm
	mov.b32 	%f13, %r12;
$L__BB69_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB69_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB69_27;
	bra.uni 	$L__BB69_21;
$L__BB69_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB69_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB69_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r22, [%rd53+-4];
	// begin inline asm
	sin.approx.ftz.f32 %r21, %r22;
    cos.approx.ftz.f32 %r20, %r22;
    div.approx.ftz.f32 %r21, %r21, %r20;
	// end inline asm
	mov.b32 	%f16, %r21;
$L__BB69_30:
	st.f32 	[%rd52], %f16;
$L__BB69_31:
	ret;
$L__BB69_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB69_22;
$L__BB69_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB69_22;
	bra.uni 	$L__BB69_27;
$L__BB69_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB69_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r16, [%rd43];
	// begin inline asm
	sin.approx.ftz.f32 %r15, %r16;
    cos.approx.ftz.f32 %r14, %r16;
    div.approx.ftz.f32 %r15, %r15, %r14;
	// end inline asm
	mov.b32 	%f14, %r15;
$L__BB69_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB69_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r19, [%rd47];
	// begin inline asm
	sin.approx.ftz.f32 %r18, %r19;
    cos.approx.ftz.f32 %r17, %r19;
    div.approx.ftz.f32 %r18, %r18, %r17;
	// end inline asm
	mov.b32 	%f15, %r18;
	bra.uni 	$L__BB69_26;

}
	// .globl	vector_asin_f32
.visible .entry vector_asin_f32(
	.param .u64 vector_asin_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_asin_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB70_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB70_3;
	bra.uni 	$L__BB70_2;
$L__BB70_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB70_4;
$L__BB70_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB70_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB70_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB70_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB70_8;
	bra.uni 	$L__BB70_7;
$L__BB70_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB70_9;
$L__BB70_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB70_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB70_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB70_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB70_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB70_16;
	bra.uni 	$L__BB70_14;
$L__BB70_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB70_16;
	bra.uni 	$L__BB70_14;
$L__BB70_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB70_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB70_17;
	bra.uni 	$L__BB70_16;
$L__BB70_17:
	ret;
$L__BB70_16:
	mov.u64 	%rd45, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_20;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 253, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 253

}
	// .globl	vector_acos_f32
.visible .entry vector_acos_f32(
	.param .u64 vector_acos_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_acos_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB71_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB71_3;
	bra.uni 	$L__BB71_2;
$L__BB71_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB71_4;
$L__BB71_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB71_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB71_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB71_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB71_8;
	bra.uni 	$L__BB71_7;
$L__BB71_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB71_9;
$L__BB71_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB71_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB71_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB71_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB71_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB71_16;
	bra.uni 	$L__BB71_14;
$L__BB71_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB71_16;
	bra.uni 	$L__BB71_14;
$L__BB71_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB71_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB71_17;
	bra.uni 	$L__BB71_16;
$L__BB71_17:
	ret;
$L__BB71_16:
	mov.u64 	%rd45, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_21;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 254, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 254

}
	// .globl	vector_atan_f32
.visible .entry vector_atan_f32(
	.param .u64 vector_atan_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_atan_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB72_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB72_3;
	bra.uni 	$L__BB72_2;
$L__BB72_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB72_4;
$L__BB72_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB72_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB72_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB72_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB72_8;
	bra.uni 	$L__BB72_7;
$L__BB72_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB72_9;
$L__BB72_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB72_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB72_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB72_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB72_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB72_16;
	bra.uni 	$L__BB72_14;
$L__BB72_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB72_16;
	bra.uni 	$L__BB72_14;
$L__BB72_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB72_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB72_17;
	bra.uni 	$L__BB72_16;
$L__BB72_17:
	ret;
$L__BB72_16:
	mov.u64 	%rd45, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_22;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 255, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 255

}
	// .globl	vector_sinh_f32
.visible .entry vector_sinh_f32(
	.param .u64 vector_sinh_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_sinh_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB73_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB73_3;
	bra.uni 	$L__BB73_2;
$L__BB73_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB73_4;
$L__BB73_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB73_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB73_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB73_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB73_8;
	bra.uni 	$L__BB73_7;
$L__BB73_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB73_9;
$L__BB73_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB73_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB73_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB73_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB73_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB73_14;
$L__BB73_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB73_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB73_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB73_17;
$L__BB73_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB73_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB73_19;
	ld.u32 	%r13, [%rd106];
	// begin inline asm
	mul.rn.ftz.f32 %r12, %r13, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r12, %r12;
    neg.ftz.f32 %r11, %r13;
    mul.rn.ftz.f32 %r11, %r11, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r11, %r11;
    sub.rn.ftz.f32 %r12, %r12, %r11;
    div.approx.ftz.f32 %r12, %r12, 0f40000000;
	// end inline asm
	mov.b32 	%f13, %r12;
$L__BB73_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB73_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB73_27;
	bra.uni 	$L__BB73_21;
$L__BB73_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB73_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB73_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r22, [%rd53+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r21, %r22, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r21, %r21;
    neg.ftz.f32 %r20, %r22;
    mul.rn.ftz.f32 %r20, %r20, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r20, %r20;
    sub.rn.ftz.f32 %r21, %r21, %r20;
    div.approx.ftz.f32 %r21, %r21, 0f40000000;
	// end inline asm
	mov.b32 	%f16, %r21;
$L__BB73_30:
	st.f32 	[%rd52], %f16;
$L__BB73_31:
	ret;
$L__BB73_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB73_22;
$L__BB73_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB73_22;
	bra.uni 	$L__BB73_27;
$L__BB73_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB73_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r16, [%rd43];
	// begin inline asm
	mul.rn.ftz.f32 %r15, %r16, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r15, %r15;
    neg.ftz.f32 %r14, %r16;
    mul.rn.ftz.f32 %r14, %r14, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r14, %r14;
    sub.rn.ftz.f32 %r15, %r15, %r14;
    div.approx.ftz.f32 %r15, %r15, 0f40000000;
	// end inline asm
	mov.b32 	%f14, %r15;
$L__BB73_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB73_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r19, [%rd47];
	// begin inline asm
	mul.rn.ftz.f32 %r18, %r19, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r18, %r18;
    neg.ftz.f32 %r17, %r19;
    mul.rn.ftz.f32 %r17, %r17, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r17, %r17;
    sub.rn.ftz.f32 %r18, %r18, %r17;
    div.approx.ftz.f32 %r18, %r18, 0f40000000;
	// end inline asm
	mov.b32 	%f15, %r18;
	bra.uni 	$L__BB73_26;

}
	// .globl	vector_cosh_f32
.visible .entry vector_cosh_f32(
	.param .u64 vector_cosh_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_cosh_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB74_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB74_3;
	bra.uni 	$L__BB74_2;
$L__BB74_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB74_4;
$L__BB74_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB74_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB74_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB74_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB74_8;
	bra.uni 	$L__BB74_7;
$L__BB74_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB74_9;
$L__BB74_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB74_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB74_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB74_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB74_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB74_14;
$L__BB74_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB74_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB74_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB74_17;
$L__BB74_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB74_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB74_19;
	ld.u32 	%r13, [%rd106];
	// begin inline asm
	mul.rn.ftz.f32 %r12, %r13, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r12, %r12;
    neg.ftz.f32 %r11, %r13;
    mul.rn.ftz.f32 %r11, %r11, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r11, %r11;
    add.rn.ftz.f32 %r12, %r12, %r11;
    div.approx.ftz.f32 %r12, %r12, 0f40000000;
	// end inline asm
	mov.b32 	%f13, %r12;
$L__BB74_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB74_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB74_27;
	bra.uni 	$L__BB74_21;
$L__BB74_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB74_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB74_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r22, [%rd53+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r21, %r22, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r21, %r21;
    neg.ftz.f32 %r20, %r22;
    mul.rn.ftz.f32 %r20, %r20, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r20, %r20;
    add.rn.ftz.f32 %r21, %r21, %r20;
    div.approx.ftz.f32 %r21, %r21, 0f40000000;
	// end inline asm
	mov.b32 	%f16, %r21;
$L__BB74_30:
	st.f32 	[%rd52], %f16;
$L__BB74_31:
	ret;
$L__BB74_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB74_22;
$L__BB74_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB74_22;
	bra.uni 	$L__BB74_27;
$L__BB74_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB74_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r16, [%rd43];
	// begin inline asm
	mul.rn.ftz.f32 %r15, %r16, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r15, %r15;
    neg.ftz.f32 %r14, %r16;
    mul.rn.ftz.f32 %r14, %r14, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r14, %r14;
    add.rn.ftz.f32 %r15, %r15, %r14;
    div.approx.ftz.f32 %r15, %r15, 0f40000000;
	// end inline asm
	mov.b32 	%f14, %r15;
$L__BB74_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB74_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r19, [%rd47];
	// begin inline asm
	mul.rn.ftz.f32 %r18, %r19, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r18, %r18;
    neg.ftz.f32 %r17, %r19;
    mul.rn.ftz.f32 %r17, %r17, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r17, %r17;
    add.rn.ftz.f32 %r18, %r18, %r17;
    div.approx.ftz.f32 %r18, %r18, 0f40000000;
	// end inline asm
	mov.b32 	%f15, %r18;
	bra.uni 	$L__BB74_26;

}
	// .globl	vector_tanh_f32
.visible .entry vector_tanh_f32(
	.param .u64 vector_tanh_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_tanh_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB75_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB75_3;
	bra.uni 	$L__BB75_2;
$L__BB75_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB75_4;
$L__BB75_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB75_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB75_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB75_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB75_8;
	bra.uni 	$L__BB75_7;
$L__BB75_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB75_9;
$L__BB75_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB75_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB75_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB75_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB75_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB75_14;
$L__BB75_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB75_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB75_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB75_17;
$L__BB75_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB75_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB75_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	tanh.approx.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB75_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB75_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB75_27;
	bra.uni 	$L__BB75_21;
$L__BB75_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB75_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB75_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	tanh.approx.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB75_30:
	st.f32 	[%rd52], %f16;
$L__BB75_31:
	ret;
$L__BB75_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB75_22;
$L__BB75_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB75_22;
	bra.uni 	$L__BB75_27;
$L__BB75_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB75_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	tanh.approx.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB75_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB75_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	tanh.approx.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB75_26;

}
	// .globl	vector_asinh_f32
.visible .entry vector_asinh_f32(
	.param .u64 vector_asinh_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_asinh_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB76_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB76_3;
	bra.uni 	$L__BB76_2;
$L__BB76_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB76_4;
$L__BB76_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB76_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB76_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB76_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB76_8;
	bra.uni 	$L__BB76_7;
$L__BB76_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB76_9;
$L__BB76_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB76_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB76_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB76_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB76_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB76_16;
	bra.uni 	$L__BB76_14;
$L__BB76_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB76_16;
	bra.uni 	$L__BB76_14;
$L__BB76_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB76_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB76_17;
	bra.uni 	$L__BB76_16;
$L__BB76_17:
	ret;
$L__BB76_16:
	mov.u64 	%rd45, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_23;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 256, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 256

}
	// .globl	vector_acosh_f32
.visible .entry vector_acosh_f32(
	.param .u64 vector_acosh_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_acosh_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB77_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB77_3;
	bra.uni 	$L__BB77_2;
$L__BB77_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB77_4;
$L__BB77_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB77_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB77_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB77_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB77_8;
	bra.uni 	$L__BB77_7;
$L__BB77_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB77_9;
$L__BB77_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB77_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB77_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB77_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB77_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB77_16;
	bra.uni 	$L__BB77_14;
$L__BB77_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB77_16;
	bra.uni 	$L__BB77_14;
$L__BB77_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB77_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB77_17;
	bra.uni 	$L__BB77_16;
$L__BB77_17:
	ret;
$L__BB77_16:
	mov.u64 	%rd45, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_24;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 257, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 257

}
	// .globl	vector_atanh_f32
.visible .entry vector_atanh_f32(
	.param .u64 vector_atanh_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_atanh_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB78_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB78_3;
	bra.uni 	$L__BB78_2;
$L__BB78_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB78_4;
$L__BB78_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB78_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB78_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB78_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB78_8;
	bra.uni 	$L__BB78_7;
$L__BB78_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB78_9;
$L__BB78_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB78_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB78_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB78_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB78_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB78_16;
	bra.uni 	$L__BB78_14;
$L__BB78_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB78_16;
	bra.uni 	$L__BB78_14;
$L__BB78_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB78_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB78_17;
	bra.uni 	$L__BB78_16;
$L__BB78_17:
	ret;
$L__BB78_16:
	mov.u64 	%rd45, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_25;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 258, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 258

}
	// .globl	vector_l1_norm_f32
.visible .entry vector_l1_norm_f32(
	.param .u64 vector_l1_norm_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_l1_norm_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB79_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB79_3;
	bra.uni 	$L__BB79_2;
$L__BB79_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB79_4;
$L__BB79_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB79_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB79_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB79_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB79_8;
	bra.uni 	$L__BB79_7;
$L__BB79_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB79_9;
$L__BB79_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB79_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB79_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB79_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB79_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB79_14;
$L__BB79_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB79_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB79_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB79_17;
$L__BB79_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB79_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB79_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	abs.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB79_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB79_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB79_27;
	bra.uni 	$L__BB79_21;
$L__BB79_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB79_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB79_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	abs.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB79_30:
	st.f32 	[%rd52], %f16;
$L__BB79_31:
	ret;
$L__BB79_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB79_22;
$L__BB79_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB79_22;
	bra.uni 	$L__BB79_27;
$L__BB79_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB79_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	abs.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB79_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB79_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	abs.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB79_26;

}
	// .globl	vector_l2_norm_f32
.visible .entry vector_l2_norm_f32(
	.param .u64 vector_l2_norm_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_l2_norm_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB80_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB80_3;
	bra.uni 	$L__BB80_2;
$L__BB80_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB80_4;
$L__BB80_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB80_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB80_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB80_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB80_8;
	bra.uni 	$L__BB80_7;
$L__BB80_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB80_9;
$L__BB80_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB80_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB80_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB80_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB80_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB80_14;
$L__BB80_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB80_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB80_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB80_17;
$L__BB80_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB80_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB80_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB80_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB80_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB80_27;
	bra.uni 	$L__BB80_21;
$L__BB80_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB80_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB80_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB80_30:
	st.f32 	[%rd52], %f16;
$L__BB80_31:
	ret;
$L__BB80_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB80_22;
$L__BB80_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB80_22;
	bra.uni 	$L__BB80_27;
$L__BB80_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB80_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	mul.rn.ftz.f32 %r13, %r14, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB80_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB80_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	mul.rn.ftz.f32 %r15, %r16, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB80_26;

}
	// .globl	vector_add_ref_f32
.visible .entry vector_add_ref_f32(
	.param .u64 vector_add_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_add_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB81_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB81_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB81_4;
$L__BB81_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB81_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB81_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	add.rn.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB81_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB81_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	add.rn.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB81_7;
$L__BB81_8:
	ret;
$L__BB81_9:
	mov.u64 	%rd37, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_50;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 259, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 259

}
	// .globl	vector_sub_ref_f32
.visible .entry vector_sub_ref_f32(
	.param .u64 vector_sub_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_sub_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB82_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB82_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB82_4;
$L__BB82_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB82_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB82_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	sub.rn.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB82_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB82_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	sub.rn.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB82_7;
$L__BB82_8:
	ret;
$L__BB82_9:
	mov.u64 	%rd37, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_51;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 260, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 260

}
	// .globl	vector_mul_ref_f32
.visible .entry vector_mul_ref_f32(
	.param .u64 vector_mul_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_mul_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB83_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB83_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB83_4;
$L__BB83_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB83_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB83_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB83_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB83_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	mul.rn.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB83_7;
$L__BB83_8:
	ret;
$L__BB83_9:
	mov.u64 	%rd37, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_52;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 261, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 261

}
	// .globl	vector_div_ref_f32
.visible .entry vector_div_ref_f32(
	.param .u64 vector_div_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_div_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB84_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB84_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB84_4;
$L__BB84_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB84_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB84_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB84_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB84_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	div.approx.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB84_7;
$L__BB84_8:
	ret;
$L__BB84_9:
	mov.u64 	%rd37, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_53;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 262, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 262

}
	// .globl	vector_neg_ref_f32
.visible .entry vector_neg_ref_f32(
	.param .u64 vector_neg_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_neg_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB85_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB85_3;
$L__BB85_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB85_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB85_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	neg.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB85_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB85_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	neg.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB85_6;
$L__BB85_7:
	ret;

}
	// .globl	vector_scale_ref_f32
.visible .entry vector_scale_ref_f32(
	.param .u64 vector_scale_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_scale_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB86_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB86_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB86_4;
$L__BB86_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB86_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB86_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB86_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB86_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	mul.rn.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB86_7;
$L__BB86_8:
	ret;
$L__BB86_9:
	mov.u64 	%rd37, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_54;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 263, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 263

}
	// .globl	vector_descale_ref_f32
.visible .entry vector_descale_ref_f32(
	.param .u64 vector_descale_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_descale_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB87_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB87_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB87_4;
$L__BB87_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB87_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB87_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB87_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB87_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	div.approx.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB87_7;
$L__BB87_8:
	ret;
$L__BB87_9:
	mov.u64 	%rd37, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_55;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 264, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 264

}
	// .globl	vector_powf_ref_f32
.visible .entry vector_powf_ref_f32(
	.param .u64 vector_powf_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_powf_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB88_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB88_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB88_4;
$L__BB88_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB88_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB88_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	lg2.approx.ftz.f32 %r5, %r6;
    mul.rn.ftz.f32 %r5, %r5, %r7;
    ex2.approx.ftz.f32 %r5, %r5;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB88_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB88_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	lg2.approx.ftz.f32 %r8, %r9;
    mul.rn.ftz.f32 %r8, %r8, %r7;
    ex2.approx.ftz.f32 %r8, %r8;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB88_7;
$L__BB88_8:
	ret;
$L__BB88_9:
	mov.u64 	%rd37, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_56;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 265, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 265

}
	// .globl	vector_ln_ref_f32
.visible .entry vector_ln_ref_f32(
	.param .u64 vector_ln_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_ln_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB89_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB89_3;
$L__BB89_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB89_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB89_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	lg2.approx.ftz.f32 %r5, %r6;
    mul.rn.ftz.f32 %r5, %r5, 0f3F317218;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB89_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB89_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	lg2.approx.ftz.f32 %r7, %r8;
    mul.rn.ftz.f32 %r7, %r7, 0f3F317218;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB89_6;
$L__BB89_7:
	ret;

}
	// .globl	vector_log2_ref_f32
.visible .entry vector_log2_ref_f32(
	.param .u64 vector_log2_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_log2_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB90_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB90_3;
$L__BB90_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB90_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB90_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	lg2.approx.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB90_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB90_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	lg2.approx.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB90_6;
$L__BB90_7:
	ret;

}
	// .globl	vector_exp_ref_f32
.visible .entry vector_exp_ref_f32(
	.param .u64 vector_exp_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_exp_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB91_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB91_3;
$L__BB91_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB91_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB91_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	mul.rn.ftz.f32 %r5, %r6, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r5, %r5;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB91_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB91_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	mul.rn.ftz.f32 %r7, %r8, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r7, %r7;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB91_6;
$L__BB91_7:
	ret;

}
	// .globl	vector_exp2_ref_f32
.visible .entry vector_exp2_ref_f32(
	.param .u64 vector_exp2_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_exp2_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB92_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB92_3;
$L__BB92_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB92_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB92_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	ex2.approx.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB92_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB92_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	ex2.approx.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB92_6;
$L__BB92_7:
	ret;

}
	// .globl	vector_recip_ref_f32
.visible .entry vector_recip_ref_f32(
	.param .u64 vector_recip_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_recip_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB93_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB93_3;
$L__BB93_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB93_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB93_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	rcp.approx.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB93_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB93_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	rcp.approx.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB93_6;
$L__BB93_7:
	ret;

}
	// .globl	vector_sin_ref_f32
.visible .entry vector_sin_ref_f32(
	.param .u64 vector_sin_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_sin_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB94_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB94_3;
$L__BB94_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB94_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB94_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	sin.approx.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB94_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB94_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	sin.approx.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB94_6;
$L__BB94_7:
	ret;

}
	// .globl	vector_cos_ref_f32
.visible .entry vector_cos_ref_f32(
	.param .u64 vector_cos_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_cos_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB95_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB95_3;
$L__BB95_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB95_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB95_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	cos.approx.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB95_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB95_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	cos.approx.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB95_6;
$L__BB95_7:
	ret;

}
	// .globl	vector_tan_ref_f32
.visible .entry vector_tan_ref_f32(
	.param .u64 vector_tan_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_tan_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB96_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB96_3;
$L__BB96_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB96_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB96_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r7, [%rd38];
	// begin inline asm
	sin.approx.ftz.f32 %r6, %r7;
    cos.approx.ftz.f32 %r5, %r7;
    div.approx.ftz.f32 %r6, %r6, %r5;
	// end inline asm
	st.u32 	[%rd38], %r6;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB96_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB96_6:
	ld.u32 	%r10, [%rd35];
	// begin inline asm
	sin.approx.ftz.f32 %r9, %r10;
    cos.approx.ftz.f32 %r8, %r10;
    div.approx.ftz.f32 %r9, %r9, %r8;
	// end inline asm
	st.u32 	[%rd35], %r9;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB96_6;
$L__BB96_7:
	ret;

}
	// .globl	vector_asin_ref_f32
.visible .entry vector_asin_ref_f32(
	.param .u64 vector_asin_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_asin_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB97_3;
	bra.uni 	$L__BB97_1;
$L__BB97_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB97_2;
	bra.uni 	$L__BB97_4;
$L__BB97_2:
	ret;
$L__BB97_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB97_2;
$L__BB97_4:
	mov.u64 	%rd7, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_20;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 266, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 266

}
	// .globl	vector_acos_ref_f32
.visible .entry vector_acos_ref_f32(
	.param .u64 vector_acos_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_acos_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB98_3;
	bra.uni 	$L__BB98_1;
$L__BB98_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB98_2;
	bra.uni 	$L__BB98_4;
$L__BB98_2:
	ret;
$L__BB98_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB98_2;
$L__BB98_4:
	mov.u64 	%rd7, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_21;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 267, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 267

}
	// .globl	vector_atan_ref_f32
.visible .entry vector_atan_ref_f32(
	.param .u64 vector_atan_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_atan_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB99_3;
	bra.uni 	$L__BB99_1;
$L__BB99_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB99_2;
	bra.uni 	$L__BB99_4;
$L__BB99_2:
	ret;
$L__BB99_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB99_2;
$L__BB99_4:
	mov.u64 	%rd7, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_22;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 268, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 268

}
	// .globl	vector_sinh_ref_f32
.visible .entry vector_sinh_ref_f32(
	.param .u64 vector_sinh_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_sinh_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB100_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB100_3;
$L__BB100_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB100_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB100_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r7, [%rd38];
	// begin inline asm
	mul.rn.ftz.f32 %r6, %r7, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r6, %r6;
    neg.ftz.f32 %r5, %r7;
    mul.rn.ftz.f32 %r5, %r5, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r5, %r5;
    sub.rn.ftz.f32 %r6, %r6, %r5;
    div.approx.ftz.f32 %r6, %r6, 0f40000000;
	// end inline asm
	st.u32 	[%rd38], %r6;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB100_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB100_6:
	ld.u32 	%r10, [%rd35];
	// begin inline asm
	mul.rn.ftz.f32 %r9, %r10, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r9, %r9;
    neg.ftz.f32 %r8, %r10;
    mul.rn.ftz.f32 %r8, %r8, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r8, %r8;
    sub.rn.ftz.f32 %r9, %r9, %r8;
    div.approx.ftz.f32 %r9, %r9, 0f40000000;
	// end inline asm
	st.u32 	[%rd35], %r9;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB100_6;
$L__BB100_7:
	ret;

}
	// .globl	vector_cosh_ref_f32
.visible .entry vector_cosh_ref_f32(
	.param .u64 vector_cosh_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_cosh_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB101_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB101_3;
$L__BB101_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB101_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB101_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r7, [%rd38];
	// begin inline asm
	mul.rn.ftz.f32 %r6, %r7, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r6, %r6;
    neg.ftz.f32 %r5, %r7;
    mul.rn.ftz.f32 %r5, %r5, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r5, %r5;
    add.rn.ftz.f32 %r6, %r6, %r5;
    div.approx.ftz.f32 %r6, %r6, 0f40000000;
	// end inline asm
	st.u32 	[%rd38], %r6;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB101_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB101_6:
	ld.u32 	%r10, [%rd35];
	// begin inline asm
	mul.rn.ftz.f32 %r9, %r10, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r9, %r9;
    neg.ftz.f32 %r8, %r10;
    mul.rn.ftz.f32 %r8, %r8, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r8, %r8;
    add.rn.ftz.f32 %r9, %r9, %r8;
    div.approx.ftz.f32 %r9, %r9, 0f40000000;
	// end inline asm
	st.u32 	[%rd35], %r9;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB101_6;
$L__BB101_7:
	ret;

}
	// .globl	vector_tanh_ref_f32
.visible .entry vector_tanh_ref_f32(
	.param .u64 vector_tanh_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_tanh_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB102_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB102_3;
$L__BB102_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB102_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB102_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	tanh.approx.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB102_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB102_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	tanh.approx.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB102_6;
$L__BB102_7:
	ret;

}
	// .globl	vector_asinh_ref_f32
.visible .entry vector_asinh_ref_f32(
	.param .u64 vector_asinh_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_asinh_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB103_3;
	bra.uni 	$L__BB103_1;
$L__BB103_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB103_2;
	bra.uni 	$L__BB103_4;
$L__BB103_2:
	ret;
$L__BB103_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB103_2;
$L__BB103_4:
	mov.u64 	%rd7, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_23;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 269, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 269

}
	// .globl	vector_acosh_ref_f32
.visible .entry vector_acosh_ref_f32(
	.param .u64 vector_acosh_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_acosh_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB104_3;
	bra.uni 	$L__BB104_1;
$L__BB104_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB104_2;
	bra.uni 	$L__BB104_4;
$L__BB104_2:
	ret;
$L__BB104_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB104_2;
$L__BB104_4:
	mov.u64 	%rd7, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_24;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 270, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 270

}
	// .globl	vector_atanh_ref_f32
.visible .entry vector_atanh_ref_f32(
	.param .u64 vector_atanh_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_atanh_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB105_3;
	bra.uni 	$L__BB105_1;
$L__BB105_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB105_2;
	bra.uni 	$L__BB105_4;
$L__BB105_2:
	ret;
$L__BB105_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB105_2;
$L__BB105_4:
	mov.u64 	%rd7, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_25;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 271, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 271

}
	// .globl	vector_add_vec_f32
.visible .entry vector_add_vec_f32(
	.param .u64 vector_add_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_add_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB106_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB106_3;
	bra.uni 	$L__BB106_2;
$L__BB106_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB106_4;
$L__BB106_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB106_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB106_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB106_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB106_8;
	bra.uni 	$L__BB106_7;
$L__BB106_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB106_9;
$L__BB106_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB106_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB106_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB106_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB106_13;
	bra.uni 	$L__BB106_12;
$L__BB106_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB106_14;
$L__BB106_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB106_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB106_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB106_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB106_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB106_19;
$L__BB106_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB106_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB106_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB106_22;
$L__BB106_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB106_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB106_27;
	@%p11 bra 	$L__BB106_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB106_26;
$L__BB106_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB106_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB106_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB106_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	add.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB106_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB106_38;
	bra.uni 	$L__BB106_30;
$L__BB106_38:
	ret;
$L__BB106_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB106_31;
$L__BB106_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB106_31;
	bra.uni 	$L__BB106_38;
$L__BB106_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB106_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB106_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB106_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB106_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB106_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	add.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB106_37;

}
	// .globl	vector_sub_vec_f32
.visible .entry vector_sub_vec_f32(
	.param .u64 vector_sub_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_sub_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB107_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB107_3;
	bra.uni 	$L__BB107_2;
$L__BB107_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB107_4;
$L__BB107_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB107_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB107_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB107_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB107_8;
	bra.uni 	$L__BB107_7;
$L__BB107_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB107_9;
$L__BB107_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB107_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB107_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB107_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB107_13;
	bra.uni 	$L__BB107_12;
$L__BB107_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB107_14;
$L__BB107_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB107_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB107_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB107_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB107_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB107_19;
$L__BB107_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB107_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB107_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB107_22;
$L__BB107_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB107_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB107_27;
	@%p11 bra 	$L__BB107_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB107_26;
$L__BB107_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB107_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB107_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB107_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	sub.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB107_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB107_38;
	bra.uni 	$L__BB107_30;
$L__BB107_38:
	ret;
$L__BB107_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB107_31;
$L__BB107_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB107_31;
	bra.uni 	$L__BB107_38;
$L__BB107_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB107_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB107_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB107_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB107_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB107_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	sub.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB107_37;

}
	// .globl	vector_mul_vec_f32
.visible .entry vector_mul_vec_f32(
	.param .u64 vector_mul_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_mul_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB108_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB108_3;
	bra.uni 	$L__BB108_2;
$L__BB108_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB108_4;
$L__BB108_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB108_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB108_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB108_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB108_8;
	bra.uni 	$L__BB108_7;
$L__BB108_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB108_9;
$L__BB108_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB108_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB108_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB108_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB108_13;
	bra.uni 	$L__BB108_12;
$L__BB108_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB108_14;
$L__BB108_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB108_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB108_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB108_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB108_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB108_19;
$L__BB108_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB108_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB108_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB108_22;
$L__BB108_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB108_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB108_27;
	@%p11 bra 	$L__BB108_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB108_26;
$L__BB108_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB108_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB108_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB108_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB108_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB108_38;
	bra.uni 	$L__BB108_30;
$L__BB108_38:
	ret;
$L__BB108_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB108_31;
$L__BB108_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB108_31;
	bra.uni 	$L__BB108_38;
$L__BB108_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB108_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB108_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB108_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB108_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB108_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB108_37;

}
	// .globl	vector_div_vec_f32
.visible .entry vector_div_vec_f32(
	.param .u64 vector_div_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_div_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB109_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB109_3;
	bra.uni 	$L__BB109_2;
$L__BB109_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB109_4;
$L__BB109_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB109_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB109_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB109_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB109_8;
	bra.uni 	$L__BB109_7;
$L__BB109_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB109_9;
$L__BB109_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB109_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB109_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB109_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB109_13;
	bra.uni 	$L__BB109_12;
$L__BB109_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB109_14;
$L__BB109_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB109_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB109_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB109_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB109_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB109_19;
$L__BB109_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB109_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB109_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB109_22;
$L__BB109_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB109_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB109_27;
	@%p11 bra 	$L__BB109_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB109_26;
$L__BB109_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB109_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB109_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB109_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB109_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB109_38;
	bra.uni 	$L__BB109_30;
$L__BB109_38:
	ret;
$L__BB109_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB109_31;
$L__BB109_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB109_31;
	bra.uni 	$L__BB109_38;
$L__BB109_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB109_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB109_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB109_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB109_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB109_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB109_37;

}
	// .globl	vector_scale_vec_f32
.visible .entry vector_scale_vec_f32(
	.param .u64 vector_scale_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_scale_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB110_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB110_3;
	bra.uni 	$L__BB110_2;
$L__BB110_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB110_4;
$L__BB110_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB110_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB110_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB110_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB110_8;
	bra.uni 	$L__BB110_7;
$L__BB110_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB110_9;
$L__BB110_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB110_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB110_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB110_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB110_13;
	bra.uni 	$L__BB110_12;
$L__BB110_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB110_14;
$L__BB110_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB110_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB110_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB110_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB110_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB110_19;
$L__BB110_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB110_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB110_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB110_22;
$L__BB110_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB110_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB110_27;
	@%p11 bra 	$L__BB110_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB110_26;
$L__BB110_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB110_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB110_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB110_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB110_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB110_38;
	bra.uni 	$L__BB110_30;
$L__BB110_38:
	ret;
$L__BB110_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB110_31;
$L__BB110_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB110_31;
	bra.uni 	$L__BB110_38;
$L__BB110_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB110_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB110_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB110_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB110_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB110_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB110_37;

}
	// .globl	vector_descale_vec_f32
.visible .entry vector_descale_vec_f32(
	.param .u64 vector_descale_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_descale_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB111_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB111_3;
	bra.uni 	$L__BB111_2;
$L__BB111_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB111_4;
$L__BB111_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB111_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB111_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB111_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB111_8;
	bra.uni 	$L__BB111_7;
$L__BB111_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB111_9;
$L__BB111_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB111_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB111_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB111_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB111_13;
	bra.uni 	$L__BB111_12;
$L__BB111_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB111_14;
$L__BB111_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB111_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB111_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB111_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB111_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB111_19;
$L__BB111_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB111_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB111_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB111_22;
$L__BB111_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB111_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB111_27;
	@%p11 bra 	$L__BB111_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB111_26;
$L__BB111_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB111_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB111_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB111_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB111_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB111_38;
	bra.uni 	$L__BB111_30;
$L__BB111_38:
	ret;
$L__BB111_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB111_31;
$L__BB111_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB111_31;
	bra.uni 	$L__BB111_38;
$L__BB111_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB111_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB111_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB111_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB111_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB111_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB111_37;

}
	// .globl	vector_powf_vec_f32
.visible .entry vector_powf_vec_f32(
	.param .u64 vector_powf_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_powf_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB112_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB112_3;
	bra.uni 	$L__BB112_2;
$L__BB112_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB112_4;
$L__BB112_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB112_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB112_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB112_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB112_8;
	bra.uni 	$L__BB112_7;
$L__BB112_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB112_9;
$L__BB112_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB112_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB112_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB112_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB112_13;
	bra.uni 	$L__BB112_12;
$L__BB112_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB112_14;
$L__BB112_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB112_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB112_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB112_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB112_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB112_19;
$L__BB112_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB112_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB112_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB112_22;
$L__BB112_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB112_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB112_27;
	@%p11 bra 	$L__BB112_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB112_26;
$L__BB112_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB112_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB112_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB112_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	lg2.approx.ftz.f32 %r14, %r15;
    mul.rn.ftz.f32 %r14, %r14, %r16;
    ex2.approx.ftz.f32 %r14, %r14;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB112_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB112_38;
	bra.uni 	$L__BB112_30;
$L__BB112_38:
	ret;
$L__BB112_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB112_31;
$L__BB112_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB112_31;
	bra.uni 	$L__BB112_38;
$L__BB112_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB112_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB112_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB112_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB112_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB112_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	lg2.approx.ftz.f32 %r17, %r18;
    mul.rn.ftz.f32 %r17, %r17, %r19;
    ex2.approx.ftz.f32 %r17, %r17;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB112_37;

}
	// .globl	vector_greater_vec_f32
.visible .entry vector_greater_vec_f32(
	.param .u64 vector_greater_vec_f32_param_0
)
{
	.reg .pred 	%p<36>;
	.reg .b32 	%r<14>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_greater_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB113_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB113_3;
	bra.uni 	$L__BB113_2;
$L__BB113_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB113_4;
$L__BB113_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB113_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB113_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB113_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB113_8;
	bra.uni 	$L__BB113_7;
$L__BB113_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB113_9;
$L__BB113_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB113_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB113_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB113_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB113_13;
	bra.uni 	$L__BB113_12;
$L__BB113_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB113_14;
$L__BB113_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB113_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB113_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB113_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB113_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB113_19;
$L__BB113_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB113_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB113_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB113_22;
$L__BB113_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB113_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p35, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p35 bra 	$L__BB113_27;
	@%p11 bra 	$L__BB113_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB113_26;
$L__BB113_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB113_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB113_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB113_29;
	ld.f32 	%f6, [%rd146];
	ld.f32 	%f7, [%rd145];
	setp.gt.f32 	%p22, %f6, %f7;
	selp.f32 	%f11, %f6, %f7, %p22;
$L__BB113_29:
	st.f32 	[%rd138], %f11;
	setp.eq.s64 	%p23, %rd31, 1;
	@%p23 bra 	$L__BB113_38;
	bra.uni 	$L__BB113_30;
$L__BB113_38:
	ret;
$L__BB113_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p28, 0;
	bra.uni 	$L__BB113_31;
$L__BB113_37:
	selp.b64 	%rd148, %rd119, %rd12, %p24;
	selp.b64 	%rd62, %rd120, 0, %p24;
	selp.b64 	%rd149, %rd123, %rd3, %p26;
	st.f32 	[%rd62], %f12;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p33, %rd147, 0;
	@%p33 bra 	$L__BB113_31;
	bra.uni 	$L__BB113_38;
$L__BB113_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p25, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p25 bra 	$L__BB113_35;
	selp.b64 	%rd152, 0, %rd6, %p35;
	setp.eq.s64 	%p27, %rd154, 0;
	@%p27 bra 	$L__BB113_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB113_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p29, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p29;
	selp.b64 	%rd155, %rd128, 0, %p29;
	setp.eq.s64 	%p30, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p30;
	mov.u64 	%rd154, 0;
	mov.pred 	%p35, %p28;
$L__BB113_35:
	setp.gt.u64 	%p24, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p26, %rd122, %rd6;
	setp.eq.s64 	%p31, %rd156, 0;
	@%p31 bra 	$L__BB113_37;
	ld.f32 	%f9, [%rd156];
	ld.f32 	%f10, [%rd155];
	setp.gt.f32 	%p32, %f9, %f10;
	selp.f32 	%f12, %f9, %f10, %p32;
	bra.uni 	$L__BB113_37;

}
	// .globl	vector_lesser_vec_f32
.visible .entry vector_lesser_vec_f32(
	.param .u64 vector_lesser_vec_f32_param_0
)
{
	.reg .pred 	%p<36>;
	.reg .b32 	%r<14>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_lesser_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB114_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB114_3;
	bra.uni 	$L__BB114_2;
$L__BB114_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB114_4;
$L__BB114_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB114_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB114_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB114_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB114_8;
	bra.uni 	$L__BB114_7;
$L__BB114_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB114_9;
$L__BB114_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB114_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB114_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB114_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB114_13;
	bra.uni 	$L__BB114_12;
$L__BB114_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB114_14;
$L__BB114_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB114_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB114_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB114_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB114_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB114_19;
$L__BB114_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB114_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB114_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB114_22;
$L__BB114_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB114_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p35, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p35 bra 	$L__BB114_27;
	@%p11 bra 	$L__BB114_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB114_26;
$L__BB114_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB114_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB114_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB114_29;
	ld.f32 	%f6, [%rd146];
	ld.f32 	%f7, [%rd145];
	setp.lt.f32 	%p22, %f6, %f7;
	selp.f32 	%f11, %f6, %f7, %p22;
$L__BB114_29:
	st.f32 	[%rd138], %f11;
	setp.eq.s64 	%p23, %rd31, 1;
	@%p23 bra 	$L__BB114_38;
	bra.uni 	$L__BB114_30;
$L__BB114_38:
	ret;
$L__BB114_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p28, 0;
	bra.uni 	$L__BB114_31;
$L__BB114_37:
	selp.b64 	%rd148, %rd119, %rd12, %p24;
	selp.b64 	%rd62, %rd120, 0, %p24;
	selp.b64 	%rd149, %rd123, %rd3, %p26;
	st.f32 	[%rd62], %f12;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p33, %rd147, 0;
	@%p33 bra 	$L__BB114_31;
	bra.uni 	$L__BB114_38;
$L__BB114_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p25, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p25 bra 	$L__BB114_35;
	selp.b64 	%rd152, 0, %rd6, %p35;
	setp.eq.s64 	%p27, %rd154, 0;
	@%p27 bra 	$L__BB114_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB114_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p29, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p29;
	selp.b64 	%rd155, %rd128, 0, %p29;
	setp.eq.s64 	%p30, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p30;
	mov.u64 	%rd154, 0;
	mov.pred 	%p35, %p28;
$L__BB114_35:
	setp.gt.u64 	%p24, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p26, %rd122, %rd6;
	setp.eq.s64 	%p31, %rd156, 0;
	@%p31 bra 	$L__BB114_37;
	ld.f32 	%f9, [%rd156];
	ld.f32 	%f10, [%rd155];
	setp.lt.f32 	%p32, %f9, %f10;
	selp.f32 	%f12, %f9, %f10, %p32;
	bra.uni 	$L__BB114_37;

}
	// .globl	vector_dot_f32
.visible .entry vector_dot_f32()
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_57;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 272, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 272

}
	// .globl	vector_quote_f32
.visible .entry vector_quote_f32()
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_18;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_58;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 273, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 273

}
	// .globl	vector_add_vec_ref_f32
.visible .entry vector_add_vec_ref_f32(
	.param .u64 vector_add_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_add_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB117_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB117_3;
	bra.uni 	$L__BB117_2;
$L__BB117_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB117_4;
$L__BB117_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB117_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB117_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB117_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB117_8;
	bra.uni 	$L__BB117_7;
$L__BB117_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB117_9;
$L__BB117_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB117_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB117_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB117_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB117_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB117_14;
$L__BB117_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB117_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB117_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB117_17;
$L__BB117_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB117_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	add.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB117_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB117_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB117_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	add.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	add.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB117_20;
$L__BB117_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB117_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	add.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB117_23:
	ret;

}
	// .globl	vector_sub_vec_ref_f32
.visible .entry vector_sub_vec_ref_f32(
	.param .u64 vector_sub_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_sub_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB118_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB118_3;
	bra.uni 	$L__BB118_2;
$L__BB118_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB118_4;
$L__BB118_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB118_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB118_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB118_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB118_8;
	bra.uni 	$L__BB118_7;
$L__BB118_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB118_9;
$L__BB118_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB118_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB118_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB118_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB118_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB118_14;
$L__BB118_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB118_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB118_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB118_17;
$L__BB118_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB118_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	sub.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB118_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB118_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB118_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	sub.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	sub.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB118_20;
$L__BB118_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB118_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	sub.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB118_23:
	ret;

}
	// .globl	vector_mul_vec_ref_f32
.visible .entry vector_mul_vec_ref_f32(
	.param .u64 vector_mul_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_mul_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB119_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB119_3;
	bra.uni 	$L__BB119_2;
$L__BB119_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB119_4;
$L__BB119_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB119_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB119_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB119_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB119_8;
	bra.uni 	$L__BB119_7;
$L__BB119_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB119_9;
$L__BB119_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB119_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB119_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB119_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB119_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB119_14;
$L__BB119_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB119_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB119_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB119_17;
$L__BB119_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB119_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB119_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB119_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB119_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB119_20;
$L__BB119_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB119_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	mul.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB119_23:
	ret;

}
	// .globl	vector_div_vec_ref_f32
.visible .entry vector_div_vec_ref_f32(
	.param .u64 vector_div_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_div_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB120_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB120_3;
	bra.uni 	$L__BB120_2;
$L__BB120_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB120_4;
$L__BB120_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB120_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB120_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB120_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB120_8;
	bra.uni 	$L__BB120_7;
$L__BB120_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB120_9;
$L__BB120_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB120_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB120_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB120_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB120_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB120_14;
$L__BB120_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB120_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB120_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB120_17;
$L__BB120_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB120_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	div.approx.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB120_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB120_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB120_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB120_20;
$L__BB120_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB120_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	div.approx.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB120_23:
	ret;

}
	// .globl	vector_scale_vec_ref_f32
.visible .entry vector_scale_vec_ref_f32(
	.param .u64 vector_scale_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_scale_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB121_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB121_3;
	bra.uni 	$L__BB121_2;
$L__BB121_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB121_4;
$L__BB121_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB121_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB121_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB121_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB121_8;
	bra.uni 	$L__BB121_7;
$L__BB121_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB121_9;
$L__BB121_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB121_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB121_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB121_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB121_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB121_14;
$L__BB121_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB121_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB121_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB121_17;
$L__BB121_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB121_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB121_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB121_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB121_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB121_20;
$L__BB121_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB121_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	mul.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB121_23:
	ret;

}
	// .globl	vector_descale_vec_ref_f32
.visible .entry vector_descale_vec_ref_f32(
	.param .u64 vector_descale_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_descale_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB122_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB122_3;
	bra.uni 	$L__BB122_2;
$L__BB122_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB122_4;
$L__BB122_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB122_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB122_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB122_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB122_8;
	bra.uni 	$L__BB122_7;
$L__BB122_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB122_9;
$L__BB122_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB122_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB122_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB122_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB122_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB122_14;
$L__BB122_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB122_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB122_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB122_17;
$L__BB122_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB122_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	div.approx.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB122_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB122_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB122_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB122_20;
$L__BB122_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB122_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	div.approx.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB122_23:
	ret;

}
	// .globl	vector_powf_vec_ref_f32
.visible .entry vector_powf_vec_ref_f32(
	.param .u64 vector_powf_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_powf_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB123_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB123_3;
	bra.uni 	$L__BB123_2;
$L__BB123_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB123_4;
$L__BB123_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB123_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB123_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB123_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB123_8;
	bra.uni 	$L__BB123_7;
$L__BB123_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB123_9;
$L__BB123_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB123_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB123_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB123_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB123_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB123_14;
$L__BB123_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB123_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB123_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB123_17;
$L__BB123_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB123_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	lg2.approx.ftz.f32 %r11, %r12;
    mul.rn.ftz.f32 %r11, %r11, %r13;
    ex2.approx.ftz.f32 %r11, %r11;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB123_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB123_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB123_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	lg2.approx.ftz.f32 %r14, %r15;
    mul.rn.ftz.f32 %r14, %r14, %r16;
    ex2.approx.ftz.f32 %r14, %r14;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	lg2.approx.ftz.f32 %r17, %r18;
    mul.rn.ftz.f32 %r17, %r17, %r19;
    ex2.approx.ftz.f32 %r17, %r17;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB123_20;
$L__BB123_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB123_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	lg2.approx.ftz.f32 %r20, %r21;
    mul.rn.ftz.f32 %r20, %r20, %r22;
    ex2.approx.ftz.f32 %r20, %r20;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB123_23:
	ret;

}
	// .globl	vector_greater_vec_ref_f32
.visible .entry vector_greater_vec_ref_f32(
	.param .u64 vector_greater_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_greater_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB124_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB124_3;
	bra.uni 	$L__BB124_2;
$L__BB124_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB124_4;
$L__BB124_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB124_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB124_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB124_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB124_8;
	bra.uni 	$L__BB124_7;
$L__BB124_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB124_9;
$L__BB124_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB124_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB124_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB124_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB124_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB124_14;
$L__BB124_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB124_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB124_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB124_17;
$L__BB124_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB124_17:
	ld.f32 	%f1, [%rd103];
	ld.f32 	%f2, [%rd101];
	setp.gt.f32 	%p12, %f2, %f1;
	selp.f32 	%f3, %f2, %f1, %p12;
	st.f32 	[%rd101], %f3;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB124_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p14 bra 	$L__BB124_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB124_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p15, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p15;
	selp.b64 	%rd76, %rd74, 0, %p15;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p16, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd79, %rd9, %p16;
	selp.b64 	%rd82, %rd80, 0, %p16;
	ld.f32 	%f4, [%rd82];
	ld.f32 	%f5, [%rd76];
	setp.gt.f32 	%p17, %f5, %f4;
	selp.f32 	%f6, %f5, %f4, %p17;
	st.f32 	[%rd76], %f6;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd81, %rd95;
	ld.f32 	%f7, [%rd107+-4];
	ld.f32 	%f8, [%rd108+-4];
	setp.gt.f32 	%p18, %f8, %f7;
	selp.f32 	%f9, %f8, %f7, %p18;
	st.f32 	[%rd108+-4], %f9;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p19, %rd104, 0;
	@%p19 bra 	$L__BB124_20;
$L__BB124_21:
	setp.eq.s64 	%p20, %rd36, 0;
	@%p20 bra 	$L__BB124_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p21, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p21;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p22, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p22;
	ld.f32 	%f10, [%rd93];
	ld.f32 	%f11, [%rd88];
	setp.gt.f32 	%p23, %f11, %f10;
	selp.f32 	%f12, %f11, %f10, %p23;
	st.f32 	[%rd88], %f12;
$L__BB124_23:
	ret;

}
	// .globl	vector_lesser_vec_ref_f32
.visible .entry vector_lesser_vec_ref_f32(
	.param .u64 vector_lesser_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_lesser_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB125_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB125_3;
	bra.uni 	$L__BB125_2;
$L__BB125_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB125_4;
$L__BB125_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB125_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB125_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB125_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB125_8;
	bra.uni 	$L__BB125_7;
$L__BB125_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB125_9;
$L__BB125_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB125_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB125_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB125_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB125_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB125_14;
$L__BB125_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB125_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB125_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB125_17;
$L__BB125_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB125_17:
	ld.f32 	%f1, [%rd103];
	ld.f32 	%f2, [%rd101];
	setp.lt.f32 	%p12, %f2, %f1;
	selp.f32 	%f3, %f2, %f1, %p12;
	st.f32 	[%rd101], %f3;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB125_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p14 bra 	$L__BB125_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB125_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p15, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p15;
	selp.b64 	%rd76, %rd74, 0, %p15;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p16, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd79, %rd9, %p16;
	selp.b64 	%rd82, %rd80, 0, %p16;
	ld.f32 	%f4, [%rd82];
	ld.f32 	%f5, [%rd76];
	setp.lt.f32 	%p17, %f5, %f4;
	selp.f32 	%f6, %f5, %f4, %p17;
	st.f32 	[%rd76], %f6;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd81, %rd95;
	ld.f32 	%f7, [%rd107+-4];
	ld.f32 	%f8, [%rd108+-4];
	setp.lt.f32 	%p18, %f8, %f7;
	selp.f32 	%f9, %f8, %f7, %p18;
	st.f32 	[%rd108+-4], %f9;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p19, %rd104, 0;
	@%p19 bra 	$L__BB125_20;
$L__BB125_21:
	setp.eq.s64 	%p20, %rd36, 0;
	@%p20 bra 	$L__BB125_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p21, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p21;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p22, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p22;
	ld.f32 	%f10, [%rd93];
	ld.f32 	%f11, [%rd88];
	setp.lt.f32 	%p23, %f11, %f10;
	selp.f32 	%f12, %f11, %f10, %p23;
	st.f32 	[%rd88], %f12;
$L__BB125_23:
	ret;

}
	// .globl	insert_hash_table_f32
.visible .entry insert_hash_table_f32(
	.param .u64 insert_hash_table_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<31>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<85>;

	ld.param.u64 	%rd37, [insert_hash_table_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd37;
	ld.global.nc.u64 	%rd83, [%rd1];
	ld.global.nc.u64 	%rd38, [%rd1+8];
	mov.u32 	%r5, %tid.x;
	cvt.u64.u32 	%rd39, %r5;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mul.wide.u32 	%rd41, %r7, %r6;
	add.s64 	%rd80, %rd41, %rd39;
	mov.u32 	%r8, %nctaid.x;
	mul.wide.u32 	%rd5, %r6, %r8;
	ld.global.nc.u64 	%rd84, [%rd1+16];
	ld.global.nc.u64 	%rd8, [%rd1+24];
	setp.le.u64 	%p2, %rd38, %rd80;
	not.b64 	%rd44, %rd80;
	mov.u64 	%rd79, 0;
	mov.u64 	%rd77, %rd79;
	@%p2 bra 	$L__BB126_5;
	max.u64 	%rd43, %rd38, %rd80;
	add.s64 	%rd10, %rd44, %rd43;
	or.b64  	%rd45, %rd10, %rd5;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.ne.s64 	%p3, %rd46, 0;
	@%p3 bra 	$L__BB126_3;
	bra.uni 	$L__BB126_2;
$L__BB126_3:
	div.u64 	%rd76, %rd10, %rd5;
	bra.uni 	$L__BB126_4;
$L__BB126_2:
	cvt.u32.u64 	%r9, %rd5;
	cvt.u32.u64 	%r10, %rd10;
	div.u32 	%r11, %r10, %r9;
	cvt.u64.u32 	%rd76, %r11;
$L__BB126_4:
	add.s64 	%rd77, %rd76, 1;
$L__BB126_5:
	setp.le.u64 	%p4, %rd8, %rd80;
	@%p4 bra 	$L__BB126_10;
	max.u64 	%rd48, %rd8, %rd80;
	add.s64 	%rd16, %rd44, %rd48;
	or.b64  	%rd50, %rd16, %rd5;
	and.b64  	%rd51, %rd50, -4294967296;
	setp.ne.s64 	%p5, %rd51, 0;
	@%p5 bra 	$L__BB126_8;
	bra.uni 	$L__BB126_7;
$L__BB126_8:
	div.u64 	%rd78, %rd16, %rd5;
	bra.uni 	$L__BB126_9;
$L__BB126_7:
	cvt.u32.u64 	%r12, %rd5;
	cvt.u32.u64 	%r13, %rd16;
	div.u32 	%r14, %r13, %r12;
	cvt.u64.u32 	%rd78, %r14;
$L__BB126_9:
	add.s64 	%rd79, %rd78, 1;
$L__BB126_10:
	min.u64 	%rd22, %rd77, %rd79;
	setp.eq.s64 	%p6, %rd22, 0;
	@%p6 bra 	$L__BB126_21;
	shl.b64 	%rd40, %rd38, 2;
	shl.b64 	%rd42, %rd8, 2;
	add.s64 	%rd3, %rd83, %rd40;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd9, %rd84, %rd42;
	ld.global.nc.u64 	%rd23, [%rd1+40];
	ld.global.nc.u64 	%rd24, [%rd1+32];
	ld.global.nc.u64 	%rd25, [%rd1+56];
	ld.global.nc.u64 	%rd26, [%rd1+48];
	mov.u64 	%rd81, 0;
	mov.b32 	%r26, 2139095039;
	mov.u64 	%rd82, %rd81;
	bra.uni 	$L__BB126_14;
$L__BB126_13:
	setp.eq.s64 	%p13, %rd82, %rd22;
	mov.u64 	%rd80, 0;
	mov.u64 	%rd81, %rd6;
	@%p13 bra 	$L__BB126_21;
$L__BB126_14:
	add.s64 	%rd82, %rd82, 1;
	add.s64 	%rd53, %rd81, %rd80;
	sub.s64 	%rd54, %rd3, %rd83;
	shr.u64 	%rd55, %rd54, 2;
	setp.gt.u64 	%p7, %rd55, %rd53;
	shl.b64 	%rd56, %rd53, 2;
	add.s64 	%rd57, %rd83, %rd56;
	add.s64 	%rd58, %rd57, 4;
	selp.b64 	%rd83, %rd58, %rd3, %p7;
	selp.b64 	%rd59, %rd57, 0, %p7;
	sub.s64 	%rd60, %rd9, %rd84;
	shr.u64 	%rd61, %rd60, 2;
	setp.gt.u64 	%p8, %rd61, %rd53;
	add.s64 	%rd62, %rd84, %rd56;
	add.s64 	%rd63, %rd62, 4;
	selp.b64 	%rd84, %rd63, %rd9, %p8;
	selp.b64 	%rd64, %rd62, 0, %p8;
	ld.f32 	%f1, [%rd59];
	ld.f32 	%f2, [%rd64];
	mov.b32 	%r15, %f1;
	shr.u32 	%r16, %r15, 16;
	xor.b32  	%r17, %r16, %r15;
	mul.lo.s32 	%r18, %r17, -2048144789;
	shr.u32 	%r19, %r18, 13;
	xor.b32  	%r20, %r19, %r18;
	mul.lo.s32 	%r21, %r20, -1028477387;
	shr.u32 	%r22, %r21, 16;
	xor.b32  	%r23, %r22, %r21;
	and.b32  	%r30, %r23, 1023;
	bra.uni 	$L__BB126_15;
$L__BB126_12:
	add.s32 	%r27, %r30, 1;
	and.b32  	%r30, %r27, 134217727;
	@%p1 bra 	$L__BB126_15;
	bra.uni 	$L__BB126_13;
$L__BB126_15:
	cvt.u64.u32 	%rd35, %r30;
	setp.gt.u64 	%p9, %rd23, %rd35;
	@%p9 bra 	$L__BB126_17;
	bra.uni 	$L__BB126_16;
$L__BB126_17:
	shl.b64 	%rd70, %rd35, 2;
	add.s64 	%rd69, %rd70, %rd24;
	// begin inline asm
	atom.global.cas.b32 %r24,[%rd69], %r26, %r15;
	// end inline asm
	mov.b32 	%f3, %r24;
	setp.neu.f32 	%p10, %f3, 0f7F7FFFFF;
	setp.neu.f32 	%p11, %f3, %f1;
	and.pred  	%p1, %p10, %p11;
	@%p1 bra 	$L__BB126_12;
	setp.gt.u64 	%p12, %rd25, %rd35;
	@%p12 bra 	$L__BB126_19;
	bra.uni 	$L__BB126_20;
$L__BB126_19:
	add.s64 	%rd74, %rd26, %rd70;
	st.f32 	[%rd74], %f2;
	@%p1 bra 	$L__BB126_15;
	bra.uni 	$L__BB126_13;
$L__BB126_21:
	ret;
$L__BB126_16:
	mov.u64 	%rd65, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_33;
	cvta.global.u64 	%rd66, %rd65;
	mov.u64 	%rd67, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_69;
	cvta.global.u64 	%rd68, %rd67;
	{ // callseq 274, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd66;
	.param .b64 param1;
	st.param.b64 	[param1+0], 40;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd68;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 274
$L__BB126_20:
	mov.u64 	%rd71, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_60;
	cvta.global.u64 	%rd72, %rd71;
	{ // callseq 275, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd35;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd25;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd72;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 275

}
	// .globl	lookup_hash_table_f32
.visible .entry lookup_hash_table_f32(
	.param .u64 lookup_hash_table_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<30>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<83>;

	ld.param.u64 	%rd38, [lookup_hash_table_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd38;
	ld.global.nc.u64 	%rd78, [%rd1];
	ld.global.nc.u64 	%rd39, [%rd1+8];
	mov.u32 	%r5, %tid.x;
	cvt.u64.u32 	%rd40, %r5;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mul.wide.u32 	%rd42, %r7, %r6;
	add.s64 	%rd79, %rd42, %rd40;
	mov.u32 	%r8, %nctaid.x;
	mul.wide.u32 	%rd5, %r6, %r8;
	ld.global.nc.u64 	%rd81, [%rd1+16];
	ld.global.nc.u64 	%rd8, [%rd1+24];
	setp.le.u64 	%p2, %rd39, %rd79;
	not.b64 	%rd45, %rd79;
	mov.u64 	%rd77, 0;
	mov.u64 	%rd75, %rd77;
	@%p2 bra 	$L__BB127_5;
	max.u64 	%rd44, %rd39, %rd79;
	add.s64 	%rd10, %rd45, %rd44;
	or.b64  	%rd46, %rd10, %rd5;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.ne.s64 	%p3, %rd47, 0;
	@%p3 bra 	$L__BB127_3;
	bra.uni 	$L__BB127_2;
$L__BB127_3:
	div.u64 	%rd74, %rd10, %rd5;
	bra.uni 	$L__BB127_4;
$L__BB127_2:
	cvt.u32.u64 	%r9, %rd5;
	cvt.u32.u64 	%r10, %rd10;
	div.u32 	%r11, %r10, %r9;
	cvt.u64.u32 	%rd74, %r11;
$L__BB127_4:
	add.s64 	%rd75, %rd74, 1;
$L__BB127_5:
	setp.le.u64 	%p4, %rd8, %rd79;
	@%p4 bra 	$L__BB127_10;
	max.u64 	%rd49, %rd8, %rd79;
	add.s64 	%rd16, %rd45, %rd49;
	or.b64  	%rd51, %rd16, %rd5;
	and.b64  	%rd52, %rd51, -4294967296;
	setp.ne.s64 	%p5, %rd52, 0;
	@%p5 bra 	$L__BB127_8;
	bra.uni 	$L__BB127_7;
$L__BB127_8:
	div.u64 	%rd76, %rd16, %rd5;
	bra.uni 	$L__BB127_9;
$L__BB127_7:
	cvt.u32.u64 	%r12, %rd5;
	cvt.u32.u64 	%r13, %rd16;
	div.u32 	%r14, %r13, %r12;
	cvt.u64.u32 	%rd76, %r14;
$L__BB127_9:
	add.s64 	%rd77, %rd76, 1;
$L__BB127_10:
	min.u64 	%rd22, %rd75, %rd77;
	setp.eq.s64 	%p6, %rd22, 0;
	@%p6 bra 	$L__BB127_22;
	shl.b64 	%rd41, %rd39, 2;
	shl.b64 	%rd43, %rd8, 2;
	add.s64 	%rd3, %rd78, %rd41;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd9, %rd81, %rd43;
	ld.global.nc.u64 	%rd23, [%rd1+40];
	ld.global.nc.u64 	%rd24, [%rd1+32];
	ld.global.nc.u64 	%rd25, [%rd1+56];
	ld.global.nc.u64 	%rd26, [%rd1+48];
	mov.u64 	%rd80, 0;
	mov.pred 	%p15, 0;
	mov.pred 	%p12, -1;
	mov.u64 	%rd82, %rd80;
	bra.uni 	$L__BB127_14;
$L__BB127_13:
	setp.eq.s64 	%p16, %rd82, %rd22;
	mov.u64 	%rd79, 0;
	mov.u64 	%rd80, %rd6;
	@%p16 bra 	$L__BB127_22;
$L__BB127_14:
	add.s64 	%rd82, %rd82, 1;
	add.s64 	%rd54, %rd80, %rd79;
	sub.s64 	%rd55, %rd3, %rd78;
	shr.u64 	%rd56, %rd55, 2;
	setp.gt.u64 	%p7, %rd56, %rd54;
	shl.b64 	%rd57, %rd54, 2;
	add.s64 	%rd58, %rd78, %rd57;
	add.s64 	%rd59, %rd58, 4;
	selp.b64 	%rd78, %rd59, %rd3, %p7;
	selp.b64 	%rd60, %rd58, 0, %p7;
	sub.s64 	%rd61, %rd9, %rd81;
	shr.u64 	%rd62, %rd61, 2;
	setp.gt.u64 	%p8, %rd62, %rd54;
	add.s64 	%rd63, %rd81, %rd57;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd81, %rd64, %rd9, %p8;
	selp.b64 	%rd35, %rd63, 0, %p8;
	ld.f32 	%f1, [%rd60];
	mov.b32 	%r15, %f1;
	shr.u32 	%r16, %r15, 16;
	xor.b32  	%r17, %r16, %r15;
	mul.lo.s32 	%r18, %r17, -2048144789;
	shr.u32 	%r19, %r18, 13;
	xor.b32  	%r20, %r19, %r18;
	mul.lo.s32 	%r21, %r20, -1028477387;
	shr.u32 	%r22, %r21, 16;
	xor.b32  	%r23, %r22, %r21;
	and.b32  	%r29, %r23, 1023;
	bra.uni 	$L__BB127_15;
$L__BB127_20:
	add.s32 	%r24, %r29, 1;
	and.b32  	%r29, %r24, 134217727;
	@%p12 bra 	$L__BB127_15;
	bra.uni 	$L__BB127_13;
$L__BB127_15:
	cvt.u64.u32 	%rd36, %r29;
	setp.le.u64 	%p9, %rd23, %rd36;
	@%p9 bra 	$L__BB127_23;
	shl.b64 	%rd67, %rd36, 2;
	add.s64 	%rd68, %rd24, %rd67;
	ld.f32 	%f2, [%rd68];
	setp.eq.f32 	%p10, %f2, %f1;
	@%p10 bra 	$L__BB127_17;
	bra.uni 	$L__BB127_19;
$L__BB127_17:
	setp.gt.u64 	%p14, %rd25, %rd36;
	@%p14 bra 	$L__BB127_18;
	bra.uni 	$L__BB127_21;
$L__BB127_18:
	add.s64 	%rd72, %rd26, %rd67;
	ld.f32 	%f3, [%rd72];
	st.f32 	[%rd35], %f3;
	@%p15 bra 	$L__BB127_15;
	bra.uni 	$L__BB127_13;
$L__BB127_19:
	setp.eq.f32 	%p11, %f2, 0f7F7FFFFF;
	@%p11 bra 	$L__BB127_12;
	bra.uni 	$L__BB127_20;
$L__BB127_12:
	mov.b32 	%r25, 0;
	st.u32 	[%rd35], %r25;
	@%p15 bra 	$L__BB127_15;
	bra.uni 	$L__BB127_13;
$L__BB127_22:
	ret;
$L__BB127_23:
	mov.u64 	%rd65, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_61;
	cvta.global.u64 	%rd66, %rd65;
	{ // callseq 276, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd36;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd23;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd66;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 276
$L__BB127_21:
	mov.u64 	%rd69, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_62;
	cvta.global.u64 	%rd70, %rd69;
	{ // callseq 277, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd36;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd25;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd70;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 277

}
	// .globl	delete_hash_table_f32
.visible .entry delete_hash_table_f32(
	.param .u64 delete_hash_table_f32_param_0
)
{
	.reg .pred 	%p<26>;
	.reg .b32 	%r<39>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<61>;

	ld.param.u64 	%rd26, [delete_hash_table_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd26;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	mov.u32 	%r9, %tid.x;
	cvt.u64.u32 	%rd27, %r9;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	shl.b64 	%rd28, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd28;
	mul.wide.u32 	%rd29, %r11, %r10;
	add.s64 	%rd5, %rd29, %rd27;
	setp.eq.s64 	%p3, %rd5, 0;
	@%p3 bra 	$L__BB128_1;
	setp.gt.u64 	%p4, %rd3, %rd5;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd31, %rd2, %rd30;
	add.s64 	%rd32, %rd31, 4;
	selp.b64 	%rd19, %rd32, %rd4, %p4;
	selp.b64 	%rd60, %rd31, 0, %p4;
	bra.uni 	$L__BB128_15;
$L__BB128_1:
	setp.eq.s64 	%p5, %rd3, 0;
	selp.b64 	%rd33, 0, 4, %p5;
	add.s64 	%rd19, %rd2, %rd33;
	selp.b64 	%rd60, 0, %rd2, %p5;
$L__BB128_15:
	setp.eq.s64 	%p6, %rd60, 0;
	@%p6 bra 	$L__BB128_25;
	mov.u32 	%r12, %nctaid.x;
	mul.wide.u32 	%rd6, %r10, %r12;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f3, [%rd60];
	mov.b32 	%r13, %f3;
	shr.u32 	%r14, %r13, 16;
	xor.b32  	%r15, %r14, %r13;
	mul.lo.s32 	%r16, %r15, -2048144789;
	shr.u32 	%r17, %r16, 13;
	xor.b32  	%r18, %r17, %r16;
	mul.lo.s32 	%r19, %r18, -1028477387;
	shr.u32 	%r20, %r19, 16;
	xor.b32  	%r21, %r20, %r19;
	and.b32  	%r38, %r21, 1023;
	ld.global.nc.u64 	%rd21, [%rd1+40];
	ld.global.nc.u64 	%rd22, [%rd1+32];
	ld.global.nc.u64 	%rd23, [%rd1+56];
	ld.global.nc.u64 	%rd24, [%rd1+48];
	mov.b32 	%r23, 2139095039;
	mov.pred 	%p13, 0;
	mov.pred 	%p11, -1;
	bra.uni 	$L__BB128_17;
$L__BB128_21:
	setp.eq.f32 	%p10, %f4, 0f00000000;
	mov.pred 	%p25, %p13;
	@%p10 bra 	$L__BB128_23;
	bra.uni 	$L__BB128_22;
$L__BB128_23:
	@%p25 bra 	$L__BB128_17;
	bra.uni 	$L__BB128_2;
$L__BB128_17:
	cvt.u64.u32 	%rd25, %r38;
	setp.le.u64 	%p7, %rd21, %rd25;
	@%p7 bra 	$L__BB128_27;
	shl.b64 	%rd36, %rd25, 2;
	add.s64 	%rd37, %rd22, %rd36;
	ld.f32 	%f4, [%rd37];
	setp.eq.f32 	%p8, %f4, %f3;
	@%p8 bra 	$L__BB128_19;
	bra.uni 	$L__BB128_21;
$L__BB128_19:
	setp.gt.u64 	%p12, %rd23, %rd25;
	@%p12 bra 	$L__BB128_20;
	bra.uni 	$L__BB128_24;
$L__BB128_20:
	add.s64 	%rd41, %rd24, %rd36;
	st.u32 	[%rd41], %r23;
	mov.pred 	%p25, %p13;
	bra.uni 	$L__BB128_23;
$L__BB128_22:
	add.s32 	%r22, %r38, 1;
	and.b32  	%r38, %r22, 134217727;
	mov.pred 	%p25, %p11;
	bra.uni 	$L__BB128_23;
$L__BB128_2:
	sub.s64 	%rd42, %rd4, %rd19;
	shr.u64 	%rd43, %rd42, 2;
	setp.le.u64 	%p14, %rd43, %rd7;
	@%p14 bra 	$L__BB128_25;
	shl.b64 	%rd44, %rd6, 2;
	add.s64 	%rd13, %rd19, %rd44;
	add.s64 	%rd57, %rd13, -4;
	mov.pred 	%p21, 0;
	mov.pred 	%p19, -1;
	bra.uni 	$L__BB128_5;
$L__BB128_4:
	sub.s64 	%rd53, %rd4, %rd13;
	shr.u64 	%rd54, %rd53, 2;
	setp.le.u64 	%p22, %rd54, %rd7;
	setp.gt.u64 	%p23, %rd54, %rd7;
	add.s64 	%rd56, %rd13, %rd44;
	add.s64 	%rd57, %rd56, -4;
	selp.b64 	%rd13, %rd56, %rd4, %p23;
	@%p22 bra 	$L__BB128_25;
$L__BB128_5:
	ld.f32 	%f1, [%rd57];
	mov.b32 	%r24, %f1;
	shr.u32 	%r25, %r24, 16;
	xor.b32  	%r26, %r25, %r24;
	mul.lo.s32 	%r27, %r26, -2048144789;
	shr.u32 	%r28, %r27, 13;
	xor.b32  	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, -1028477387;
	shr.u32 	%r31, %r30, 16;
	xor.b32  	%r32, %r31, %r30;
	and.b32  	%r36, %r32, 1023;
	bra.uni 	$L__BB128_6;
$L__BB128_10:
	setp.eq.f32 	%p18, %f2, 0f00000000;
	mov.pred 	%p24, %p21;
	@%p18 bra 	$L__BB128_12;
	bra.uni 	$L__BB128_11;
$L__BB128_12:
	@%p24 bra 	$L__BB128_6;
	bra.uni 	$L__BB128_4;
$L__BB128_6:
	cvt.u64.u32 	%rd14, %r36;
	setp.le.u64 	%p15, %rd21, %rd14;
	@%p15 bra 	$L__BB128_26;
	shl.b64 	%rd47, %rd14, 2;
	add.s64 	%rd48, %rd22, %rd47;
	ld.f32 	%f2, [%rd48];
	setp.eq.f32 	%p16, %f2, %f1;
	@%p16 bra 	$L__BB128_8;
	bra.uni 	$L__BB128_10;
$L__BB128_8:
	setp.gt.u64 	%p20, %rd23, %rd14;
	@%p20 bra 	$L__BB128_9;
	bra.uni 	$L__BB128_13;
$L__BB128_9:
	add.s64 	%rd52, %rd24, %rd47;
	st.u32 	[%rd52], %r23;
	mov.pred 	%p24, %p21;
	bra.uni 	$L__BB128_12;
$L__BB128_11:
	add.s32 	%r33, %r36, 1;
	and.b32  	%r36, %r33, 134217727;
	mov.pred 	%p24, %p19;
	bra.uni 	$L__BB128_12;
$L__BB128_25:
	ret;
$L__BB128_26:
	mov.u64 	%rd45, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_63;
	cvta.global.u64 	%rd46, %rd45;
	{ // callseq 280, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd14;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd21;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd46;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 280
$L__BB128_13:
	mov.u64 	%rd49, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_64;
	cvta.global.u64 	%rd50, %rd49;
	{ // callseq 281, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd14;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd23;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd50;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 281
$L__BB128_27:
	mov.u64 	%rd34, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_63;
	cvta.global.u64 	%rd35, %rd34;
	{ // callseq 278, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd25;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd21;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd35;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 278
$L__BB128_24:
	mov.u64 	%rd38, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_64;
	cvta.global.u64 	%rd39, %rd38;
	{ // callseq 279, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd25;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd23;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd39;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 279

}
	// .globl	count_hash_table_f32
.visible .entry count_hash_table_f32(
	.param .u64 count_hash_table_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<73>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<47>;

	ld.param.u64 	%rd22, [count_hash_table_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd22;
	// begin inline asm
	.shared .align 4 .b8 nonphysical[128];
    mov.u32 %r14, nonphysical;
	// end inline asm
	mov.u32 	%r2, %tid.x;
	cvt.u64.u32 	%rd23, %r2;
	mov.u32 	%r3, %ntid.x;
	ld.global.nc.u64 	%rd3, [%rd1+32];
	ld.global.nc.u64 	%rd4, [%rd1+40];
	mov.u32 	%r15, %ctaid.x;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd5, %rd3, %rd24;
	mul.wide.u32 	%rd25, %r15, %r3;
	add.s64 	%rd6, %rd25, %rd23;
	setp.eq.s64 	%p1, %rd6, 0;
	@%p1 bra 	$L__BB129_1;
	setp.gt.u64 	%p2, %rd4, %rd6;
	shl.b64 	%rd26, %rd6, 2;
	add.s64 	%rd27, %rd3, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd45, %rd28, %rd5, %p2;
	selp.b64 	%rd46, %rd27, 0, %p2;
	bra.uni 	$L__BB129_3;
$L__BB129_1:
	setp.eq.s64 	%p3, %rd4, 0;
	selp.b64 	%rd29, 0, 4, %p3;
	add.s64 	%rd45, %rd3, %rd29;
	selp.b64 	%rd46, 0, %rd3, %p3;
$L__BB129_3:
	and.b64  	%rd2, %rd23, 31;
	setp.eq.s64 	%p4, %rd46, 0;
	mov.b32 	%r71, 0;
	@%p4 bra 	$L__BB129_7;
	mov.u32 	%r16, %nctaid.x;
	mul.wide.u32 	%rd7, %r3, %r16;
	add.s64 	%rd8, %rd7, -1;
	ld.f32 	%f1, [%rd46];
	setp.neu.f32 	%p5, %f1, 0f7F7FFFFF;
	selp.u32 	%r71, 1, 0, %p5;
	sub.s64 	%rd30, %rd5, %rd45;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd8;
	@%p6 bra 	$L__BB129_7;
	shl.b64 	%rd19, %rd7, 2;
	sub.s64 	%rd34, %rd5, %rd19;
	sub.s64 	%rd44, %rd34, %rd45;
	add.s64 	%rd35, %rd19, %rd45;
	add.s64 	%rd43, %rd35, -4;
$L__BB129_6:
	ld.f32 	%f2, [%rd43];
	setp.neu.f32 	%p7, %f2, 0f7F7FFFFF;
	selp.u32 	%r18, 1, 0, %p7;
	add.s32 	%r71, %r71, %r18;
	shr.u64 	%rd36, %rd44, 2;
	setp.gt.u64 	%p8, %rd36, %rd8;
	sub.s64 	%rd44, %rd44, %rd19;
	add.s64 	%rd43, %rd43, %rd19;
	@%p8 bra 	$L__BB129_6;
$L__BB129_7:
	mov.b32 	%r20, 16;
	mov.b32 	%r51, 31;
	// begin inline asm
	shfl.sync.bfly.b32 %r19,%r71, %r20, %r51, 4294967295;
	// end inline asm
	add.s32 	%r26, %r19, %r71;
	mov.b32 	%r24, 8;
	// begin inline asm
	shfl.sync.bfly.b32 %r23,%r26, %r24, %r51, 4294967295;
	// end inline asm
	add.s32 	%r30, %r23, %r26;
	mov.b32 	%r28, 4;
	// begin inline asm
	shfl.sync.bfly.b32 %r27,%r30, %r28, %r51, 4294967295;
	// end inline asm
	add.s32 	%r34, %r27, %r30;
	mov.b32 	%r32, 2;
	// begin inline asm
	shfl.sync.bfly.b32 %r31,%r34, %r32, %r51, 4294967295;
	// end inline asm
	add.s32 	%r38, %r31, %r34;
	mov.b32 	%r36, 1;
	// begin inline asm
	shfl.sync.bfly.b32 %r35,%r38, %r36, %r51, 4294967295;
	// end inline asm
	setp.ne.s64 	%p9, %rd2, 0;
	@%p9 bra 	$L__BB129_9;
	add.s32 	%r42, %r35, %r38;
	shr.u32 	%r39, %r2, 3;
	and.b32  	%r40, %r39, 124;
	add.s32 	%r41, %r14, %r40;
	// begin inline asm
	st.shared.u32 [%r41], %r42;
	// end inline asm
$L__BB129_9:
	bar.sync 	0;
	shr.u32 	%r44, %r3, 5;
	setp.ge.u32 	%p10, %r2, %r44;
	mov.b32 	%r72, 0;
	@%p10 bra 	$L__BB129_11;
	cvt.u32.u64 	%r45, %rd2;
	shl.b32 	%r46, %r45, 2;
	add.s32 	%r48, %r14, %r46;
	// begin inline asm
	ld.shared.u32 %r72, [%r48];
	// end inline asm
$L__BB129_11:
	setp.gt.u32 	%p11, %r2, 31;
	@%p11 bra 	$L__BB129_13;
	setp.eq.s64 	%p12, %rd2, 0;
	// begin inline asm
	shfl.sync.bfly.b32 %r49,%r72, %r20, %r51, 4294967295;
	// end inline asm
	add.s32 	%r56, %r49, %r72;
	// begin inline asm
	shfl.sync.bfly.b32 %r53,%r56, %r24, %r51, 4294967295;
	// end inline asm
	add.s32 	%r60, %r53, %r56;
	// begin inline asm
	shfl.sync.bfly.b32 %r57,%r60, %r28, %r51, 4294967295;
	// end inline asm
	add.s32 	%r64, %r57, %r60;
	// begin inline asm
	shfl.sync.bfly.b32 %r61,%r64, %r32, %r51, 4294967295;
	// end inline asm
	add.s32 	%r68, %r61, %r64;
	// begin inline asm
	shfl.sync.bfly.b32 %r65,%r68, %r36, %r51, 4294967295;
	// end inline asm
	@%p12 bra 	$L__BB129_14;
	bra.uni 	$L__BB129_13;
$L__BB129_14:
	ld.global.nc.u64 	%rd37, [%rd1+72];
	setp.ne.s64 	%p13, %rd37, 0;
	@%p13 bra 	$L__BB129_16;
	bra.uni 	$L__BB129_15;
$L__BB129_16:
	add.s32 	%r69, %r65, %r68;
	ld.global.nc.u64 	%rd38, [%rd1+64];
	// begin inline asm
	red.global.add.u32 [%rd38], %r69;
	// end inline asm
$L__BB129_13:
	ret;
$L__BB129_15:
	mov.u64 	%rd39, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_33;
	cvta.global.u64 	%rd40, %rd39;
	mov.u64 	%rd41, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_65;
	cvta.global.u64 	%rd42, %rd41;
	{ // callseq 282, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd40;
	.param .b64 param1;
	st.param.b64 	[param1+0], 40;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd42;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 282

}
	// .globl	iterate_hash_table_f32
.visible .entry iterate_hash_table_f32(
	.param .u64 iterate_hash_table_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<16>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<128>;

	ld.param.u64 	%rd58, [iterate_hash_table_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd58;
	ld.global.nc.u64 	%rd2, [%rd1+32];
	ld.global.nc.u64 	%rd3, [%rd1+40];
	mov.u32 	%r2, %tid.x;
	cvt.u64.u32 	%rd59, %r2;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mul.wide.u32 	%rd61, %r4, %r3;
	add.s64 	%rd5, %rd61, %rd59;
	mov.u32 	%r5, %nctaid.x;
	mul.wide.u32 	%rd6, %r3, %r5;
	ld.global.nc.u64 	%rd8, [%rd1+48];
	ld.global.nc.u64 	%rd9, [%rd1+56];
	setp.le.u64 	%p1, %rd3, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd116, 0;
	mov.u64 	%rd114, %rd116;
	@%p1 bra 	$L__BB130_5;
	max.u64 	%rd63, %rd3, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p2, %rd66, 0;
	@%p2 bra 	$L__BB130_3;
	bra.uni 	$L__BB130_2;
$L__BB130_3:
	div.u64 	%rd113, %rd11, %rd6;
	bra.uni 	$L__BB130_4;
$L__BB130_2:
	cvt.u32.u64 	%r6, %rd6;
	cvt.u32.u64 	%r7, %rd11;
	div.u32 	%r8, %r7, %r6;
	cvt.u64.u32 	%rd113, %r8;
$L__BB130_4:
	add.s64 	%rd114, %rd113, 1;
$L__BB130_5:
	setp.le.u64 	%p3, %rd9, %rd5;
	@%p3 bra 	$L__BB130_10;
	max.u64 	%rd68, %rd9, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p4, %rd71, 0;
	@%p4 bra 	$L__BB130_8;
	bra.uni 	$L__BB130_7;
$L__BB130_8:
	div.u64 	%rd115, %rd17, %rd6;
	bra.uni 	$L__BB130_9;
$L__BB130_7:
	cvt.u32.u64 	%r9, %rd6;
	cvt.u32.u64 	%r10, %rd17;
	div.u32 	%r11, %r10, %r9;
	cvt.u64.u32 	%rd115, %r11;
$L__BB130_9:
	add.s64 	%rd116, %rd115, 1;
$L__BB130_10:
	min.u64 	%rd23, %rd114, %rd116;
	setp.eq.s64 	%p5, %rd23, 0;
	@%p5 bra 	$L__BB130_31;
	shl.b64 	%rd60, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd4, %rd2, %rd60;
	setp.eq.s64 	%p6, %rd5, 0;
	shl.b64 	%rd112, %rd5, 2;
	@%p6 bra 	$L__BB130_13;
	setp.gt.u64 	%p7, %rd3, %rd5;
	add.s64 	%rd73, %rd2, %rd112;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd122, %rd74, %rd4, %p7;
	selp.b64 	%rd118, %rd73, 0, %p7;
	bra.uni 	$L__BB130_14;
$L__BB130_13:
	setp.eq.s64 	%p8, %rd3, 0;
	selp.b64 	%rd75, 0, 4, %p8;
	add.s64 	%rd122, %rd2, %rd75;
	selp.b64 	%rd118, 0, %rd2, %p8;
$L__BB130_14:
	add.s64 	%rd10, %rd8, %rd62;
	@%p6 bra 	$L__BB130_16;
	setp.gt.u64 	%p10, %rd9, %rd5;
	add.s64 	%rd77, %rd8, %rd112;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd123, %rd78, %rd10, %p10;
	selp.b64 	%rd120, %rd77, 0, %p10;
	bra.uni 	$L__BB130_17;
$L__BB130_16:
	setp.eq.s64 	%p11, %rd9, 0;
	selp.b64 	%rd79, 0, 4, %p11;
	add.s64 	%rd123, %rd8, %rd79;
	selp.b64 	%rd120, 0, %rd8, %p11;
$L__BB130_17:
	ld.f32 	%f1, [%rd118];
	setp.eq.f32 	%p12, %f1, 0f7F7FFFFF;
	@%p12 bra 	$L__BB130_22;
	ld.global.nc.u64 	%rd80, [%rd1+72];
	setp.eq.s64 	%p13, %rd80, 0;
	@%p13 bra 	$L__BB130_26;
	ld.f32 	%f2, [%rd120];
	ld.global.nc.u64 	%rd40, [%rd1+8];
	cvt.u32.u64 	%r13, %rd40;
	ld.global.nc.u64 	%rd81, [%rd1+64];
	// begin inline asm
	atom.global.inc.u32 %r12,[%rd81], %r13;
	// end inline asm
	cvt.u64.u32 	%rd127, %r12;
	setp.le.u64 	%p14, %rd40, %rd127;
	@%p14 bra 	$L__BB130_32;
	ld.global.nc.u64 	%rd82, [%rd1];
	shl.b64 	%rd83, %rd127, 2;
	add.s64 	%rd84, %rd82, %rd83;
	st.f32 	[%rd84], %f1;
	ld.global.nc.u64 	%rd43, [%rd1+24];
	setp.le.u64 	%p15, %rd43, %rd127;
	@%p15 bra 	$L__BB130_33;
	ld.global.nc.u64 	%rd85, [%rd1+16];
	add.s64 	%rd87, %rd85, %rd83;
	st.f32 	[%rd87], %f2;
$L__BB130_22:
	setp.eq.s64 	%p16, %rd23, 1;
	@%p16 bra 	$L__BB130_31;
	bra.uni 	$L__BB130_23;
$L__BB130_31:
	ret;
$L__BB130_23:
	add.s64 	%rd7, %rd6, -1;
	ld.global.nc.u64 	%rd39, [%rd1+72];
	ld.global.nc.u64 	%rd40, [%rd1+8];
	cvt.u32.u64 	%r15, %rd40;
	ld.global.nc.u64 	%rd99, [%rd1+64];
	ld.global.nc.u64 	%rd42, [%rd1];
	ld.global.nc.u64 	%rd43, [%rd1+24];
	ld.global.nc.u64 	%rd44, [%rd1+16];
	add.s64 	%rd46, %rd23, -1;
	setp.ne.s64 	%p20, %rd39, 0;
	bra.uni 	$L__BB130_24;
$L__BB130_30:
	selp.b64 	%rd122, %rd91, %rd4, %p17;
	selp.b64 	%rd123, %rd96, %rd10, %p18;
	add.s64 	%rd46, %rd46, -1;
	setp.ne.s64 	%p23, %rd46, 0;
	@%p23 bra 	$L__BB130_24;
	bra.uni 	$L__BB130_31;
$L__BB130_24:
	sub.s64 	%rd88, %rd4, %rd122;
	shr.u64 	%rd89, %rd88, 2;
	setp.gt.u64 	%p17, %rd89, %rd7;
	shl.b64 	%rd90, %rd6, 2;
	add.s64 	%rd91, %rd122, %rd90;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p17;
	sub.s64 	%rd94, %rd10, %rd123;
	shr.u64 	%rd95, %rd94, 2;
	setp.gt.u64 	%p18, %rd95, %rd7;
	add.s64 	%rd96, %rd123, %rd90;
	ld.f32 	%f3, [%rd93];
	setp.eq.f32 	%p19, %f3, 0f7F7FFFFF;
	@%p19 bra 	$L__BB130_30;
	@%p20 bra 	$L__BB130_27;
	bra.uni 	$L__BB130_26;
$L__BB130_27:
	add.s64 	%rd97, %rd96, -4;
	selp.b64 	%rd98, %rd97, 0, %p18;
	ld.f32 	%f4, [%rd98];
	// begin inline asm
	atom.global.inc.u32 %r14,[%rd99], %r15;
	// end inline asm
	cvt.u64.u32 	%rd127, %r14;
	setp.le.u64 	%p21, %rd40, %rd127;
	@%p21 bra 	$L__BB130_32;
	shl.b64 	%rd102, %rd127, 2;
	add.s64 	%rd103, %rd42, %rd102;
	st.f32 	[%rd103], %f3;
	setp.gt.u64 	%p22, %rd43, %rd127;
	@%p22 bra 	$L__BB130_29;
	bra.uni 	$L__BB130_33;
$L__BB130_29:
	add.s64 	%rd107, %rd44, %rd102;
	st.f32 	[%rd107], %f4;
	bra.uni 	$L__BB130_30;
$L__BB130_33:
	mov.u64 	%rd104, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_68;
	cvta.global.u64 	%rd105, %rd104;
	{ // callseq 284, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd127;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd43;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd105;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 284
$L__BB130_32:
	mov.u64 	%rd100, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_67;
	cvta.global.u64 	%rd101, %rd100;
	{ // callseq 283, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd127;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd40;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd101;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 283
$L__BB130_26:
	mov.u64 	%rd108, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_33;
	cvta.global.u64 	%rd109, %rd108;
	mov.u64 	%rd110, anon_$_3b3c5c5838314d1657e7e64c8d33d59b_$_66;
	cvta.global.u64 	%rd111, %rd110;
	{ // callseq 285, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd109;
	.param .b64 param1;
	st.param.b64 	[param1+0], 40;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd111;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 285

}