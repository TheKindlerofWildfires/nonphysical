//
// Generated by LLVM NVPTX Back-End
//

.version 7.0
.target sm_80
.address_size 64

	// .globl	fft_forward_128_kernel
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E
(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_3
)
;
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E
(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_3
)
;
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E
(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_3
)
;
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E
(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_3
)
;
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E
(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_3
)
;
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE
(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_3
)
;
.visible .func _ZN4core9panicking5panic17hc7c8a74e6511bb99E
(
.param .b64 _ZN4core9panicking5panic17hc7c8a74e6511bb99E_param_0,
	.param .b64 _ZN4core9panicking5panic17hc7c8a74e6511bb99E_param_1,
	.param .b64 _ZN4core9panicking5panic17hc7c8a74e6511bb99E_param_2
)
.noreturn{
	trap;
	exit;
}
.visible .func _ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E
(
.param .b64 _ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E_param_0
)
.noreturn{
	trap;
	exit;
}
.visible .func _ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E
(
.param .b64 _ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E_param_0
)
.noreturn{
	trap;
	exit;
}
.visible .func _ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E
(
.param .b64 _ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E_param_0,
	.param .b64 _ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E_param_1,
	.param .b64 _ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E_param_2
)
.noreturn{
	trap;
	exit;
}
.global .align 1 .b8 anon_$_9b271fbca47443a0c783d86781d13fba_$_0[89] = {47, 114, 117, 115, 116, 99, 47, 56, 101, 56, 54, 99, 57, 53, 54, 55, 49, 53, 52, 100, 99, 53, 97, 57, 97, 100, 97, 49, 53, 97, 98, 49, 57, 54, 100, 50, 51, 101, 97, 101, 50, 98, 100, 55, 100, 56, 57, 47, 108, 105, 98, 114, 97, 114, 121, 47, 99, 111, 114, 101, 47, 115, 114, 99, 47, 105, 116, 101, 114, 47, 97, 100, 97, 112, 116, 101, 114, 115, 47, 115, 116, 101, 112, 95, 98, 121, 46, 114, 115};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_1[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_0), 89, 107374182681};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_2[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_0), 89, 120259084570};
.global .align 1 .b8 anon_$_9b271fbca47443a0c783d86781d13fba_$_3[49] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 115, 105, 103, 110, 97, 108, 92, 102, 111, 117, 114, 105, 101, 114, 92, 112, 116, 120, 95, 102, 111, 117, 114, 105, 101, 114, 46, 114, 115};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_4[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_3), 49, 244813136094};
.global .align 1 .b8 anon_$_9b271fbca47443a0c783d86781d13fba_$_5[27] = {97, 115, 115, 101, 114, 116, 105, 111, 110, 32, 102, 97, 105, 108, 101, 100, 58, 32, 115, 116, 101, 112, 32, 33, 61, 32, 48};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_6[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_0), 89, 38654705699};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_7[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_3), 49, 244813136132};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_8[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_3), 49, 244813136170};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_9[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_3), 49, 244813136207};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_10[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_3), 49, 266287972781};
.global .align 1 .b8 anon_$_9b271fbca47443a0c783d86781d13fba_$_11[19] = {110, 111, 116, 32, 121, 101, 116, 32, 105, 109, 112, 108, 101, 109, 101, 110, 116, 101, 100};
.global .align 1 .b8 anon_$_9b271fbca47443a0c783d86781d13fba_$_12[35] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 115, 104, 97, 114, 101, 100, 92, 102, 108, 111, 97, 116, 46, 114, 115};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_13[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_12), 35, 38654705867};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_14[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_12), 35, 38654705872};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_15[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_12), 35, 38654705877};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_16[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_12), 35, 38654705940};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_17[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_12), 35, 38654705945};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_18[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_12), 35, 38654705950};
.global .align 1 .b8 anon_$_9b271fbca47443a0c783d86781d13fba_$_19[39] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 115, 104, 97, 114, 101, 100, 92, 112, 114, 105, 109, 105, 116, 105, 118, 101, 46, 114, 115};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_20[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_19), 39, 38654706006};
.global .align 1 .b8 anon_$_9b271fbca47443a0c783d86781d13fba_$_21[38] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 115, 104, 97, 114, 101, 100, 92, 117, 110, 115, 105, 103, 110, 101, 100, 46, 114, 115};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_22[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_21), 38, 60129542290};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_23[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_21), 38, 38654705833};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_24[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_21), 38, 60129542519};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_25[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_21), 38, 38654706062};
.global .align 1 .b8 anon_$_9b271fbca47443a0c783d86781d13fba_$_26[40] = {97, 115, 115, 101, 114, 116, 105, 111, 110, 32, 102, 97, 105, 108, 101, 100, 58, 32, 105, 110, 100, 101, 120, 32, 60, 32, 115, 101, 108, 102, 46, 112, 116, 114, 46, 108, 101, 110, 40, 41};
.global .align 1 .b8 anon_$_9b271fbca47443a0c783d86781d13fba_$_27[41] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 99, 117, 100, 97, 92, 103, 108, 111, 98, 97, 108, 92, 97, 116, 111, 109, 105, 99, 46, 114, 115};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_28[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_27), 41, 38654705859};
.global .align 1 .b8 anon_$_9b271fbca47443a0c783d86781d13fba_$_29[47] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 115, 104, 97, 114, 101, 100, 92, 118, 101, 99, 116, 111, 114, 92, 112, 116, 120, 95, 118, 101, 99, 116, 111, 114, 46, 114, 115};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_30[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 21474836543};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_31[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 21474836590};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_32[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 21474836639};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_33[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 21474836686};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_34[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 21474836692};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_35[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 21474836698};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_36[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 73014444256};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_37[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 73014444267};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_38[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 73014444277};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_39[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 73014444288};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_40[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 73014444311};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_41[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 73014444324};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_42[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 73014444337};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_43[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 73014444568};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_44[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 73014444577};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_45[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 73014444585};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_46[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 73014444594};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_47[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 73014444611};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_48[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 73014444620};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_49[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 73014444629};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_50[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 21474837333};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_51[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_29), 47, 21474837338};
.global .align 1 .b8 anon_$_9b271fbca47443a0c783d86781d13fba_$_52[54] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 103, 114, 97, 112, 104, 92, 104, 97, 115, 104, 95, 116, 97, 98, 108, 101, 92, 112, 116, 120, 95, 104, 97, 115, 104, 95, 116, 97, 98, 108, 101, 46, 114, 115};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_53[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_52), 54, 73014444068};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_54[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_52), 54, 68719476791};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_55[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_52), 54, 111669149752};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_56[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_52), 54, 68719476813};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_57[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_52), 54, 73014444110};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_58[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_27), 41, 38654706152};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_59[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_27), 41, 38654706042};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_60[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_52), 54, 55834574995};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_61[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_52), 54, 55834574996};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_62[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_27), 41, 38654705779};
.global .align 1 .b8 anon_$_9b271fbca47443a0c783d86781d13fba_$_63[27] = {97, 115, 115, 101, 114, 116, 105, 111, 110, 32, 102, 97, 105, 108, 101, 100, 58, 32, 105, 110, 100, 101, 120, 32, 60, 32, 78};
.global .align 1 .b8 anon_$_9b271fbca47443a0c783d86781d13fba_$_64[34] = {110, 111, 110, 112, 104, 121, 115, 105, 99, 97, 108, 95, 112, 116, 120, 92, 115, 114, 99, 92, 99, 117, 100, 97, 92, 115, 104, 97, 114, 101, 100, 46, 114, 115};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_65[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_64), 34, 38654705781};
.global .align 8 .u64 anon_$_9b271fbca47443a0c783d86781d13fba_$_66[3] = {generic(anon_$_9b271fbca47443a0c783d86781d13fba_$_64), 34, 38654705800};

.visible .entry fft_forward_128_kernel(
	.param .u64 fft_forward_128_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot0[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<18>;
	.reg .b32 	%r<50>;
	.reg .b64 	%rd<170>;

	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd67, [fft_forward_128_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd67;
	add.u64 	%rd69, %SP, 16;
	add.u64 	%rd70, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[1024];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd70], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd71, [%rd1+8];
	and.b64  	%rd72, %rd71, -128;
	mul.wide.u32 	%rd73, %r5, 128;
	setp.lt.u64 	%p3, %rd73, %rd72;
	sub.s64 	%rd75, %rd71, %rd73;
	setp.gt.u64 	%p4, %rd75, 127;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB0_2;
	bra.uni 	$L__BB0_1;
$L__BB0_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd77, %rd3, 128;
	setp.gt.u32 	%p6, %r6, 127;
	not.b64 	%rd78, %rd3;
	add.s64 	%rd9, %rd78, %rd77;
	mov.u64 	%rd156, 0;
	and.b64  	%rd152, %rd9, -4294967296;
	mov.u64 	%rd154, %rd156;
	@%p6 bra 	$L__BB0_7;
	setp.ne.s64 	%p7, %rd152, 0;
	@%p7 bra 	$L__BB0_5;
	bra.uni 	$L__BB0_4;
$L__BB0_5:
	div.u64 	%rd153, %rd9, %rd4;
	bra.uni 	$L__BB0_6;
$L__BB0_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd9;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd153, %r9;
$L__BB0_6:
	add.s64 	%rd154, %rd153, 1;
$L__BB0_7:
	shl.b64 	%rd74, %rd73, 3;
	@%p6 bra 	$L__BB0_12;
	setp.ne.s64 	%p9, %rd152, 0;
	@%p9 bra 	$L__BB0_10;
	bra.uni 	$L__BB0_9;
$L__BB0_10:
	div.u64 	%rd155, %rd9, %rd4;
	bra.uni 	$L__BB0_11;
$L__BB0_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd9;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd155, %r13;
$L__BB0_11:
	add.s64 	%rd156, %rd155, 1;
$L__BB0_12:
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd7, %rd6, %rd74;
	add.s64 	%rd8, %rd4, -1;
	min.u64 	%rd20, %rd154, %rd156;
	setp.eq.s64 	%p10, %rd20, 0;
	shl.b64 	%rd149, %rd3, 3;
	@%p10 bra 	$L__BB0_18;
	add.s64 	%rd83, %rd7, %rd149;
	shl.b32 	%r19, %r6, 3;
	add.s32 	%r14, %r2, %r19;
	add.s32 	%r15, %r14, 4;
	ld.u32 	%r16, [%rd83];
	ld.u32 	%r17, [%rd83+4];
	// begin inline asm
	st.shared.f32 [%r14], %r16;
st.shared.f32 [%r15], %r17;
	// end inline asm
	setp.eq.s64 	%p11, %rd20, 1;
	@%p11 bra 	$L__BB0_18;
	add.s64 	%rd160, %rd3, 1;
	add.s64 	%rd159, %rd20, -1;
	shl.b64 	%rd84, %rd5, 10;
	shl.b64 	%rd23, %rd4, 3;
	add.s64 	%rd85, %rd84, %rd23;
	add.s64 	%rd87, %rd85, %rd149;
	add.s64 	%rd158, %rd6, %rd87;
	mov.u64 	%rd88, 1016;
	sub.s64 	%rd157, %rd88, %rd149;
$L__BB0_15:
	add.s64 	%rd31, %rd160, %rd8;
	setp.lt.u64 	%p13, %rd31, 128;
	@%p13 bra 	$L__BB0_17;
	bra.uni 	$L__BB0_16;
$L__BB0_17:
	shr.u64 	%rd89, %rd157, 3;
	setp.gt.u64 	%p12, %rd89, %rd8;
	selp.b64 	%rd30, %rd158, 0, %p12;
	setp.lt.u64 	%p1, %rd31, %rd160;
	add.s64 	%rd94, %rd160, %rd4;
	selp.b64 	%rd160, 128, %rd94, %p1;
	cvt.u32.u64 	%r24, %rd31;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r20, %r25, %r2;
	add.s32 	%r21, %r20, 4;
	ld.u32 	%r22, [%rd30];
	ld.u32 	%r23, [%rd30+4];
	// begin inline asm
	st.shared.f32 [%r20], %r22;
st.shared.f32 [%r21], %r23;
	// end inline asm
	add.s64 	%rd159, %rd159, -1;
	add.s64 	%rd158, %rd158, %rd23;
	sub.s64 	%rd157, %rd157, %rd23;
	setp.ne.s64 	%p14, %rd159, 0;
	@%p14 bra 	$L__BB0_15;
$L__BB0_18:
	add.u64 	%rd68, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd96, [%rd1+16];
	ld.global.nc.u64 	%rd97, [%rd1+24];
	st.local.u64 	[%rd2], %rd96;
	st.local.u64 	[%rd2+8], %rd97;
	bar.sync 	0;
	{ // callseq 1, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd68;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd69;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 1
	bar.sync 	0;
	mov.u64 	%rd164, 0;
	mov.u64 	%rd162, %rd164;
	@%p6 bra 	$L__BB0_23;
	setp.ne.s64 	%p16, %rd152, 0;
	@%p16 bra 	$L__BB0_21;
	bra.uni 	$L__BB0_20;
$L__BB0_21:
	div.u64 	%rd161, %rd9, %rd4;
	bra.uni 	$L__BB0_22;
$L__BB0_20:
	cvt.u32.u64 	%r27, %rd4;
	cvt.u32.u64 	%r28, %rd9;
	div.u32 	%r29, %r28, %r27;
	cvt.u64.u32 	%rd161, %r29;
$L__BB0_22:
	add.s64 	%rd162, %rd161, 1;
$L__BB0_23:
	@%p6 bra 	$L__BB0_28;
	setp.ne.s64 	%p18, %rd152, 0;
	@%p18 bra 	$L__BB0_26;
	bra.uni 	$L__BB0_25;
$L__BB0_26:
	div.u64 	%rd163, %rd9, %rd4;
	bra.uni 	$L__BB0_27;
$L__BB0_25:
	cvt.u32.u64 	%r31, %rd4;
	cvt.u32.u64 	%r32, %rd9;
	div.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd163, %r33;
$L__BB0_27:
	add.s64 	%rd164, %rd163, 1;
$L__BB0_28:
	min.u64 	%rd46, %rd162, %rd164;
	setp.eq.s64 	%p19, %rd46, 0;
	@%p19 bra 	$L__BB0_36;
	cvt.u16.u64 	%rs1, %rd3;
	or.b16  	%rs2, %rs1, -128;
	and.b16  	%rs3, %rs2, 240;
	and.b16  	%rs4, %rs2, 15;
	shl.b16 	%rs5, %rs4, 4;
	shr.u16 	%rs6, %rs3, 4;
	or.b16  	%rs7, %rs6, %rs5;
	and.b16  	%rs8, %rs7, 51;
	shl.b16 	%rs9, %rs8, 2;
	shr.u16 	%rs10, %rs7, 2;
	and.b16  	%rs11, %rs10, 51;
	or.b16  	%rs12, %rs11, %rs9;
	and.b16  	%rs13, %rs12, 85;
	add.s64 	%rd104, %rd7, %rd149;
	shl.b16 	%rs14, %rs12, 1;
	and.b16  	%rs15, %rs14, 340;
	shl.b16 	%rs16, %rs13, 3;
	or.b16  	%rs17, %rs16, %rs15;
	cvt.u32.u16 	%r38, %rs17;
	add.s32 	%r39, %r38, 1020;
	and.b32  	%r40, %r39, 1016;
	add.s32 	%r36, %r2, %r40;
	add.s32 	%r37, %r36, 4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd104], %r34;
	st.u32 	[%rd104+4], %r35;
	setp.eq.s64 	%p20, %rd46, 1;
	@%p20 bra 	$L__BB0_36;
	add.s64 	%rd168, %rd3, 1;
	shl.b64 	%rd105, %rd5, 10;
	shl.b64 	%rd48, %rd4, 3;
	add.s64 	%rd106, %rd105, %rd48;
	add.s64 	%rd108, %rd106, %rd149;
	add.s64 	%rd167, %rd6, %rd108;
	mov.u64 	%rd109, 1016;
	sub.s64 	%rd166, %rd109, %rd149;
	add.s64 	%rd165, %rd46, -1;
$L__BB0_31:
	add.s64 	%rd57, %rd168, %rd8;
	add.s64 	%rd112, %rd57, 128;
	shr.u64 	%rd113, %rd112, 1;
	and.b64  	%rd114, %rd113, 1431655765;
	shl.b64 	%rd115, %rd112, 1;
	and.b64  	%rd116, %rd115, 2863311530;
	or.b64  	%rd117, %rd114, %rd116;
	shr.u64 	%rd118, %rd117, 2;
	and.b64  	%rd119, %rd118, 858993459;
	shl.b64 	%rd120, %rd117, 2;
	and.b64  	%rd121, %rd120, 3435973836;
	or.b64  	%rd122, %rd119, %rd121;
	shr.u64 	%rd123, %rd122, 4;
	and.b64  	%rd124, %rd123, 252645135;
	shl.b64 	%rd125, %rd122, 4;
	and.b64  	%rd126, %rd125, 4042322160;
	or.b64  	%rd127, %rd124, %rd126;
	shr.u64 	%rd128, %rd127, 8;
	and.b64  	%rd129, %rd128, 16711935;
	shl.b64 	%rd130, %rd127, 8;
	and.b64  	%rd131, %rd130, 4278255360;
	or.b64  	%rd132, %rd129, %rd131;
	shr.u64 	%rd133, %rd132, 16;
	shl.b64 	%rd134, %rd132, 16;
	or.b64  	%rd58, %rd133, %rd134;
	setp.eq.s64 	%p22, %rd58, 0;
	mov.u64 	%rd169, 64;
	@%p22 bra 	$L__BB0_33;
	add.s64 	%rd135, %rd58, -1;
	not.b64 	%rd136, %rd58;
	and.b64  	%rd137, %rd136, %rd135;
	popc.b64 	%r41, %rd137;
	cvt.u64.u32 	%rd169, %r41;
$L__BB0_33:
	setp.lt.u64 	%p23, %rd169, 31;
	and.b64  	%rd138, %rd169, 63;
	selp.b64 	%rd139, %rd138, 31, %p23;
	cvt.u32.u64 	%r42, %rd139;
	shr.u64 	%rd140, %rd58, %r42;
	add.s64 	%rd62, %rd140, -1;
	setp.lt.u64 	%p24, %rd62, 256;
	@%p24 bra 	$L__BB0_35;
	bra.uni 	$L__BB0_34;
$L__BB0_35:
	shr.u64 	%rd111, %rd166, 3;
	setp.gt.u64 	%p21, %rd111, %rd8;
	selp.b64 	%rd56, %rd167, 0, %p21;
	setp.lt.u64 	%p2, %rd57, %rd168;
	setp.gt.u64 	%p25, %rd57, 127;
	add.s64 	%rd145, %rd57, 1;
	selp.b64 	%rd146, 128, %rd145, %p25;
	selp.b64 	%rd168, 128, %rd146, %p2;
	cvt.u32.u64 	%r47, %rd62;
	shl.b32 	%r48, %r47, 2;
	and.b32  	%r49, %r48, 1016;
	add.s32 	%r45, %r49, %r2;
	add.s32 	%r46, %r45, 4;
	// begin inline asm
	ld.shared.f32 %r43, [%r45];
ld.shared.f32 %r44, [%r46];
	// end inline asm
	st.u32 	[%rd56], %r43;
	st.u32 	[%rd56+4], %r44;
	add.s64 	%rd167, %rd167, %rd48;
	sub.s64 	%rd166, %rd166, %rd48;
	add.s64 	%rd165, %rd165, -1;
	setp.ne.s64 	%p26, %rd165, 0;
	@%p26 bra 	$L__BB0_31;
$L__BB0_36:
	ret;
$L__BB0_16:
	mov.u64 	%rd90, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd91, %rd90;
	mov.u64 	%rd92, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd93, %rd92;
	{ // callseq 0, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd93;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 0
$L__BB0_34:
	mov.u64 	%rd141, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd142, %rd141;
	mov.u64 	%rd143, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd144, %rd143;
	{ // callseq 2, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd144;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 2
$L__BB0_1:
	mov.u64 	%rd147, anon_$_9b271fbca47443a0c783d86781d13fba_$_4;
	cvta.global.u64 	%rd148, %rd147;
	{ // callseq 3, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd148;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 3

}
	// .globl	fft_forward_256_kernel
.visible .entry fft_forward_256_kernel(
	.param .u64 fft_forward_256_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot1[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<48>;
	.reg .b64 	%rd<170>;

	mov.u64 	%SPL, __local_depot1;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd67, [fft_forward_256_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd67;
	add.u64 	%rd69, %SP, 16;
	add.u64 	%rd70, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[2048];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd70], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd71, [%rd1+8];
	and.b64  	%rd72, %rd71, -256;
	mul.wide.u32 	%rd73, %r5, 256;
	setp.lt.u64 	%p3, %rd73, %rd72;
	sub.s64 	%rd75, %rd71, %rd73;
	setp.gt.u64 	%p4, %rd75, 255;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB1_2;
	bra.uni 	$L__BB1_1;
$L__BB1_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd77, %rd3, 256;
	setp.gt.u32 	%p6, %r6, 255;
	not.b64 	%rd78, %rd3;
	add.s64 	%rd9, %rd78, %rd77;
	mov.u64 	%rd156, 0;
	and.b64  	%rd152, %rd9, -4294967296;
	mov.u64 	%rd154, %rd156;
	@%p6 bra 	$L__BB1_7;
	setp.ne.s64 	%p7, %rd152, 0;
	@%p7 bra 	$L__BB1_5;
	bra.uni 	$L__BB1_4;
$L__BB1_5:
	div.u64 	%rd153, %rd9, %rd4;
	bra.uni 	$L__BB1_6;
$L__BB1_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd9;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd153, %r9;
$L__BB1_6:
	add.s64 	%rd154, %rd153, 1;
$L__BB1_7:
	shl.b64 	%rd74, %rd73, 3;
	@%p6 bra 	$L__BB1_12;
	setp.ne.s64 	%p9, %rd152, 0;
	@%p9 bra 	$L__BB1_10;
	bra.uni 	$L__BB1_9;
$L__BB1_10:
	div.u64 	%rd155, %rd9, %rd4;
	bra.uni 	$L__BB1_11;
$L__BB1_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd9;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd155, %r13;
$L__BB1_11:
	add.s64 	%rd156, %rd155, 1;
$L__BB1_12:
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd7, %rd6, %rd74;
	add.s64 	%rd8, %rd4, -1;
	min.u64 	%rd20, %rd154, %rd156;
	setp.eq.s64 	%p10, %rd20, 0;
	shl.b64 	%rd149, %rd3, 3;
	@%p10 bra 	$L__BB1_18;
	add.s64 	%rd83, %rd7, %rd149;
	shl.b32 	%r19, %r6, 3;
	add.s32 	%r14, %r2, %r19;
	add.s32 	%r15, %r14, 4;
	ld.u32 	%r16, [%rd83];
	ld.u32 	%r17, [%rd83+4];
	// begin inline asm
	st.shared.f32 [%r14], %r16;
st.shared.f32 [%r15], %r17;
	// end inline asm
	setp.eq.s64 	%p11, %rd20, 1;
	@%p11 bra 	$L__BB1_18;
	add.s64 	%rd160, %rd3, 1;
	add.s64 	%rd159, %rd20, -1;
	shl.b64 	%rd84, %rd5, 11;
	shl.b64 	%rd23, %rd4, 3;
	add.s64 	%rd85, %rd84, %rd23;
	add.s64 	%rd87, %rd85, %rd149;
	add.s64 	%rd158, %rd6, %rd87;
	mov.u64 	%rd88, 2040;
	sub.s64 	%rd157, %rd88, %rd149;
$L__BB1_15:
	add.s64 	%rd31, %rd160, %rd8;
	setp.lt.u64 	%p13, %rd31, 256;
	@%p13 bra 	$L__BB1_17;
	bra.uni 	$L__BB1_16;
$L__BB1_17:
	shr.u64 	%rd89, %rd157, 3;
	setp.gt.u64 	%p12, %rd89, %rd8;
	selp.b64 	%rd30, %rd158, 0, %p12;
	setp.lt.u64 	%p1, %rd31, %rd160;
	add.s64 	%rd94, %rd160, %rd4;
	selp.b64 	%rd160, 256, %rd94, %p1;
	cvt.u32.u64 	%r24, %rd31;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r20, %r25, %r2;
	add.s32 	%r21, %r20, 4;
	ld.u32 	%r22, [%rd30];
	ld.u32 	%r23, [%rd30+4];
	// begin inline asm
	st.shared.f32 [%r20], %r22;
st.shared.f32 [%r21], %r23;
	// end inline asm
	add.s64 	%rd159, %rd159, -1;
	add.s64 	%rd158, %rd158, %rd23;
	sub.s64 	%rd157, %rd157, %rd23;
	setp.ne.s64 	%p14, %rd159, 0;
	@%p14 bra 	$L__BB1_15;
$L__BB1_18:
	add.u64 	%rd68, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd96, [%rd1+16];
	ld.global.nc.u64 	%rd97, [%rd1+24];
	st.local.u64 	[%rd2], %rd96;
	st.local.u64 	[%rd2+8], %rd97;
	bar.sync 	0;
	{ // callseq 5, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd68;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd69;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 5
	bar.sync 	0;
	mov.u64 	%rd164, 0;
	mov.u64 	%rd162, %rd164;
	@%p6 bra 	$L__BB1_23;
	setp.ne.s64 	%p16, %rd152, 0;
	@%p16 bra 	$L__BB1_21;
	bra.uni 	$L__BB1_20;
$L__BB1_21:
	div.u64 	%rd161, %rd9, %rd4;
	bra.uni 	$L__BB1_22;
$L__BB1_20:
	cvt.u32.u64 	%r27, %rd4;
	cvt.u32.u64 	%r28, %rd9;
	div.u32 	%r29, %r28, %r27;
	cvt.u64.u32 	%rd161, %r29;
$L__BB1_22:
	add.s64 	%rd162, %rd161, 1;
$L__BB1_23:
	@%p6 bra 	$L__BB1_28;
	setp.ne.s64 	%p18, %rd152, 0;
	@%p18 bra 	$L__BB1_26;
	bra.uni 	$L__BB1_25;
$L__BB1_26:
	div.u64 	%rd163, %rd9, %rd4;
	bra.uni 	$L__BB1_27;
$L__BB1_25:
	cvt.u32.u64 	%r31, %rd4;
	cvt.u32.u64 	%r32, %rd9;
	div.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd163, %r33;
$L__BB1_27:
	add.s64 	%rd164, %rd163, 1;
$L__BB1_28:
	min.u64 	%rd46, %rd162, %rd164;
	setp.eq.s64 	%p19, %rd46, 0;
	@%p19 bra 	$L__BB1_36;
	cvt.u16.u64 	%rs1, %rd3;
	and.b16  	%rs2, %rs1, 240;
	and.b16  	%rs3, %rs1, 15;
	shl.b16 	%rs4, %rs3, 4;
	shr.u16 	%rs5, %rs2, 4;
	or.b16  	%rs6, %rs5, %rs4;
	and.b16  	%rs7, %rs6, 51;
	shl.b16 	%rs8, %rs7, 2;
	shr.u16 	%rs9, %rs6, 2;
	and.b16  	%rs10, %rs9, 51;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 85;
	shl.b16 	%rs13, %rs12, 1;
	shr.u16 	%rs14, %rs11, 1;
	and.b16  	%rs15, %rs14, 85;
	or.b16  	%rs16, %rs15, %rs13;
	add.s64 	%rd104, %rd7, %rd149;
	mul.wide.u16 	%r38, %rs16, 8;
	add.s32 	%r36, %r2, %r38;
	add.s32 	%r37, %r36, 4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd104], %r34;
	st.u32 	[%rd104+4], %r35;
	setp.eq.s64 	%p20, %rd46, 1;
	@%p20 bra 	$L__BB1_36;
	add.s64 	%rd168, %rd3, 1;
	shl.b64 	%rd105, %rd5, 11;
	shl.b64 	%rd48, %rd4, 3;
	add.s64 	%rd106, %rd105, %rd48;
	add.s64 	%rd108, %rd106, %rd149;
	add.s64 	%rd167, %rd6, %rd108;
	mov.u64 	%rd109, 2040;
	sub.s64 	%rd166, %rd109, %rd149;
	add.s64 	%rd165, %rd46, -1;
$L__BB1_31:
	add.s64 	%rd57, %rd168, %rd8;
	add.s64 	%rd112, %rd57, 256;
	shr.u64 	%rd113, %rd112, 1;
	and.b64  	%rd114, %rd113, 1431655765;
	shl.b64 	%rd115, %rd112, 1;
	and.b64  	%rd116, %rd115, 2863311530;
	or.b64  	%rd117, %rd114, %rd116;
	shr.u64 	%rd118, %rd117, 2;
	and.b64  	%rd119, %rd118, 858993459;
	shl.b64 	%rd120, %rd117, 2;
	and.b64  	%rd121, %rd120, 3435973836;
	or.b64  	%rd122, %rd119, %rd121;
	shr.u64 	%rd123, %rd122, 4;
	and.b64  	%rd124, %rd123, 252645135;
	shl.b64 	%rd125, %rd122, 4;
	and.b64  	%rd126, %rd125, 4042322160;
	or.b64  	%rd127, %rd124, %rd126;
	shr.u64 	%rd128, %rd127, 8;
	and.b64  	%rd129, %rd128, 16711935;
	shl.b64 	%rd130, %rd127, 8;
	and.b64  	%rd131, %rd130, 4278255360;
	or.b64  	%rd132, %rd129, %rd131;
	shr.u64 	%rd133, %rd132, 16;
	shl.b64 	%rd134, %rd132, 16;
	or.b64  	%rd58, %rd133, %rd134;
	setp.eq.s64 	%p22, %rd58, 0;
	mov.u64 	%rd169, 64;
	@%p22 bra 	$L__BB1_33;
	add.s64 	%rd135, %rd58, -1;
	not.b64 	%rd136, %rd58;
	and.b64  	%rd137, %rd136, %rd135;
	popc.b64 	%r39, %rd137;
	cvt.u64.u32 	%rd169, %r39;
$L__BB1_33:
	setp.lt.u64 	%p23, %rd169, 31;
	and.b64  	%rd138, %rd169, 63;
	selp.b64 	%rd139, %rd138, 31, %p23;
	cvt.u32.u64 	%r40, %rd139;
	shr.u64 	%rd140, %rd58, %r40;
	add.s64 	%rd62, %rd140, -1;
	setp.lt.u64 	%p24, %rd62, 512;
	@%p24 bra 	$L__BB1_35;
	bra.uni 	$L__BB1_34;
$L__BB1_35:
	shr.u64 	%rd111, %rd166, 3;
	setp.gt.u64 	%p21, %rd111, %rd8;
	selp.b64 	%rd56, %rd167, 0, %p21;
	setp.lt.u64 	%p2, %rd57, %rd168;
	setp.gt.u64 	%p25, %rd57, 255;
	add.s64 	%rd145, %rd57, 1;
	selp.b64 	%rd146, 256, %rd145, %p25;
	selp.b64 	%rd168, 256, %rd146, %p2;
	cvt.u32.u64 	%r45, %rd62;
	shl.b32 	%r46, %r45, 2;
	and.b32  	%r47, %r46, 2040;
	add.s32 	%r43, %r47, %r2;
	add.s32 	%r44, %r43, 4;
	// begin inline asm
	ld.shared.f32 %r41, [%r43];
ld.shared.f32 %r42, [%r44];
	// end inline asm
	st.u32 	[%rd56], %r41;
	st.u32 	[%rd56+4], %r42;
	add.s64 	%rd167, %rd167, %rd48;
	sub.s64 	%rd166, %rd166, %rd48;
	add.s64 	%rd165, %rd165, -1;
	setp.ne.s64 	%p26, %rd165, 0;
	@%p26 bra 	$L__BB1_31;
$L__BB1_36:
	ret;
$L__BB1_16:
	mov.u64 	%rd90, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd91, %rd90;
	mov.u64 	%rd92, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd93, %rd92;
	{ // callseq 4, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd93;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 4
$L__BB1_34:
	mov.u64 	%rd141, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd142, %rd141;
	mov.u64 	%rd143, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd144, %rd143;
	{ // callseq 6, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd144;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 6
$L__BB1_1:
	mov.u64 	%rd147, anon_$_9b271fbca47443a0c783d86781d13fba_$_4;
	cvta.global.u64 	%rd148, %rd147;
	{ // callseq 7, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd148;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 7

}
	// .globl	fft_forward_512_kernel
.visible .entry fft_forward_512_kernel(
	.param .u64 fft_forward_512_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot2[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<170>;

	mov.u64 	%SPL, __local_depot2;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd67, [fft_forward_512_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd67;
	add.u64 	%rd69, %SP, 16;
	add.u64 	%rd70, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[4096];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd70], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd71, [%rd1+8];
	and.b64  	%rd72, %rd71, -512;
	mul.wide.u32 	%rd73, %r5, 512;
	setp.lt.u64 	%p3, %rd73, %rd72;
	sub.s64 	%rd75, %rd71, %rd73;
	setp.gt.u64 	%p4, %rd75, 511;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB2_2;
	bra.uni 	$L__BB2_1;
$L__BB2_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd77, %rd3, 512;
	setp.gt.u32 	%p6, %r6, 511;
	not.b64 	%rd78, %rd3;
	add.s64 	%rd9, %rd78, %rd77;
	mov.u64 	%rd156, 0;
	and.b64  	%rd152, %rd9, -4294967296;
	mov.u64 	%rd154, %rd156;
	@%p6 bra 	$L__BB2_7;
	setp.ne.s64 	%p7, %rd152, 0;
	@%p7 bra 	$L__BB2_5;
	bra.uni 	$L__BB2_4;
$L__BB2_5:
	div.u64 	%rd153, %rd9, %rd4;
	bra.uni 	$L__BB2_6;
$L__BB2_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd9;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd153, %r9;
$L__BB2_6:
	add.s64 	%rd154, %rd153, 1;
$L__BB2_7:
	shl.b64 	%rd74, %rd73, 3;
	@%p6 bra 	$L__BB2_12;
	setp.ne.s64 	%p9, %rd152, 0;
	@%p9 bra 	$L__BB2_10;
	bra.uni 	$L__BB2_9;
$L__BB2_10:
	div.u64 	%rd155, %rd9, %rd4;
	bra.uni 	$L__BB2_11;
$L__BB2_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd9;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd155, %r13;
$L__BB2_11:
	add.s64 	%rd156, %rd155, 1;
$L__BB2_12:
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd7, %rd6, %rd74;
	add.s64 	%rd8, %rd4, -1;
	min.u64 	%rd20, %rd154, %rd156;
	setp.eq.s64 	%p10, %rd20, 0;
	shl.b64 	%rd149, %rd3, 3;
	@%p10 bra 	$L__BB2_18;
	add.s64 	%rd83, %rd7, %rd149;
	shl.b32 	%r19, %r6, 3;
	add.s32 	%r14, %r2, %r19;
	add.s32 	%r15, %r14, 4;
	ld.u32 	%r16, [%rd83];
	ld.u32 	%r17, [%rd83+4];
	// begin inline asm
	st.shared.f32 [%r14], %r16;
st.shared.f32 [%r15], %r17;
	// end inline asm
	setp.eq.s64 	%p11, %rd20, 1;
	@%p11 bra 	$L__BB2_18;
	add.s64 	%rd160, %rd3, 1;
	add.s64 	%rd159, %rd20, -1;
	shl.b64 	%rd84, %rd5, 12;
	shl.b64 	%rd23, %rd4, 3;
	add.s64 	%rd85, %rd84, %rd23;
	add.s64 	%rd87, %rd85, %rd149;
	add.s64 	%rd158, %rd6, %rd87;
	mov.u64 	%rd88, 4088;
	sub.s64 	%rd157, %rd88, %rd149;
$L__BB2_15:
	add.s64 	%rd31, %rd160, %rd8;
	setp.lt.u64 	%p13, %rd31, 512;
	@%p13 bra 	$L__BB2_17;
	bra.uni 	$L__BB2_16;
$L__BB2_17:
	shr.u64 	%rd89, %rd157, 3;
	setp.gt.u64 	%p12, %rd89, %rd8;
	selp.b64 	%rd30, %rd158, 0, %p12;
	setp.lt.u64 	%p1, %rd31, %rd160;
	add.s64 	%rd94, %rd160, %rd4;
	selp.b64 	%rd160, 512, %rd94, %p1;
	cvt.u32.u64 	%r24, %rd31;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r20, %r25, %r2;
	add.s32 	%r21, %r20, 4;
	ld.u32 	%r22, [%rd30];
	ld.u32 	%r23, [%rd30+4];
	// begin inline asm
	st.shared.f32 [%r20], %r22;
st.shared.f32 [%r21], %r23;
	// end inline asm
	add.s64 	%rd159, %rd159, -1;
	add.s64 	%rd158, %rd158, %rd23;
	sub.s64 	%rd157, %rd157, %rd23;
	setp.ne.s64 	%p14, %rd159, 0;
	@%p14 bra 	$L__BB2_15;
$L__BB2_18:
	add.u64 	%rd68, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd96, [%rd1+16];
	ld.global.nc.u64 	%rd97, [%rd1+24];
	st.local.u64 	[%rd2], %rd96;
	st.local.u64 	[%rd2+8], %rd97;
	bar.sync 	0;
	{ // callseq 9, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd68;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd69;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 9
	bar.sync 	0;
	mov.u64 	%rd164, 0;
	mov.u64 	%rd162, %rd164;
	@%p6 bra 	$L__BB2_23;
	setp.ne.s64 	%p16, %rd152, 0;
	@%p16 bra 	$L__BB2_21;
	bra.uni 	$L__BB2_20;
$L__BB2_21:
	div.u64 	%rd161, %rd9, %rd4;
	bra.uni 	$L__BB2_22;
$L__BB2_20:
	cvt.u32.u64 	%r27, %rd4;
	cvt.u32.u64 	%r28, %rd9;
	div.u32 	%r29, %r28, %r27;
	cvt.u64.u32 	%rd161, %r29;
$L__BB2_22:
	add.s64 	%rd162, %rd161, 1;
$L__BB2_23:
	@%p6 bra 	$L__BB2_28;
	setp.ne.s64 	%p18, %rd152, 0;
	@%p18 bra 	$L__BB2_26;
	bra.uni 	$L__BB2_25;
$L__BB2_26:
	div.u64 	%rd163, %rd9, %rd4;
	bra.uni 	$L__BB2_27;
$L__BB2_25:
	cvt.u32.u64 	%r31, %rd4;
	cvt.u32.u64 	%r32, %rd9;
	div.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd163, %r33;
$L__BB2_27:
	add.s64 	%rd164, %rd163, 1;
$L__BB2_28:
	min.u64 	%rd46, %rd162, %rd164;
	setp.eq.s64 	%p19, %rd46, 0;
	@%p19 bra 	$L__BB2_36;
	cvt.u16.u64 	%rs1, %rd3;
	and.b16  	%rs2, %rs1, 240;
	and.b16  	%rs3, %rs1, 15;
	shl.b16 	%rs4, %rs3, 4;
	shr.u16 	%rs5, %rs2, 4;
	or.b16  	%rs6, %rs5, %rs4;
	and.b16  	%rs7, %rs6, 51;
	shl.b16 	%rs8, %rs7, 2;
	shr.u16 	%rs9, %rs6, 2;
	and.b16  	%rs10, %rs9, 51;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 85;
	add.s64 	%rd104, %rd7, %rd149;
	shl.b16 	%rs13, %rs11, 3;
	and.b16  	%rs14, %rs13, 1360;
	shl.b16 	%rs15, %rs12, 5;
	or.b16  	%rs16, %rs15, %rs14;
	cvt.u32.u16 	%r39, %rs16;
	shr.u32 	%r40, %r6, 5;
	and.b32  	%r41, %r40, 8;
	or.b32  	%r42, %r41, %r39;
	and.b32  	%r43, %r42, 4088;
	add.s32 	%r36, %r2, %r43;
	add.s32 	%r37, %r36, 4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd104], %r34;
	st.u32 	[%rd104+4], %r35;
	setp.eq.s64 	%p20, %rd46, 1;
	@%p20 bra 	$L__BB2_36;
	add.s64 	%rd168, %rd3, 1;
	shl.b64 	%rd105, %rd5, 12;
	shl.b64 	%rd48, %rd4, 3;
	add.s64 	%rd106, %rd105, %rd48;
	add.s64 	%rd108, %rd106, %rd149;
	add.s64 	%rd167, %rd6, %rd108;
	mov.u64 	%rd109, 4088;
	sub.s64 	%rd166, %rd109, %rd149;
	add.s64 	%rd165, %rd46, -1;
$L__BB2_31:
	add.s64 	%rd57, %rd168, %rd8;
	add.s64 	%rd112, %rd57, 512;
	shr.u64 	%rd113, %rd112, 1;
	and.b64  	%rd114, %rd113, 1431655765;
	shl.b64 	%rd115, %rd112, 1;
	and.b64  	%rd116, %rd115, 2863311530;
	or.b64  	%rd117, %rd114, %rd116;
	shr.u64 	%rd118, %rd117, 2;
	and.b64  	%rd119, %rd118, 858993459;
	shl.b64 	%rd120, %rd117, 2;
	and.b64  	%rd121, %rd120, 3435973836;
	or.b64  	%rd122, %rd119, %rd121;
	shr.u64 	%rd123, %rd122, 4;
	and.b64  	%rd124, %rd123, 252645135;
	shl.b64 	%rd125, %rd122, 4;
	and.b64  	%rd126, %rd125, 4042322160;
	or.b64  	%rd127, %rd124, %rd126;
	shr.u64 	%rd128, %rd127, 8;
	and.b64  	%rd129, %rd128, 16711935;
	shl.b64 	%rd130, %rd127, 8;
	and.b64  	%rd131, %rd130, 4278255360;
	or.b64  	%rd132, %rd129, %rd131;
	shr.u64 	%rd133, %rd132, 16;
	shl.b64 	%rd134, %rd132, 16;
	or.b64  	%rd58, %rd133, %rd134;
	setp.eq.s64 	%p22, %rd58, 0;
	mov.u64 	%rd169, 64;
	@%p22 bra 	$L__BB2_33;
	add.s64 	%rd135, %rd58, -1;
	not.b64 	%rd136, %rd58;
	and.b64  	%rd137, %rd136, %rd135;
	popc.b64 	%r44, %rd137;
	cvt.u64.u32 	%rd169, %r44;
$L__BB2_33:
	setp.lt.u64 	%p23, %rd169, 31;
	and.b64  	%rd138, %rd169, 63;
	selp.b64 	%rd139, %rd138, 31, %p23;
	cvt.u32.u64 	%r45, %rd139;
	shr.u64 	%rd140, %rd58, %r45;
	add.s64 	%rd62, %rd140, -1;
	setp.lt.u64 	%p24, %rd62, 1024;
	@%p24 bra 	$L__BB2_35;
	bra.uni 	$L__BB2_34;
$L__BB2_35:
	shr.u64 	%rd111, %rd166, 3;
	setp.gt.u64 	%p21, %rd111, %rd8;
	selp.b64 	%rd56, %rd167, 0, %p21;
	setp.lt.u64 	%p2, %rd57, %rd168;
	setp.gt.u64 	%p25, %rd57, 511;
	add.s64 	%rd145, %rd57, 1;
	selp.b64 	%rd146, 512, %rd145, %p25;
	selp.b64 	%rd168, 512, %rd146, %p2;
	cvt.u32.u64 	%r50, %rd62;
	shl.b32 	%r51, %r50, 2;
	and.b32  	%r52, %r51, 4088;
	add.s32 	%r48, %r52, %r2;
	add.s32 	%r49, %r48, 4;
	// begin inline asm
	ld.shared.f32 %r46, [%r48];
ld.shared.f32 %r47, [%r49];
	// end inline asm
	st.u32 	[%rd56], %r46;
	st.u32 	[%rd56+4], %r47;
	add.s64 	%rd167, %rd167, %rd48;
	sub.s64 	%rd166, %rd166, %rd48;
	add.s64 	%rd165, %rd165, -1;
	setp.ne.s64 	%p26, %rd165, 0;
	@%p26 bra 	$L__BB2_31;
$L__BB2_36:
	ret;
$L__BB2_16:
	mov.u64 	%rd90, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd91, %rd90;
	mov.u64 	%rd92, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd93, %rd92;
	{ // callseq 8, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd93;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 8
$L__BB2_34:
	mov.u64 	%rd141, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd142, %rd141;
	mov.u64 	%rd143, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd144, %rd143;
	{ // callseq 10, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd144;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 10
$L__BB2_1:
	mov.u64 	%rd147, anon_$_9b271fbca47443a0c783d86781d13fba_$_4;
	cvta.global.u64 	%rd148, %rd147;
	{ // callseq 11, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd148;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 11

}
	// .globl	fft_forward_1024_kernel
.visible .entry fft_forward_1024_kernel(
	.param .u64 fft_forward_1024_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot3[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<20>;
	.reg .b32 	%r<49>;
	.reg .b64 	%rd<122>;

	mov.u64 	%SPL, __local_depot3;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd45, [fft_forward_1024_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd45;
	add.u64 	%rd47, %SP, 16;
	add.u64 	%rd48, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[8192];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd48], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd49, [%rd1+8];
	and.b64  	%rd50, %rd49, -1024;
	mul.wide.u32 	%rd6, %r5, 1024;
	setp.lt.u64 	%p3, %rd6, %rd50;
	sub.s64 	%rd51, %rd49, %rd6;
	setp.gt.u64 	%p4, %rd51, 1023;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB3_2;
	bra.uni 	$L__BB3_1;
$L__BB3_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r10, %rd3;
	ld.global.nc.u64 	%rd7, [%rd1];
	shl.b64 	%rd52, %rd6, 3;
	add.s64 	%rd53, %rd7, %rd52;
	add.s64 	%rd8, %rd4, -1;
	cvt.u16.u64 	%rs2, %rd3;
	xor.b16  	%rs1, %rs2, 1023;
	cvt.u16.u64 	%rs3, %rd4;
	shl.b64 	%rd54, %rd3, 3;
	add.s64 	%rd9, %rd53, %rd54;
	shl.b32 	%r11, %r10, 3;
	add.s32 	%r6, %r2, %r11;
	add.s32 	%r7, %r6, 4;
	ld.u32 	%r8, [%rd9];
	ld.u32 	%r9, [%rd9+4];
	// begin inline asm
	st.shared.f32 [%r6], %r8;
st.shared.f32 [%r7], %r9;
	// end inline asm
	setp.lt.u16 	%p6, %rs1, %rs3;
	cvt.u32.u64 	%r48, %rd4;
	@%p6 bra 	$L__BB3_7;
	add.s64 	%rd116, %rd3, 1;
	cvt.u32.u16 	%r12, %rs1;
	div.u32 	%r14, %r12, %r48;
	cvt.u64.u32 	%rd115, %r14;
	shl.b64 	%rd55, %rd5, 13;
	shl.b64 	%rd12, %rd4, 3;
	add.s64 	%rd56, %rd55, %rd12;
	add.s64 	%rd58, %rd56, %rd54;
	add.s64 	%rd114, %rd7, %rd58;
	xor.b64  	%rd113, %rd54, 8184;
$L__BB3_4:
	add.s64 	%rd20, %rd116, %rd8;
	setp.lt.u64 	%p8, %rd20, 1024;
	@%p8 bra 	$L__BB3_6;
	bra.uni 	$L__BB3_5;
$L__BB3_6:
	shr.u64 	%rd59, %rd113, 3;
	setp.gt.u64 	%p7, %rd59, %rd8;
	selp.b64 	%rd19, %rd114, 0, %p7;
	setp.lt.u64 	%p1, %rd20, %rd116;
	add.s64 	%rd64, %rd116, %rd4;
	selp.b64 	%rd116, 1024, %rd64, %p1;
	cvt.u32.u64 	%r19, %rd20;
	shl.b32 	%r20, %r19, 3;
	add.s32 	%r15, %r20, %r2;
	add.s32 	%r16, %r15, 4;
	ld.u32 	%r17, [%rd19];
	ld.u32 	%r18, [%rd19+4];
	// begin inline asm
	st.shared.f32 [%r15], %r17;
st.shared.f32 [%r16], %r18;
	// end inline asm
	add.s64 	%rd115, %rd115, -1;
	add.s64 	%rd114, %rd114, %rd12;
	sub.s64 	%rd113, %rd113, %rd12;
	setp.ne.s64 	%p9, %rd115, 0;
	@%p9 bra 	$L__BB3_4;
$L__BB3_7:
	add.u64 	%rd46, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd65, [%rd1+16];
	ld.global.nc.u64 	%rd66, [%rd1+24];
	st.local.u64 	[%rd2], %rd65;
	st.local.u64 	[%rd2+8], %rd66;
	bar.sync 	0;
	{ // callseq 13, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd47;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 13
	bar.sync 	0;
	xor.b64  	%rd69, %rd3, 1023;
	and.b16  	%rs5, %rs2, 240;
	and.b16  	%rs6, %rs2, 15;
	shl.b16 	%rs7, %rs6, 4;
	shr.u16 	%rs8, %rs5, 4;
	or.b16  	%rs9, %rs8, %rs7;
	and.b16  	%rs10, %rs9, 51;
	shl.b16 	%rs11, %rs10, 2;
	shr.u16 	%rs12, %rs9, 2;
	and.b16  	%rs13, %rs12, 51;
	or.b16  	%rs14, %rs13, %rs11;
	and.b16  	%rs15, %rs14, 85;
	shr.u32 	%r29, %r10, 8;
	shr.u32 	%r30, %r10, 6;
	and.b32  	%r31, %r30, 4;
	or.b32  	%r32, %r29, %r31;
	shl.b16 	%rs16, %rs14, 4;
	and.b16  	%rs17, %rs16, 2720;
	shl.b16 	%rs18, %rs15, 6;
	or.b16  	%rs19, %rs18, %rs17;
	cvt.u32.u16 	%r33, %rs19;
	shl.b32 	%r34, %r32, 2;
	or.b32  	%r35, %r34, %r33;
	or.b32  	%r36, %r35, 4;
	add.s32 	%r37, %r36, 8188;
	and.b32  	%r38, %r37, 8184;
	add.s32 	%r23, %r2, %r38;
	add.s32 	%r24, %r23, 4;
	// begin inline asm
	ld.shared.f32 %r21, [%r23];
ld.shared.f32 %r22, [%r24];
	// end inline asm
	st.u32 	[%rd9], %r21;
	st.u32 	[%rd9+4], %r22;
	setp.lt.u64 	%p10, %rd69, %rd4;
	@%p10 bra 	$L__BB3_14;
	cvt.u32.u64 	%r26, %rd69;
	div.u32 	%r28, %r26, %r48;
	cvt.u64.u32 	%rd117, %r28;
	add.s64 	%rd120, %rd3, 1;
	shl.b64 	%rd70, %rd5, 13;
	shl.b64 	%rd27, %rd4, 3;
	add.s64 	%rd71, %rd70, %rd27;
	add.s64 	%rd73, %rd71, %rd54;
	add.s64 	%rd119, %rd7, %rd73;
	xor.b64  	%rd118, %rd54, 8184;
$L__BB3_9:
	add.s64 	%rd35, %rd120, %rd8;
	add.s64 	%rd76, %rd35, 1024;
	shr.u64 	%rd77, %rd76, 1;
	and.b64  	%rd78, %rd77, 1431655765;
	shl.b64 	%rd79, %rd76, 1;
	and.b64  	%rd80, %rd79, 2863311530;
	or.b64  	%rd81, %rd78, %rd80;
	shr.u64 	%rd82, %rd81, 2;
	and.b64  	%rd83, %rd82, 858993459;
	shl.b64 	%rd84, %rd81, 2;
	and.b64  	%rd85, %rd84, 3435973836;
	or.b64  	%rd86, %rd83, %rd85;
	shr.u64 	%rd87, %rd86, 4;
	and.b64  	%rd88, %rd87, 252645135;
	shl.b64 	%rd89, %rd86, 4;
	and.b64  	%rd90, %rd89, 4042322160;
	or.b64  	%rd91, %rd88, %rd90;
	shr.u64 	%rd92, %rd91, 8;
	and.b64  	%rd93, %rd92, 16711935;
	shl.b64 	%rd94, %rd91, 8;
	and.b64  	%rd95, %rd94, 4278255360;
	or.b64  	%rd96, %rd93, %rd95;
	shr.u64 	%rd97, %rd96, 16;
	shl.b64 	%rd98, %rd96, 16;
	or.b64  	%rd36, %rd97, %rd98;
	setp.eq.s64 	%p12, %rd36, 0;
	mov.u64 	%rd121, 64;
	@%p12 bra 	$L__BB3_11;
	add.s64 	%rd99, %rd36, -1;
	not.b64 	%rd100, %rd36;
	and.b64  	%rd101, %rd100, %rd99;
	popc.b64 	%r39, %rd101;
	cvt.u64.u32 	%rd121, %r39;
$L__BB3_11:
	setp.lt.u64 	%p13, %rd121, 31;
	and.b64  	%rd102, %rd121, 63;
	selp.b64 	%rd103, %rd102, 31, %p13;
	cvt.u32.u64 	%r40, %rd103;
	shr.u64 	%rd104, %rd36, %r40;
	add.s64 	%rd40, %rd104, -1;
	setp.lt.u64 	%p14, %rd40, 2048;
	@%p14 bra 	$L__BB3_13;
	bra.uni 	$L__BB3_12;
$L__BB3_13:
	shr.u64 	%rd75, %rd118, 3;
	setp.gt.u64 	%p11, %rd75, %rd8;
	selp.b64 	%rd34, %rd119, 0, %p11;
	setp.lt.u64 	%p2, %rd35, %rd120;
	setp.gt.u64 	%p15, %rd35, 1023;
	add.s64 	%rd109, %rd35, 1;
	selp.b64 	%rd110, 1024, %rd109, %p15;
	selp.b64 	%rd120, 1024, %rd110, %p2;
	cvt.u32.u64 	%r45, %rd40;
	shl.b32 	%r46, %r45, 2;
	and.b32  	%r47, %r46, 8184;
	add.s32 	%r43, %r47, %r2;
	add.s32 	%r44, %r43, 4;
	// begin inline asm
	ld.shared.f32 %r41, [%r43];
ld.shared.f32 %r42, [%r44];
	// end inline asm
	st.u32 	[%rd34], %r41;
	st.u32 	[%rd34+4], %r42;
	add.s64 	%rd119, %rd119, %rd27;
	sub.s64 	%rd118, %rd118, %rd27;
	add.s64 	%rd117, %rd117, -1;
	setp.ne.s64 	%p16, %rd117, 0;
	@%p16 bra 	$L__BB3_9;
$L__BB3_14:
	ret;
$L__BB3_5:
	mov.u64 	%rd60, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd61, %rd60;
	mov.u64 	%rd62, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd63, %rd62;
	{ // callseq 12, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd61;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd63;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 12
$L__BB3_12:
	mov.u64 	%rd105, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd106, %rd105;
	mov.u64 	%rd107, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd108, %rd107;
	{ // callseq 14, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd106;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd108;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 14
$L__BB3_1:
	mov.u64 	%rd111, anon_$_9b271fbca47443a0c783d86781d13fba_$_4;
	cvta.global.u64 	%rd112, %rd111;
	{ // callseq 15, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd112;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 15

}
	// .globl	fft_forward_2048_kernel
.visible .entry fft_forward_2048_kernel(
	.param .u64 fft_forward_2048_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot4[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<24>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<112>;

	mov.u64 	%SPL, __local_depot4;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd40, [fft_forward_2048_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd40;
	add.u64 	%rd42, %SP, 16;
	add.u64 	%rd43, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[16384];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd43], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd44, [%rd1+8];
	and.b64  	%rd45, %rd44, -2048;
	mul.wide.u32 	%rd6, %r5, 2048;
	setp.lt.u64 	%p3, %rd6, %rd45;
	sub.s64 	%rd46, %rd44, %rd6;
	setp.gt.u64 	%p4, %rd46, 2047;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB4_2;
	bra.uni 	$L__BB4_1;
$L__BB4_2:
	add.u64 	%rd41, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r10, %rd3;
	ld.global.nc.u64 	%rd47, [%rd1];
	shl.b64 	%rd48, %rd6, 3;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd7, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 2047;
	shl.b64 	%rd50, %rd3, 3;
	add.s64 	%rd8, %rd49, %rd50;
	shl.b32 	%r11, %r10, 3;
	add.s32 	%r6, %r2, %r11;
	add.s32 	%r7, %r6, 4;
	ld.u32 	%r8, [%rd8];
	ld.u32 	%r9, [%rd8+4];
	// begin inline asm
	st.shared.f32 [%r6], %r8;
st.shared.f32 [%r7], %r9;
	// end inline asm
	add.s64 	%rd110, %rd3, 1;
	cvt.u32.u16 	%r12, %rs2;
	cvt.u32.u64 	%r13, %rd4;
	div.u32 	%r14, %r12, %r13;
	cvt.u64.u32 	%rd105, %r14;
	shl.b64 	%rd51, %rd5, 14;
	shl.b64 	%rd11, %rd4, 3;
	add.s64 	%rd52, %rd51, %rd11;
	add.s64 	%rd53, %rd52, %rd50;
	add.s64 	%rd109, %rd47, %rd53;
	xor.b64  	%rd108, %rd50, 16376;
	mov.u64 	%rd103, %rd108;
	mov.u64 	%rd104, %rd109;
	mov.u64 	%rd106, %rd110;
$L__BB4_3:
	add.s64 	%rd19, %rd106, %rd7;
	setp.lt.u64 	%p7, %rd19, 2048;
	@%p7 bra 	$L__BB4_5;
	bra.uni 	$L__BB4_4;
$L__BB4_5:
	shr.u64 	%rd54, %rd103, 3;
	setp.gt.u64 	%p6, %rd54, %rd7;
	selp.b64 	%rd18, %rd104, 0, %p6;
	setp.lt.u64 	%p1, %rd19, %rd106;
	add.s64 	%rd59, %rd106, %rd4;
	selp.b64 	%rd106, 2048, %rd59, %p1;
	cvt.u32.u64 	%r19, %rd19;
	shl.b32 	%r20, %r19, 3;
	add.s32 	%r15, %r20, %r2;
	add.s32 	%r16, %r15, 4;
	ld.u32 	%r17, [%rd18];
	ld.u32 	%r18, [%rd18+4];
	// begin inline asm
	st.shared.f32 [%r15], %r17;
st.shared.f32 [%r16], %r18;
	// end inline asm
	add.s64 	%rd105, %rd105, -1;
	add.s64 	%rd104, %rd104, %rd11;
	sub.s64 	%rd103, %rd103, %rd11;
	setp.ne.s64 	%p8, %rd105, 0;
	@%p8 bra 	$L__BB4_3;
	ld.global.nc.u64 	%rd60, [%rd1+16];
	ld.global.nc.u64 	%rd61, [%rd1+24];
	st.local.u64 	[%rd2], %rd60;
	st.local.u64 	[%rd2+8], %rd61;
	bar.sync 	0;
	{ // callseq 17, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd41;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd42;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 17
	bar.sync 	0;
	xor.b32  	%r26, %r10, 2047;
	div.u32 	%r28, %r26, %r13;
	cvt.u64.u32 	%rd107, %r28;
	or.b16  	%rs3, %rs1, 2048;
	shl.b16 	%rs4, %rs1, 12;
	shr.u16 	%rs5, %rs3, 4;
	and.b16  	%rs6, %rs5, 176;
	or.b16  	%rs7, %rs4, %rs6;
	shl.b16 	%rs8, %rs1, 4;
	and.b16  	%rs9, %rs8, 3840;
	or.b16  	%rs10, %rs9, %rs7;
	and.b16  	%rs11, %rs10, 13107;
	shl.b16 	%rs12, %rs11, 2;
	shr.u16 	%rs13, %rs10, 2;
	and.b16  	%rs14, %rs13, 13107;
	or.b16  	%rs15, %rs14, %rs12;
	and.b16  	%rs16, %rs15, 21845;
	shl.b16 	%rs17, %rs16, 1;
	shr.u16 	%rs18, %rs15, 1;
	and.b16  	%rs19, %rs18, 21845;
	or.b16  	%rs20, %rs19, %rs17;
	shr.u16 	%rs21, %rs20, 2;
	add.s16 	%rs22, %rs21, 16380;
	and.b16  	%rs23, %rs22, 16368;
	cvt.u32.u16 	%r29, %rs23;
	add.s32 	%r23, %r2, %r29;
	add.s32 	%r24, %r23, 4;
	// begin inline asm
	ld.shared.f32 %r21, [%r23];
ld.shared.f32 %r22, [%r24];
	// end inline asm
	st.u32 	[%rd8], %r21;
	st.u32 	[%rd8+4], %r22;
$L__BB4_7:
	add.s64 	%rd30, %rd110, %rd7;
	add.s64 	%rd66, %rd30, 2048;
	shr.u64 	%rd67, %rd66, 1;
	and.b64  	%rd68, %rd67, 1431655765;
	shl.b64 	%rd69, %rd66, 1;
	and.b64  	%rd70, %rd69, 2863311530;
	or.b64  	%rd71, %rd68, %rd70;
	shr.u64 	%rd72, %rd71, 2;
	and.b64  	%rd73, %rd72, 858993459;
	shl.b64 	%rd74, %rd71, 2;
	and.b64  	%rd75, %rd74, 3435973836;
	or.b64  	%rd76, %rd73, %rd75;
	shr.u64 	%rd77, %rd76, 4;
	and.b64  	%rd78, %rd77, 252645135;
	shl.b64 	%rd79, %rd76, 4;
	and.b64  	%rd80, %rd79, 4042322160;
	or.b64  	%rd81, %rd78, %rd80;
	shr.u64 	%rd82, %rd81, 8;
	and.b64  	%rd83, %rd82, 16711935;
	shl.b64 	%rd84, %rd81, 8;
	and.b64  	%rd85, %rd84, 4278255360;
	or.b64  	%rd86, %rd83, %rd85;
	shr.u64 	%rd87, %rd86, 16;
	shl.b64 	%rd88, %rd86, 16;
	or.b64  	%rd31, %rd87, %rd88;
	setp.eq.s64 	%p10, %rd31, 0;
	mov.u64 	%rd111, 64;
	@%p10 bra 	$L__BB4_9;
	add.s64 	%rd89, %rd31, -1;
	not.b64 	%rd90, %rd31;
	and.b64  	%rd91, %rd90, %rd89;
	popc.b64 	%r30, %rd91;
	cvt.u64.u32 	%rd111, %r30;
$L__BB4_9:
	setp.lt.u64 	%p11, %rd111, 31;
	and.b64  	%rd92, %rd111, 63;
	selp.b64 	%rd93, %rd92, 31, %p11;
	cvt.u32.u64 	%r31, %rd93;
	shr.u64 	%rd94, %rd31, %r31;
	add.s64 	%rd35, %rd94, -1;
	setp.lt.u64 	%p12, %rd35, 4096;
	@%p12 bra 	$L__BB4_11;
	bra.uni 	$L__BB4_10;
$L__BB4_11:
	shr.u64 	%rd65, %rd108, 3;
	setp.gt.u64 	%p9, %rd65, %rd7;
	selp.b64 	%rd29, %rd109, 0, %p9;
	setp.lt.u64 	%p2, %rd30, %rd110;
	setp.gt.u64 	%p13, %rd30, 2047;
	add.s64 	%rd99, %rd30, 1;
	selp.b64 	%rd100, 2048, %rd99, %p13;
	selp.b64 	%rd110, 2048, %rd100, %p2;
	cvt.u32.u64 	%r36, %rd35;
	shl.b32 	%r37, %r36, 2;
	and.b32  	%r38, %r37, 16376;
	add.s32 	%r34, %r38, %r2;
	add.s32 	%r35, %r34, 4;
	// begin inline asm
	ld.shared.f32 %r32, [%r34];
ld.shared.f32 %r33, [%r35];
	// end inline asm
	st.u32 	[%rd29], %r32;
	st.u32 	[%rd29+4], %r33;
	add.s64 	%rd109, %rd109, %rd11;
	sub.s64 	%rd108, %rd108, %rd11;
	add.s64 	%rd107, %rd107, -1;
	setp.ne.s64 	%p14, %rd107, 0;
	@%p14 bra 	$L__BB4_7;
	ret;
$L__BB4_4:
	mov.u64 	%rd55, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd56, %rd55;
	mov.u64 	%rd57, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd58, %rd57;
	{ // callseq 16, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd56;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd58;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 16
$L__BB4_10:
	mov.u64 	%rd95, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd96, %rd95;
	mov.u64 	%rd97, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd98, %rd97;
	{ // callseq 18, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd96;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd98;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 18
$L__BB4_1:
	mov.u64 	%rd101, anon_$_9b271fbca47443a0c783d86781d13fba_$_4;
	cvta.global.u64 	%rd102, %rd101;
	{ // callseq 19, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd102;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 19

}
	// .globl	fft_forward_4096_kernel
.visible .entry fft_forward_4096_kernel(
	.param .u64 fft_forward_4096_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot5[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<46>;
	.reg .b64 	%rd<136>;

	mov.u64 	%SPL, __local_depot5;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd41, [fft_forward_4096_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd41;
	add.u64 	%rd43, %SP, 16;
	add.u64 	%rd44, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[32768];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd44], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd45, [%rd1+8];
	and.b64  	%rd46, %rd45, -4096;
	mul.wide.u32 	%rd6, %r5, 4096;
	setp.lt.u64 	%p3, %rd6, %rd46;
	sub.s64 	%rd47, %rd45, %rd6;
	setp.gt.u64 	%p4, %rd47, 4095;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB5_2;
	bra.uni 	$L__BB5_1;
$L__BB5_2:
	add.u64 	%rd42, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r10, %rd3;
	ld.global.nc.u64 	%rd48, [%rd1];
	shl.b64 	%rd49, %rd6, 3;
	add.s64 	%rd50, %rd48, %rd49;
	add.s64 	%rd7, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 4095;
	shl.b64 	%rd51, %rd3, 3;
	add.s64 	%rd8, %rd50, %rd51;
	add.s64 	%rd134, %rd3, 1;
	shl.b32 	%r11, %r10, 3;
	add.s32 	%r6, %r2, %r11;
	add.s32 	%r7, %r6, 4;
	ld.u32 	%r8, [%rd8];
	ld.u32 	%r9, [%rd8+4];
	// begin inline asm
	st.shared.f32 [%r6], %r8;
st.shared.f32 [%r7], %r9;
	// end inline asm
	cvt.u32.u16 	%r12, %rs2;
	cvt.u32.u64 	%r13, %rd4;
	div.u32 	%r14, %r12, %r13;
	cvt.u64.u32 	%rd129, %r14;
	shl.b64 	%rd52, %rd5, 15;
	shl.b64 	%rd11, %rd4, 3;
	add.s64 	%rd53, %rd52, %rd11;
	add.s64 	%rd54, %rd53, %rd51;
	add.s64 	%rd133, %rd48, %rd54;
	xor.b64  	%rd132, %rd51, 32760;
	mov.u64 	%rd127, %rd132;
	mov.u64 	%rd128, %rd133;
	mov.u64 	%rd130, %rd134;
$L__BB5_3:
	add.s64 	%rd19, %rd130, %rd7;
	setp.lt.u64 	%p7, %rd19, 4096;
	@%p7 bra 	$L__BB5_5;
	bra.uni 	$L__BB5_4;
$L__BB5_5:
	shr.u64 	%rd55, %rd127, 3;
	setp.gt.u64 	%p6, %rd55, %rd7;
	selp.b64 	%rd18, %rd128, 0, %p6;
	setp.lt.u64 	%p1, %rd19, %rd130;
	add.s64 	%rd60, %rd130, %rd4;
	selp.b64 	%rd130, 4096, %rd60, %p1;
	cvt.u32.u64 	%r19, %rd19;
	shl.b32 	%r20, %r19, 3;
	add.s32 	%r15, %r20, %r2;
	add.s32 	%r16, %r15, 4;
	ld.u32 	%r17, [%rd18];
	ld.u32 	%r18, [%rd18+4];
	// begin inline asm
	st.shared.f32 [%r15], %r17;
st.shared.f32 [%r16], %r18;
	// end inline asm
	add.s64 	%rd129, %rd129, -1;
	add.s64 	%rd128, %rd128, %rd11;
	sub.s64 	%rd127, %rd127, %rd11;
	setp.ne.s64 	%p8, %rd129, 0;
	@%p8 bra 	$L__BB5_3;
	ld.global.nc.u64 	%rd61, [%rd1+16];
	ld.global.nc.u64 	%rd62, [%rd1+24];
	st.local.u64 	[%rd2], %rd61;
	st.local.u64 	[%rd2+8], %rd62;
	bar.sync 	0;
	{ // callseq 21, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd42;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd43;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 21
	bar.sync 	0;
	shr.u64 	%rd65, %rd3, 1;
	and.b64  	%rd66, %rd65, 341;
	shl.b64 	%rd67, %rd3, 1;
	and.b64  	%rd68, %rd67, 682;
	or.b64  	%rd69, %rd68, %rd66;
	shr.u64 	%rd70, %rd69, 2;
	and.b64  	%rd71, %rd70, 51;
	shl.b64 	%rd72, %rd69, 2;
	and.b64  	%rd73, %rd72, 3276;
	or.b64  	%rd74, %rd73, %rd71;
	or.b64  	%rd75, %rd74, 32768;
	shr.u64 	%rd76, %rd75, 4;
	and.b64  	%rd77, %rd76, 2063;
	shl.b64 	%rd78, %rd74, 4;
	and.b64  	%rd79, %rd78, 49392;
	or.b64  	%rd80, %rd77, %rd79;
	{ .reg .b32 tmp; mov.b64 {%r25, tmp}, %rd80; }
	prmt.b32 	%r26, %r25, 0, 291;
	{ .reg .b32 tmp; mov.b64 {tmp, %r27}, %rd80; }
	prmt.b32 	%r28, %r27, 0, 291;
	mov.b64 	%rd81, {%r28, %r26};
	shr.u64 	%rd82, %rd81, 32;
	or.b64  	%rd83, %rd82, 2147483648;
	add.s64 	%rd84, %rd83, -1;
	not.b64 	%rd85, %rd83;
	and.b64  	%rd86, %rd85, %rd84;
	popc.b64 	%r29, %rd86;
	shr.u64 	%rd87, %rd82, %r29;
	add.s64 	%rd25, %rd87, -1;
	setp.gt.u64 	%p9, %rd25, 8191;
	@%p9 bra 	$L__BB5_11;
	xor.b32  	%r22, %r10, 4095;
	div.u32 	%r24, %r22, %r13;
	cvt.u64.u32 	%rd131, %r24;
	cvt.u32.u64 	%r34, %rd25;
	shl.b32 	%r35, %r34, 2;
	and.b32  	%r36, %r35, 32760;
	add.s32 	%r32, %r2, %r36;
	add.s32 	%r33, %r32, 4;
	// begin inline asm
	ld.shared.f32 %r30, [%r32];
ld.shared.f32 %r31, [%r33];
	// end inline asm
	st.u32 	[%rd8], %r30;
	st.u32 	[%rd8+4], %r31;
$L__BB5_8:
	add.s64 	%rd31, %rd134, %rd7;
	add.s64 	%rd90, %rd31, 4096;
	shr.u64 	%rd91, %rd90, 1;
	and.b64  	%rd92, %rd91, 1431655765;
	shl.b64 	%rd93, %rd90, 1;
	and.b64  	%rd94, %rd93, 2863311530;
	or.b64  	%rd95, %rd92, %rd94;
	shr.u64 	%rd96, %rd95, 2;
	and.b64  	%rd97, %rd96, 858993459;
	shl.b64 	%rd98, %rd95, 2;
	and.b64  	%rd99, %rd98, 3435973836;
	or.b64  	%rd100, %rd97, %rd99;
	shr.u64 	%rd101, %rd100, 4;
	and.b64  	%rd102, %rd101, 252645135;
	shl.b64 	%rd103, %rd100, 4;
	and.b64  	%rd104, %rd103, 4042322160;
	or.b64  	%rd105, %rd102, %rd104;
	shr.u64 	%rd106, %rd105, 8;
	and.b64  	%rd107, %rd106, 16711935;
	shl.b64 	%rd108, %rd105, 8;
	and.b64  	%rd109, %rd108, 4278255360;
	or.b64  	%rd110, %rd107, %rd109;
	shr.u64 	%rd111, %rd110, 16;
	shl.b64 	%rd112, %rd110, 16;
	or.b64  	%rd32, %rd111, %rd112;
	setp.eq.s64 	%p11, %rd32, 0;
	mov.u64 	%rd135, 64;
	@%p11 bra 	$L__BB5_10;
	add.s64 	%rd113, %rd32, -1;
	not.b64 	%rd114, %rd32;
	and.b64  	%rd115, %rd114, %rd113;
	popc.b64 	%r37, %rd115;
	cvt.u64.u32 	%rd135, %r37;
$L__BB5_10:
	setp.lt.u64 	%p12, %rd135, 31;
	and.b64  	%rd116, %rd135, 63;
	selp.b64 	%rd117, %rd116, 31, %p12;
	cvt.u32.u64 	%r38, %rd117;
	shr.u64 	%rd118, %rd32, %r38;
	add.s64 	%rd36, %rd118, -1;
	setp.lt.u64 	%p13, %rd36, 8192;
	@%p13 bra 	$L__BB5_12;
	bra.uni 	$L__BB5_11;
$L__BB5_12:
	shr.u64 	%rd89, %rd132, 3;
	setp.gt.u64 	%p10, %rd89, %rd7;
	selp.b64 	%rd30, %rd133, 0, %p10;
	setp.lt.u64 	%p2, %rd31, %rd134;
	setp.gt.u64 	%p14, %rd31, 4095;
	add.s64 	%rd123, %rd31, 1;
	selp.b64 	%rd124, 4096, %rd123, %p14;
	selp.b64 	%rd134, 4096, %rd124, %p2;
	cvt.u32.u64 	%r43, %rd36;
	shl.b32 	%r44, %r43, 2;
	and.b32  	%r45, %r44, 32760;
	add.s32 	%r41, %r45, %r2;
	add.s32 	%r42, %r41, 4;
	// begin inline asm
	ld.shared.f32 %r39, [%r41];
ld.shared.f32 %r40, [%r42];
	// end inline asm
	st.u32 	[%rd30], %r39;
	st.u32 	[%rd30+4], %r40;
	add.s64 	%rd133, %rd133, %rd11;
	sub.s64 	%rd132, %rd132, %rd11;
	add.s64 	%rd131, %rd131, -1;
	setp.ne.s64 	%p15, %rd131, 0;
	@%p15 bra 	$L__BB5_8;
	ret;
$L__BB5_11:
	mov.u64 	%rd119, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd120, %rd119;
	mov.u64 	%rd121, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd122, %rd121;
	{ // callseq 22, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd120;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd122;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 22
$L__BB5_4:
	mov.u64 	%rd56, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd57, %rd56;
	mov.u64 	%rd58, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd59, %rd58;
	{ // callseq 20, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd57;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd59;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 20
$L__BB5_1:
	mov.u64 	%rd125, anon_$_9b271fbca47443a0c783d86781d13fba_$_4;
	cvta.global.u64 	%rd126, %rd125;
	{ // callseq 23, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd126;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 23

}
	// .globl	fft_backward_128_kernel
.visible .entry fft_backward_128_kernel(
	.param .u64 fft_backward_128_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot6[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<18>;
	.reg .b32 	%r<74>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<170>;

	mov.u64 	%SPL, __local_depot6;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd66, [fft_backward_128_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd66;
	add.u64 	%rd68, %SP, 16;
	add.u64 	%rd69, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[1024];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd69], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd70, [%rd1+8];
	and.b64  	%rd71, %rd70, -128;
	mul.wide.u32 	%rd72, %r5, 128;
	setp.lt.u64 	%p3, %rd72, %rd71;
	sub.s64 	%rd74, %rd70, %rd72;
	setp.gt.u64 	%p4, %rd74, 127;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB6_2;
	bra.uni 	$L__BB6_1;
$L__BB6_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd76, %rd3, 128;
	setp.gt.u32 	%p6, %r6, 127;
	not.b64 	%rd77, %rd3;
	add.s64 	%rd9, %rd77, %rd76;
	mov.u64 	%rd156, 0;
	and.b64  	%rd152, %rd9, -4294967296;
	mov.u64 	%rd154, %rd156;
	@%p6 bra 	$L__BB6_7;
	setp.ne.s64 	%p7, %rd152, 0;
	@%p7 bra 	$L__BB6_5;
	bra.uni 	$L__BB6_4;
$L__BB6_5:
	div.u64 	%rd153, %rd9, %rd4;
	bra.uni 	$L__BB6_6;
$L__BB6_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd9;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd153, %r9;
$L__BB6_6:
	add.s64 	%rd154, %rd153, 1;
$L__BB6_7:
	shl.b64 	%rd73, %rd72, 3;
	@%p6 bra 	$L__BB6_12;
	setp.ne.s64 	%p9, %rd152, 0;
	@%p9 bra 	$L__BB6_10;
	bra.uni 	$L__BB6_9;
$L__BB6_10:
	div.u64 	%rd155, %rd9, %rd4;
	bra.uni 	$L__BB6_11;
$L__BB6_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd9;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd155, %r13;
$L__BB6_11:
	add.s64 	%rd156, %rd155, 1;
$L__BB6_12:
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd7, %rd6, %rd73;
	add.s64 	%rd8, %rd4, -1;
	min.u64 	%rd20, %rd154, %rd156;
	setp.eq.s64 	%p10, %rd20, 0;
	shl.b64 	%rd149, %rd3, 3;
	@%p10 bra 	$L__BB6_18;
	add.s64 	%rd82, %rd7, %rd149;
	ld.u32 	%r15, [%rd82+4];
	ld.u32 	%r18, [%rd82];
	// begin inline asm
	neg.ftz.f32 %r14, %r15;
	// end inline asm
	shl.b32 	%r21, %r6, 3;
	add.s32 	%r16, %r2, %r21;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	st.shared.f32 [%r16], %r18;
st.shared.f32 [%r17], %r14;
	// end inline asm
	setp.eq.s64 	%p11, %rd20, 1;
	@%p11 bra 	$L__BB6_18;
	add.s64 	%rd160, %rd3, 1;
	add.s64 	%rd159, %rd20, -1;
	shl.b64 	%rd83, %rd5, 10;
	shl.b64 	%rd23, %rd4, 3;
	add.s64 	%rd84, %rd83, %rd23;
	add.s64 	%rd86, %rd84, %rd149;
	add.s64 	%rd158, %rd6, %rd86;
	mov.u64 	%rd87, 1016;
	sub.s64 	%rd157, %rd87, %rd149;
$L__BB6_15:
	shr.u64 	%rd88, %rd157, 3;
	setp.gt.u64 	%p12, %rd88, %rd8;
	selp.b64 	%rd89, %rd158, 0, %p12;
	add.s64 	%rd30, %rd160, %rd8;
	ld.f32 	%f1, [%rd89];
	ld.u32 	%r23, [%rd89+4];
	// begin inline asm
	neg.ftz.f32 %r22, %r23;
	// end inline asm
	setp.lt.u64 	%p13, %rd30, 128;
	@%p13 bra 	$L__BB6_17;
	bra.uni 	$L__BB6_16;
$L__BB6_17:
	setp.lt.u64 	%p1, %rd30, %rd160;
	mov.b32 	%f2, %r22;
	add.s64 	%rd94, %rd160, %rd4;
	selp.b64 	%rd160, 128, %rd94, %p1;
	cvt.u32.u64 	%r28, %rd30;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r24, %r29, %r2;
	add.s32 	%r25, %r24, 4;
	mov.b32 	%r26, %f1;
	// begin inline asm
	st.shared.f32 [%r24], %r26;
st.shared.f32 [%r25], %r22;
	// end inline asm
	add.s64 	%rd159, %rd159, -1;
	add.s64 	%rd158, %rd158, %rd23;
	sub.s64 	%rd157, %rd157, %rd23;
	setp.ne.s64 	%p14, %rd159, 0;
	@%p14 bra 	$L__BB6_15;
$L__BB6_18:
	add.u64 	%rd67, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd96, [%rd1+16];
	ld.global.nc.u64 	%rd97, [%rd1+24];
	st.local.u64 	[%rd2], %rd96;
	st.local.u64 	[%rd2+8], %rd97;
	bar.sync 	0;
	{ // callseq 25, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd67;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd68;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 25
	mov.b32 	%r31, 1124073472;
	// begin inline asm
	rcp.approx.ftz.f32 %r73, %r31;
	// end inline asm
	bar.sync 	0;
	mov.u64 	%rd164, 0;
	mov.u64 	%rd162, %rd164;
	@%p6 bra 	$L__BB6_23;
	setp.ne.s64 	%p16, %rd152, 0;
	@%p16 bra 	$L__BB6_21;
	bra.uni 	$L__BB6_20;
$L__BB6_21:
	div.u64 	%rd161, %rd9, %rd4;
	bra.uni 	$L__BB6_22;
$L__BB6_20:
	cvt.u32.u64 	%r33, %rd4;
	cvt.u32.u64 	%r34, %rd9;
	div.u32 	%r35, %r34, %r33;
	cvt.u64.u32 	%rd161, %r35;
$L__BB6_22:
	add.s64 	%rd162, %rd161, 1;
$L__BB6_23:
	@%p6 bra 	$L__BB6_28;
	setp.ne.s64 	%p18, %rd152, 0;
	@%p18 bra 	$L__BB6_26;
	bra.uni 	$L__BB6_25;
$L__BB6_26:
	div.u64 	%rd163, %rd9, %rd4;
	bra.uni 	$L__BB6_27;
$L__BB6_25:
	cvt.u32.u64 	%r37, %rd4;
	cvt.u32.u64 	%r38, %rd9;
	div.u32 	%r39, %r38, %r37;
	cvt.u64.u32 	%rd163, %r39;
$L__BB6_27:
	add.s64 	%rd164, %rd163, 1;
$L__BB6_28:
	min.u64 	%rd45, %rd162, %rd164;
	setp.eq.s64 	%p19, %rd45, 0;
	@%p19 bra 	$L__BB6_36;
	mov.b32 	%f3, %r73;
	cvt.u16.u64 	%rs1, %rd3;
	or.b16  	%rs2, %rs1, -128;
	and.b16  	%rs3, %rs2, 240;
	and.b16  	%rs4, %rs2, 15;
	shl.b16 	%rs5, %rs4, 4;
	shr.u16 	%rs6, %rs3, 4;
	or.b16  	%rs7, %rs6, %rs5;
	and.b16  	%rs8, %rs7, 51;
	shl.b16 	%rs9, %rs8, 2;
	shr.u16 	%rs10, %rs7, 2;
	and.b16  	%rs11, %rs10, 51;
	or.b16  	%rs12, %rs11, %rs9;
	and.b16  	%rs13, %rs12, 85;
	add.s64 	%rd104, %rd7, %rd149;
	shl.b16 	%rs14, %rs12, 1;
	and.b16  	%rs15, %rs14, 340;
	shl.b16 	%rs16, %rs13, 3;
	or.b16  	%rs17, %rs16, %rs15;
	cvt.u32.u16 	%r52, %rs17;
	add.s32 	%r53, %r52, 1020;
	and.b32  	%r54, %r53, 1016;
	add.s32 	%r42, %r2, %r54;
	add.s32 	%r43, %r42, 4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r44, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r46, %r40, %r73;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r44, %r73;
	// end inline asm
	st.u32 	[%rd104], %r46;
	st.u32 	[%rd104+4], %r49;
	setp.eq.s64 	%p20, %rd45, 1;
	@%p20 bra 	$L__BB6_36;
	add.s64 	%rd168, %rd3, 1;
	shl.b64 	%rd105, %rd5, 10;
	shl.b64 	%rd47, %rd4, 3;
	add.s64 	%rd106, %rd105, %rd47;
	add.s64 	%rd108, %rd106, %rd149;
	add.s64 	%rd167, %rd6, %rd108;
	mov.u64 	%rd109, 1016;
	sub.s64 	%rd166, %rd109, %rd149;
	add.s64 	%rd165, %rd45, -1;
$L__BB6_31:
	add.s64 	%rd56, %rd168, %rd8;
	add.s64 	%rd112, %rd56, 128;
	shr.u64 	%rd113, %rd112, 1;
	and.b64  	%rd114, %rd113, 1431655765;
	shl.b64 	%rd115, %rd112, 1;
	and.b64  	%rd116, %rd115, 2863311530;
	or.b64  	%rd117, %rd114, %rd116;
	shr.u64 	%rd118, %rd117, 2;
	and.b64  	%rd119, %rd118, 858993459;
	shl.b64 	%rd120, %rd117, 2;
	and.b64  	%rd121, %rd120, 3435973836;
	or.b64  	%rd122, %rd119, %rd121;
	shr.u64 	%rd123, %rd122, 4;
	and.b64  	%rd124, %rd123, 252645135;
	shl.b64 	%rd125, %rd122, 4;
	and.b64  	%rd126, %rd125, 4042322160;
	or.b64  	%rd127, %rd124, %rd126;
	shr.u64 	%rd128, %rd127, 8;
	and.b64  	%rd129, %rd128, 16711935;
	shl.b64 	%rd130, %rd127, 8;
	and.b64  	%rd131, %rd130, 4278255360;
	or.b64  	%rd132, %rd129, %rd131;
	shr.u64 	%rd133, %rd132, 16;
	shl.b64 	%rd134, %rd132, 16;
	or.b64  	%rd57, %rd133, %rd134;
	setp.eq.s64 	%p22, %rd57, 0;
	mov.u64 	%rd169, 64;
	@%p22 bra 	$L__BB6_33;
	add.s64 	%rd135, %rd57, -1;
	not.b64 	%rd136, %rd57;
	and.b64  	%rd137, %rd136, %rd135;
	popc.b64 	%r55, %rd137;
	cvt.u64.u32 	%rd169, %r55;
$L__BB6_33:
	setp.lt.u64 	%p23, %rd169, 31;
	and.b64  	%rd138, %rd169, 63;
	selp.b64 	%rd139, %rd138, 31, %p23;
	cvt.u32.u64 	%r56, %rd139;
	shr.u64 	%rd140, %rd57, %r56;
	add.s64 	%rd61, %rd140, -1;
	setp.lt.u64 	%p24, %rd61, 256;
	@%p24 bra 	$L__BB6_35;
	bra.uni 	$L__BB6_34;
$L__BB6_35:
	shr.u64 	%rd111, %rd166, 3;
	setp.gt.u64 	%p21, %rd111, %rd8;
	selp.b64 	%rd55, %rd167, 0, %p21;
	setp.lt.u64 	%p2, %rd56, %rd168;
	setp.gt.u64 	%p25, %rd56, 127;
	add.s64 	%rd145, %rd56, 1;
	selp.b64 	%rd146, 128, %rd145, %p25;
	selp.b64 	%rd168, 128, %rd146, %p2;
	cvt.u32.u64 	%r69, %rd61;
	shl.b32 	%r70, %r69, 2;
	and.b32  	%r71, %r70, 1016;
	add.s32 	%r59, %r71, %r2;
	add.s32 	%r60, %r59, 4;
	// begin inline asm
	ld.shared.f32 %r57, [%r59];
ld.shared.f32 %r58, [%r60];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r61, %r58;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r63, %r57, %r73;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r66, %r61, %r73;
	// end inline asm
	st.u32 	[%rd55], %r63;
	st.u32 	[%rd55+4], %r66;
	add.s64 	%rd167, %rd167, %rd47;
	sub.s64 	%rd166, %rd166, %rd47;
	add.s64 	%rd165, %rd165, -1;
	setp.ne.s64 	%p26, %rd165, 0;
	@%p26 bra 	$L__BB6_31;
$L__BB6_36:
	ret;
$L__BB6_16:
	mov.u64 	%rd90, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd91, %rd90;
	mov.u64 	%rd92, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd93, %rd92;
	{ // callseq 24, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd93;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 24
$L__BB6_34:
	mov.u64 	%rd141, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd142, %rd141;
	mov.u64 	%rd143, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd144, %rd143;
	{ // callseq 26, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd144;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 26
$L__BB6_1:
	mov.u64 	%rd147, anon_$_9b271fbca47443a0c783d86781d13fba_$_8;
	cvta.global.u64 	%rd148, %rd147;
	{ // callseq 27, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd148;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 27

}
	// .globl	fft_backward_256_kernel
.visible .entry fft_backward_256_kernel(
	.param .u64 fft_backward_256_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot7[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<72>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<170>;

	mov.u64 	%SPL, __local_depot7;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd66, [fft_backward_256_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd66;
	add.u64 	%rd68, %SP, 16;
	add.u64 	%rd69, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[2048];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd69], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd70, [%rd1+8];
	and.b64  	%rd71, %rd70, -256;
	mul.wide.u32 	%rd72, %r5, 256;
	setp.lt.u64 	%p3, %rd72, %rd71;
	sub.s64 	%rd74, %rd70, %rd72;
	setp.gt.u64 	%p4, %rd74, 255;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB7_2;
	bra.uni 	$L__BB7_1;
$L__BB7_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd76, %rd3, 256;
	setp.gt.u32 	%p6, %r6, 255;
	not.b64 	%rd77, %rd3;
	add.s64 	%rd9, %rd77, %rd76;
	mov.u64 	%rd156, 0;
	and.b64  	%rd152, %rd9, -4294967296;
	mov.u64 	%rd154, %rd156;
	@%p6 bra 	$L__BB7_7;
	setp.ne.s64 	%p7, %rd152, 0;
	@%p7 bra 	$L__BB7_5;
	bra.uni 	$L__BB7_4;
$L__BB7_5:
	div.u64 	%rd153, %rd9, %rd4;
	bra.uni 	$L__BB7_6;
$L__BB7_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd9;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd153, %r9;
$L__BB7_6:
	add.s64 	%rd154, %rd153, 1;
$L__BB7_7:
	shl.b64 	%rd73, %rd72, 3;
	@%p6 bra 	$L__BB7_12;
	setp.ne.s64 	%p9, %rd152, 0;
	@%p9 bra 	$L__BB7_10;
	bra.uni 	$L__BB7_9;
$L__BB7_10:
	div.u64 	%rd155, %rd9, %rd4;
	bra.uni 	$L__BB7_11;
$L__BB7_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd9;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd155, %r13;
$L__BB7_11:
	add.s64 	%rd156, %rd155, 1;
$L__BB7_12:
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd7, %rd6, %rd73;
	add.s64 	%rd8, %rd4, -1;
	min.u64 	%rd20, %rd154, %rd156;
	setp.eq.s64 	%p10, %rd20, 0;
	shl.b64 	%rd149, %rd3, 3;
	@%p10 bra 	$L__BB7_18;
	add.s64 	%rd82, %rd7, %rd149;
	ld.u32 	%r15, [%rd82+4];
	ld.u32 	%r18, [%rd82];
	// begin inline asm
	neg.ftz.f32 %r14, %r15;
	// end inline asm
	shl.b32 	%r21, %r6, 3;
	add.s32 	%r16, %r2, %r21;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	st.shared.f32 [%r16], %r18;
st.shared.f32 [%r17], %r14;
	// end inline asm
	setp.eq.s64 	%p11, %rd20, 1;
	@%p11 bra 	$L__BB7_18;
	add.s64 	%rd160, %rd3, 1;
	add.s64 	%rd159, %rd20, -1;
	shl.b64 	%rd83, %rd5, 11;
	shl.b64 	%rd23, %rd4, 3;
	add.s64 	%rd84, %rd83, %rd23;
	add.s64 	%rd86, %rd84, %rd149;
	add.s64 	%rd158, %rd6, %rd86;
	mov.u64 	%rd87, 2040;
	sub.s64 	%rd157, %rd87, %rd149;
$L__BB7_15:
	shr.u64 	%rd88, %rd157, 3;
	setp.gt.u64 	%p12, %rd88, %rd8;
	selp.b64 	%rd89, %rd158, 0, %p12;
	add.s64 	%rd30, %rd160, %rd8;
	ld.f32 	%f1, [%rd89];
	ld.u32 	%r23, [%rd89+4];
	// begin inline asm
	neg.ftz.f32 %r22, %r23;
	// end inline asm
	setp.lt.u64 	%p13, %rd30, 256;
	@%p13 bra 	$L__BB7_17;
	bra.uni 	$L__BB7_16;
$L__BB7_17:
	setp.lt.u64 	%p1, %rd30, %rd160;
	mov.b32 	%f2, %r22;
	add.s64 	%rd94, %rd160, %rd4;
	selp.b64 	%rd160, 256, %rd94, %p1;
	cvt.u32.u64 	%r28, %rd30;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r24, %r29, %r2;
	add.s32 	%r25, %r24, 4;
	mov.b32 	%r26, %f1;
	// begin inline asm
	st.shared.f32 [%r24], %r26;
st.shared.f32 [%r25], %r22;
	// end inline asm
	add.s64 	%rd159, %rd159, -1;
	add.s64 	%rd158, %rd158, %rd23;
	sub.s64 	%rd157, %rd157, %rd23;
	setp.ne.s64 	%p14, %rd159, 0;
	@%p14 bra 	$L__BB7_15;
$L__BB7_18:
	add.u64 	%rd67, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd96, [%rd1+16];
	ld.global.nc.u64 	%rd97, [%rd1+24];
	st.local.u64 	[%rd2], %rd96;
	st.local.u64 	[%rd2+8], %rd97;
	bar.sync 	0;
	{ // callseq 29, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd67;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd68;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 29
	mov.b32 	%r31, 1132462080;
	// begin inline asm
	rcp.approx.ftz.f32 %r71, %r31;
	// end inline asm
	bar.sync 	0;
	mov.u64 	%rd164, 0;
	mov.u64 	%rd162, %rd164;
	@%p6 bra 	$L__BB7_23;
	setp.ne.s64 	%p16, %rd152, 0;
	@%p16 bra 	$L__BB7_21;
	bra.uni 	$L__BB7_20;
$L__BB7_21:
	div.u64 	%rd161, %rd9, %rd4;
	bra.uni 	$L__BB7_22;
$L__BB7_20:
	cvt.u32.u64 	%r33, %rd4;
	cvt.u32.u64 	%r34, %rd9;
	div.u32 	%r35, %r34, %r33;
	cvt.u64.u32 	%rd161, %r35;
$L__BB7_22:
	add.s64 	%rd162, %rd161, 1;
$L__BB7_23:
	@%p6 bra 	$L__BB7_28;
	setp.ne.s64 	%p18, %rd152, 0;
	@%p18 bra 	$L__BB7_26;
	bra.uni 	$L__BB7_25;
$L__BB7_26:
	div.u64 	%rd163, %rd9, %rd4;
	bra.uni 	$L__BB7_27;
$L__BB7_25:
	cvt.u32.u64 	%r37, %rd4;
	cvt.u32.u64 	%r38, %rd9;
	div.u32 	%r39, %r38, %r37;
	cvt.u64.u32 	%rd163, %r39;
$L__BB7_27:
	add.s64 	%rd164, %rd163, 1;
$L__BB7_28:
	min.u64 	%rd45, %rd162, %rd164;
	setp.eq.s64 	%p19, %rd45, 0;
	@%p19 bra 	$L__BB7_36;
	mov.b32 	%f3, %r71;
	cvt.u16.u64 	%rs1, %rd3;
	and.b16  	%rs2, %rs1, 240;
	and.b16  	%rs3, %rs1, 15;
	shl.b16 	%rs4, %rs3, 4;
	shr.u16 	%rs5, %rs2, 4;
	or.b16  	%rs6, %rs5, %rs4;
	and.b16  	%rs7, %rs6, 51;
	shl.b16 	%rs8, %rs7, 2;
	shr.u16 	%rs9, %rs6, 2;
	and.b16  	%rs10, %rs9, 51;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 85;
	shl.b16 	%rs13, %rs12, 1;
	shr.u16 	%rs14, %rs11, 1;
	and.b16  	%rs15, %rs14, 85;
	or.b16  	%rs16, %rs15, %rs13;
	add.s64 	%rd104, %rd7, %rd149;
	mul.wide.u16 	%r52, %rs16, 8;
	add.s32 	%r42, %r2, %r52;
	add.s32 	%r43, %r42, 4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r44, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r46, %r40, %r71;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r44, %r71;
	// end inline asm
	st.u32 	[%rd104], %r46;
	st.u32 	[%rd104+4], %r49;
	setp.eq.s64 	%p20, %rd45, 1;
	@%p20 bra 	$L__BB7_36;
	add.s64 	%rd168, %rd3, 1;
	shl.b64 	%rd105, %rd5, 11;
	shl.b64 	%rd47, %rd4, 3;
	add.s64 	%rd106, %rd105, %rd47;
	add.s64 	%rd108, %rd106, %rd149;
	add.s64 	%rd167, %rd6, %rd108;
	mov.u64 	%rd109, 2040;
	sub.s64 	%rd166, %rd109, %rd149;
	add.s64 	%rd165, %rd45, -1;
$L__BB7_31:
	add.s64 	%rd56, %rd168, %rd8;
	add.s64 	%rd112, %rd56, 256;
	shr.u64 	%rd113, %rd112, 1;
	and.b64  	%rd114, %rd113, 1431655765;
	shl.b64 	%rd115, %rd112, 1;
	and.b64  	%rd116, %rd115, 2863311530;
	or.b64  	%rd117, %rd114, %rd116;
	shr.u64 	%rd118, %rd117, 2;
	and.b64  	%rd119, %rd118, 858993459;
	shl.b64 	%rd120, %rd117, 2;
	and.b64  	%rd121, %rd120, 3435973836;
	or.b64  	%rd122, %rd119, %rd121;
	shr.u64 	%rd123, %rd122, 4;
	and.b64  	%rd124, %rd123, 252645135;
	shl.b64 	%rd125, %rd122, 4;
	and.b64  	%rd126, %rd125, 4042322160;
	or.b64  	%rd127, %rd124, %rd126;
	shr.u64 	%rd128, %rd127, 8;
	and.b64  	%rd129, %rd128, 16711935;
	shl.b64 	%rd130, %rd127, 8;
	and.b64  	%rd131, %rd130, 4278255360;
	or.b64  	%rd132, %rd129, %rd131;
	shr.u64 	%rd133, %rd132, 16;
	shl.b64 	%rd134, %rd132, 16;
	or.b64  	%rd57, %rd133, %rd134;
	setp.eq.s64 	%p22, %rd57, 0;
	mov.u64 	%rd169, 64;
	@%p22 bra 	$L__BB7_33;
	add.s64 	%rd135, %rd57, -1;
	not.b64 	%rd136, %rd57;
	and.b64  	%rd137, %rd136, %rd135;
	popc.b64 	%r53, %rd137;
	cvt.u64.u32 	%rd169, %r53;
$L__BB7_33:
	setp.lt.u64 	%p23, %rd169, 31;
	and.b64  	%rd138, %rd169, 63;
	selp.b64 	%rd139, %rd138, 31, %p23;
	cvt.u32.u64 	%r54, %rd139;
	shr.u64 	%rd140, %rd57, %r54;
	add.s64 	%rd61, %rd140, -1;
	setp.lt.u64 	%p24, %rd61, 512;
	@%p24 bra 	$L__BB7_35;
	bra.uni 	$L__BB7_34;
$L__BB7_35:
	shr.u64 	%rd111, %rd166, 3;
	setp.gt.u64 	%p21, %rd111, %rd8;
	selp.b64 	%rd55, %rd167, 0, %p21;
	setp.lt.u64 	%p2, %rd56, %rd168;
	setp.gt.u64 	%p25, %rd56, 255;
	add.s64 	%rd145, %rd56, 1;
	selp.b64 	%rd146, 256, %rd145, %p25;
	selp.b64 	%rd168, 256, %rd146, %p2;
	cvt.u32.u64 	%r67, %rd61;
	shl.b32 	%r68, %r67, 2;
	and.b32  	%r69, %r68, 2040;
	add.s32 	%r57, %r69, %r2;
	add.s32 	%r58, %r57, 4;
	// begin inline asm
	ld.shared.f32 %r55, [%r57];
ld.shared.f32 %r56, [%r58];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r59, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r61, %r55, %r71;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r64, %r59, %r71;
	// end inline asm
	st.u32 	[%rd55], %r61;
	st.u32 	[%rd55+4], %r64;
	add.s64 	%rd167, %rd167, %rd47;
	sub.s64 	%rd166, %rd166, %rd47;
	add.s64 	%rd165, %rd165, -1;
	setp.ne.s64 	%p26, %rd165, 0;
	@%p26 bra 	$L__BB7_31;
$L__BB7_36:
	ret;
$L__BB7_16:
	mov.u64 	%rd90, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd91, %rd90;
	mov.u64 	%rd92, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd93, %rd92;
	{ // callseq 28, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd93;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 28
$L__BB7_34:
	mov.u64 	%rd141, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd142, %rd141;
	mov.u64 	%rd143, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd144, %rd143;
	{ // callseq 30, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd144;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 30
$L__BB7_1:
	mov.u64 	%rd147, anon_$_9b271fbca47443a0c783d86781d13fba_$_8;
	cvta.global.u64 	%rd148, %rd147;
	{ // callseq 31, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd148;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 31

}
	// .globl	fft_backward_512_kernel
.visible .entry fft_backward_512_kernel(
	.param .u64 fft_backward_512_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot8[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<27>;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<77>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<170>;

	mov.u64 	%SPL, __local_depot8;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd66, [fft_backward_512_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd66;
	add.u64 	%rd68, %SP, 16;
	add.u64 	%rd69, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[4096];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd69], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd70, [%rd1+8];
	and.b64  	%rd71, %rd70, -512;
	mul.wide.u32 	%rd72, %r5, 512;
	setp.lt.u64 	%p3, %rd72, %rd71;
	sub.s64 	%rd74, %rd70, %rd72;
	setp.gt.u64 	%p4, %rd74, 511;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB8_2;
	bra.uni 	$L__BB8_1;
$L__BB8_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd76, %rd3, 512;
	setp.gt.u32 	%p6, %r6, 511;
	not.b64 	%rd77, %rd3;
	add.s64 	%rd9, %rd77, %rd76;
	mov.u64 	%rd156, 0;
	and.b64  	%rd152, %rd9, -4294967296;
	mov.u64 	%rd154, %rd156;
	@%p6 bra 	$L__BB8_7;
	setp.ne.s64 	%p7, %rd152, 0;
	@%p7 bra 	$L__BB8_5;
	bra.uni 	$L__BB8_4;
$L__BB8_5:
	div.u64 	%rd153, %rd9, %rd4;
	bra.uni 	$L__BB8_6;
$L__BB8_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd9;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd153, %r9;
$L__BB8_6:
	add.s64 	%rd154, %rd153, 1;
$L__BB8_7:
	shl.b64 	%rd73, %rd72, 3;
	@%p6 bra 	$L__BB8_12;
	setp.ne.s64 	%p9, %rd152, 0;
	@%p9 bra 	$L__BB8_10;
	bra.uni 	$L__BB8_9;
$L__BB8_10:
	div.u64 	%rd155, %rd9, %rd4;
	bra.uni 	$L__BB8_11;
$L__BB8_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd9;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd155, %r13;
$L__BB8_11:
	add.s64 	%rd156, %rd155, 1;
$L__BB8_12:
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd7, %rd6, %rd73;
	add.s64 	%rd8, %rd4, -1;
	min.u64 	%rd20, %rd154, %rd156;
	setp.eq.s64 	%p10, %rd20, 0;
	shl.b64 	%rd149, %rd3, 3;
	@%p10 bra 	$L__BB8_18;
	add.s64 	%rd82, %rd7, %rd149;
	ld.u32 	%r15, [%rd82+4];
	ld.u32 	%r18, [%rd82];
	// begin inline asm
	neg.ftz.f32 %r14, %r15;
	// end inline asm
	shl.b32 	%r21, %r6, 3;
	add.s32 	%r16, %r2, %r21;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	st.shared.f32 [%r16], %r18;
st.shared.f32 [%r17], %r14;
	// end inline asm
	setp.eq.s64 	%p11, %rd20, 1;
	@%p11 bra 	$L__BB8_18;
	add.s64 	%rd160, %rd3, 1;
	add.s64 	%rd159, %rd20, -1;
	shl.b64 	%rd83, %rd5, 12;
	shl.b64 	%rd23, %rd4, 3;
	add.s64 	%rd84, %rd83, %rd23;
	add.s64 	%rd86, %rd84, %rd149;
	add.s64 	%rd158, %rd6, %rd86;
	mov.u64 	%rd87, 4088;
	sub.s64 	%rd157, %rd87, %rd149;
$L__BB8_15:
	shr.u64 	%rd88, %rd157, 3;
	setp.gt.u64 	%p12, %rd88, %rd8;
	selp.b64 	%rd89, %rd158, 0, %p12;
	add.s64 	%rd30, %rd160, %rd8;
	ld.f32 	%f1, [%rd89];
	ld.u32 	%r23, [%rd89+4];
	// begin inline asm
	neg.ftz.f32 %r22, %r23;
	// end inline asm
	setp.lt.u64 	%p13, %rd30, 512;
	@%p13 bra 	$L__BB8_17;
	bra.uni 	$L__BB8_16;
$L__BB8_17:
	setp.lt.u64 	%p1, %rd30, %rd160;
	mov.b32 	%f2, %r22;
	add.s64 	%rd94, %rd160, %rd4;
	selp.b64 	%rd160, 512, %rd94, %p1;
	cvt.u32.u64 	%r28, %rd30;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r24, %r29, %r2;
	add.s32 	%r25, %r24, 4;
	mov.b32 	%r26, %f1;
	// begin inline asm
	st.shared.f32 [%r24], %r26;
st.shared.f32 [%r25], %r22;
	// end inline asm
	add.s64 	%rd159, %rd159, -1;
	add.s64 	%rd158, %rd158, %rd23;
	sub.s64 	%rd157, %rd157, %rd23;
	setp.ne.s64 	%p14, %rd159, 0;
	@%p14 bra 	$L__BB8_15;
$L__BB8_18:
	add.u64 	%rd67, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd96, [%rd1+16];
	ld.global.nc.u64 	%rd97, [%rd1+24];
	st.local.u64 	[%rd2], %rd96;
	st.local.u64 	[%rd2+8], %rd97;
	bar.sync 	0;
	{ // callseq 33, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd67;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd68;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 33
	mov.b32 	%r31, 1140850688;
	// begin inline asm
	rcp.approx.ftz.f32 %r76, %r31;
	// end inline asm
	bar.sync 	0;
	mov.u64 	%rd164, 0;
	mov.u64 	%rd162, %rd164;
	@%p6 bra 	$L__BB8_23;
	setp.ne.s64 	%p16, %rd152, 0;
	@%p16 bra 	$L__BB8_21;
	bra.uni 	$L__BB8_20;
$L__BB8_21:
	div.u64 	%rd161, %rd9, %rd4;
	bra.uni 	$L__BB8_22;
$L__BB8_20:
	cvt.u32.u64 	%r33, %rd4;
	cvt.u32.u64 	%r34, %rd9;
	div.u32 	%r35, %r34, %r33;
	cvt.u64.u32 	%rd161, %r35;
$L__BB8_22:
	add.s64 	%rd162, %rd161, 1;
$L__BB8_23:
	@%p6 bra 	$L__BB8_28;
	setp.ne.s64 	%p18, %rd152, 0;
	@%p18 bra 	$L__BB8_26;
	bra.uni 	$L__BB8_25;
$L__BB8_26:
	div.u64 	%rd163, %rd9, %rd4;
	bra.uni 	$L__BB8_27;
$L__BB8_25:
	cvt.u32.u64 	%r37, %rd4;
	cvt.u32.u64 	%r38, %rd9;
	div.u32 	%r39, %r38, %r37;
	cvt.u64.u32 	%rd163, %r39;
$L__BB8_27:
	add.s64 	%rd164, %rd163, 1;
$L__BB8_28:
	min.u64 	%rd45, %rd162, %rd164;
	setp.eq.s64 	%p19, %rd45, 0;
	@%p19 bra 	$L__BB8_36;
	mov.b32 	%f3, %r76;
	cvt.u16.u64 	%rs1, %rd3;
	and.b16  	%rs2, %rs1, 240;
	and.b16  	%rs3, %rs1, 15;
	shl.b16 	%rs4, %rs3, 4;
	shr.u16 	%rs5, %rs2, 4;
	or.b16  	%rs6, %rs5, %rs4;
	and.b16  	%rs7, %rs6, 51;
	shl.b16 	%rs8, %rs7, 2;
	shr.u16 	%rs9, %rs6, 2;
	and.b16  	%rs10, %rs9, 51;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 85;
	add.s64 	%rd104, %rd7, %rd149;
	shl.b16 	%rs13, %rs11, 3;
	and.b16  	%rs14, %rs13, 1360;
	shl.b16 	%rs15, %rs12, 5;
	or.b16  	%rs16, %rs15, %rs14;
	cvt.u32.u16 	%r53, %rs16;
	shr.u32 	%r54, %r6, 5;
	and.b32  	%r55, %r54, 8;
	or.b32  	%r56, %r55, %r53;
	and.b32  	%r57, %r56, 4088;
	add.s32 	%r42, %r2, %r57;
	add.s32 	%r43, %r42, 4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r44, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r46, %r40, %r76;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r44, %r76;
	// end inline asm
	st.u32 	[%rd104], %r46;
	st.u32 	[%rd104+4], %r49;
	setp.eq.s64 	%p20, %rd45, 1;
	@%p20 bra 	$L__BB8_36;
	add.s64 	%rd168, %rd3, 1;
	shl.b64 	%rd105, %rd5, 12;
	shl.b64 	%rd47, %rd4, 3;
	add.s64 	%rd106, %rd105, %rd47;
	add.s64 	%rd108, %rd106, %rd149;
	add.s64 	%rd167, %rd6, %rd108;
	mov.u64 	%rd109, 4088;
	sub.s64 	%rd166, %rd109, %rd149;
	add.s64 	%rd165, %rd45, -1;
$L__BB8_31:
	add.s64 	%rd56, %rd168, %rd8;
	add.s64 	%rd112, %rd56, 512;
	shr.u64 	%rd113, %rd112, 1;
	and.b64  	%rd114, %rd113, 1431655765;
	shl.b64 	%rd115, %rd112, 1;
	and.b64  	%rd116, %rd115, 2863311530;
	or.b64  	%rd117, %rd114, %rd116;
	shr.u64 	%rd118, %rd117, 2;
	and.b64  	%rd119, %rd118, 858993459;
	shl.b64 	%rd120, %rd117, 2;
	and.b64  	%rd121, %rd120, 3435973836;
	or.b64  	%rd122, %rd119, %rd121;
	shr.u64 	%rd123, %rd122, 4;
	and.b64  	%rd124, %rd123, 252645135;
	shl.b64 	%rd125, %rd122, 4;
	and.b64  	%rd126, %rd125, 4042322160;
	or.b64  	%rd127, %rd124, %rd126;
	shr.u64 	%rd128, %rd127, 8;
	and.b64  	%rd129, %rd128, 16711935;
	shl.b64 	%rd130, %rd127, 8;
	and.b64  	%rd131, %rd130, 4278255360;
	or.b64  	%rd132, %rd129, %rd131;
	shr.u64 	%rd133, %rd132, 16;
	shl.b64 	%rd134, %rd132, 16;
	or.b64  	%rd57, %rd133, %rd134;
	setp.eq.s64 	%p22, %rd57, 0;
	mov.u64 	%rd169, 64;
	@%p22 bra 	$L__BB8_33;
	add.s64 	%rd135, %rd57, -1;
	not.b64 	%rd136, %rd57;
	and.b64  	%rd137, %rd136, %rd135;
	popc.b64 	%r58, %rd137;
	cvt.u64.u32 	%rd169, %r58;
$L__BB8_33:
	setp.lt.u64 	%p23, %rd169, 31;
	and.b64  	%rd138, %rd169, 63;
	selp.b64 	%rd139, %rd138, 31, %p23;
	cvt.u32.u64 	%r59, %rd139;
	shr.u64 	%rd140, %rd57, %r59;
	add.s64 	%rd61, %rd140, -1;
	setp.lt.u64 	%p24, %rd61, 1024;
	@%p24 bra 	$L__BB8_35;
	bra.uni 	$L__BB8_34;
$L__BB8_35:
	shr.u64 	%rd111, %rd166, 3;
	setp.gt.u64 	%p21, %rd111, %rd8;
	selp.b64 	%rd55, %rd167, 0, %p21;
	setp.lt.u64 	%p2, %rd56, %rd168;
	setp.gt.u64 	%p25, %rd56, 511;
	add.s64 	%rd145, %rd56, 1;
	selp.b64 	%rd146, 512, %rd145, %p25;
	selp.b64 	%rd168, 512, %rd146, %p2;
	cvt.u32.u64 	%r72, %rd61;
	shl.b32 	%r73, %r72, 2;
	and.b32  	%r74, %r73, 4088;
	add.s32 	%r62, %r74, %r2;
	add.s32 	%r63, %r62, 4;
	// begin inline asm
	ld.shared.f32 %r60, [%r62];
ld.shared.f32 %r61, [%r63];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r64, %r61;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r66, %r60, %r76;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r69, %r64, %r76;
	// end inline asm
	st.u32 	[%rd55], %r66;
	st.u32 	[%rd55+4], %r69;
	add.s64 	%rd167, %rd167, %rd47;
	sub.s64 	%rd166, %rd166, %rd47;
	add.s64 	%rd165, %rd165, -1;
	setp.ne.s64 	%p26, %rd165, 0;
	@%p26 bra 	$L__BB8_31;
$L__BB8_36:
	ret;
$L__BB8_16:
	mov.u64 	%rd90, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd91, %rd90;
	mov.u64 	%rd92, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd93, %rd92;
	{ // callseq 32, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd91;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd93;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 32
$L__BB8_34:
	mov.u64 	%rd141, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd142, %rd141;
	mov.u64 	%rd143, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd144, %rd143;
	{ // callseq 34, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd142;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd144;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 34
$L__BB8_1:
	mov.u64 	%rd147, anon_$_9b271fbca47443a0c783d86781d13fba_$_8;
	cvta.global.u64 	%rd148, %rd147;
	{ // callseq 35, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd148;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 35

}
	// .globl	fft_backward_1024_kernel
.visible .entry fft_backward_1024_kernel(
	.param .u64 fft_backward_1024_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot9[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .b16 	%rs<20>;
	.reg .b32 	%r<73>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<122>;

	mov.u64 	%SPL, __local_depot9;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd44, [fft_backward_1024_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd44;
	add.u64 	%rd46, %SP, 16;
	add.u64 	%rd47, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[8192];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd47], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd48, [%rd1+8];
	and.b64  	%rd49, %rd48, -1024;
	mul.wide.u32 	%rd6, %r5, 1024;
	setp.lt.u64 	%p3, %rd6, %rd49;
	sub.s64 	%rd50, %rd48, %rd6;
	setp.gt.u64 	%p4, %rd50, 1023;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB9_2;
	bra.uni 	$L__BB9_1;
$L__BB9_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r12, %rd3;
	ld.global.nc.u64 	%rd7, [%rd1];
	shl.b64 	%rd51, %rd6, 3;
	add.s64 	%rd52, %rd7, %rd51;
	add.s64 	%rd8, %rd4, -1;
	cvt.u16.u64 	%rs2, %rd3;
	xor.b16  	%rs1, %rs2, 1023;
	cvt.u16.u64 	%rs3, %rd4;
	shl.b64 	%rd53, %rd3, 3;
	add.s64 	%rd9, %rd52, %rd53;
	shl.b32 	%r13, %r12, 3;
	ld.u32 	%r7, [%rd9+4];
	ld.u32 	%r10, [%rd9];
	// begin inline asm
	neg.ftz.f32 %r6, %r7;
	// end inline asm
	add.s32 	%r8, %r2, %r13;
	add.s32 	%r9, %r8, 4;
	// begin inline asm
	st.shared.f32 [%r8], %r10;
st.shared.f32 [%r9], %r6;
	// end inline asm
	setp.lt.u16 	%p6, %rs1, %rs3;
	cvt.u32.u64 	%r70, %rd4;
	@%p6 bra 	$L__BB9_7;
	add.s64 	%rd116, %rd3, 1;
	cvt.u32.u16 	%r14, %rs1;
	div.u32 	%r16, %r14, %r70;
	cvt.u64.u32 	%rd115, %r16;
	shl.b64 	%rd54, %rd5, 13;
	shl.b64 	%rd12, %rd4, 3;
	add.s64 	%rd55, %rd54, %rd12;
	add.s64 	%rd57, %rd55, %rd53;
	add.s64 	%rd114, %rd7, %rd57;
	xor.b64  	%rd113, %rd53, 8184;
$L__BB9_4:
	shr.u64 	%rd58, %rd113, 3;
	setp.gt.u64 	%p7, %rd58, %rd8;
	selp.b64 	%rd59, %rd114, 0, %p7;
	add.s64 	%rd19, %rd116, %rd8;
	ld.f32 	%f1, [%rd59];
	ld.u32 	%r18, [%rd59+4];
	// begin inline asm
	neg.ftz.f32 %r17, %r18;
	// end inline asm
	setp.lt.u64 	%p8, %rd19, 1024;
	@%p8 bra 	$L__BB9_6;
	bra.uni 	$L__BB9_5;
$L__BB9_6:
	setp.lt.u64 	%p1, %rd19, %rd116;
	mov.b32 	%f2, %r17;
	add.s64 	%rd64, %rd116, %rd4;
	selp.b64 	%rd116, 1024, %rd64, %p1;
	cvt.u32.u64 	%r23, %rd19;
	shl.b32 	%r24, %r23, 3;
	add.s32 	%r19, %r24, %r2;
	add.s32 	%r20, %r19, 4;
	mov.b32 	%r21, %f1;
	// begin inline asm
	st.shared.f32 [%r19], %r21;
st.shared.f32 [%r20], %r17;
	// end inline asm
	add.s64 	%rd115, %rd115, -1;
	add.s64 	%rd114, %rd114, %rd12;
	sub.s64 	%rd113, %rd113, %rd12;
	setp.ne.s64 	%p9, %rd115, 0;
	@%p9 bra 	$L__BB9_4;
$L__BB9_7:
	add.u64 	%rd45, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd65, [%rd1+16];
	ld.global.nc.u64 	%rd66, [%rd1+24];
	st.local.u64 	[%rd2], %rd65;
	st.local.u64 	[%rd2+8], %rd66;
	bar.sync 	0;
	{ // callseq 37, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd45;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd46;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 37
	mov.b32 	%r26, 1149239296;
	// begin inline asm
	rcp.approx.ftz.f32 %r25, %r26;
	// end inline asm
	bar.sync 	0;
	xor.b64  	%rd69, %rd3, 1023;
	and.b16  	%rs5, %rs2, 240;
	and.b16  	%rs6, %rs2, 15;
	shl.b16 	%rs7, %rs6, 4;
	shr.u16 	%rs8, %rs5, 4;
	or.b16  	%rs9, %rs8, %rs7;
	and.b16  	%rs10, %rs9, 51;
	shl.b16 	%rs11, %rs10, 2;
	shr.u16 	%rs12, %rs9, 2;
	and.b16  	%rs13, %rs12, 51;
	or.b16  	%rs14, %rs13, %rs11;
	and.b16  	%rs15, %rs14, 85;
	shr.u32 	%r43, %r12, 8;
	shr.u32 	%r44, %r12, 6;
	and.b32  	%r45, %r44, 4;
	or.b32  	%r46, %r43, %r45;
	shl.b16 	%rs16, %rs14, 4;
	and.b16  	%rs17, %rs16, 2720;
	shl.b16 	%rs18, %rs15, 6;
	or.b16  	%rs19, %rs18, %rs17;
	cvt.u32.u16 	%r47, %rs19;
	shl.b32 	%r48, %r46, 2;
	or.b32  	%r49, %r48, %r47;
	or.b32  	%r50, %r49, 4;
	add.s32 	%r51, %r50, 8188;
	and.b32  	%r52, %r51, 8184;
	add.s32 	%r29, %r2, %r52;
	add.s32 	%r30, %r29, 4;
	// begin inline asm
	ld.shared.f32 %r27, [%r29];
ld.shared.f32 %r28, [%r30];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r31, %r28;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r33, %r27, %r25;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r36, %r31, %r25;
	// end inline asm
	st.u32 	[%rd9], %r33;
	st.u32 	[%rd9+4], %r36;
	setp.lt.u64 	%p10, %rd69, %rd4;
	@%p10 bra 	$L__BB9_14;
	mov.b32 	%f3, %r25;
	cvt.u32.u64 	%r40, %rd69;
	div.u32 	%r42, %r40, %r70;
	cvt.u64.u32 	%rd117, %r42;
	add.s64 	%rd120, %rd3, 1;
	shl.b64 	%rd70, %rd5, 13;
	shl.b64 	%rd26, %rd4, 3;
	add.s64 	%rd71, %rd70, %rd26;
	add.s64 	%rd73, %rd71, %rd53;
	add.s64 	%rd119, %rd7, %rd73;
	xor.b64  	%rd118, %rd53, 8184;
$L__BB9_9:
	add.s64 	%rd34, %rd120, %rd8;
	add.s64 	%rd76, %rd34, 1024;
	shr.u64 	%rd77, %rd76, 1;
	and.b64  	%rd78, %rd77, 1431655765;
	shl.b64 	%rd79, %rd76, 1;
	and.b64  	%rd80, %rd79, 2863311530;
	or.b64  	%rd81, %rd78, %rd80;
	shr.u64 	%rd82, %rd81, 2;
	and.b64  	%rd83, %rd82, 858993459;
	shl.b64 	%rd84, %rd81, 2;
	and.b64  	%rd85, %rd84, 3435973836;
	or.b64  	%rd86, %rd83, %rd85;
	shr.u64 	%rd87, %rd86, 4;
	and.b64  	%rd88, %rd87, 252645135;
	shl.b64 	%rd89, %rd86, 4;
	and.b64  	%rd90, %rd89, 4042322160;
	or.b64  	%rd91, %rd88, %rd90;
	shr.u64 	%rd92, %rd91, 8;
	and.b64  	%rd93, %rd92, 16711935;
	shl.b64 	%rd94, %rd91, 8;
	and.b64  	%rd95, %rd94, 4278255360;
	or.b64  	%rd96, %rd93, %rd95;
	shr.u64 	%rd97, %rd96, 16;
	shl.b64 	%rd98, %rd96, 16;
	or.b64  	%rd35, %rd97, %rd98;
	setp.eq.s64 	%p12, %rd35, 0;
	mov.u64 	%rd121, 64;
	@%p12 bra 	$L__BB9_11;
	add.s64 	%rd99, %rd35, -1;
	not.b64 	%rd100, %rd35;
	and.b64  	%rd101, %rd100, %rd99;
	popc.b64 	%r53, %rd101;
	cvt.u64.u32 	%rd121, %r53;
$L__BB9_11:
	setp.lt.u64 	%p13, %rd121, 31;
	and.b64  	%rd102, %rd121, 63;
	selp.b64 	%rd103, %rd102, 31, %p13;
	cvt.u32.u64 	%r54, %rd103;
	shr.u64 	%rd104, %rd35, %r54;
	add.s64 	%rd39, %rd104, -1;
	setp.lt.u64 	%p14, %rd39, 2048;
	@%p14 bra 	$L__BB9_13;
	bra.uni 	$L__BB9_12;
$L__BB9_13:
	shr.u64 	%rd75, %rd118, 3;
	setp.gt.u64 	%p11, %rd75, %rd8;
	selp.b64 	%rd33, %rd119, 0, %p11;
	setp.lt.u64 	%p2, %rd34, %rd120;
	setp.gt.u64 	%p15, %rd34, 1023;
	add.s64 	%rd109, %rd34, 1;
	selp.b64 	%rd110, 1024, %rd109, %p15;
	selp.b64 	%rd120, 1024, %rd110, %p2;
	cvt.u32.u64 	%r67, %rd39;
	shl.b32 	%r68, %r67, 2;
	and.b32  	%r69, %r68, 8184;
	add.s32 	%r57, %r69, %r2;
	add.s32 	%r58, %r57, 4;
	// begin inline asm
	ld.shared.f32 %r55, [%r57];
ld.shared.f32 %r56, [%r58];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r59, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r61, %r55, %r25;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r64, %r59, %r25;
	// end inline asm
	st.u32 	[%rd33], %r61;
	st.u32 	[%rd33+4], %r64;
	add.s64 	%rd119, %rd119, %rd26;
	sub.s64 	%rd118, %rd118, %rd26;
	add.s64 	%rd117, %rd117, -1;
	setp.ne.s64 	%p16, %rd117, 0;
	@%p16 bra 	$L__BB9_9;
$L__BB9_14:
	ret;
$L__BB9_5:
	mov.u64 	%rd60, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd61, %rd60;
	mov.u64 	%rd62, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd63, %rd62;
	{ // callseq 36, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd61;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd63;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 36
$L__BB9_12:
	mov.u64 	%rd105, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd106, %rd105;
	mov.u64 	%rd107, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd108, %rd107;
	{ // callseq 38, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd106;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd108;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 38
$L__BB9_1:
	mov.u64 	%rd111, anon_$_9b271fbca47443a0c783d86781d13fba_$_8;
	cvta.global.u64 	%rd112, %rd111;
	{ // callseq 39, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd112;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 39

}
	// .globl	fft_backward_2048_kernel
.visible .entry fft_backward_2048_kernel(
	.param .u64 fft_backward_2048_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot10[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .b16 	%rs<24>;
	.reg .b32 	%r<63>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<112>;

	mov.u64 	%SPL, __local_depot10;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd39, [fft_backward_2048_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd39;
	add.u64 	%rd41, %SP, 16;
	add.u64 	%rd42, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[16384];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd42], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd43, [%rd1+8];
	and.b64  	%rd44, %rd43, -2048;
	mul.wide.u32 	%rd6, %r5, 2048;
	setp.lt.u64 	%p3, %rd6, %rd44;
	sub.s64 	%rd45, %rd43, %rd6;
	setp.gt.u64 	%p4, %rd45, 2047;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB10_2;
	bra.uni 	$L__BB10_1;
$L__BB10_2:
	add.u64 	%rd40, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r12, %rd3;
	ld.global.nc.u64 	%rd46, [%rd1];
	shl.b64 	%rd47, %rd6, 3;
	add.s64 	%rd48, %rd46, %rd47;
	add.s64 	%rd7, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 2047;
	shl.b64 	%rd49, %rd3, 3;
	add.s64 	%rd8, %rd48, %rd49;
	shl.b32 	%r13, %r12, 3;
	ld.u32 	%r7, [%rd8+4];
	ld.u32 	%r10, [%rd8];
	// begin inline asm
	neg.ftz.f32 %r11, %r7;
	// end inline asm
	add.s32 	%r8, %r2, %r13;
	add.s32 	%r9, %r8, 4;
	// begin inline asm
	st.shared.f32 [%r8], %r10;
st.shared.f32 [%r9], %r11;
	// end inline asm
	add.s64 	%rd110, %rd3, 1;
	cvt.u32.u16 	%r14, %rs2;
	cvt.u32.u64 	%r15, %rd4;
	div.u32 	%r16, %r14, %r15;
	cvt.u64.u32 	%rd105, %r16;
	shl.b64 	%rd50, %rd5, 14;
	shl.b64 	%rd11, %rd4, 3;
	add.s64 	%rd51, %rd50, %rd11;
	add.s64 	%rd52, %rd51, %rd49;
	add.s64 	%rd109, %rd46, %rd52;
	xor.b64  	%rd108, %rd49, 16376;
	mov.u64 	%rd103, %rd108;
	mov.u64 	%rd104, %rd109;
	mov.u64 	%rd106, %rd110;
$L__BB10_3:
	shr.u64 	%rd53, %rd103, 3;
	setp.gt.u64 	%p6, %rd53, %rd7;
	selp.b64 	%rd54, %rd104, 0, %p6;
	add.s64 	%rd18, %rd106, %rd7;
	ld.f32 	%f1, [%rd54];
	ld.u32 	%r18, [%rd54+4];
	// begin inline asm
	neg.ftz.f32 %r17, %r18;
	// end inline asm
	setp.lt.u64 	%p7, %rd18, 2048;
	@%p7 bra 	$L__BB10_5;
	bra.uni 	$L__BB10_4;
$L__BB10_5:
	setp.lt.u64 	%p1, %rd18, %rd106;
	mov.b32 	%f2, %r17;
	add.s64 	%rd59, %rd106, %rd4;
	selp.b64 	%rd106, 2048, %rd59, %p1;
	cvt.u32.u64 	%r23, %rd18;
	shl.b32 	%r24, %r23, 3;
	add.s32 	%r19, %r24, %r2;
	add.s32 	%r20, %r19, 4;
	mov.b32 	%r21, %f1;
	// begin inline asm
	st.shared.f32 [%r19], %r21;
st.shared.f32 [%r20], %r17;
	// end inline asm
	add.s64 	%rd105, %rd105, -1;
	add.s64 	%rd104, %rd104, %rd11;
	sub.s64 	%rd103, %rd103, %rd11;
	setp.ne.s64 	%p8, %rd105, 0;
	@%p8 bra 	$L__BB10_3;
	ld.global.nc.u64 	%rd60, [%rd1+16];
	ld.global.nc.u64 	%rd61, [%rd1+24];
	st.local.u64 	[%rd2], %rd60;
	st.local.u64 	[%rd2+8], %rd61;
	bar.sync 	0;
	{ // callseq 41, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd40;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd41;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 41
	mov.b32 	%r26, 1157627904;
	// begin inline asm
	rcp.approx.ftz.f32 %r25, %r26;
	// end inline asm
	mov.b32 	%f3, %r25;
	bar.sync 	0;
	xor.b32  	%r40, %r12, 2047;
	div.u32 	%r42, %r40, %r15;
	cvt.u64.u32 	%rd107, %r42;
	or.b16  	%rs3, %rs1, 2048;
	shl.b16 	%rs4, %rs1, 12;
	shr.u16 	%rs5, %rs3, 4;
	and.b16  	%rs6, %rs5, 176;
	or.b16  	%rs7, %rs4, %rs6;
	shl.b16 	%rs8, %rs1, 4;
	and.b16  	%rs9, %rs8, 3840;
	or.b16  	%rs10, %rs9, %rs7;
	and.b16  	%rs11, %rs10, 13107;
	shl.b16 	%rs12, %rs11, 2;
	shr.u16 	%rs13, %rs10, 2;
	and.b16  	%rs14, %rs13, 13107;
	or.b16  	%rs15, %rs14, %rs12;
	and.b16  	%rs16, %rs15, 21845;
	shl.b16 	%rs17, %rs16, 1;
	shr.u16 	%rs18, %rs15, 1;
	and.b16  	%rs19, %rs18, 21845;
	or.b16  	%rs20, %rs19, %rs17;
	shr.u16 	%rs21, %rs20, 2;
	add.s16 	%rs22, %rs21, 16380;
	and.b16  	%rs23, %rs22, 16368;
	cvt.u32.u16 	%r43, %rs23;
	add.s32 	%r29, %r2, %r43;
	add.s32 	%r30, %r29, 4;
	// begin inline asm
	ld.shared.f32 %r34, [%r29];
ld.shared.f32 %r32, [%r30];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r37, %r32;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r33, %r34, %r25;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r36, %r37, %r25;
	// end inline asm
	st.u32 	[%rd8], %r33;
	st.u32 	[%rd8+4], %r36;
$L__BB10_7:
	add.s64 	%rd29, %rd110, %rd7;
	add.s64 	%rd66, %rd29, 2048;
	shr.u64 	%rd67, %rd66, 1;
	and.b64  	%rd68, %rd67, 1431655765;
	shl.b64 	%rd69, %rd66, 1;
	and.b64  	%rd70, %rd69, 2863311530;
	or.b64  	%rd71, %rd68, %rd70;
	shr.u64 	%rd72, %rd71, 2;
	and.b64  	%rd73, %rd72, 858993459;
	shl.b64 	%rd74, %rd71, 2;
	and.b64  	%rd75, %rd74, 3435973836;
	or.b64  	%rd76, %rd73, %rd75;
	shr.u64 	%rd77, %rd76, 4;
	and.b64  	%rd78, %rd77, 252645135;
	shl.b64 	%rd79, %rd76, 4;
	and.b64  	%rd80, %rd79, 4042322160;
	or.b64  	%rd81, %rd78, %rd80;
	shr.u64 	%rd82, %rd81, 8;
	and.b64  	%rd83, %rd82, 16711935;
	shl.b64 	%rd84, %rd81, 8;
	and.b64  	%rd85, %rd84, 4278255360;
	or.b64  	%rd86, %rd83, %rd85;
	shr.u64 	%rd87, %rd86, 16;
	shl.b64 	%rd88, %rd86, 16;
	or.b64  	%rd30, %rd87, %rd88;
	setp.eq.s64 	%p10, %rd30, 0;
	mov.u64 	%rd111, 64;
	@%p10 bra 	$L__BB10_9;
	add.s64 	%rd89, %rd30, -1;
	not.b64 	%rd90, %rd30;
	and.b64  	%rd91, %rd90, %rd89;
	popc.b64 	%r44, %rd91;
	cvt.u64.u32 	%rd111, %r44;
$L__BB10_9:
	setp.lt.u64 	%p11, %rd111, 31;
	and.b64  	%rd92, %rd111, 63;
	selp.b64 	%rd93, %rd92, 31, %p11;
	cvt.u32.u64 	%r45, %rd93;
	shr.u64 	%rd94, %rd30, %r45;
	add.s64 	%rd34, %rd94, -1;
	setp.lt.u64 	%p12, %rd34, 4096;
	@%p12 bra 	$L__BB10_11;
	bra.uni 	$L__BB10_10;
$L__BB10_11:
	shr.u64 	%rd65, %rd108, 3;
	setp.gt.u64 	%p9, %rd65, %rd7;
	selp.b64 	%rd28, %rd109, 0, %p9;
	setp.lt.u64 	%p2, %rd29, %rd110;
	setp.gt.u64 	%p13, %rd29, 2047;
	add.s64 	%rd99, %rd29, 1;
	selp.b64 	%rd100, 2048, %rd99, %p13;
	selp.b64 	%rd110, 2048, %rd100, %p2;
	cvt.u32.u64 	%r58, %rd34;
	shl.b32 	%r59, %r58, 2;
	and.b32  	%r60, %r59, 16376;
	add.s32 	%r48, %r60, %r2;
	add.s32 	%r49, %r48, 4;
	// begin inline asm
	ld.shared.f32 %r46, [%r48];
ld.shared.f32 %r47, [%r49];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r50, %r47;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r52, %r46, %r25;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r55, %r50, %r25;
	// end inline asm
	st.u32 	[%rd28], %r52;
	st.u32 	[%rd28+4], %r55;
	add.s64 	%rd109, %rd109, %rd11;
	sub.s64 	%rd108, %rd108, %rd11;
	add.s64 	%rd107, %rd107, -1;
	setp.ne.s64 	%p14, %rd107, 0;
	@%p14 bra 	$L__BB10_7;
	ret;
$L__BB10_4:
	mov.u64 	%rd55, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd56, %rd55;
	mov.u64 	%rd57, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd58, %rd57;
	{ // callseq 40, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd56;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd58;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 40
$L__BB10_10:
	mov.u64 	%rd95, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd96, %rd95;
	mov.u64 	%rd97, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd98, %rd97;
	{ // callseq 42, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd96;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd98;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 42
$L__BB10_1:
	mov.u64 	%rd101, anon_$_9b271fbca47443a0c783d86781d13fba_$_8;
	cvta.global.u64 	%rd102, %rd101;
	{ // callseq 43, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd102;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 43

}
	// .globl	fft_backward_4096_kernel
.visible .entry fft_backward_4096_kernel(
	.param .u64 fft_backward_4096_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot11[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<70>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<136>;

	mov.u64 	%SPL, __local_depot11;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd40, [fft_backward_4096_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd40;
	add.u64 	%rd42, %SP, 16;
	add.u64 	%rd43, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[32768];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd43], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd44, [%rd1+8];
	and.b64  	%rd45, %rd44, -4096;
	mul.wide.u32 	%rd6, %r5, 4096;
	setp.lt.u64 	%p3, %rd6, %rd45;
	sub.s64 	%rd46, %rd44, %rd6;
	setp.gt.u64 	%p4, %rd46, 4095;
	and.pred  	%p5, %p3, %p4;
	@%p5 bra 	$L__BB11_2;
	bra.uni 	$L__BB11_1;
$L__BB11_2:
	add.u64 	%rd41, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r12, %rd3;
	ld.global.nc.u64 	%rd47, [%rd1];
	shl.b64 	%rd48, %rd6, 3;
	add.s64 	%rd49, %rd47, %rd48;
	add.s64 	%rd7, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 4095;
	shl.b64 	%rd50, %rd3, 3;
	add.s64 	%rd8, %rd49, %rd50;
	add.s64 	%rd134, %rd3, 1;
	shl.b32 	%r13, %r12, 3;
	ld.u32 	%r7, [%rd8+4];
	ld.u32 	%r10, [%rd8];
	// begin inline asm
	neg.ftz.f32 %r11, %r7;
	// end inline asm
	add.s32 	%r8, %r2, %r13;
	add.s32 	%r9, %r8, 4;
	// begin inline asm
	st.shared.f32 [%r8], %r10;
st.shared.f32 [%r9], %r11;
	// end inline asm
	cvt.u32.u16 	%r14, %rs2;
	cvt.u32.u64 	%r15, %rd4;
	div.u32 	%r16, %r14, %r15;
	cvt.u64.u32 	%rd129, %r16;
	shl.b64 	%rd51, %rd5, 15;
	shl.b64 	%rd11, %rd4, 3;
	add.s64 	%rd52, %rd51, %rd11;
	add.s64 	%rd53, %rd52, %rd50;
	add.s64 	%rd133, %rd47, %rd53;
	xor.b64  	%rd132, %rd50, 32760;
	mov.u64 	%rd127, %rd132;
	mov.u64 	%rd128, %rd133;
	mov.u64 	%rd130, %rd134;
$L__BB11_3:
	shr.u64 	%rd54, %rd127, 3;
	setp.gt.u64 	%p6, %rd54, %rd7;
	selp.b64 	%rd55, %rd128, 0, %p6;
	add.s64 	%rd18, %rd130, %rd7;
	ld.f32 	%f1, [%rd55];
	ld.u32 	%r18, [%rd55+4];
	// begin inline asm
	neg.ftz.f32 %r17, %r18;
	// end inline asm
	setp.lt.u64 	%p7, %rd18, 4096;
	@%p7 bra 	$L__BB11_5;
	bra.uni 	$L__BB11_4;
$L__BB11_5:
	setp.lt.u64 	%p1, %rd18, %rd130;
	mov.b32 	%f2, %r17;
	add.s64 	%rd60, %rd130, %rd4;
	selp.b64 	%rd130, 4096, %rd60, %p1;
	cvt.u32.u64 	%r23, %rd18;
	shl.b32 	%r24, %r23, 3;
	add.s32 	%r19, %r24, %r2;
	add.s32 	%r20, %r19, 4;
	mov.b32 	%r21, %f1;
	// begin inline asm
	st.shared.f32 [%r19], %r21;
st.shared.f32 [%r20], %r17;
	// end inline asm
	add.s64 	%rd129, %rd129, -1;
	add.s64 	%rd128, %rd128, %rd11;
	sub.s64 	%rd127, %rd127, %rd11;
	setp.ne.s64 	%p8, %rd129, 0;
	@%p8 bra 	$L__BB11_3;
	ld.global.nc.u64 	%rd61, [%rd1+16];
	ld.global.nc.u64 	%rd62, [%rd1+24];
	st.local.u64 	[%rd2], %rd61;
	st.local.u64 	[%rd2+8], %rd62;
	bar.sync 	0;
	{ // callseq 45, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd41;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd42;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 45
	mov.b32 	%r26, 1166016512;
	// begin inline asm
	rcp.approx.ftz.f32 %r69, %r26;
	// end inline asm
	bar.sync 	0;
	shr.u64 	%rd65, %rd3, 1;
	and.b64  	%rd66, %rd65, 341;
	shl.b64 	%rd67, %rd3, 1;
	and.b64  	%rd68, %rd67, 682;
	or.b64  	%rd69, %rd68, %rd66;
	shr.u64 	%rd70, %rd69, 2;
	and.b64  	%rd71, %rd70, 51;
	shl.b64 	%rd72, %rd69, 2;
	and.b64  	%rd73, %rd72, 3276;
	or.b64  	%rd74, %rd73, %rd71;
	or.b64  	%rd75, %rd74, 32768;
	shr.u64 	%rd76, %rd75, 4;
	and.b64  	%rd77, %rd76, 2063;
	shl.b64 	%rd78, %rd74, 4;
	and.b64  	%rd79, %rd78, 49392;
	or.b64  	%rd80, %rd77, %rd79;
	{ .reg .b32 tmp; mov.b64 {%r31, tmp}, %rd80; }
	prmt.b32 	%r32, %r31, 0, 291;
	{ .reg .b32 tmp; mov.b64 {tmp, %r33}, %rd80; }
	prmt.b32 	%r34, %r33, 0, 291;
	mov.b64 	%rd81, {%r34, %r32};
	shr.u64 	%rd82, %rd81, 32;
	or.b64  	%rd83, %rd82, 2147483648;
	add.s64 	%rd84, %rd83, -1;
	not.b64 	%rd85, %rd83;
	and.b64  	%rd86, %rd85, %rd84;
	popc.b64 	%r35, %rd86;
	shr.u64 	%rd87, %rd82, %r35;
	add.s64 	%rd24, %rd87, -1;
	setp.gt.u64 	%p9, %rd24, 8191;
	@%p9 bra 	$L__BB11_11;
	mov.b32 	%f3, %r69;
	xor.b32  	%r28, %r12, 4095;
	div.u32 	%r30, %r28, %r15;
	cvt.u64.u32 	%rd131, %r30;
	cvt.u32.u64 	%r48, %rd24;
	shl.b32 	%r49, %r48, 2;
	and.b32  	%r50, %r49, 32760;
	add.s32 	%r38, %r2, %r50;
	add.s32 	%r39, %r38, 4;
	// begin inline asm
	ld.shared.f32 %r43, [%r38];
ld.shared.f32 %r41, [%r39];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r46, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r42, %r43, %r69;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r46, %r69;
	// end inline asm
	st.u32 	[%rd8], %r42;
	st.u32 	[%rd8+4], %r45;
$L__BB11_8:
	add.s64 	%rd30, %rd134, %rd7;
	add.s64 	%rd90, %rd30, 4096;
	shr.u64 	%rd91, %rd90, 1;
	and.b64  	%rd92, %rd91, 1431655765;
	shl.b64 	%rd93, %rd90, 1;
	and.b64  	%rd94, %rd93, 2863311530;
	or.b64  	%rd95, %rd92, %rd94;
	shr.u64 	%rd96, %rd95, 2;
	and.b64  	%rd97, %rd96, 858993459;
	shl.b64 	%rd98, %rd95, 2;
	and.b64  	%rd99, %rd98, 3435973836;
	or.b64  	%rd100, %rd97, %rd99;
	shr.u64 	%rd101, %rd100, 4;
	and.b64  	%rd102, %rd101, 252645135;
	shl.b64 	%rd103, %rd100, 4;
	and.b64  	%rd104, %rd103, 4042322160;
	or.b64  	%rd105, %rd102, %rd104;
	shr.u64 	%rd106, %rd105, 8;
	and.b64  	%rd107, %rd106, 16711935;
	shl.b64 	%rd108, %rd105, 8;
	and.b64  	%rd109, %rd108, 4278255360;
	or.b64  	%rd110, %rd107, %rd109;
	shr.u64 	%rd111, %rd110, 16;
	shl.b64 	%rd112, %rd110, 16;
	or.b64  	%rd31, %rd111, %rd112;
	setp.eq.s64 	%p11, %rd31, 0;
	mov.u64 	%rd135, 64;
	@%p11 bra 	$L__BB11_10;
	add.s64 	%rd113, %rd31, -1;
	not.b64 	%rd114, %rd31;
	and.b64  	%rd115, %rd114, %rd113;
	popc.b64 	%r51, %rd115;
	cvt.u64.u32 	%rd135, %r51;
$L__BB11_10:
	setp.lt.u64 	%p12, %rd135, 31;
	and.b64  	%rd116, %rd135, 63;
	selp.b64 	%rd117, %rd116, 31, %p12;
	cvt.u32.u64 	%r52, %rd117;
	shr.u64 	%rd118, %rd31, %r52;
	add.s64 	%rd35, %rd118, -1;
	setp.lt.u64 	%p13, %rd35, 8192;
	@%p13 bra 	$L__BB11_12;
	bra.uni 	$L__BB11_11;
$L__BB11_12:
	shr.u64 	%rd89, %rd132, 3;
	setp.gt.u64 	%p10, %rd89, %rd7;
	selp.b64 	%rd29, %rd133, 0, %p10;
	setp.lt.u64 	%p2, %rd30, %rd134;
	setp.gt.u64 	%p14, %rd30, 4095;
	add.s64 	%rd123, %rd30, 1;
	selp.b64 	%rd124, 4096, %rd123, %p14;
	selp.b64 	%rd134, 4096, %rd124, %p2;
	cvt.u32.u64 	%r65, %rd35;
	shl.b32 	%r66, %r65, 2;
	and.b32  	%r67, %r66, 32760;
	add.s32 	%r55, %r67, %r2;
	add.s32 	%r56, %r55, 4;
	// begin inline asm
	ld.shared.f32 %r53, [%r55];
ld.shared.f32 %r54, [%r56];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r57, %r54;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r59, %r53, %r69;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r62, %r57, %r69;
	// end inline asm
	st.u32 	[%rd29], %r59;
	st.u32 	[%rd29+4], %r62;
	add.s64 	%rd133, %rd133, %rd11;
	sub.s64 	%rd132, %rd132, %rd11;
	add.s64 	%rd131, %rd131, -1;
	setp.ne.s64 	%p15, %rd131, 0;
	@%p15 bra 	$L__BB11_8;
	ret;
$L__BB11_11:
	mov.u64 	%rd119, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd120, %rd119;
	mov.u64 	%rd121, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd122, %rd121;
	{ // callseq 46, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd120;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd122;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 46
$L__BB11_4:
	mov.u64 	%rd56, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd57, %rd56;
	mov.u64 	%rd58, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd59, %rd58;
	{ // callseq 44, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd57;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd59;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 44
$L__BB11_1:
	mov.u64 	%rd125, anon_$_9b271fbca47443a0c783d86781d13fba_$_8;
	cvta.global.u64 	%rd126, %rd125;
	{ // callseq 47, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd126;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 47

}
	// .globl	fft_forward_shifted_128_kernel
.visible .entry fft_forward_shifted_128_kernel(
	.param .u64 fft_forward_shifted_128_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot12[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<73>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<146>;

	mov.u64 	%SPL, __local_depot12;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd62, [fft_forward_shifted_128_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd62;
	add.u64 	%rd64, %SP, 16;
	add.u64 	%rd65, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[1024];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd65], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd66, [%rd1+8];
	and.b64  	%rd67, %rd66, -128;
	mul.wide.u32 	%rd68, %r5, 128;
	setp.lt.u64 	%p2, %rd68, %rd67;
	sub.s64 	%rd70, %rd66, %rd68;
	setp.gt.u64 	%p3, %rd70, 127;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB12_2;
	bra.uni 	$L__BB12_1;
$L__BB12_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd72, %rd3, 128;
	setp.gt.u32 	%p5, %r6, 127;
	not.b64 	%rd73, %rd3;
	add.s64 	%rd10, %rd73, %rd72;
	mov.u64 	%rd132, 0;
	and.b64  	%rd128, %rd10, -4294967296;
	mov.u64 	%rd130, %rd132;
	@%p5 bra 	$L__BB12_7;
	setp.ne.s64 	%p6, %rd128, 0;
	@%p6 bra 	$L__BB12_5;
	bra.uni 	$L__BB12_4;
$L__BB12_5:
	div.u64 	%rd129, %rd10, %rd4;
	bra.uni 	$L__BB12_6;
$L__BB12_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd10;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd129, %r9;
$L__BB12_6:
	add.s64 	%rd130, %rd129, 1;
$L__BB12_7:
	shl.b64 	%rd69, %rd68, 3;
	@%p5 bra 	$L__BB12_12;
	setp.ne.s64 	%p8, %rd128, 0;
	@%p8 bra 	$L__BB12_10;
	bra.uni 	$L__BB12_9;
$L__BB12_10:
	div.u64 	%rd131, %rd10, %rd4;
	bra.uni 	$L__BB12_11;
$L__BB12_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd10;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd131, %r13;
$L__BB12_11:
	add.s64 	%rd132, %rd131, 1;
$L__BB12_12:
	add.s64 	%rd7, %rd6, %rd69;
	add.s64 	%rd9, %rd4, -1;
	min.u64 	%rd21, %rd130, %rd132;
	setp.eq.s64 	%p9, %rd21, 0;
	shl.b64 	%rd124, %rd3, 3;
	shl.b64 	%rd125, %rd4, 3;
	@%p9 bra 	$L__BB12_18;
	add.s64 	%rd78, %rd7, %rd124;
	shl.b32 	%r19, %r6, 3;
	add.s32 	%r14, %r2, %r19;
	add.s32 	%r15, %r14, 4;
	ld.u32 	%r16, [%rd78];
	ld.u32 	%r17, [%rd78+4];
	// begin inline asm
	st.shared.f32 [%r14], %r16;
st.shared.f32 [%r15], %r17;
	// end inline asm
	setp.eq.s64 	%p10, %rd21, 1;
	@%p10 bra 	$L__BB12_18;
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd136, %rd3, 1;
	add.s64 	%rd135, %rd21, -1;
	shl.b64 	%rd79, %rd5, 10;
	add.s64 	%rd80, %rd79, %rd125;
	add.s64 	%rd82, %rd80, %rd124;
	add.s64 	%rd134, %rd6, %rd82;
	mov.u64 	%rd83, 1016;
	sub.s64 	%rd133, %rd83, %rd124;
$L__BB12_15:
	add.s64 	%rd32, %rd136, %rd9;
	setp.lt.u64 	%p12, %rd32, 128;
	@%p12 bra 	$L__BB12_17;
	bra.uni 	$L__BB12_16;
$L__BB12_17:
	shr.u64 	%rd84, %rd133, 3;
	setp.gt.u64 	%p11, %rd84, %rd9;
	selp.b64 	%rd31, %rd134, 0, %p11;
	setp.lt.u64 	%p1, %rd32, %rd136;
	add.s64 	%rd89, %rd136, %rd4;
	selp.b64 	%rd136, 128, %rd89, %p1;
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r20, %r25, %r2;
	add.s32 	%r21, %r20, 4;
	ld.u32 	%r22, [%rd31];
	ld.u32 	%r23, [%rd31+4];
	// begin inline asm
	st.shared.f32 [%r20], %r22;
st.shared.f32 [%r21], %r23;
	// end inline asm
	add.s64 	%rd135, %rd135, -1;
	add.s64 	%rd134, %rd134, %rd125;
	sub.s64 	%rd133, %rd133, %rd125;
	setp.ne.s64 	%p13, %rd135, 0;
	@%p13 bra 	$L__BB12_15;
$L__BB12_18:
	add.u64 	%rd63, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd91, [%rd1+16];
	ld.global.nc.u64 	%rd92, [%rd1+24];
	st.local.u64 	[%rd2], %rd91;
	st.local.u64 	[%rd2+8], %rd92;
	bar.sync 	0;
	{ // callseq 49, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd63;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd64;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 49
	bar.sync 	0;
	mov.u64 	%rd140, 0;
	mov.u64 	%rd138, %rd140;
	@%p5 bra 	$L__BB12_23;
	setp.ne.s64 	%p15, %rd128, 0;
	@%p15 bra 	$L__BB12_21;
	bra.uni 	$L__BB12_20;
$L__BB12_21:
	div.u64 	%rd137, %rd10, %rd4;
	bra.uni 	$L__BB12_22;
$L__BB12_20:
	cvt.u32.u64 	%r27, %rd4;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	cvt.u64.u32 	%rd137, %r29;
$L__BB12_22:
	add.s64 	%rd138, %rd137, 1;
$L__BB12_23:
	@%p5 bra 	$L__BB12_28;
	setp.ne.s64 	%p17, %rd128, 0;
	@%p17 bra 	$L__BB12_26;
	bra.uni 	$L__BB12_25;
$L__BB12_26:
	div.u64 	%rd139, %rd10, %rd4;
	bra.uni 	$L__BB12_27;
$L__BB12_25:
	cvt.u32.u64 	%r31, %rd4;
	cvt.u32.u64 	%r32, %rd10;
	div.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd139, %r33;
$L__BB12_27:
	add.s64 	%rd140, %rd139, 1;
$L__BB12_28:
	min.u64 	%rd47, %rd138, %rd140;
	setp.eq.s64 	%p18, %rd47, 0;
	@%p18 bra 	$L__BB12_35;
	add.s64 	%rd48, %rd7, %rd124;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, -64;
	and.b16  	%rs3, %rs2, 240;
	and.b16  	%rs4, %rs2, 15;
	shl.b16 	%rs5, %rs4, 4;
	shr.u16 	%rs6, %rs3, 4;
	or.b16  	%rs7, %rs6, %rs5;
	and.b16  	%rs8, %rs7, 51;
	shl.b16 	%rs9, %rs8, 2;
	shr.u16 	%rs10, %rs7, 2;
	and.b16  	%rs11, %rs10, 51;
	or.b16  	%rs12, %rs11, %rs9;
	and.b16  	%rs13, %rs12, 85;
	shl.b16 	%rs14, %rs13, 1;
	shr.u16 	%rs15, %rs12, 1;
	and.b16  	%rs16, %rs15, 85;
	or.b16  	%rs17, %rs16, %rs14;
	mul.wide.u16 	%r38, %rs17, 4;
	add.s32 	%r37, %r2, %r38;
	add.s32 	%r36, %r37, -4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd48], %r34;
	st.u32 	[%rd48+4], %r35;
	setp.eq.s64 	%p19, %rd47, 1;
	@%p19 bra 	$L__BB12_35;
	add.s64 	%rd8, %rd7, 1024;
	add.s64 	%rd145, %rd3, 1;
	add.s64 	%rd144, %rd48, 8;
	add.s64 	%rd51, %rd47, -1;
	and.b64  	%rd52, %rd51, 1;
	setp.eq.s64 	%p20, %rd47, 2;
	@%p20 bra 	$L__BB12_33;
	and.b64  	%rd141, %rd51, -2;
$L__BB12_32:
	sub.s64 	%rd99, %rd8, %rd144;
	shr.u64 	%rd100, %rd99, 3;
	setp.gt.u64 	%p21, %rd100, %rd9;
	add.s64 	%rd103, %rd144, %rd125;
	add.s64 	%rd104, %rd103, -8;
	selp.b64 	%rd105, %rd103, %rd8, %p21;
	selp.b64 	%rd106, %rd104, 0, %p21;
	add.s64 	%rd107, %rd145, %rd9;
	setp.lt.u64 	%p22, %rd107, %rd145;
	setp.gt.u64 	%p23, %rd107, 127;
	add.s64 	%rd108, %rd107, 1;
	selp.b64 	%rd109, 128, %rd108, %p23;
	selp.b64 	%rd110, 128, %rd109, %p22;
	cvt.u16.u64 	%rs18, %rd107;
	and.b16  	%rs19, %rs18, 127;
	xor.b16  	%rs20, %rs19, -64;
	and.b16  	%rs21, %rs20, 240;
	and.b16  	%rs22, %rs20, 15;
	shl.b16 	%rs23, %rs22, 4;
	shr.u16 	%rs24, %rs21, 4;
	or.b16  	%rs25, %rs24, %rs23;
	and.b16  	%rs26, %rs25, 51;
	shl.b16 	%rs27, %rs26, 2;
	shr.u16 	%rs28, %rs25, 2;
	and.b16  	%rs29, %rs28, 51;
	or.b16  	%rs30, %rs29, %rs27;
	and.b16  	%rs31, %rs30, 85;
	shl.b16 	%rs32, %rs31, 1;
	shr.u16 	%rs33, %rs30, 1;
	and.b16  	%rs34, %rs33, 85;
	or.b16  	%rs35, %rs34, %rs32;
	mul.wide.u16 	%r47, %rs35, 4;
	add.s32 	%r42, %r47, %r2;
	add.s32 	%r41, %r42, -4;
	// begin inline asm
	ld.shared.f32 %r39, [%r41];
ld.shared.f32 %r40, [%r42];
	// end inline asm
	st.u32 	[%rd106], %r39;
	st.u32 	[%rd106+4], %r40;
	add.s64 	%rd144, %rd105, %rd125;
	add.s64 	%rd112, %rd110, %rd9;
	setp.lt.u64 	%p24, %rd112, %rd110;
	setp.gt.u64 	%p25, %rd112, 127;
	add.s64 	%rd113, %rd112, 1;
	selp.b64 	%rd114, 128, %rd113, %p25;
	selp.b64 	%rd145, 128, %rd114, %p24;
	cvt.u16.u64 	%rs36, %rd112;
	and.b16  	%rs37, %rs36, 127;
	xor.b16  	%rs38, %rs37, -64;
	and.b16  	%rs39, %rs38, 240;
	and.b16  	%rs40, %rs38, 15;
	shl.b16 	%rs41, %rs40, 4;
	shr.u16 	%rs42, %rs39, 4;
	or.b16  	%rs43, %rs42, %rs41;
	and.b16  	%rs44, %rs43, 51;
	shl.b16 	%rs45, %rs44, 2;
	shr.u16 	%rs46, %rs43, 2;
	and.b16  	%rs47, %rs46, 51;
	or.b16  	%rs48, %rs47, %rs45;
	and.b16  	%rs49, %rs48, 85;
	shl.b16 	%rs50, %rs49, 1;
	shr.u16 	%rs51, %rs48, 1;
	and.b16  	%rs52, %rs51, 85;
	or.b16  	%rs53, %rs52, %rs50;
	mul.wide.u16 	%r48, %rs53, 4;
	add.s32 	%r46, %r48, %r2;
	add.s32 	%r45, %r46, -4;
	// begin inline asm
	ld.shared.f32 %r43, [%r45];
ld.shared.f32 %r44, [%r46];
	// end inline asm
	st.u32 	[%rd144+-8], %r43;
	st.u32 	[%rd144+-4], %r44;
	add.s64 	%rd141, %rd141, -2;
	setp.ne.s64 	%p26, %rd141, 0;
	@%p26 bra 	$L__BB12_32;
$L__BB12_33:
	setp.eq.s64 	%p27, %rd52, 0;
	@%p27 bra 	$L__BB12_35;
	sub.s64 	%rd115, %rd8, %rd144;
	shr.u64 	%rd116, %rd115, 3;
	setp.gt.u64 	%p28, %rd116, %rd9;
	add.s64 	%rd118, %rd144, %rd125;
	add.s64 	%rd119, %rd118, -8;
	selp.b64 	%rd120, %rd119, 0, %p28;
	add.s64 	%rd121, %rd145, %rd9;
	setp.lt.u64 	%p29, %rd121, %rd145;
	cvt.u16.u64 	%rs54, %rd121;
	and.b16  	%rs55, %rs54, 127;
	xor.b16  	%rs56, %rs55, -64;
	selp.b16 	%rs57, -64, %rs56, %p29;
	and.b16  	%rs58, %rs57, 240;
	and.b16  	%rs59, %rs57, 15;
	shl.b16 	%rs60, %rs59, 4;
	shr.u16 	%rs61, %rs58, 4;
	or.b16  	%rs62, %rs61, %rs60;
	and.b16  	%rs63, %rs62, 51;
	shl.b16 	%rs64, %rs63, 2;
	shr.u16 	%rs65, %rs62, 2;
	and.b16  	%rs66, %rs65, 51;
	or.b16  	%rs67, %rs66, %rs64;
	and.b16  	%rs68, %rs67, 85;
	shl.b16 	%rs69, %rs68, 1;
	shr.u16 	%rs70, %rs67, 1;
	and.b16  	%rs71, %rs70, 85;
	or.b16  	%rs72, %rs71, %rs69;
	mul.wide.u16 	%r53, %rs72, 4;
	add.s32 	%r52, %r53, %r2;
	add.s32 	%r51, %r52, -4;
	// begin inline asm
	ld.shared.f32 %r49, [%r51];
ld.shared.f32 %r50, [%r52];
	// end inline asm
	st.u32 	[%rd120], %r49;
	st.u32 	[%rd120+4], %r50;
$L__BB12_35:
	ret;
$L__BB12_16:
	mov.u64 	%rd85, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd86, %rd85;
	mov.u64 	%rd87, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd88, %rd87;
	{ // callseq 48, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd86;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd88;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 48
$L__BB12_1:
	mov.u64 	%rd122, anon_$_9b271fbca47443a0c783d86781d13fba_$_7;
	cvta.global.u64 	%rd123, %rd122;
	{ // callseq 50, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd123;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 50

}
	// .globl	fft_forward_shifted_256_kernel
.visible .entry fft_forward_shifted_256_kernel(
	.param .u64 fft_forward_shifted_256_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot13[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<76>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<146>;

	mov.u64 	%SPL, __local_depot13;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd62, [fft_forward_shifted_256_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd62;
	add.u64 	%rd64, %SP, 16;
	add.u64 	%rd65, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[2048];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd65], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd66, [%rd1+8];
	and.b64  	%rd67, %rd66, -256;
	mul.wide.u32 	%rd68, %r5, 256;
	setp.lt.u64 	%p2, %rd68, %rd67;
	sub.s64 	%rd70, %rd66, %rd68;
	setp.gt.u64 	%p3, %rd70, 255;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB13_2;
	bra.uni 	$L__BB13_1;
$L__BB13_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd72, %rd3, 256;
	setp.gt.u32 	%p5, %r6, 255;
	not.b64 	%rd73, %rd3;
	add.s64 	%rd10, %rd73, %rd72;
	mov.u64 	%rd132, 0;
	and.b64  	%rd128, %rd10, -4294967296;
	mov.u64 	%rd130, %rd132;
	@%p5 bra 	$L__BB13_7;
	setp.ne.s64 	%p6, %rd128, 0;
	@%p6 bra 	$L__BB13_5;
	bra.uni 	$L__BB13_4;
$L__BB13_5:
	div.u64 	%rd129, %rd10, %rd4;
	bra.uni 	$L__BB13_6;
$L__BB13_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd10;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd129, %r9;
$L__BB13_6:
	add.s64 	%rd130, %rd129, 1;
$L__BB13_7:
	shl.b64 	%rd69, %rd68, 3;
	@%p5 bra 	$L__BB13_12;
	setp.ne.s64 	%p8, %rd128, 0;
	@%p8 bra 	$L__BB13_10;
	bra.uni 	$L__BB13_9;
$L__BB13_10:
	div.u64 	%rd131, %rd10, %rd4;
	bra.uni 	$L__BB13_11;
$L__BB13_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd10;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd131, %r13;
$L__BB13_11:
	add.s64 	%rd132, %rd131, 1;
$L__BB13_12:
	add.s64 	%rd7, %rd6, %rd69;
	add.s64 	%rd9, %rd4, -1;
	min.u64 	%rd21, %rd130, %rd132;
	setp.eq.s64 	%p9, %rd21, 0;
	shl.b64 	%rd124, %rd3, 3;
	shl.b64 	%rd125, %rd4, 3;
	@%p9 bra 	$L__BB13_18;
	add.s64 	%rd78, %rd7, %rd124;
	shl.b32 	%r19, %r6, 3;
	add.s32 	%r14, %r2, %r19;
	add.s32 	%r15, %r14, 4;
	ld.u32 	%r16, [%rd78];
	ld.u32 	%r17, [%rd78+4];
	// begin inline asm
	st.shared.f32 [%r14], %r16;
st.shared.f32 [%r15], %r17;
	// end inline asm
	setp.eq.s64 	%p10, %rd21, 1;
	@%p10 bra 	$L__BB13_18;
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd136, %rd3, 1;
	add.s64 	%rd135, %rd21, -1;
	shl.b64 	%rd79, %rd5, 11;
	add.s64 	%rd80, %rd79, %rd125;
	add.s64 	%rd82, %rd80, %rd124;
	add.s64 	%rd134, %rd6, %rd82;
	mov.u64 	%rd83, 2040;
	sub.s64 	%rd133, %rd83, %rd124;
$L__BB13_15:
	add.s64 	%rd32, %rd136, %rd9;
	setp.lt.u64 	%p12, %rd32, 256;
	@%p12 bra 	$L__BB13_17;
	bra.uni 	$L__BB13_16;
$L__BB13_17:
	shr.u64 	%rd84, %rd133, 3;
	setp.gt.u64 	%p11, %rd84, %rd9;
	selp.b64 	%rd31, %rd134, 0, %p11;
	setp.lt.u64 	%p1, %rd32, %rd136;
	add.s64 	%rd89, %rd136, %rd4;
	selp.b64 	%rd136, 256, %rd89, %p1;
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r20, %r25, %r2;
	add.s32 	%r21, %r20, 4;
	ld.u32 	%r22, [%rd31];
	ld.u32 	%r23, [%rd31+4];
	// begin inline asm
	st.shared.f32 [%r20], %r22;
st.shared.f32 [%r21], %r23;
	// end inline asm
	add.s64 	%rd135, %rd135, -1;
	add.s64 	%rd134, %rd134, %rd125;
	sub.s64 	%rd133, %rd133, %rd125;
	setp.ne.s64 	%p13, %rd135, 0;
	@%p13 bra 	$L__BB13_15;
$L__BB13_18:
	add.u64 	%rd63, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd91, [%rd1+16];
	ld.global.nc.u64 	%rd92, [%rd1+24];
	st.local.u64 	[%rd2], %rd91;
	st.local.u64 	[%rd2+8], %rd92;
	bar.sync 	0;
	{ // callseq 52, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd63;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd64;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 52
	bar.sync 	0;
	mov.u64 	%rd140, 0;
	mov.u64 	%rd138, %rd140;
	@%p5 bra 	$L__BB13_23;
	setp.ne.s64 	%p15, %rd128, 0;
	@%p15 bra 	$L__BB13_21;
	bra.uni 	$L__BB13_20;
$L__BB13_21:
	div.u64 	%rd137, %rd10, %rd4;
	bra.uni 	$L__BB13_22;
$L__BB13_20:
	cvt.u32.u64 	%r27, %rd4;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	cvt.u64.u32 	%rd137, %r29;
$L__BB13_22:
	add.s64 	%rd138, %rd137, 1;
$L__BB13_23:
	@%p5 bra 	$L__BB13_28;
	setp.ne.s64 	%p17, %rd128, 0;
	@%p17 bra 	$L__BB13_26;
	bra.uni 	$L__BB13_25;
$L__BB13_26:
	div.u64 	%rd139, %rd10, %rd4;
	bra.uni 	$L__BB13_27;
$L__BB13_25:
	cvt.u32.u64 	%r31, %rd4;
	cvt.u32.u64 	%r32, %rd10;
	div.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd139, %r33;
$L__BB13_27:
	add.s64 	%rd140, %rd139, 1;
$L__BB13_28:
	min.u64 	%rd47, %rd138, %rd140;
	setp.eq.s64 	%p18, %rd47, 0;
	@%p18 bra 	$L__BB13_35;
	add.s64 	%rd48, %rd7, %rd124;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 384;
	shl.b16 	%rs3, %rs1, 8;
	shr.u16 	%rs4, %rs2, 8;
	or.b16  	%rs5, %rs3, %rs4;
	shl.b16 	%rs6, %rs5, 4;
	shl.b16 	%rs7, %rs2, 4;
	and.b16  	%rs8, %rs7, 3840;
	or.b16  	%rs9, %rs8, %rs6;
	and.b16  	%rs10, %rs9, 13107;
	shl.b16 	%rs11, %rs10, 2;
	shr.u16 	%rs12, %rs9, 2;
	and.b16  	%rs13, %rs12, 13107;
	or.b16  	%rs14, %rs13, %rs11;
	and.b16  	%rs15, %rs14, 21845;
	shl.b16 	%rs16, %rs15, 1;
	shr.u16 	%rs17, %rs14, 1;
	and.b16  	%rs18, %rs17, 21845;
	or.b16  	%rs19, %rs18, %rs16;
	shr.u16 	%rs20, %rs19, 5;
	cvt.u32.u16 	%r38, %rs20;
	add.s32 	%r37, %r2, %r38;
	add.s32 	%r36, %r37, -4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd48], %r34;
	st.u32 	[%rd48+4], %r35;
	setp.eq.s64 	%p19, %rd47, 1;
	@%p19 bra 	$L__BB13_35;
	add.s64 	%rd8, %rd7, 2048;
	add.s64 	%rd145, %rd3, 1;
	add.s64 	%rd144, %rd48, 8;
	add.s64 	%rd51, %rd47, -1;
	and.b64  	%rd52, %rd51, 1;
	setp.eq.s64 	%p20, %rd47, 2;
	@%p20 bra 	$L__BB13_33;
	and.b64  	%rd141, %rd51, -2;
$L__BB13_32:
	sub.s64 	%rd99, %rd8, %rd144;
	shr.u64 	%rd100, %rd99, 3;
	setp.gt.u64 	%p21, %rd100, %rd9;
	add.s64 	%rd103, %rd144, %rd125;
	add.s64 	%rd104, %rd103, -8;
	selp.b64 	%rd105, %rd103, %rd8, %p21;
	selp.b64 	%rd106, %rd104, 0, %p21;
	add.s64 	%rd107, %rd145, %rd9;
	setp.lt.u64 	%p22, %rd107, %rd145;
	setp.gt.u64 	%p23, %rd107, 255;
	add.s64 	%rd108, %rd107, 1;
	selp.b64 	%rd109, 256, %rd108, %p23;
	selp.b64 	%rd110, 256, %rd109, %p22;
	cvt.u16.u64 	%rs21, %rd107;
	xor.b16  	%rs22, %rs21, 128;
	shl.b16 	%rs23, %rs21, 12;
	shl.b16 	%rs24, %rs22, 4;
	and.b16  	%rs25, %rs24, 3840;
	or.b16  	%rs26, %rs23, %rs25;
	shr.u16 	%rs27, %rs26, 2;
	and.b16  	%rs28, %rs27, 13056;
	or.b16  	%rs29, %rs26, 16;
	and.b16  	%rs30, %rs29, 13072;
	shl.b16 	%rs31, %rs30, 2;
	or.b16  	%rs32, %rs28, %rs31;
	and.b16  	%rs33, %rs32, 21824;
	shl.b16 	%rs34, %rs33, 1;
	shr.u16 	%rs35, %rs32, 1;
	and.b16  	%rs36, %rs35, 21760;
	or.b16  	%rs37, %rs36, %rs34;
	shr.u16 	%rs38, %rs37, 5;
	cvt.u32.u16 	%r47, %rs38;
	add.s32 	%r42, %r2, %r47;
	add.s32 	%r41, %r42, -4;
	// begin inline asm
	ld.shared.f32 %r39, [%r41];
ld.shared.f32 %r40, [%r42];
	// end inline asm
	st.u32 	[%rd106], %r39;
	st.u32 	[%rd106+4], %r40;
	add.s64 	%rd144, %rd105, %rd125;
	add.s64 	%rd111, %rd110, %rd9;
	setp.lt.u64 	%p24, %rd111, %rd110;
	setp.gt.u64 	%p25, %rd111, 255;
	add.s64 	%rd113, %rd111, 1;
	selp.b64 	%rd114, 256, %rd113, %p25;
	selp.b64 	%rd145, 256, %rd114, %p24;
	cvt.u16.u64 	%rs39, %rd111;
	xor.b16  	%rs40, %rs39, 128;
	shl.b16 	%rs41, %rs39, 12;
	shl.b16 	%rs42, %rs40, 4;
	and.b16  	%rs43, %rs42, 3840;
	or.b16  	%rs44, %rs41, %rs43;
	shr.u16 	%rs45, %rs44, 2;
	and.b16  	%rs46, %rs45, 13056;
	or.b16  	%rs47, %rs44, 16;
	and.b16  	%rs48, %rs47, 13072;
	shl.b16 	%rs49, %rs48, 2;
	or.b16  	%rs50, %rs46, %rs49;
	and.b16  	%rs51, %rs50, 21824;
	shl.b16 	%rs52, %rs51, 1;
	shr.u16 	%rs53, %rs50, 1;
	and.b16  	%rs54, %rs53, 21760;
	or.b16  	%rs55, %rs54, %rs52;
	shr.u16 	%rs56, %rs55, 5;
	cvt.u32.u16 	%r48, %rs56;
	add.s32 	%r46, %r2, %r48;
	add.s32 	%r45, %r46, -4;
	// begin inline asm
	ld.shared.f32 %r43, [%r45];
ld.shared.f32 %r44, [%r46];
	// end inline asm
	st.u32 	[%rd144+-8], %r43;
	st.u32 	[%rd144+-4], %r44;
	add.s64 	%rd141, %rd141, -2;
	setp.ne.s64 	%p26, %rd141, 0;
	@%p26 bra 	$L__BB13_32;
$L__BB13_33:
	setp.eq.s64 	%p27, %rd52, 0;
	@%p27 bra 	$L__BB13_35;
	sub.s64 	%rd115, %rd8, %rd144;
	shr.u64 	%rd116, %rd115, 3;
	setp.gt.u64 	%p28, %rd116, %rd9;
	add.s64 	%rd118, %rd144, %rd125;
	add.s64 	%rd119, %rd118, -8;
	selp.b64 	%rd120, %rd119, 0, %p28;
	add.s64 	%rd121, %rd145, %rd9;
	setp.lt.u64 	%p29, %rd121, %rd145;
	cvt.u16.u64 	%rs57, %rd121;
	xor.b16  	%rs58, %rs57, 128;
	selp.b16 	%rs59, 384, %rs58, %p29;
	shl.b16 	%rs60, %rs59, 12;
	shl.b16 	%rs61, %rs59, 4;
	and.b16  	%rs62, %rs61, 3840;
	or.b16  	%rs63, %rs60, %rs62;
	shr.u16 	%rs64, %rs63, 2;
	and.b16  	%rs65, %rs64, 13056;
	or.b16  	%rs66, %rs63, 16;
	and.b16  	%rs67, %rs66, 13072;
	shl.b16 	%rs68, %rs67, 2;
	or.b16  	%rs69, %rs65, %rs68;
	and.b16  	%rs70, %rs69, 21824;
	shl.b16 	%rs71, %rs70, 1;
	shr.u16 	%rs72, %rs69, 1;
	and.b16  	%rs73, %rs72, 21760;
	or.b16  	%rs74, %rs73, %rs71;
	shr.u16 	%rs75, %rs74, 5;
	cvt.u32.u16 	%r53, %rs75;
	add.s32 	%r52, %r2, %r53;
	add.s32 	%r51, %r52, -4;
	// begin inline asm
	ld.shared.f32 %r49, [%r51];
ld.shared.f32 %r50, [%r52];
	// end inline asm
	st.u32 	[%rd120], %r49;
	st.u32 	[%rd120+4], %r50;
$L__BB13_35:
	ret;
$L__BB13_16:
	mov.u64 	%rd85, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd86, %rd85;
	mov.u64 	%rd87, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd88, %rd87;
	{ // callseq 51, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd86;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd88;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 51
$L__BB13_1:
	mov.u64 	%rd122, anon_$_9b271fbca47443a0c783d86781d13fba_$_7;
	cvta.global.u64 	%rd123, %rd122;
	{ // callseq 53, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd123;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 53

}
	// .globl	fft_forward_shifted_512_kernel
.visible .entry fft_forward_shifted_512_kernel(
	.param .u64 fft_forward_shifted_512_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot14[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<30>;
	.reg .b16 	%rs<83>;
	.reg .b32 	%r<54>;
	.reg .b64 	%rd<146>;

	mov.u64 	%SPL, __local_depot14;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd62, [fft_forward_shifted_512_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd62;
	add.u64 	%rd64, %SP, 16;
	add.u64 	%rd65, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[4096];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd65], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd66, [%rd1+8];
	and.b64  	%rd67, %rd66, -512;
	mul.wide.u32 	%rd68, %r5, 512;
	setp.lt.u64 	%p2, %rd68, %rd67;
	sub.s64 	%rd70, %rd66, %rd68;
	setp.gt.u64 	%p3, %rd70, 511;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB14_2;
	bra.uni 	$L__BB14_1;
$L__BB14_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd72, %rd3, 512;
	setp.gt.u32 	%p5, %r6, 511;
	not.b64 	%rd73, %rd3;
	add.s64 	%rd10, %rd73, %rd72;
	mov.u64 	%rd132, 0;
	and.b64  	%rd128, %rd10, -4294967296;
	mov.u64 	%rd130, %rd132;
	@%p5 bra 	$L__BB14_7;
	setp.ne.s64 	%p6, %rd128, 0;
	@%p6 bra 	$L__BB14_5;
	bra.uni 	$L__BB14_4;
$L__BB14_5:
	div.u64 	%rd129, %rd10, %rd4;
	bra.uni 	$L__BB14_6;
$L__BB14_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd10;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd129, %r9;
$L__BB14_6:
	add.s64 	%rd130, %rd129, 1;
$L__BB14_7:
	shl.b64 	%rd69, %rd68, 3;
	@%p5 bra 	$L__BB14_12;
	setp.ne.s64 	%p8, %rd128, 0;
	@%p8 bra 	$L__BB14_10;
	bra.uni 	$L__BB14_9;
$L__BB14_10:
	div.u64 	%rd131, %rd10, %rd4;
	bra.uni 	$L__BB14_11;
$L__BB14_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd10;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd131, %r13;
$L__BB14_11:
	add.s64 	%rd132, %rd131, 1;
$L__BB14_12:
	add.s64 	%rd7, %rd6, %rd69;
	add.s64 	%rd9, %rd4, -1;
	min.u64 	%rd21, %rd130, %rd132;
	setp.eq.s64 	%p9, %rd21, 0;
	shl.b64 	%rd124, %rd3, 3;
	shl.b64 	%rd125, %rd4, 3;
	@%p9 bra 	$L__BB14_18;
	add.s64 	%rd78, %rd7, %rd124;
	shl.b32 	%r19, %r6, 3;
	add.s32 	%r14, %r2, %r19;
	add.s32 	%r15, %r14, 4;
	ld.u32 	%r16, [%rd78];
	ld.u32 	%r17, [%rd78+4];
	// begin inline asm
	st.shared.f32 [%r14], %r16;
st.shared.f32 [%r15], %r17;
	// end inline asm
	setp.eq.s64 	%p10, %rd21, 1;
	@%p10 bra 	$L__BB14_18;
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd136, %rd3, 1;
	add.s64 	%rd135, %rd21, -1;
	shl.b64 	%rd79, %rd5, 12;
	add.s64 	%rd80, %rd79, %rd125;
	add.s64 	%rd82, %rd80, %rd124;
	add.s64 	%rd134, %rd6, %rd82;
	mov.u64 	%rd83, 4088;
	sub.s64 	%rd133, %rd83, %rd124;
$L__BB14_15:
	add.s64 	%rd32, %rd136, %rd9;
	setp.lt.u64 	%p12, %rd32, 512;
	@%p12 bra 	$L__BB14_17;
	bra.uni 	$L__BB14_16;
$L__BB14_17:
	shr.u64 	%rd84, %rd133, 3;
	setp.gt.u64 	%p11, %rd84, %rd9;
	selp.b64 	%rd31, %rd134, 0, %p11;
	setp.lt.u64 	%p1, %rd32, %rd136;
	add.s64 	%rd89, %rd136, %rd4;
	selp.b64 	%rd136, 512, %rd89, %p1;
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r20, %r25, %r2;
	add.s32 	%r21, %r20, 4;
	ld.u32 	%r22, [%rd31];
	ld.u32 	%r23, [%rd31+4];
	// begin inline asm
	st.shared.f32 [%r20], %r22;
st.shared.f32 [%r21], %r23;
	// end inline asm
	add.s64 	%rd135, %rd135, -1;
	add.s64 	%rd134, %rd134, %rd125;
	sub.s64 	%rd133, %rd133, %rd125;
	setp.ne.s64 	%p13, %rd135, 0;
	@%p13 bra 	$L__BB14_15;
$L__BB14_18:
	add.u64 	%rd63, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd91, [%rd1+16];
	ld.global.nc.u64 	%rd92, [%rd1+24];
	st.local.u64 	[%rd2], %rd91;
	st.local.u64 	[%rd2+8], %rd92;
	bar.sync 	0;
	{ // callseq 55, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd63;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd64;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 55
	bar.sync 	0;
	mov.u64 	%rd140, 0;
	mov.u64 	%rd138, %rd140;
	@%p5 bra 	$L__BB14_23;
	setp.ne.s64 	%p15, %rd128, 0;
	@%p15 bra 	$L__BB14_21;
	bra.uni 	$L__BB14_20;
$L__BB14_21:
	div.u64 	%rd137, %rd10, %rd4;
	bra.uni 	$L__BB14_22;
$L__BB14_20:
	cvt.u32.u64 	%r27, %rd4;
	cvt.u32.u64 	%r28, %rd10;
	div.u32 	%r29, %r28, %r27;
	cvt.u64.u32 	%rd137, %r29;
$L__BB14_22:
	add.s64 	%rd138, %rd137, 1;
$L__BB14_23:
	@%p5 bra 	$L__BB14_28;
	setp.ne.s64 	%p17, %rd128, 0;
	@%p17 bra 	$L__BB14_26;
	bra.uni 	$L__BB14_25;
$L__BB14_26:
	div.u64 	%rd139, %rd10, %rd4;
	bra.uni 	$L__BB14_27;
$L__BB14_25:
	cvt.u32.u64 	%r31, %rd4;
	cvt.u32.u64 	%r32, %rd10;
	div.u32 	%r33, %r32, %r31;
	cvt.u64.u32 	%rd139, %r33;
$L__BB14_27:
	add.s64 	%rd140, %rd139, 1;
$L__BB14_28:
	min.u64 	%rd47, %rd138, %rd140;
	setp.eq.s64 	%p18, %rd47, 0;
	@%p18 bra 	$L__BB14_35;
	add.s64 	%rd48, %rd7, %rd124;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 768;
	shl.b16 	%rs3, %rs1, 8;
	shr.u16 	%rs4, %rs2, 8;
	or.b16  	%rs5, %rs3, %rs4;
	shl.b16 	%rs6, %rs5, 4;
	shl.b16 	%rs7, %rs1, 4;
	and.b16  	%rs8, %rs7, 3840;
	or.b16  	%rs9, %rs8, %rs6;
	and.b16  	%rs10, %rs9, 13107;
	shl.b16 	%rs11, %rs10, 2;
	shr.u16 	%rs12, %rs9, 2;
	and.b16  	%rs13, %rs12, 13107;
	or.b16  	%rs14, %rs13, %rs11;
	and.b16  	%rs15, %rs14, 21845;
	shl.b16 	%rs16, %rs15, 1;
	shr.u16 	%rs17, %rs14, 1;
	and.b16  	%rs18, %rs17, 21845;
	or.b16  	%rs19, %rs18, %rs16;
	shr.u16 	%rs20, %rs19, 4;
	cvt.u32.u16 	%r38, %rs20;
	add.s32 	%r37, %r2, %r38;
	add.s32 	%r36, %r37, -4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd48], %r34;
	st.u32 	[%rd48+4], %r35;
	setp.eq.s64 	%p19, %rd47, 1;
	@%p19 bra 	$L__BB14_35;
	add.s64 	%rd8, %rd7, 4096;
	add.s64 	%rd145, %rd3, 1;
	add.s64 	%rd144, %rd48, 8;
	add.s64 	%rd51, %rd47, -1;
	and.b64  	%rd52, %rd51, 1;
	setp.eq.s64 	%p20, %rd47, 2;
	@%p20 bra 	$L__BB14_33;
	and.b64  	%rd141, %rd51, -2;
$L__BB14_32:
	sub.s64 	%rd99, %rd8, %rd144;
	shr.u64 	%rd100, %rd99, 3;
	setp.gt.u64 	%p21, %rd100, %rd9;
	add.s64 	%rd103, %rd144, %rd125;
	add.s64 	%rd104, %rd103, -8;
	selp.b64 	%rd105, %rd103, %rd8, %p21;
	selp.b64 	%rd106, %rd104, 0, %p21;
	add.s64 	%rd107, %rd145, %rd9;
	setp.lt.u64 	%p22, %rd107, %rd145;
	setp.gt.u64 	%p23, %rd107, 511;
	add.s64 	%rd108, %rd107, 1;
	selp.b64 	%rd109, 512, %rd108, %p23;
	selp.b64 	%rd110, 512, %rd109, %p22;
	cvt.u16.u64 	%rs21, %rd107;
	and.b16  	%rs22, %rs21, 256;
	xor.b16  	%rs23, %rs22, 768;
	shl.b16 	%rs24, %rs21, 12;
	shr.u16 	%rs25, %rs23, 4;
	or.b16  	%rs26, %rs24, %rs25;
	shl.b16 	%rs27, %rs21, 4;
	and.b16  	%rs28, %rs27, 3840;
	or.b16  	%rs29, %rs28, %rs26;
	and.b16  	%rs30, %rs29, 13107;
	shl.b16 	%rs31, %rs30, 2;
	shr.u16 	%rs32, %rs29, 2;
	and.b16  	%rs33, %rs32, 13107;
	or.b16  	%rs34, %rs33, %rs31;
	and.b16  	%rs35, %rs34, 21845;
	shl.b16 	%rs36, %rs35, 1;
	shr.u16 	%rs37, %rs34, 1;
	and.b16  	%rs38, %rs37, 21845;
	or.b16  	%rs39, %rs38, %rs36;
	shr.u16 	%rs40, %rs39, 4;
	cvt.u32.u16 	%r47, %rs40;
	add.s32 	%r42, %r2, %r47;
	add.s32 	%r41, %r42, -4;
	// begin inline asm
	ld.shared.f32 %r39, [%r41];
ld.shared.f32 %r40, [%r42];
	// end inline asm
	st.u32 	[%rd106], %r39;
	st.u32 	[%rd106+4], %r40;
	add.s64 	%rd144, %rd105, %rd125;
	add.s64 	%rd111, %rd110, %rd9;
	setp.lt.u64 	%p24, %rd111, %rd110;
	setp.gt.u64 	%p25, %rd111, 511;
	add.s64 	%rd113, %rd111, 1;
	selp.b64 	%rd114, 512, %rd113, %p25;
	selp.b64 	%rd145, 512, %rd114, %p24;
	cvt.u16.u64 	%rs41, %rd111;
	and.b16  	%rs42, %rs41, 256;
	xor.b16  	%rs43, %rs42, 768;
	shl.b16 	%rs44, %rs41, 12;
	shr.u16 	%rs45, %rs43, 4;
	or.b16  	%rs46, %rs44, %rs45;
	shl.b16 	%rs47, %rs41, 4;
	and.b16  	%rs48, %rs47, 3840;
	or.b16  	%rs49, %rs48, %rs46;
	and.b16  	%rs50, %rs49, 13107;
	shl.b16 	%rs51, %rs50, 2;
	shr.u16 	%rs52, %rs49, 2;
	and.b16  	%rs53, %rs52, 13107;
	or.b16  	%rs54, %rs53, %rs51;
	and.b16  	%rs55, %rs54, 21845;
	shl.b16 	%rs56, %rs55, 1;
	shr.u16 	%rs57, %rs54, 1;
	and.b16  	%rs58, %rs57, 21845;
	or.b16  	%rs59, %rs58, %rs56;
	shr.u16 	%rs60, %rs59, 4;
	cvt.u32.u16 	%r48, %rs60;
	add.s32 	%r46, %r2, %r48;
	add.s32 	%r45, %r46, -4;
	// begin inline asm
	ld.shared.f32 %r43, [%r45];
ld.shared.f32 %r44, [%r46];
	// end inline asm
	st.u32 	[%rd144+-8], %r43;
	st.u32 	[%rd144+-4], %r44;
	add.s64 	%rd141, %rd141, -2;
	setp.ne.s64 	%p26, %rd141, 0;
	@%p26 bra 	$L__BB14_32;
$L__BB14_33:
	setp.eq.s64 	%p27, %rd52, 0;
	@%p27 bra 	$L__BB14_35;
	sub.s64 	%rd115, %rd8, %rd144;
	shr.u64 	%rd116, %rd115, 3;
	setp.gt.u64 	%p28, %rd116, %rd9;
	add.s64 	%rd118, %rd144, %rd125;
	add.s64 	%rd119, %rd118, -8;
	selp.b64 	%rd120, %rd119, 0, %p28;
	add.s64 	%rd121, %rd145, %rd9;
	setp.lt.u64 	%p29, %rd121, %rd145;
	cvt.u16.u64 	%rs61, %rd121;
	and.b16  	%rs62, %rs61, 511;
	xor.b16  	%rs63, %rs62, 768;
	selp.b16 	%rs64, 768, %rs63, %p29;
	shr.u16 	%rs65, %rs64, 8;
	shl.b16 	%rs66, %rs64, 8;
	or.b16  	%rs67, %rs66, %rs65;
	shl.b16 	%rs68, %rs67, 4;
	shl.b16 	%rs69, %rs64, 4;
	and.b16  	%rs70, %rs69, 3840;
	or.b16  	%rs71, %rs70, %rs68;
	and.b16  	%rs72, %rs71, 13107;
	shl.b16 	%rs73, %rs72, 2;
	shr.u16 	%rs74, %rs71, 2;
	and.b16  	%rs75, %rs74, 13107;
	or.b16  	%rs76, %rs75, %rs73;
	and.b16  	%rs77, %rs76, 21845;
	shl.b16 	%rs78, %rs77, 1;
	shr.u16 	%rs79, %rs76, 1;
	and.b16  	%rs80, %rs79, 21845;
	or.b16  	%rs81, %rs80, %rs78;
	shr.u16 	%rs82, %rs81, 4;
	cvt.u32.u16 	%r53, %rs82;
	add.s32 	%r52, %r2, %r53;
	add.s32 	%r51, %r52, -4;
	// begin inline asm
	ld.shared.f32 %r49, [%r51];
ld.shared.f32 %r50, [%r52];
	// end inline asm
	st.u32 	[%rd120], %r49;
	st.u32 	[%rd120+4], %r50;
$L__BB14_35:
	ret;
$L__BB14_16:
	mov.u64 	%rd85, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd86, %rd85;
	mov.u64 	%rd87, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd88, %rd87;
	{ // callseq 54, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd86;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd88;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 54
$L__BB14_1:
	mov.u64 	%rd122, anon_$_9b271fbca47443a0c783d86781d13fba_$_7;
	cvta.global.u64 	%rd123, %rd122;
	{ // callseq 56, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd123;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 56

}
	// .globl	fft_forward_shifted_1024_kernel
.visible .entry fft_forward_shifted_1024_kernel(
	.param .u64 fft_forward_shifted_1024_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot15[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<20>;
	.reg .b16 	%rs<85>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<98>;

	mov.u64 	%SPL, __local_depot15;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd39, [fft_forward_shifted_1024_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd39;
	add.u64 	%rd41, %SP, 16;
	add.u64 	%rd42, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[8192];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd42], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd43, [%rd1+8];
	and.b64  	%rd44, %rd43, -1024;
	mul.wide.u32 	%rd6, %r5, 1024;
	setp.lt.u64 	%p2, %rd6, %rd44;
	sub.s64 	%rd45, %rd43, %rd6;
	setp.gt.u64 	%p3, %rd45, 1023;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB15_2;
	bra.uni 	$L__BB15_1;
$L__BB15_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r10, %rd3;
	ld.global.nc.u64 	%rd7, [%rd1];
	shl.b64 	%rd46, %rd6, 3;
	add.s64 	%rd47, %rd7, %rd46;
	add.s64 	%rd9, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 1023;
	cvt.u16.u64 	%rs3, %rd4;
	shl.b64 	%rd48, %rd3, 3;
	add.s64 	%rd10, %rd47, %rd48;
	shl.b32 	%r11, %r10, 3;
	add.s32 	%r6, %r2, %r11;
	add.s32 	%r7, %r6, 4;
	ld.u32 	%r8, [%rd10];
	ld.u32 	%r9, [%rd10+4];
	// begin inline asm
	st.shared.f32 [%r6], %r8;
st.shared.f32 [%r7], %r9;
	// end inline asm
	setp.lt.u16 	%p5, %rs2, %rs3;
	add.s64 	%rd97, %rd3, 1;
	shl.b64 	%rd88, %rd4, 3;
	@%p5 bra 	$L__BB15_7;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u16 	%r12, %rs2;
	cvt.u32.u64 	%r13, %rd4;
	div.u32 	%r14, %r12, %r13;
	cvt.u64.u32 	%rd91, %r14;
	shl.b64 	%rd49, %rd5, 13;
	add.s64 	%rd50, %rd49, %rd88;
	add.s64 	%rd52, %rd50, %rd48;
	add.s64 	%rd90, %rd7, %rd52;
	xor.b64  	%rd89, %rd48, 8184;
	mov.u64 	%rd92, %rd97;
$L__BB15_4:
	add.s64 	%rd21, %rd92, %rd9;
	setp.lt.u64 	%p7, %rd21, 1024;
	@%p7 bra 	$L__BB15_6;
	bra.uni 	$L__BB15_5;
$L__BB15_6:
	shr.u64 	%rd53, %rd89, 3;
	setp.gt.u64 	%p6, %rd53, %rd9;
	selp.b64 	%rd20, %rd90, 0, %p6;
	setp.lt.u64 	%p1, %rd21, %rd92;
	add.s64 	%rd58, %rd92, %rd4;
	selp.b64 	%rd92, 1024, %rd58, %p1;
	cvt.u32.u64 	%r19, %rd21;
	shl.b32 	%r20, %r19, 3;
	add.s32 	%r15, %r20, %r2;
	add.s32 	%r16, %r15, 4;
	ld.u32 	%r17, [%rd20];
	ld.u32 	%r18, [%rd20+4];
	// begin inline asm
	st.shared.f32 [%r15], %r17;
st.shared.f32 [%r16], %r18;
	// end inline asm
	add.s64 	%rd91, %rd91, -1;
	add.s64 	%rd90, %rd90, %rd88;
	sub.s64 	%rd89, %rd89, %rd88;
	setp.ne.s64 	%p8, %rd91, 0;
	@%p8 bra 	$L__BB15_4;
$L__BB15_7:
	add.u64 	%rd40, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd59, [%rd1+16];
	ld.global.nc.u64 	%rd60, [%rd1+24];
	st.local.u64 	[%rd2], %rd59;
	st.local.u64 	[%rd2+8], %rd60;
	bar.sync 	0;
	{ // callseq 58, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd40;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd41;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 58
	bar.sync 	0;
	xor.b64  	%rd26, %rd3, 1023;
	xor.b16  	%rs4, %rs1, 1536;
	shl.b16 	%rs5, %rs1, 12;
	shr.u16 	%rs6, %rs4, 4;
	and.b16  	%rs7, %rs6, 112;
	or.b16  	%rs8, %rs5, %rs7;
	shl.b16 	%rs9, %rs1, 4;
	and.b16  	%rs10, %rs9, 3840;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 13107;
	shl.b16 	%rs13, %rs12, 2;
	shr.u16 	%rs14, %rs11, 2;
	and.b16  	%rs15, %rs14, 13107;
	or.b16  	%rs16, %rs15, %rs13;
	and.b16  	%rs17, %rs16, 21845;
	shl.b16 	%rs18, %rs17, 1;
	shr.u16 	%rs19, %rs16, 1;
	and.b16  	%rs20, %rs19, 21845;
	or.b16  	%rs21, %rs20, %rs18;
	shr.u16 	%rs22, %rs21, 3;
	cvt.u32.u16 	%r25, %rs22;
	add.s32 	%r24, %r2, %r25;
	add.s32 	%r23, %r24, -4;
	// begin inline asm
	ld.shared.f32 %r21, [%r23];
ld.shared.f32 %r22, [%r24];
	// end inline asm
	st.u32 	[%rd10], %r21;
	st.u32 	[%rd10+4], %r22;
	setp.lt.u64 	%p9, %rd26, %rd4;
	@%p9 bra 	$L__BB15_13;
	add.s64 	%rd8, %rd47, 8192;
	cvt.u32.u64 	%r26, %rd26;
	cvt.u32.u64 	%r27, %rd4;
	div.u32 	%r28, %r26, %r27;
	cvt.u64.u32 	%rd27, %r28;
	add.s64 	%rd96, %rd10, 8;
	and.b64  	%rd29, %rd27, 1;
	setp.eq.s32 	%p10, %r28, 1;
	@%p10 bra 	$L__BB15_11;
	and.b64  	%rd93, %rd27, 1022;
$L__BB15_10:
	sub.s64 	%rd63, %rd8, %rd96;
	shr.u64 	%rd64, %rd63, 3;
	setp.gt.u64 	%p11, %rd64, %rd9;
	add.s64 	%rd67, %rd96, %rd88;
	add.s64 	%rd68, %rd67, -8;
	selp.b64 	%rd69, %rd67, %rd8, %p11;
	selp.b64 	%rd70, %rd68, 0, %p11;
	add.s64 	%rd71, %rd97, %rd9;
	setp.lt.u64 	%p12, %rd71, %rd97;
	setp.gt.u64 	%p13, %rd71, 1023;
	add.s64 	%rd72, %rd71, 1;
	selp.b64 	%rd73, 1024, %rd72, %p13;
	selp.b64 	%rd74, 1024, %rd73, %p12;
	cvt.u16.u64 	%rs23, %rd71;
	and.b16  	%rs24, %rs23, 768;
	xor.b16  	%rs25, %rs24, 1536;
	shl.b16 	%rs26, %rs23, 12;
	shr.u16 	%rs27, %rs25, 4;
	or.b16  	%rs28, %rs26, %rs27;
	shl.b16 	%rs29, %rs23, 4;
	and.b16  	%rs30, %rs29, 3840;
	or.b16  	%rs31, %rs30, %rs28;
	and.b16  	%rs32, %rs31, 13107;
	shl.b16 	%rs33, %rs32, 2;
	shr.u16 	%rs34, %rs31, 2;
	and.b16  	%rs35, %rs34, 13107;
	or.b16  	%rs36, %rs35, %rs33;
	and.b16  	%rs37, %rs36, 21845;
	shl.b16 	%rs38, %rs37, 1;
	shr.u16 	%rs39, %rs36, 1;
	and.b16  	%rs40, %rs39, 21845;
	or.b16  	%rs41, %rs40, %rs38;
	shr.u16 	%rs42, %rs41, 3;
	cvt.u32.u16 	%r37, %rs42;
	add.s32 	%r32, %r2, %r37;
	add.s32 	%r31, %r32, -4;
	// begin inline asm
	ld.shared.f32 %r29, [%r31];
ld.shared.f32 %r30, [%r32];
	// end inline asm
	st.u32 	[%rd70], %r29;
	st.u32 	[%rd70+4], %r30;
	add.s64 	%rd96, %rd69, %rd88;
	add.s64 	%rd75, %rd74, %rd9;
	setp.lt.u64 	%p14, %rd75, %rd74;
	setp.gt.u64 	%p15, %rd75, 1023;
	add.s64 	%rd77, %rd75, 1;
	selp.b64 	%rd78, 1024, %rd77, %p15;
	selp.b64 	%rd97, 1024, %rd78, %p14;
	cvt.u16.u64 	%rs43, %rd75;
	and.b16  	%rs44, %rs43, 768;
	xor.b16  	%rs45, %rs44, 1536;
	shl.b16 	%rs46, %rs43, 12;
	shr.u16 	%rs47, %rs45, 4;
	or.b16  	%rs48, %rs46, %rs47;
	shl.b16 	%rs49, %rs43, 4;
	and.b16  	%rs50, %rs49, 3840;
	or.b16  	%rs51, %rs50, %rs48;
	and.b16  	%rs52, %rs51, 13107;
	shl.b16 	%rs53, %rs52, 2;
	shr.u16 	%rs54, %rs51, 2;
	and.b16  	%rs55, %rs54, 13107;
	or.b16  	%rs56, %rs55, %rs53;
	and.b16  	%rs57, %rs56, 21845;
	shl.b16 	%rs58, %rs57, 1;
	shr.u16 	%rs59, %rs56, 1;
	and.b16  	%rs60, %rs59, 21845;
	or.b16  	%rs61, %rs60, %rs58;
	shr.u16 	%rs62, %rs61, 3;
	cvt.u32.u16 	%r38, %rs62;
	add.s32 	%r36, %r2, %r38;
	add.s32 	%r35, %r36, -4;
	// begin inline asm
	ld.shared.f32 %r33, [%r35];
ld.shared.f32 %r34, [%r36];
	// end inline asm
	st.u32 	[%rd96+-8], %r33;
	st.u32 	[%rd96+-4], %r34;
	add.s64 	%rd93, %rd93, -2;
	setp.ne.s64 	%p16, %rd93, 0;
	@%p16 bra 	$L__BB15_10;
$L__BB15_11:
	setp.eq.s64 	%p17, %rd29, 0;
	@%p17 bra 	$L__BB15_13;
	sub.s64 	%rd79, %rd8, %rd96;
	shr.u64 	%rd80, %rd79, 3;
	setp.gt.u64 	%p18, %rd80, %rd9;
	add.s64 	%rd82, %rd96, %rd88;
	add.s64 	%rd83, %rd82, -8;
	selp.b64 	%rd84, %rd83, 0, %p18;
	add.s64 	%rd85, %rd97, %rd9;
	setp.lt.u64 	%p19, %rd85, %rd97;
	cvt.u16.u64 	%rs63, %rd85;
	and.b16  	%rs64, %rs63, 1023;
	xor.b16  	%rs65, %rs64, 1536;
	selp.b16 	%rs66, 1536, %rs65, %p19;
	shr.u16 	%rs67, %rs66, 8;
	shl.b16 	%rs68, %rs66, 8;
	or.b16  	%rs69, %rs68, %rs67;
	shl.b16 	%rs70, %rs69, 4;
	shl.b16 	%rs71, %rs66, 4;
	and.b16  	%rs72, %rs71, 3840;
	or.b16  	%rs73, %rs72, %rs70;
	and.b16  	%rs74, %rs73, 13107;
	shl.b16 	%rs75, %rs74, 2;
	shr.u16 	%rs76, %rs73, 2;
	and.b16  	%rs77, %rs76, 13107;
	or.b16  	%rs78, %rs77, %rs75;
	and.b16  	%rs79, %rs78, 21845;
	shl.b16 	%rs80, %rs79, 1;
	shr.u16 	%rs81, %rs78, 1;
	and.b16  	%rs82, %rs81, 21845;
	or.b16  	%rs83, %rs82, %rs80;
	shr.u16 	%rs84, %rs83, 3;
	cvt.u32.u16 	%r43, %rs84;
	add.s32 	%r42, %r2, %r43;
	add.s32 	%r41, %r42, -4;
	// begin inline asm
	ld.shared.f32 %r39, [%r41];
ld.shared.f32 %r40, [%r42];
	// end inline asm
	st.u32 	[%rd84], %r39;
	st.u32 	[%rd84+4], %r40;
$L__BB15_13:
	ret;
$L__BB15_5:
	mov.u64 	%rd54, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd55, %rd54;
	mov.u64 	%rd56, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd57, %rd56;
	{ // callseq 57, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd55;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd57;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 57
$L__BB15_1:
	mov.u64 	%rd86, anon_$_9b271fbca47443a0c783d86781d13fba_$_7;
	cvta.global.u64 	%rd87, %rd86;
	{ // callseq 59, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd87;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 59

}
	// .globl	fft_forward_shifted_2048_kernel
.visible .entry fft_forward_shifted_2048_kernel(
	.param .u64 fft_forward_shifted_2048_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot16[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<84>;
	.reg .b32 	%r<45>;
	.reg .b64 	%rd<95>;

	mov.u64 	%SPL, __local_depot16;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd37, [fft_forward_shifted_2048_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd37;
	add.u64 	%rd39, %SP, 16;
	add.u64 	%rd40, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[16384];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd40], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd41, [%rd1+8];
	and.b64  	%rd42, %rd41, -2048;
	mul.wide.u32 	%rd6, %r5, 2048;
	setp.lt.u64 	%p2, %rd6, %rd42;
	sub.s64 	%rd43, %rd41, %rd6;
	setp.gt.u64 	%p3, %rd43, 2047;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB16_2;
	bra.uni 	$L__BB16_1;
$L__BB16_2:
	add.u64 	%rd38, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r10, %rd3;
	ld.global.nc.u64 	%rd44, [%rd1];
	shl.b64 	%rd45, %rd6, 3;
	add.s64 	%rd46, %rd44, %rd45;
	add.s64 	%rd7, %rd46, 16384;
	add.s64 	%rd8, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 2047;
	shl.b64 	%rd47, %rd3, 3;
	add.s64 	%rd9, %rd46, %rd47;
	shl.b32 	%r11, %r10, 3;
	add.s32 	%r6, %r2, %r11;
	add.s32 	%r7, %r6, 4;
	ld.u32 	%r8, [%rd9];
	ld.u32 	%r9, [%rd9+4];
	// begin inline asm
	st.shared.f32 [%r6], %r8;
st.shared.f32 [%r7], %r9;
	// end inline asm
	add.s64 	%rd94, %rd3, 1;
	add.s64 	%rd93, %rd9, 8;
	cvt.u32.u16 	%r12, %rs2;
	cvt.u32.u64 	%r13, %rd4;
	div.u32 	%r14, %r12, %r13;
	cvt.u64.u32 	%rd88, %r14;
	shl.b64 	%rd48, %rd5, 14;
	shl.b64 	%rd13, %rd4, 3;
	add.s64 	%rd49, %rd48, %rd13;
	add.s64 	%rd50, %rd49, %rd47;
	add.s64 	%rd87, %rd44, %rd50;
	xor.b64  	%rd86, %rd47, 16376;
	mov.u64 	%rd89, %rd94;
$L__BB16_3:
	add.s64 	%rd21, %rd89, %rd8;
	setp.lt.u64 	%p6, %rd21, 2048;
	@%p6 bra 	$L__BB16_5;
	bra.uni 	$L__BB16_4;
$L__BB16_5:
	shr.u64 	%rd51, %rd86, 3;
	setp.gt.u64 	%p5, %rd51, %rd8;
	selp.b64 	%rd20, %rd87, 0, %p5;
	setp.lt.u64 	%p1, %rd21, %rd89;
	add.s64 	%rd56, %rd89, %rd4;
	selp.b64 	%rd89, 2048, %rd56, %p1;
	cvt.u32.u64 	%r19, %rd21;
	shl.b32 	%r20, %r19, 3;
	add.s32 	%r15, %r20, %r2;
	add.s32 	%r16, %r15, 4;
	ld.u32 	%r17, [%rd20];
	ld.u32 	%r18, [%rd20+4];
	// begin inline asm
	st.shared.f32 [%r15], %r17;
st.shared.f32 [%r16], %r18;
	// end inline asm
	add.s64 	%rd88, %rd88, -1;
	add.s64 	%rd87, %rd87, %rd13;
	sub.s64 	%rd86, %rd86, %rd13;
	setp.ne.s64 	%p7, %rd88, 0;
	@%p7 bra 	$L__BB16_3;
	ld.global.nc.u64 	%rd57, [%rd1+16];
	ld.global.nc.u64 	%rd58, [%rd1+24];
	st.local.u64 	[%rd2], %rd57;
	st.local.u64 	[%rd2+8], %rd58;
	bar.sync 	0;
	{ // callseq 61, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd38;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd39;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 61
	bar.sync 	0;
	xor.b32  	%r26, %r10, 2047;
	div.u32 	%r28, %r26, %r13;
	cvt.u64.u32 	%rd26, %r28;
	or.b16  	%rs3, %rs1, 3072;
	shl.b16 	%rs4, %rs1, 12;
	shr.u16 	%rs5, %rs3, 4;
	and.b16  	%rs6, %rs5, 240;
	or.b16  	%rs7, %rs4, %rs6;
	shl.b16 	%rs8, %rs1, 4;
	and.b16  	%rs9, %rs8, 3840;
	or.b16  	%rs10, %rs9, %rs7;
	and.b16  	%rs11, %rs10, 13107;
	shl.b16 	%rs12, %rs11, 2;
	shr.u16 	%rs13, %rs10, 2;
	and.b16  	%rs14, %rs13, 13107;
	or.b16  	%rs15, %rs14, %rs12;
	and.b16  	%rs16, %rs15, 21845;
	shl.b16 	%rs17, %rs16, 1;
	shr.u16 	%rs18, %rs15, 1;
	and.b16  	%rs19, %rs18, 21845;
	or.b16  	%rs20, %rs19, %rs17;
	shr.u16 	%rs21, %rs20, 2;
	cvt.u32.u16 	%r29, %rs21;
	add.s32 	%r24, %r2, %r29;
	add.s32 	%r23, %r24, -4;
	// begin inline asm
	ld.shared.f32 %r21, [%r23];
ld.shared.f32 %r22, [%r24];
	// end inline asm
	st.u32 	[%rd9], %r21;
	st.u32 	[%rd9+4], %r22;
	and.b64  	%rd27, %rd26, 1;
	setp.eq.s32 	%p8, %r28, 1;
	@%p8 bra 	$L__BB16_9;
	and.b64  	%rd90, %rd26, 2046;
$L__BB16_8:
	sub.s64 	%rd61, %rd7, %rd93;
	shr.u64 	%rd62, %rd61, 3;
	setp.gt.u64 	%p9, %rd62, %rd8;
	add.s64 	%rd65, %rd93, %rd13;
	add.s64 	%rd66, %rd65, -8;
	selp.b64 	%rd67, %rd65, %rd7, %p9;
	selp.b64 	%rd68, %rd66, 0, %p9;
	add.s64 	%rd69, %rd94, %rd8;
	setp.lt.u64 	%p10, %rd69, %rd94;
	setp.gt.u64 	%p11, %rd69, 2047;
	add.s64 	%rd70, %rd69, 1;
	selp.b64 	%rd71, 2048, %rd70, %p11;
	selp.b64 	%rd72, 2048, %rd71, %p10;
	cvt.u16.u64 	%rs22, %rd69;
	and.b16  	%rs23, %rs22, 1792;
	xor.b16  	%rs24, %rs23, 3072;
	shl.b16 	%rs25, %rs22, 12;
	shr.u16 	%rs26, %rs24, 4;
	or.b16  	%rs27, %rs25, %rs26;
	shl.b16 	%rs28, %rs22, 4;
	and.b16  	%rs29, %rs28, 3840;
	or.b16  	%rs30, %rs29, %rs27;
	and.b16  	%rs31, %rs30, 13107;
	shl.b16 	%rs32, %rs31, 2;
	shr.u16 	%rs33, %rs30, 2;
	and.b16  	%rs34, %rs33, 13107;
	or.b16  	%rs35, %rs34, %rs32;
	and.b16  	%rs36, %rs35, 21845;
	shl.b16 	%rs37, %rs36, 1;
	shr.u16 	%rs38, %rs35, 1;
	and.b16  	%rs39, %rs38, 21845;
	or.b16  	%rs40, %rs39, %rs37;
	shr.u16 	%rs41, %rs40, 2;
	cvt.u32.u16 	%r38, %rs41;
	add.s32 	%r33, %r2, %r38;
	add.s32 	%r32, %r33, -4;
	// begin inline asm
	ld.shared.f32 %r30, [%r32];
ld.shared.f32 %r31, [%r33];
	// end inline asm
	st.u32 	[%rd68], %r30;
	st.u32 	[%rd68+4], %r31;
	add.s64 	%rd93, %rd67, %rd13;
	add.s64 	%rd73, %rd72, %rd8;
	setp.lt.u64 	%p12, %rd73, %rd72;
	setp.gt.u64 	%p13, %rd73, 2047;
	add.s64 	%rd75, %rd73, 1;
	selp.b64 	%rd76, 2048, %rd75, %p13;
	selp.b64 	%rd94, 2048, %rd76, %p12;
	cvt.u16.u64 	%rs42, %rd73;
	and.b16  	%rs43, %rs42, 1792;
	xor.b16  	%rs44, %rs43, 3072;
	shl.b16 	%rs45, %rs42, 12;
	shr.u16 	%rs46, %rs44, 4;
	or.b16  	%rs47, %rs45, %rs46;
	shl.b16 	%rs48, %rs42, 4;
	and.b16  	%rs49, %rs48, 3840;
	or.b16  	%rs50, %rs49, %rs47;
	and.b16  	%rs51, %rs50, 13107;
	shl.b16 	%rs52, %rs51, 2;
	shr.u16 	%rs53, %rs50, 2;
	and.b16  	%rs54, %rs53, 13107;
	or.b16  	%rs55, %rs54, %rs52;
	and.b16  	%rs56, %rs55, 21845;
	shl.b16 	%rs57, %rs56, 1;
	shr.u16 	%rs58, %rs55, 1;
	and.b16  	%rs59, %rs58, 21845;
	or.b16  	%rs60, %rs59, %rs57;
	shr.u16 	%rs61, %rs60, 2;
	cvt.u32.u16 	%r39, %rs61;
	add.s32 	%r37, %r2, %r39;
	add.s32 	%r36, %r37, -4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd93+-8], %r34;
	st.u32 	[%rd93+-4], %r35;
	add.s64 	%rd90, %rd90, -2;
	setp.ne.s64 	%p14, %rd90, 0;
	@%p14 bra 	$L__BB16_8;
$L__BB16_9:
	setp.eq.s64 	%p15, %rd27, 0;
	@%p15 bra 	$L__BB16_11;
	sub.s64 	%rd77, %rd7, %rd93;
	shr.u64 	%rd78, %rd77, 3;
	setp.gt.u64 	%p16, %rd78, %rd8;
	add.s64 	%rd80, %rd93, %rd13;
	add.s64 	%rd81, %rd80, -8;
	selp.b64 	%rd82, %rd81, 0, %p16;
	add.s64 	%rd83, %rd94, %rd8;
	setp.lt.u64 	%p17, %rd83, %rd94;
	cvt.u16.u64 	%rs62, %rd83;
	and.b16  	%rs63, %rs62, 2047;
	xor.b16  	%rs64, %rs63, 3072;
	selp.b16 	%rs65, 3072, %rs64, %p17;
	shr.u16 	%rs66, %rs65, 8;
	shl.b16 	%rs67, %rs65, 8;
	or.b16  	%rs68, %rs67, %rs66;
	shl.b16 	%rs69, %rs68, 4;
	shl.b16 	%rs70, %rs65, 4;
	and.b16  	%rs71, %rs70, 3840;
	or.b16  	%rs72, %rs71, %rs69;
	and.b16  	%rs73, %rs72, 13107;
	shl.b16 	%rs74, %rs73, 2;
	shr.u16 	%rs75, %rs72, 2;
	and.b16  	%rs76, %rs75, 13107;
	or.b16  	%rs77, %rs76, %rs74;
	and.b16  	%rs78, %rs77, 21845;
	shl.b16 	%rs79, %rs78, 1;
	shr.u16 	%rs80, %rs77, 1;
	and.b16  	%rs81, %rs80, 21845;
	or.b16  	%rs82, %rs81, %rs79;
	shr.u16 	%rs83, %rs82, 2;
	cvt.u32.u16 	%r44, %rs83;
	add.s32 	%r43, %r2, %r44;
	add.s32 	%r42, %r43, -4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	st.u32 	[%rd82], %r40;
	st.u32 	[%rd82+4], %r41;
$L__BB16_11:
	ret;
$L__BB16_4:
	mov.u64 	%rd52, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd53, %rd52;
	mov.u64 	%rd54, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd55, %rd54;
	{ // callseq 60, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd53;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd55;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 60
$L__BB16_1:
	mov.u64 	%rd84, anon_$_9b271fbca47443a0c783d86781d13fba_$_7;
	cvta.global.u64 	%rd85, %rd84;
	{ // callseq 62, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd85;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 62

}
	// .globl	fft_forward_shifted_4096_kernel
.visible .entry fft_forward_shifted_4096_kernel(
	.param .u64 fft_forward_shifted_4096_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot17[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<18>;
	.reg .b16 	%rs<90>;
	.reg .b32 	%r<45>;
	.reg .b64 	%rd<95>;

	mov.u64 	%SPL, __local_depot17;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd37, [fft_forward_shifted_4096_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd37;
	add.u64 	%rd39, %SP, 16;
	add.u64 	%rd40, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[32768];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd40], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd41, [%rd1+8];
	and.b64  	%rd42, %rd41, -4096;
	mul.wide.u32 	%rd6, %r5, 4096;
	setp.lt.u64 	%p2, %rd6, %rd42;
	sub.s64 	%rd43, %rd41, %rd6;
	setp.gt.u64 	%p3, %rd43, 4095;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB17_2;
	bra.uni 	$L__BB17_1;
$L__BB17_2:
	add.u64 	%rd38, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r10, %rd3;
	ld.global.nc.u64 	%rd44, [%rd1];
	shl.b64 	%rd45, %rd6, 3;
	add.s64 	%rd46, %rd44, %rd45;
	add.s64 	%rd7, %rd46, 32768;
	add.s64 	%rd8, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 4095;
	shl.b64 	%rd47, %rd3, 3;
	add.s64 	%rd9, %rd46, %rd47;
	add.s64 	%rd93, %rd9, 8;
	add.s64 	%rd94, %rd3, 1;
	shl.b32 	%r11, %r10, 3;
	add.s32 	%r6, %r2, %r11;
	add.s32 	%r7, %r6, 4;
	ld.u32 	%r8, [%rd9];
	ld.u32 	%r9, [%rd9+4];
	// begin inline asm
	st.shared.f32 [%r6], %r8;
st.shared.f32 [%r7], %r9;
	// end inline asm
	cvt.u32.u16 	%r12, %rs2;
	cvt.u32.u64 	%r13, %rd4;
	div.u32 	%r14, %r12, %r13;
	cvt.u64.u32 	%rd88, %r14;
	shl.b64 	%rd48, %rd5, 15;
	shl.b64 	%rd13, %rd4, 3;
	add.s64 	%rd49, %rd48, %rd13;
	add.s64 	%rd50, %rd49, %rd47;
	add.s64 	%rd87, %rd44, %rd50;
	xor.b64  	%rd86, %rd47, 32760;
	mov.u64 	%rd89, %rd94;
$L__BB17_3:
	add.s64 	%rd21, %rd89, %rd8;
	setp.lt.u64 	%p6, %rd21, 4096;
	@%p6 bra 	$L__BB17_5;
	bra.uni 	$L__BB17_4;
$L__BB17_5:
	shr.u64 	%rd51, %rd86, 3;
	setp.gt.u64 	%p5, %rd51, %rd8;
	selp.b64 	%rd20, %rd87, 0, %p5;
	setp.lt.u64 	%p1, %rd21, %rd89;
	add.s64 	%rd56, %rd89, %rd4;
	selp.b64 	%rd89, 4096, %rd56, %p1;
	cvt.u32.u64 	%r19, %rd21;
	shl.b32 	%r20, %r19, 3;
	add.s32 	%r15, %r20, %r2;
	add.s32 	%r16, %r15, 4;
	ld.u32 	%r17, [%rd20];
	ld.u32 	%r18, [%rd20+4];
	// begin inline asm
	st.shared.f32 [%r15], %r17;
st.shared.f32 [%r16], %r18;
	// end inline asm
	add.s64 	%rd88, %rd88, -1;
	add.s64 	%rd87, %rd87, %rd13;
	sub.s64 	%rd86, %rd86, %rd13;
	setp.ne.s64 	%p7, %rd88, 0;
	@%p7 bra 	$L__BB17_3;
	ld.global.nc.u64 	%rd57, [%rd1+16];
	ld.global.nc.u64 	%rd58, [%rd1+24];
	st.local.u64 	[%rd2], %rd57;
	st.local.u64 	[%rd2+8], %rd58;
	bar.sync 	0;
	{ // callseq 64, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd38;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd39;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 64
	bar.sync 	0;
	xor.b32  	%r26, %r10, 4095;
	div.u32 	%r28, %r26, %r13;
	cvt.u64.u32 	%rd26, %r28;
	or.b16  	%rs3, %rs1, 6144;
	shl.b16 	%rs4, %rs1, 8;
	shr.u16 	%rs5, %rs3, 8;
	or.b16  	%rs6, %rs4, %rs5;
	and.b16  	%rs7, %rs6, 3851;
	shl.b16 	%rs8, %rs7, 4;
	shr.u16 	%rs9, %rs6, 4;
	and.b16  	%rs10, %rs9, 3841;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 13107;
	shl.b16 	%rs13, %rs12, 2;
	shr.u16 	%rs14, %rs11, 2;
	and.b16  	%rs15, %rs14, 13107;
	or.b16  	%rs16, %rs15, %rs13;
	and.b16  	%rs17, %rs16, 21845;
	shl.b16 	%rs18, %rs17, 1;
	shr.u16 	%rs19, %rs16, 1;
	and.b16  	%rs20, %rs19, 21845;
	or.b16  	%rs21, %rs20, %rs18;
	shr.u16 	%rs22, %rs21, 1;
	cvt.u32.u16 	%r29, %rs22;
	add.s32 	%r24, %r2, %r29;
	add.s32 	%r23, %r24, -4;
	// begin inline asm
	ld.shared.f32 %r21, [%r23];
ld.shared.f32 %r22, [%r24];
	// end inline asm
	st.u32 	[%rd9], %r21;
	st.u32 	[%rd9+4], %r22;
	and.b64  	%rd27, %rd26, 1;
	setp.eq.s32 	%p8, %r28, 1;
	@%p8 bra 	$L__BB17_9;
	and.b64  	%rd90, %rd26, 4094;
$L__BB17_8:
	sub.s64 	%rd61, %rd7, %rd93;
	shr.u64 	%rd62, %rd61, 3;
	setp.gt.u64 	%p9, %rd62, %rd8;
	add.s64 	%rd65, %rd93, %rd13;
	add.s64 	%rd66, %rd65, -8;
	selp.b64 	%rd67, %rd65, %rd7, %p9;
	selp.b64 	%rd68, %rd66, 0, %p9;
	add.s64 	%rd69, %rd94, %rd8;
	setp.lt.u64 	%p10, %rd69, %rd94;
	setp.gt.u64 	%p11, %rd69, 4095;
	add.s64 	%rd70, %rd69, 1;
	selp.b64 	%rd71, 4096, %rd70, %p11;
	selp.b64 	%rd72, 4096, %rd71, %p10;
	cvt.u16.u64 	%rs23, %rd69;
	and.b16  	%rs24, %rs23, 3840;
	xor.b16  	%rs25, %rs24, 6144;
	shl.b16 	%rs26, %rs23, 8;
	shr.u16 	%rs27, %rs25, 8;
	or.b16  	%rs28, %rs26, %rs27;
	and.b16  	%rs29, %rs28, 3855;
	shl.b16 	%rs30, %rs29, 4;
	shr.u16 	%rs31, %rs28, 4;
	and.b16  	%rs32, %rs31, 3841;
	or.b16  	%rs33, %rs32, %rs30;
	and.b16  	%rs34, %rs33, 13107;
	shl.b16 	%rs35, %rs34, 2;
	shr.u16 	%rs36, %rs33, 2;
	and.b16  	%rs37, %rs36, 13107;
	or.b16  	%rs38, %rs37, %rs35;
	and.b16  	%rs39, %rs38, 21845;
	shl.b16 	%rs40, %rs39, 1;
	shr.u16 	%rs41, %rs38, 1;
	and.b16  	%rs42, %rs41, 21845;
	or.b16  	%rs43, %rs42, %rs40;
	shr.u16 	%rs44, %rs43, 1;
	cvt.u32.u16 	%r38, %rs44;
	add.s32 	%r33, %r2, %r38;
	add.s32 	%r32, %r33, -4;
	// begin inline asm
	ld.shared.f32 %r30, [%r32];
ld.shared.f32 %r31, [%r33];
	// end inline asm
	st.u32 	[%rd68], %r30;
	st.u32 	[%rd68+4], %r31;
	add.s64 	%rd93, %rd67, %rd13;
	add.s64 	%rd73, %rd72, %rd8;
	setp.lt.u64 	%p12, %rd73, %rd72;
	setp.gt.u64 	%p13, %rd73, 4095;
	add.s64 	%rd75, %rd73, 1;
	selp.b64 	%rd76, 4096, %rd75, %p13;
	selp.b64 	%rd94, 4096, %rd76, %p12;
	cvt.u16.u64 	%rs45, %rd73;
	and.b16  	%rs46, %rs45, 3840;
	xor.b16  	%rs47, %rs46, 6144;
	shl.b16 	%rs48, %rs45, 8;
	shr.u16 	%rs49, %rs47, 8;
	or.b16  	%rs50, %rs48, %rs49;
	and.b16  	%rs51, %rs50, 3855;
	shl.b16 	%rs52, %rs51, 4;
	shr.u16 	%rs53, %rs50, 4;
	and.b16  	%rs54, %rs53, 3841;
	or.b16  	%rs55, %rs54, %rs52;
	and.b16  	%rs56, %rs55, 13107;
	shl.b16 	%rs57, %rs56, 2;
	shr.u16 	%rs58, %rs55, 2;
	and.b16  	%rs59, %rs58, 13107;
	or.b16  	%rs60, %rs59, %rs57;
	and.b16  	%rs61, %rs60, 21845;
	shl.b16 	%rs62, %rs61, 1;
	shr.u16 	%rs63, %rs60, 1;
	and.b16  	%rs64, %rs63, 21845;
	or.b16  	%rs65, %rs64, %rs62;
	shr.u16 	%rs66, %rs65, 1;
	cvt.u32.u16 	%r39, %rs66;
	add.s32 	%r37, %r2, %r39;
	add.s32 	%r36, %r37, -4;
	// begin inline asm
	ld.shared.f32 %r34, [%r36];
ld.shared.f32 %r35, [%r37];
	// end inline asm
	st.u32 	[%rd93+-8], %r34;
	st.u32 	[%rd93+-4], %r35;
	add.s64 	%rd90, %rd90, -2;
	setp.ne.s64 	%p14, %rd90, 0;
	@%p14 bra 	$L__BB17_8;
$L__BB17_9:
	setp.eq.s64 	%p15, %rd27, 0;
	@%p15 bra 	$L__BB17_11;
	sub.s64 	%rd77, %rd7, %rd93;
	shr.u64 	%rd78, %rd77, 3;
	setp.gt.u64 	%p16, %rd78, %rd8;
	add.s64 	%rd80, %rd93, %rd13;
	add.s64 	%rd81, %rd80, -8;
	selp.b64 	%rd82, %rd81, 0, %p16;
	add.s64 	%rd83, %rd94, %rd8;
	setp.lt.u64 	%p17, %rd83, %rd94;
	cvt.u16.u64 	%rs67, %rd83;
	and.b16  	%rs68, %rs67, 4095;
	xor.b16  	%rs69, %rs68, 6144;
	selp.b16 	%rs70, 6144, %rs69, %p17;
	shr.u16 	%rs71, %rs70, 8;
	shl.b16 	%rs72, %rs70, 8;
	or.b16  	%rs73, %rs72, %rs71;
	and.b16  	%rs74, %rs73, 3855;
	shl.b16 	%rs75, %rs74, 4;
	shr.u16 	%rs76, %rs73, 4;
	and.b16  	%rs77, %rs76, 3841;
	or.b16  	%rs78, %rs77, %rs75;
	and.b16  	%rs79, %rs78, 13107;
	shl.b16 	%rs80, %rs79, 2;
	shr.u16 	%rs81, %rs78, 2;
	and.b16  	%rs82, %rs81, 13107;
	or.b16  	%rs83, %rs82, %rs80;
	and.b16  	%rs84, %rs83, 21845;
	shl.b16 	%rs85, %rs84, 1;
	shr.u16 	%rs86, %rs83, 1;
	and.b16  	%rs87, %rs86, 21845;
	or.b16  	%rs88, %rs87, %rs85;
	shr.u16 	%rs89, %rs88, 1;
	cvt.u32.u16 	%r44, %rs89;
	add.s32 	%r43, %r2, %r44;
	add.s32 	%r42, %r43, -4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	st.u32 	[%rd82], %r40;
	st.u32 	[%rd82+4], %r41;
$L__BB17_11:
	ret;
$L__BB17_4:
	mov.u64 	%rd52, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd53, %rd52;
	mov.u64 	%rd54, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd55, %rd54;
	{ // callseq 63, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd53;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd55;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 63
$L__BB17_1:
	mov.u64 	%rd84, anon_$_9b271fbca47443a0c783d86781d13fba_$_7;
	cvta.global.u64 	%rd85, %rd84;
	{ // callseq 65, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd85;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 65

}
	// .globl	fft_backward_shifted_128_kernel
.visible .entry fft_backward_shifted_128_kernel(
	.param .u64 fft_backward_shifted_128_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot18[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<36>;
	.reg .b32 	%r<68>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<127>;

	mov.u64 	%SPL, __local_depot18;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd57, [fft_backward_shifted_128_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd57;
	add.u64 	%rd59, %SP, 16;
	add.u64 	%rd60, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[1024];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd60], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd61, [%rd1+8];
	and.b64  	%rd62, %rd61, -128;
	mul.wide.u32 	%rd63, %r5, 128;
	setp.lt.u64 	%p2, %rd63, %rd62;
	sub.s64 	%rd65, %rd61, %rd63;
	setp.gt.u64 	%p3, %rd65, 127;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB18_2;
	bra.uni 	$L__BB18_1;
$L__BB18_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd67, %rd3, 128;
	setp.gt.u32 	%p5, %r6, 127;
	not.b64 	%rd68, %rd3;
	add.s64 	%rd10, %rd68, %rd67;
	mov.u64 	%rd115, 0;
	and.b64  	%rd111, %rd10, -4294967296;
	mov.u64 	%rd113, %rd115;
	@%p5 bra 	$L__BB18_7;
	setp.ne.s64 	%p6, %rd111, 0;
	@%p6 bra 	$L__BB18_5;
	bra.uni 	$L__BB18_4;
$L__BB18_5:
	div.u64 	%rd112, %rd10, %rd4;
	bra.uni 	$L__BB18_6;
$L__BB18_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd10;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd112, %r9;
$L__BB18_6:
	add.s64 	%rd113, %rd112, 1;
$L__BB18_7:
	shl.b64 	%rd64, %rd63, 3;
	@%p5 bra 	$L__BB18_12;
	setp.ne.s64 	%p8, %rd111, 0;
	@%p8 bra 	$L__BB18_10;
	bra.uni 	$L__BB18_9;
$L__BB18_10:
	div.u64 	%rd114, %rd10, %rd4;
	bra.uni 	$L__BB18_11;
$L__BB18_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd10;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd114, %r13;
$L__BB18_11:
	add.s64 	%rd115, %rd114, 1;
$L__BB18_12:
	add.s64 	%rd7, %rd6, %rd64;
	add.s64 	%rd9, %rd4, -1;
	min.u64 	%rd21, %rd113, %rd115;
	setp.eq.s64 	%p9, %rd21, 0;
	shl.b64 	%rd107, %rd3, 3;
	shl.b64 	%rd108, %rd4, 3;
	@%p9 bra 	$L__BB18_18;
	add.s64 	%rd73, %rd7, %rd107;
	ld.u32 	%r15, [%rd73+4];
	ld.u32 	%r18, [%rd73];
	// begin inline asm
	neg.ftz.f32 %r14, %r15;
	// end inline asm
	shl.b32 	%r21, %r6, 3;
	add.s32 	%r16, %r2, %r21;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	st.shared.f32 [%r16], %r18;
st.shared.f32 [%r17], %r14;
	// end inline asm
	setp.eq.s64 	%p10, %rd21, 1;
	@%p10 bra 	$L__BB18_18;
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd119, %rd3, 1;
	add.s64 	%rd118, %rd21, -1;
	shl.b64 	%rd74, %rd5, 10;
	add.s64 	%rd75, %rd74, %rd108;
	add.s64 	%rd77, %rd75, %rd107;
	add.s64 	%rd117, %rd6, %rd77;
	mov.u64 	%rd78, 1016;
	sub.s64 	%rd116, %rd78, %rd107;
$L__BB18_15:
	shr.u64 	%rd79, %rd116, 3;
	setp.gt.u64 	%p11, %rd79, %rd9;
	selp.b64 	%rd80, %rd117, 0, %p11;
	add.s64 	%rd31, %rd119, %rd9;
	ld.f32 	%f1, [%rd80];
	ld.u32 	%r23, [%rd80+4];
	// begin inline asm
	neg.ftz.f32 %r22, %r23;
	// end inline asm
	setp.lt.u64 	%p12, %rd31, 128;
	@%p12 bra 	$L__BB18_17;
	bra.uni 	$L__BB18_16;
$L__BB18_17:
	setp.lt.u64 	%p1, %rd31, %rd119;
	mov.b32 	%f2, %r22;
	add.s64 	%rd85, %rd119, %rd4;
	selp.b64 	%rd119, 128, %rd85, %p1;
	cvt.u32.u64 	%r28, %rd31;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r24, %r29, %r2;
	add.s32 	%r25, %r24, 4;
	mov.b32 	%r26, %f1;
	// begin inline asm
	st.shared.f32 [%r24], %r26;
st.shared.f32 [%r25], %r22;
	// end inline asm
	add.s64 	%rd118, %rd118, -1;
	add.s64 	%rd117, %rd117, %rd108;
	sub.s64 	%rd116, %rd116, %rd108;
	setp.ne.s64 	%p13, %rd118, 0;
	@%p13 bra 	$L__BB18_15;
$L__BB18_18:
	add.u64 	%rd58, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd87, [%rd1+16];
	ld.global.nc.u64 	%rd88, [%rd1+24];
	st.local.u64 	[%rd2], %rd87;
	st.local.u64 	[%rd2+8], %rd88;
	bar.sync 	0;
	{ // callseq 67, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd58;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd59;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 67
	mov.b32 	%r31, 1124073472;
	// begin inline asm
	rcp.approx.ftz.f32 %r48, %r31;
	// end inline asm
	bar.sync 	0;
	mov.u64 	%rd123, 0;
	mov.u64 	%rd121, %rd123;
	@%p5 bra 	$L__BB18_23;
	setp.ne.s64 	%p15, %rd111, 0;
	@%p15 bra 	$L__BB18_21;
	bra.uni 	$L__BB18_20;
$L__BB18_21:
	div.u64 	%rd120, %rd10, %rd4;
	bra.uni 	$L__BB18_22;
$L__BB18_20:
	cvt.u32.u64 	%r33, %rd4;
	cvt.u32.u64 	%r34, %rd10;
	div.u32 	%r35, %r34, %r33;
	cvt.u64.u32 	%rd120, %r35;
$L__BB18_22:
	add.s64 	%rd121, %rd120, 1;
$L__BB18_23:
	@%p5 bra 	$L__BB18_28;
	setp.ne.s64 	%p17, %rd111, 0;
	@%p17 bra 	$L__BB18_26;
	bra.uni 	$L__BB18_25;
$L__BB18_26:
	div.u64 	%rd122, %rd10, %rd4;
	bra.uni 	$L__BB18_27;
$L__BB18_25:
	cvt.u32.u64 	%r37, %rd4;
	cvt.u32.u64 	%r38, %rd10;
	div.u32 	%r39, %r38, %r37;
	cvt.u64.u32 	%rd122, %r39;
$L__BB18_27:
	add.s64 	%rd123, %rd122, 1;
$L__BB18_28:
	min.u64 	%rd46, %rd121, %rd123;
	setp.eq.s64 	%p18, %rd46, 0;
	@%p18 bra 	$L__BB18_32;
	mov.b32 	%f3, %r48;
	add.s64 	%rd47, %rd7, %rd107;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, -64;
	and.b16  	%rs3, %rs2, 240;
	and.b16  	%rs4, %rs2, 15;
	shl.b16 	%rs5, %rs4, 4;
	shr.u16 	%rs6, %rs3, 4;
	or.b16  	%rs7, %rs6, %rs5;
	and.b16  	%rs8, %rs7, 51;
	shl.b16 	%rs9, %rs8, 2;
	shr.u16 	%rs10, %rs7, 2;
	and.b16  	%rs11, %rs10, 51;
	or.b16  	%rs12, %rs11, %rs9;
	and.b16  	%rs13, %rs12, 85;
	shl.b16 	%rs14, %rs13, 1;
	shr.u16 	%rs15, %rs12, 1;
	and.b16  	%rs16, %rs15, 85;
	or.b16  	%rs17, %rs16, %rs14;
	mul.wide.u16 	%r52, %rs17, 4;
	add.s32 	%r43, %r2, %r52;
	add.s32 	%r42, %r43, -4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r44, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r46, %r40, %r48;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r44, %r48;
	// end inline asm
	st.u32 	[%rd47], %r46;
	st.u32 	[%rd47+4], %r49;
	setp.eq.s64 	%p19, %rd46, 1;
	@%p19 bra 	$L__BB18_32;
	add.s64 	%rd8, %rd7, 1024;
	add.s64 	%rd126, %rd3, 1;
	add.s64 	%rd125, %rd47, 8;
	add.s64 	%rd124, %rd46, -1;
$L__BB18_31:
	sub.s64 	%rd95, %rd8, %rd125;
	shr.u64 	%rd96, %rd95, 3;
	setp.gt.u64 	%p20, %rd96, %rd9;
	add.s64 	%rd99, %rd125, %rd108;
	add.s64 	%rd100, %rd99, -8;
	selp.b64 	%rd125, %rd99, %rd8, %p20;
	selp.b64 	%rd101, %rd100, 0, %p20;
	add.s64 	%rd102, %rd126, %rd9;
	setp.lt.u64 	%p21, %rd102, %rd126;
	setp.gt.u64 	%p22, %rd102, 127;
	add.s64 	%rd103, %rd102, 1;
	selp.b64 	%rd104, 128, %rd103, %p22;
	selp.b64 	%rd126, 128, %rd104, %p21;
	cvt.u16.u64 	%rs18, %rd102;
	and.b16  	%rs19, %rs18, 127;
	xor.b16  	%rs20, %rs19, -64;
	and.b16  	%rs21, %rs20, 240;
	and.b16  	%rs22, %rs20, 15;
	shl.b16 	%rs23, %rs22, 4;
	shr.u16 	%rs24, %rs21, 4;
	or.b16  	%rs25, %rs24, %rs23;
	and.b16  	%rs26, %rs25, 51;
	shl.b16 	%rs27, %rs26, 2;
	shr.u16 	%rs28, %rs25, 2;
	and.b16  	%rs29, %rs28, 51;
	or.b16  	%rs30, %rs29, %rs27;
	and.b16  	%rs31, %rs30, 85;
	shl.b16 	%rs32, %rs31, 1;
	shr.u16 	%rs33, %rs30, 1;
	and.b16  	%rs34, %rs33, 85;
	or.b16  	%rs35, %rs34, %rs32;
	mul.wide.u16 	%r65, %rs35, 4;
	add.s32 	%r56, %r65, %r2;
	add.s32 	%r55, %r56, -4;
	// begin inline asm
	ld.shared.f32 %r53, [%r55];
ld.shared.f32 %r54, [%r56];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r57, %r54;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r59, %r53, %r48;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r62, %r57, %r48;
	// end inline asm
	st.u32 	[%rd101], %r59;
	st.u32 	[%rd101+4], %r62;
	add.s64 	%rd124, %rd124, -1;
	setp.ne.s64 	%p23, %rd124, 0;
	@%p23 bra 	$L__BB18_31;
$L__BB18_32:
	ret;
$L__BB18_16:
	mov.u64 	%rd81, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd82, %rd81;
	mov.u64 	%rd83, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd84, %rd83;
	{ // callseq 66, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd82;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd84;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 66
$L__BB18_1:
	mov.u64 	%rd105, anon_$_9b271fbca47443a0c783d86781d13fba_$_9;
	cvta.global.u64 	%rd106, %rd105;
	{ // callseq 68, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd106;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 68

}
	// .globl	fft_backward_shifted_256_kernel
.visible .entry fft_backward_shifted_256_kernel(
	.param .u64 fft_backward_shifted_256_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot19[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<39>;
	.reg .b32 	%r<68>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<127>;

	mov.u64 	%SPL, __local_depot19;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd57, [fft_backward_shifted_256_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd57;
	add.u64 	%rd59, %SP, 16;
	add.u64 	%rd60, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[2048];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd60], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd61, [%rd1+8];
	and.b64  	%rd62, %rd61, -256;
	mul.wide.u32 	%rd63, %r5, 256;
	setp.lt.u64 	%p2, %rd63, %rd62;
	sub.s64 	%rd65, %rd61, %rd63;
	setp.gt.u64 	%p3, %rd65, 255;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB19_2;
	bra.uni 	$L__BB19_1;
$L__BB19_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd67, %rd3, 256;
	setp.gt.u32 	%p5, %r6, 255;
	not.b64 	%rd68, %rd3;
	add.s64 	%rd10, %rd68, %rd67;
	mov.u64 	%rd115, 0;
	and.b64  	%rd111, %rd10, -4294967296;
	mov.u64 	%rd113, %rd115;
	@%p5 bra 	$L__BB19_7;
	setp.ne.s64 	%p6, %rd111, 0;
	@%p6 bra 	$L__BB19_5;
	bra.uni 	$L__BB19_4;
$L__BB19_5:
	div.u64 	%rd112, %rd10, %rd4;
	bra.uni 	$L__BB19_6;
$L__BB19_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd10;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd112, %r9;
$L__BB19_6:
	add.s64 	%rd113, %rd112, 1;
$L__BB19_7:
	shl.b64 	%rd64, %rd63, 3;
	@%p5 bra 	$L__BB19_12;
	setp.ne.s64 	%p8, %rd111, 0;
	@%p8 bra 	$L__BB19_10;
	bra.uni 	$L__BB19_9;
$L__BB19_10:
	div.u64 	%rd114, %rd10, %rd4;
	bra.uni 	$L__BB19_11;
$L__BB19_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd10;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd114, %r13;
$L__BB19_11:
	add.s64 	%rd115, %rd114, 1;
$L__BB19_12:
	add.s64 	%rd7, %rd6, %rd64;
	add.s64 	%rd9, %rd4, -1;
	min.u64 	%rd21, %rd113, %rd115;
	setp.eq.s64 	%p9, %rd21, 0;
	shl.b64 	%rd107, %rd3, 3;
	shl.b64 	%rd108, %rd4, 3;
	@%p9 bra 	$L__BB19_18;
	add.s64 	%rd73, %rd7, %rd107;
	ld.u32 	%r15, [%rd73+4];
	ld.u32 	%r18, [%rd73];
	// begin inline asm
	neg.ftz.f32 %r14, %r15;
	// end inline asm
	shl.b32 	%r21, %r6, 3;
	add.s32 	%r16, %r2, %r21;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	st.shared.f32 [%r16], %r18;
st.shared.f32 [%r17], %r14;
	// end inline asm
	setp.eq.s64 	%p10, %rd21, 1;
	@%p10 bra 	$L__BB19_18;
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd119, %rd3, 1;
	add.s64 	%rd118, %rd21, -1;
	shl.b64 	%rd74, %rd5, 11;
	add.s64 	%rd75, %rd74, %rd108;
	add.s64 	%rd77, %rd75, %rd107;
	add.s64 	%rd117, %rd6, %rd77;
	mov.u64 	%rd78, 2040;
	sub.s64 	%rd116, %rd78, %rd107;
$L__BB19_15:
	shr.u64 	%rd79, %rd116, 3;
	setp.gt.u64 	%p11, %rd79, %rd9;
	selp.b64 	%rd80, %rd117, 0, %p11;
	add.s64 	%rd31, %rd119, %rd9;
	ld.f32 	%f1, [%rd80];
	ld.u32 	%r23, [%rd80+4];
	// begin inline asm
	neg.ftz.f32 %r22, %r23;
	// end inline asm
	setp.lt.u64 	%p12, %rd31, 256;
	@%p12 bra 	$L__BB19_17;
	bra.uni 	$L__BB19_16;
$L__BB19_17:
	setp.lt.u64 	%p1, %rd31, %rd119;
	mov.b32 	%f2, %r22;
	add.s64 	%rd85, %rd119, %rd4;
	selp.b64 	%rd119, 256, %rd85, %p1;
	cvt.u32.u64 	%r28, %rd31;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r24, %r29, %r2;
	add.s32 	%r25, %r24, 4;
	mov.b32 	%r26, %f1;
	// begin inline asm
	st.shared.f32 [%r24], %r26;
st.shared.f32 [%r25], %r22;
	// end inline asm
	add.s64 	%rd118, %rd118, -1;
	add.s64 	%rd117, %rd117, %rd108;
	sub.s64 	%rd116, %rd116, %rd108;
	setp.ne.s64 	%p13, %rd118, 0;
	@%p13 bra 	$L__BB19_15;
$L__BB19_18:
	add.u64 	%rd58, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd87, [%rd1+16];
	ld.global.nc.u64 	%rd88, [%rd1+24];
	st.local.u64 	[%rd2], %rd87;
	st.local.u64 	[%rd2+8], %rd88;
	bar.sync 	0;
	{ // callseq 70, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd58;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd59;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 70
	mov.b32 	%r31, 1132462080;
	// begin inline asm
	rcp.approx.ftz.f32 %r48, %r31;
	// end inline asm
	bar.sync 	0;
	mov.u64 	%rd123, 0;
	mov.u64 	%rd121, %rd123;
	@%p5 bra 	$L__BB19_23;
	setp.ne.s64 	%p15, %rd111, 0;
	@%p15 bra 	$L__BB19_21;
	bra.uni 	$L__BB19_20;
$L__BB19_21:
	div.u64 	%rd120, %rd10, %rd4;
	bra.uni 	$L__BB19_22;
$L__BB19_20:
	cvt.u32.u64 	%r33, %rd4;
	cvt.u32.u64 	%r34, %rd10;
	div.u32 	%r35, %r34, %r33;
	cvt.u64.u32 	%rd120, %r35;
$L__BB19_22:
	add.s64 	%rd121, %rd120, 1;
$L__BB19_23:
	@%p5 bra 	$L__BB19_28;
	setp.ne.s64 	%p17, %rd111, 0;
	@%p17 bra 	$L__BB19_26;
	bra.uni 	$L__BB19_25;
$L__BB19_26:
	div.u64 	%rd122, %rd10, %rd4;
	bra.uni 	$L__BB19_27;
$L__BB19_25:
	cvt.u32.u64 	%r37, %rd4;
	cvt.u32.u64 	%r38, %rd10;
	div.u32 	%r39, %r38, %r37;
	cvt.u64.u32 	%rd122, %r39;
$L__BB19_27:
	add.s64 	%rd123, %rd122, 1;
$L__BB19_28:
	min.u64 	%rd46, %rd121, %rd123;
	setp.eq.s64 	%p18, %rd46, 0;
	@%p18 bra 	$L__BB19_32;
	mov.b32 	%f3, %r48;
	add.s64 	%rd47, %rd7, %rd107;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 384;
	shl.b16 	%rs3, %rs1, 8;
	shr.u16 	%rs4, %rs2, 8;
	or.b16  	%rs5, %rs3, %rs4;
	shl.b16 	%rs6, %rs5, 4;
	shl.b16 	%rs7, %rs2, 4;
	and.b16  	%rs8, %rs7, 3840;
	or.b16  	%rs9, %rs8, %rs6;
	and.b16  	%rs10, %rs9, 13107;
	shl.b16 	%rs11, %rs10, 2;
	shr.u16 	%rs12, %rs9, 2;
	and.b16  	%rs13, %rs12, 13107;
	or.b16  	%rs14, %rs13, %rs11;
	and.b16  	%rs15, %rs14, 21845;
	shl.b16 	%rs16, %rs15, 1;
	shr.u16 	%rs17, %rs14, 1;
	and.b16  	%rs18, %rs17, 21845;
	or.b16  	%rs19, %rs18, %rs16;
	shr.u16 	%rs20, %rs19, 5;
	cvt.u32.u16 	%r52, %rs20;
	add.s32 	%r43, %r2, %r52;
	add.s32 	%r42, %r43, -4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r44, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r46, %r40, %r48;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r44, %r48;
	// end inline asm
	st.u32 	[%rd47], %r46;
	st.u32 	[%rd47+4], %r49;
	setp.eq.s64 	%p19, %rd46, 1;
	@%p19 bra 	$L__BB19_32;
	add.s64 	%rd8, %rd7, 2048;
	add.s64 	%rd126, %rd3, 1;
	add.s64 	%rd125, %rd47, 8;
	add.s64 	%rd124, %rd46, -1;
$L__BB19_31:
	sub.s64 	%rd95, %rd8, %rd125;
	shr.u64 	%rd96, %rd95, 3;
	setp.gt.u64 	%p20, %rd96, %rd9;
	add.s64 	%rd99, %rd125, %rd108;
	add.s64 	%rd100, %rd99, -8;
	selp.b64 	%rd125, %rd99, %rd8, %p20;
	selp.b64 	%rd101, %rd100, 0, %p20;
	add.s64 	%rd102, %rd126, %rd9;
	setp.lt.u64 	%p21, %rd102, %rd126;
	setp.gt.u64 	%p22, %rd102, 255;
	add.s64 	%rd103, %rd102, 1;
	selp.b64 	%rd104, 256, %rd103, %p22;
	selp.b64 	%rd126, 256, %rd104, %p21;
	cvt.u16.u64 	%rs21, %rd102;
	xor.b16  	%rs22, %rs21, 128;
	shl.b16 	%rs23, %rs21, 12;
	shl.b16 	%rs24, %rs22, 4;
	and.b16  	%rs25, %rs24, 3840;
	or.b16  	%rs26, %rs23, %rs25;
	shr.u16 	%rs27, %rs26, 2;
	and.b16  	%rs28, %rs27, 13056;
	or.b16  	%rs29, %rs26, 16;
	and.b16  	%rs30, %rs29, 13072;
	shl.b16 	%rs31, %rs30, 2;
	or.b16  	%rs32, %rs28, %rs31;
	and.b16  	%rs33, %rs32, 21824;
	shl.b16 	%rs34, %rs33, 1;
	shr.u16 	%rs35, %rs32, 1;
	and.b16  	%rs36, %rs35, 21760;
	or.b16  	%rs37, %rs36, %rs34;
	shr.u16 	%rs38, %rs37, 5;
	cvt.u32.u16 	%r65, %rs38;
	add.s32 	%r56, %r2, %r65;
	add.s32 	%r55, %r56, -4;
	// begin inline asm
	ld.shared.f32 %r53, [%r55];
ld.shared.f32 %r54, [%r56];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r57, %r54;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r59, %r53, %r48;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r62, %r57, %r48;
	// end inline asm
	st.u32 	[%rd101], %r59;
	st.u32 	[%rd101+4], %r62;
	add.s64 	%rd124, %rd124, -1;
	setp.ne.s64 	%p23, %rd124, 0;
	@%p23 bra 	$L__BB19_31;
$L__BB19_32:
	ret;
$L__BB19_16:
	mov.u64 	%rd81, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd82, %rd81;
	mov.u64 	%rd83, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd84, %rd83;
	{ // callseq 69, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd82;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd84;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 69
$L__BB19_1:
	mov.u64 	%rd105, anon_$_9b271fbca47443a0c783d86781d13fba_$_9;
	cvta.global.u64 	%rd106, %rd105;
	{ // callseq 71, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd106;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 71

}
	// .globl	fft_backward_shifted_512_kernel
.visible .entry fft_backward_shifted_512_kernel(
	.param .u64 fft_backward_shifted_512_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot20[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<41>;
	.reg .b32 	%r<68>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<127>;

	mov.u64 	%SPL, __local_depot20;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd57, [fft_backward_shifted_512_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd57;
	add.u64 	%rd59, %SP, 16;
	add.u64 	%rd60, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[4096];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd60], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd6, [%rd1];
	ld.global.nc.u64 	%rd61, [%rd1+8];
	and.b64  	%rd62, %rd61, -512;
	mul.wide.u32 	%rd63, %r5, 512;
	setp.lt.u64 	%p2, %rd63, %rd62;
	sub.s64 	%rd65, %rd61, %rd63;
	setp.gt.u64 	%p3, %rd65, 511;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB20_2;
	bra.uni 	$L__BB20_1;
$L__BB20_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r6, %rd3;
	max.u64 	%rd67, %rd3, 512;
	setp.gt.u32 	%p5, %r6, 511;
	not.b64 	%rd68, %rd3;
	add.s64 	%rd10, %rd68, %rd67;
	mov.u64 	%rd115, 0;
	and.b64  	%rd111, %rd10, -4294967296;
	mov.u64 	%rd113, %rd115;
	@%p5 bra 	$L__BB20_7;
	setp.ne.s64 	%p6, %rd111, 0;
	@%p6 bra 	$L__BB20_5;
	bra.uni 	$L__BB20_4;
$L__BB20_5:
	div.u64 	%rd112, %rd10, %rd4;
	bra.uni 	$L__BB20_6;
$L__BB20_4:
	cvt.u32.u64 	%r7, %rd4;
	cvt.u32.u64 	%r8, %rd10;
	div.u32 	%r9, %r8, %r7;
	cvt.u64.u32 	%rd112, %r9;
$L__BB20_6:
	add.s64 	%rd113, %rd112, 1;
$L__BB20_7:
	shl.b64 	%rd64, %rd63, 3;
	@%p5 bra 	$L__BB20_12;
	setp.ne.s64 	%p8, %rd111, 0;
	@%p8 bra 	$L__BB20_10;
	bra.uni 	$L__BB20_9;
$L__BB20_10:
	div.u64 	%rd114, %rd10, %rd4;
	bra.uni 	$L__BB20_11;
$L__BB20_9:
	cvt.u32.u64 	%r11, %rd4;
	cvt.u32.u64 	%r12, %rd10;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd114, %r13;
$L__BB20_11:
	add.s64 	%rd115, %rd114, 1;
$L__BB20_12:
	add.s64 	%rd7, %rd6, %rd64;
	add.s64 	%rd9, %rd4, -1;
	min.u64 	%rd21, %rd113, %rd115;
	setp.eq.s64 	%p9, %rd21, 0;
	shl.b64 	%rd107, %rd3, 3;
	shl.b64 	%rd108, %rd4, 3;
	@%p9 bra 	$L__BB20_18;
	add.s64 	%rd73, %rd7, %rd107;
	ld.u32 	%r15, [%rd73+4];
	ld.u32 	%r18, [%rd73];
	// begin inline asm
	neg.ftz.f32 %r14, %r15;
	// end inline asm
	shl.b32 	%r21, %r6, 3;
	add.s32 	%r16, %r2, %r21;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	st.shared.f32 [%r16], %r18;
st.shared.f32 [%r17], %r14;
	// end inline asm
	setp.eq.s64 	%p10, %rd21, 1;
	@%p10 bra 	$L__BB20_18;
	cvt.u64.u32 	%rd5, %r5;
	add.s64 	%rd119, %rd3, 1;
	add.s64 	%rd118, %rd21, -1;
	shl.b64 	%rd74, %rd5, 12;
	add.s64 	%rd75, %rd74, %rd108;
	add.s64 	%rd77, %rd75, %rd107;
	add.s64 	%rd117, %rd6, %rd77;
	mov.u64 	%rd78, 4088;
	sub.s64 	%rd116, %rd78, %rd107;
$L__BB20_15:
	shr.u64 	%rd79, %rd116, 3;
	setp.gt.u64 	%p11, %rd79, %rd9;
	selp.b64 	%rd80, %rd117, 0, %p11;
	add.s64 	%rd31, %rd119, %rd9;
	ld.f32 	%f1, [%rd80];
	ld.u32 	%r23, [%rd80+4];
	// begin inline asm
	neg.ftz.f32 %r22, %r23;
	// end inline asm
	setp.lt.u64 	%p12, %rd31, 512;
	@%p12 bra 	$L__BB20_17;
	bra.uni 	$L__BB20_16;
$L__BB20_17:
	setp.lt.u64 	%p1, %rd31, %rd119;
	mov.b32 	%f2, %r22;
	add.s64 	%rd85, %rd119, %rd4;
	selp.b64 	%rd119, 512, %rd85, %p1;
	cvt.u32.u64 	%r28, %rd31;
	shl.b32 	%r29, %r28, 3;
	add.s32 	%r24, %r29, %r2;
	add.s32 	%r25, %r24, 4;
	mov.b32 	%r26, %f1;
	// begin inline asm
	st.shared.f32 [%r24], %r26;
st.shared.f32 [%r25], %r22;
	// end inline asm
	add.s64 	%rd118, %rd118, -1;
	add.s64 	%rd117, %rd117, %rd108;
	sub.s64 	%rd116, %rd116, %rd108;
	setp.ne.s64 	%p13, %rd118, 0;
	@%p13 bra 	$L__BB20_15;
$L__BB20_18:
	add.u64 	%rd58, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd87, [%rd1+16];
	ld.global.nc.u64 	%rd88, [%rd1+24];
	st.local.u64 	[%rd2], %rd87;
	st.local.u64 	[%rd2+8], %rd88;
	bar.sync 	0;
	{ // callseq 73, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd58;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd59;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 73
	mov.b32 	%r31, 1140850688;
	// begin inline asm
	rcp.approx.ftz.f32 %r48, %r31;
	// end inline asm
	bar.sync 	0;
	mov.u64 	%rd123, 0;
	mov.u64 	%rd121, %rd123;
	@%p5 bra 	$L__BB20_23;
	setp.ne.s64 	%p15, %rd111, 0;
	@%p15 bra 	$L__BB20_21;
	bra.uni 	$L__BB20_20;
$L__BB20_21:
	div.u64 	%rd120, %rd10, %rd4;
	bra.uni 	$L__BB20_22;
$L__BB20_20:
	cvt.u32.u64 	%r33, %rd4;
	cvt.u32.u64 	%r34, %rd10;
	div.u32 	%r35, %r34, %r33;
	cvt.u64.u32 	%rd120, %r35;
$L__BB20_22:
	add.s64 	%rd121, %rd120, 1;
$L__BB20_23:
	@%p5 bra 	$L__BB20_28;
	setp.ne.s64 	%p17, %rd111, 0;
	@%p17 bra 	$L__BB20_26;
	bra.uni 	$L__BB20_25;
$L__BB20_26:
	div.u64 	%rd122, %rd10, %rd4;
	bra.uni 	$L__BB20_27;
$L__BB20_25:
	cvt.u32.u64 	%r37, %rd4;
	cvt.u32.u64 	%r38, %rd10;
	div.u32 	%r39, %r38, %r37;
	cvt.u64.u32 	%rd122, %r39;
$L__BB20_27:
	add.s64 	%rd123, %rd122, 1;
$L__BB20_28:
	min.u64 	%rd46, %rd121, %rd123;
	setp.eq.s64 	%p18, %rd46, 0;
	@%p18 bra 	$L__BB20_32;
	mov.b32 	%f3, %r48;
	add.s64 	%rd47, %rd7, %rd107;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 768;
	shl.b16 	%rs3, %rs1, 8;
	shr.u16 	%rs4, %rs2, 8;
	or.b16  	%rs5, %rs3, %rs4;
	shl.b16 	%rs6, %rs5, 4;
	shl.b16 	%rs7, %rs1, 4;
	and.b16  	%rs8, %rs7, 3840;
	or.b16  	%rs9, %rs8, %rs6;
	and.b16  	%rs10, %rs9, 13107;
	shl.b16 	%rs11, %rs10, 2;
	shr.u16 	%rs12, %rs9, 2;
	and.b16  	%rs13, %rs12, 13107;
	or.b16  	%rs14, %rs13, %rs11;
	and.b16  	%rs15, %rs14, 21845;
	shl.b16 	%rs16, %rs15, 1;
	shr.u16 	%rs17, %rs14, 1;
	and.b16  	%rs18, %rs17, 21845;
	or.b16  	%rs19, %rs18, %rs16;
	shr.u16 	%rs20, %rs19, 4;
	cvt.u32.u16 	%r52, %rs20;
	add.s32 	%r43, %r2, %r52;
	add.s32 	%r42, %r43, -4;
	// begin inline asm
	ld.shared.f32 %r40, [%r42];
ld.shared.f32 %r41, [%r43];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r44, %r41;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r46, %r40, %r48;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r44, %r48;
	// end inline asm
	st.u32 	[%rd47], %r46;
	st.u32 	[%rd47+4], %r49;
	setp.eq.s64 	%p19, %rd46, 1;
	@%p19 bra 	$L__BB20_32;
	add.s64 	%rd8, %rd7, 4096;
	add.s64 	%rd126, %rd3, 1;
	add.s64 	%rd125, %rd47, 8;
	add.s64 	%rd124, %rd46, -1;
$L__BB20_31:
	sub.s64 	%rd95, %rd8, %rd125;
	shr.u64 	%rd96, %rd95, 3;
	setp.gt.u64 	%p20, %rd96, %rd9;
	add.s64 	%rd99, %rd125, %rd108;
	add.s64 	%rd100, %rd99, -8;
	selp.b64 	%rd125, %rd99, %rd8, %p20;
	selp.b64 	%rd101, %rd100, 0, %p20;
	add.s64 	%rd102, %rd126, %rd9;
	setp.lt.u64 	%p21, %rd102, %rd126;
	setp.gt.u64 	%p22, %rd102, 511;
	add.s64 	%rd103, %rd102, 1;
	selp.b64 	%rd104, 512, %rd103, %p22;
	selp.b64 	%rd126, 512, %rd104, %p21;
	cvt.u16.u64 	%rs21, %rd102;
	and.b16  	%rs22, %rs21, 256;
	xor.b16  	%rs23, %rs22, 768;
	shl.b16 	%rs24, %rs21, 12;
	shr.u16 	%rs25, %rs23, 4;
	or.b16  	%rs26, %rs24, %rs25;
	shl.b16 	%rs27, %rs21, 4;
	and.b16  	%rs28, %rs27, 3840;
	or.b16  	%rs29, %rs28, %rs26;
	and.b16  	%rs30, %rs29, 13107;
	shl.b16 	%rs31, %rs30, 2;
	shr.u16 	%rs32, %rs29, 2;
	and.b16  	%rs33, %rs32, 13107;
	or.b16  	%rs34, %rs33, %rs31;
	and.b16  	%rs35, %rs34, 21845;
	shl.b16 	%rs36, %rs35, 1;
	shr.u16 	%rs37, %rs34, 1;
	and.b16  	%rs38, %rs37, 21845;
	or.b16  	%rs39, %rs38, %rs36;
	shr.u16 	%rs40, %rs39, 4;
	cvt.u32.u16 	%r65, %rs40;
	add.s32 	%r56, %r2, %r65;
	add.s32 	%r55, %r56, -4;
	// begin inline asm
	ld.shared.f32 %r53, [%r55];
ld.shared.f32 %r54, [%r56];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r57, %r54;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r59, %r53, %r48;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r62, %r57, %r48;
	// end inline asm
	st.u32 	[%rd101], %r59;
	st.u32 	[%rd101+4], %r62;
	add.s64 	%rd124, %rd124, -1;
	setp.ne.s64 	%p23, %rd124, 0;
	@%p23 bra 	$L__BB20_31;
$L__BB20_32:
	ret;
$L__BB20_16:
	mov.u64 	%rd81, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd82, %rd81;
	mov.u64 	%rd83, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd84, %rd83;
	{ // callseq 72, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd82;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd84;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 72
$L__BB20_1:
	mov.u64 	%rd105, anon_$_9b271fbca47443a0c783d86781d13fba_$_9;
	cvta.global.u64 	%rd106, %rd105;
	{ // callseq 74, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd106;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 74

}
	// .globl	fft_backward_shifted_1024_kernel
.visible .entry fft_backward_shifted_1024_kernel(
	.param .u64 fft_backward_shifted_1024_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot21[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<14>;
	.reg .b16 	%rs<43>;
	.reg .b32 	%r<59>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<79>;

	mov.u64 	%SPL, __local_depot21;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd33, [fft_backward_shifted_1024_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd33;
	add.u64 	%rd35, %SP, 16;
	add.u64 	%rd36, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[8192];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd36], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd37, [%rd1+8];
	and.b64  	%rd38, %rd37, -1024;
	mul.wide.u32 	%rd6, %r5, 1024;
	setp.lt.u64 	%p2, %rd6, %rd38;
	sub.s64 	%rd39, %rd37, %rd6;
	setp.gt.u64 	%p3, %rd39, 1023;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB21_2;
	bra.uni 	$L__BB21_1;
$L__BB21_2:
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u32.u64 	%r12, %rd3;
	ld.global.nc.u64 	%rd7, [%rd1];
	shl.b64 	%rd40, %rd6, 3;
	add.s64 	%rd41, %rd7, %rd40;
	add.s64 	%rd9, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 1023;
	cvt.u16.u64 	%rs3, %rd4;
	shl.b64 	%rd42, %rd3, 3;
	add.s64 	%rd10, %rd41, %rd42;
	shl.b32 	%r13, %r12, 3;
	ld.u32 	%r7, [%rd10+4];
	ld.u32 	%r10, [%rd10];
	// begin inline asm
	neg.ftz.f32 %r6, %r7;
	// end inline asm
	add.s32 	%r8, %r2, %r13;
	add.s32 	%r9, %r8, 4;
	// begin inline asm
	st.shared.f32 [%r8], %r10;
st.shared.f32 [%r9], %r6;
	// end inline asm
	setp.lt.u16 	%p5, %rs2, %rs3;
	add.s64 	%rd78, %rd3, 1;
	cvt.u32.u64 	%r56, %rd4;
	shl.b64 	%rd71, %rd4, 3;
	@%p5 bra 	$L__BB21_7;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u16 	%r14, %rs2;
	div.u32 	%r16, %r14, %r56;
	cvt.u64.u32 	%rd74, %r16;
	shl.b64 	%rd43, %rd5, 13;
	add.s64 	%rd44, %rd43, %rd71;
	add.s64 	%rd46, %rd44, %rd42;
	add.s64 	%rd73, %rd7, %rd46;
	xor.b64  	%rd72, %rd42, 8184;
	mov.u64 	%rd75, %rd78;
$L__BB21_4:
	shr.u64 	%rd47, %rd72, 3;
	setp.gt.u64 	%p6, %rd47, %rd9;
	selp.b64 	%rd48, %rd73, 0, %p6;
	add.s64 	%rd20, %rd75, %rd9;
	ld.f32 	%f1, [%rd48];
	ld.u32 	%r18, [%rd48+4];
	// begin inline asm
	neg.ftz.f32 %r17, %r18;
	// end inline asm
	setp.lt.u64 	%p7, %rd20, 1024;
	@%p7 bra 	$L__BB21_6;
	bra.uni 	$L__BB21_5;
$L__BB21_6:
	setp.lt.u64 	%p1, %rd20, %rd75;
	mov.b32 	%f2, %r17;
	add.s64 	%rd53, %rd75, %rd4;
	selp.b64 	%rd75, 1024, %rd53, %p1;
	cvt.u32.u64 	%r23, %rd20;
	shl.b32 	%r24, %r23, 3;
	add.s32 	%r19, %r24, %r2;
	add.s32 	%r20, %r19, 4;
	mov.b32 	%r21, %f1;
	// begin inline asm
	st.shared.f32 [%r19], %r21;
st.shared.f32 [%r20], %r17;
	// end inline asm
	add.s64 	%rd74, %rd74, -1;
	add.s64 	%rd73, %rd73, %rd71;
	sub.s64 	%rd72, %rd72, %rd71;
	setp.ne.s64 	%p8, %rd74, 0;
	@%p8 bra 	$L__BB21_4;
$L__BB21_7:
	add.u64 	%rd34, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	ld.global.nc.u64 	%rd54, [%rd1+16];
	ld.global.nc.u64 	%rd55, [%rd1+24];
	st.local.u64 	[%rd2], %rd54;
	st.local.u64 	[%rd2+8], %rd55;
	bar.sync 	0;
	{ // callseq 76, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd34;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd35;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 76
	mov.b32 	%r26, 1149239296;
	// begin inline asm
	rcp.approx.ftz.f32 %r35, %r26;
	// end inline asm
	bar.sync 	0;
	xor.b64  	%rd58, %rd3, 1023;
	xor.b16  	%rs4, %rs1, 1536;
	shl.b16 	%rs5, %rs1, 12;
	shr.u16 	%rs6, %rs4, 4;
	and.b16  	%rs7, %rs6, 112;
	or.b16  	%rs8, %rs5, %rs7;
	shl.b16 	%rs9, %rs1, 4;
	and.b16  	%rs10, %rs9, 3840;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 13107;
	shl.b16 	%rs13, %rs12, 2;
	shr.u16 	%rs14, %rs11, 2;
	and.b16  	%rs15, %rs14, 13107;
	or.b16  	%rs16, %rs15, %rs13;
	and.b16  	%rs17, %rs16, 21845;
	shl.b16 	%rs18, %rs17, 1;
	shr.u16 	%rs19, %rs16, 1;
	and.b16  	%rs20, %rs19, 21845;
	or.b16  	%rs21, %rs20, %rs18;
	shr.u16 	%rs22, %rs21, 3;
	cvt.u32.u16 	%r42, %rs22;
	add.s32 	%r30, %r2, %r42;
	add.s32 	%r29, %r30, -4;
	// begin inline asm
	ld.shared.f32 %r27, [%r29];
ld.shared.f32 %r28, [%r30];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r31, %r28;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r33, %r27, %r35;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r36, %r31, %r35;
	// end inline asm
	st.u32 	[%rd10], %r33;
	st.u32 	[%rd10+4], %r36;
	setp.lt.u64 	%p9, %rd58, %rd4;
	@%p9 bra 	$L__BB21_10;
	add.s64 	%rd8, %rd41, 8192;
	mov.b32 	%f3, %r35;
	cvt.u32.u64 	%r39, %rd58;
	div.u32 	%r41, %r39, %r56;
	cvt.u64.u32 	%rd76, %r41;
	add.s64 	%rd77, %rd10, 8;
$L__BB21_9:
	sub.s64 	%rd59, %rd8, %rd77;
	shr.u64 	%rd60, %rd59, 3;
	setp.gt.u64 	%p10, %rd60, %rd9;
	add.s64 	%rd63, %rd77, %rd71;
	add.s64 	%rd64, %rd63, -8;
	selp.b64 	%rd77, %rd63, %rd8, %p10;
	selp.b64 	%rd65, %rd64, 0, %p10;
	add.s64 	%rd66, %rd78, %rd9;
	setp.lt.u64 	%p11, %rd66, %rd78;
	setp.gt.u64 	%p12, %rd66, 1023;
	add.s64 	%rd67, %rd66, 1;
	selp.b64 	%rd68, 1024, %rd67, %p12;
	selp.b64 	%rd78, 1024, %rd68, %p11;
	cvt.u16.u64 	%rs23, %rd66;
	and.b16  	%rs24, %rs23, 768;
	xor.b16  	%rs25, %rs24, 1536;
	shl.b16 	%rs26, %rs23, 12;
	shr.u16 	%rs27, %rs25, 4;
	or.b16  	%rs28, %rs26, %rs27;
	shl.b16 	%rs29, %rs23, 4;
	and.b16  	%rs30, %rs29, 3840;
	or.b16  	%rs31, %rs30, %rs28;
	and.b16  	%rs32, %rs31, 13107;
	shl.b16 	%rs33, %rs32, 2;
	shr.u16 	%rs34, %rs31, 2;
	and.b16  	%rs35, %rs34, 13107;
	or.b16  	%rs36, %rs35, %rs33;
	and.b16  	%rs37, %rs36, 21845;
	shl.b16 	%rs38, %rs37, 1;
	shr.u16 	%rs39, %rs36, 1;
	and.b16  	%rs40, %rs39, 21845;
	or.b16  	%rs41, %rs40, %rs38;
	shr.u16 	%rs42, %rs41, 3;
	cvt.u32.u16 	%r55, %rs42;
	add.s32 	%r46, %r2, %r55;
	add.s32 	%r45, %r46, -4;
	// begin inline asm
	ld.shared.f32 %r43, [%r45];
ld.shared.f32 %r44, [%r46];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r47, %r44;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r49, %r43, %r35;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r52, %r47, %r35;
	// end inline asm
	st.u32 	[%rd65], %r49;
	st.u32 	[%rd65+4], %r52;
	add.s64 	%rd76, %rd76, -1;
	setp.ne.s64 	%p13, %rd76, 0;
	@%p13 bra 	$L__BB21_9;
$L__BB21_10:
	ret;
$L__BB21_5:
	mov.u64 	%rd49, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd50, %rd49;
	mov.u64 	%rd51, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd52, %rd51;
	{ // callseq 75, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd50;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd52;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 75
$L__BB21_1:
	mov.u64 	%rd69, anon_$_9b271fbca47443a0c783d86781d13fba_$_9;
	cvta.global.u64 	%rd70, %rd69;
	{ // callseq 77, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd70;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 77

}
	// .globl	fft_backward_shifted_2048_kernel
.visible .entry fft_backward_shifted_2048_kernel(
	.param .u64 fft_backward_shifted_2048_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot22[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<42>;
	.reg .b32 	%r<59>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<76>;

	mov.u64 	%SPL, __local_depot22;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd32, [fft_backward_shifted_2048_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd32;
	add.u64 	%rd34, %SP, 16;
	add.u64 	%rd35, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[16384];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd35], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd36, [%rd1+8];
	and.b64  	%rd37, %rd36, -2048;
	mul.wide.u32 	%rd6, %r5, 2048;
	setp.lt.u64 	%p2, %rd6, %rd37;
	sub.s64 	%rd38, %rd36, %rd6;
	setp.gt.u64 	%p3, %rd38, 2047;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB22_2;
	bra.uni 	$L__BB22_1;
$L__BB22_2:
	add.u64 	%rd33, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r12, %rd3;
	ld.global.nc.u64 	%rd39, [%rd1];
	shl.b64 	%rd40, %rd6, 3;
	add.s64 	%rd41, %rd39, %rd40;
	add.s64 	%rd7, %rd41, 16384;
	add.s64 	%rd8, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 2047;
	shl.b64 	%rd42, %rd3, 3;
	add.s64 	%rd9, %rd41, %rd42;
	shl.b32 	%r13, %r12, 3;
	ld.u32 	%r7, [%rd9+4];
	ld.u32 	%r10, [%rd9];
	// begin inline asm
	neg.ftz.f32 %r11, %r7;
	// end inline asm
	add.s32 	%r8, %r2, %r13;
	add.s32 	%r9, %r8, 4;
	// begin inline asm
	st.shared.f32 [%r8], %r10;
st.shared.f32 [%r9], %r11;
	// end inline asm
	add.s64 	%rd75, %rd3, 1;
	add.s64 	%rd74, %rd9, 8;
	cvt.u32.u16 	%r14, %rs2;
	cvt.u32.u64 	%r15, %rd4;
	div.u32 	%r16, %r14, %r15;
	cvt.u64.u32 	%rd71, %r16;
	shl.b64 	%rd43, %rd5, 14;
	shl.b64 	%rd13, %rd4, 3;
	add.s64 	%rd44, %rd43, %rd13;
	add.s64 	%rd45, %rd44, %rd42;
	add.s64 	%rd70, %rd39, %rd45;
	xor.b64  	%rd69, %rd42, 16376;
	mov.u64 	%rd72, %rd75;
$L__BB22_3:
	shr.u64 	%rd46, %rd69, 3;
	setp.gt.u64 	%p5, %rd46, %rd8;
	selp.b64 	%rd47, %rd70, 0, %p5;
	add.s64 	%rd20, %rd72, %rd8;
	ld.f32 	%f1, [%rd47];
	ld.u32 	%r18, [%rd47+4];
	// begin inline asm
	neg.ftz.f32 %r17, %r18;
	// end inline asm
	setp.lt.u64 	%p6, %rd20, 2048;
	@%p6 bra 	$L__BB22_5;
	bra.uni 	$L__BB22_4;
$L__BB22_5:
	setp.lt.u64 	%p1, %rd20, %rd72;
	mov.b32 	%f2, %r17;
	add.s64 	%rd52, %rd72, %rd4;
	selp.b64 	%rd72, 2048, %rd52, %p1;
	cvt.u32.u64 	%r23, %rd20;
	shl.b32 	%r24, %r23, 3;
	add.s32 	%r19, %r24, %r2;
	add.s32 	%r20, %r19, 4;
	mov.b32 	%r21, %f1;
	// begin inline asm
	st.shared.f32 [%r19], %r21;
st.shared.f32 [%r20], %r17;
	// end inline asm
	add.s64 	%rd71, %rd71, -1;
	add.s64 	%rd70, %rd70, %rd13;
	sub.s64 	%rd69, %rd69, %rd13;
	setp.ne.s64 	%p7, %rd71, 0;
	@%p7 bra 	$L__BB22_3;
	ld.global.nc.u64 	%rd53, [%rd1+16];
	ld.global.nc.u64 	%rd54, [%rd1+24];
	st.local.u64 	[%rd2], %rd53;
	st.local.u64 	[%rd2+8], %rd54;
	bar.sync 	0;
	{ // callseq 79, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd33;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd34;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 79
	mov.b32 	%r26, 1157627904;
	// begin inline asm
	rcp.approx.ftz.f32 %r35, %r26;
	// end inline asm
	mov.b32 	%f3, %r35;
	bar.sync 	0;
	xor.b32  	%r40, %r12, 2047;
	div.u32 	%r42, %r40, %r15;
	cvt.u64.u32 	%rd73, %r42;
	or.b16  	%rs3, %rs1, 3072;
	shl.b16 	%rs4, %rs1, 12;
	shr.u16 	%rs5, %rs3, 4;
	and.b16  	%rs6, %rs5, 240;
	or.b16  	%rs7, %rs4, %rs6;
	shl.b16 	%rs8, %rs1, 4;
	and.b16  	%rs9, %rs8, 3840;
	or.b16  	%rs10, %rs9, %rs7;
	and.b16  	%rs11, %rs10, 13107;
	shl.b16 	%rs12, %rs11, 2;
	shr.u16 	%rs13, %rs10, 2;
	and.b16  	%rs14, %rs13, 13107;
	or.b16  	%rs15, %rs14, %rs12;
	and.b16  	%rs16, %rs15, 21845;
	shl.b16 	%rs17, %rs16, 1;
	shr.u16 	%rs18, %rs15, 1;
	and.b16  	%rs19, %rs18, 21845;
	or.b16  	%rs20, %rs19, %rs17;
	shr.u16 	%rs21, %rs20, 2;
	cvt.u32.u16 	%r43, %rs21;
	add.s32 	%r30, %r2, %r43;
	add.s32 	%r29, %r30, -4;
	// begin inline asm
	ld.shared.f32 %r34, [%r29];
ld.shared.f32 %r32, [%r30];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r37, %r32;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r33, %r34, %r35;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r36, %r37, %r35;
	// end inline asm
	st.u32 	[%rd9], %r33;
	st.u32 	[%rd9+4], %r36;
$L__BB22_7:
	sub.s64 	%rd57, %rd7, %rd74;
	shr.u64 	%rd58, %rd57, 3;
	setp.gt.u64 	%p8, %rd58, %rd8;
	add.s64 	%rd61, %rd74, %rd13;
	add.s64 	%rd62, %rd61, -8;
	selp.b64 	%rd74, %rd61, %rd7, %p8;
	selp.b64 	%rd63, %rd62, 0, %p8;
	add.s64 	%rd64, %rd75, %rd8;
	setp.lt.u64 	%p9, %rd64, %rd75;
	setp.gt.u64 	%p10, %rd64, 2047;
	add.s64 	%rd65, %rd64, 1;
	selp.b64 	%rd66, 2048, %rd65, %p10;
	selp.b64 	%rd75, 2048, %rd66, %p9;
	cvt.u16.u64 	%rs22, %rd64;
	and.b16  	%rs23, %rs22, 1792;
	xor.b16  	%rs24, %rs23, 3072;
	shl.b16 	%rs25, %rs22, 12;
	shr.u16 	%rs26, %rs24, 4;
	or.b16  	%rs27, %rs25, %rs26;
	shl.b16 	%rs28, %rs22, 4;
	and.b16  	%rs29, %rs28, 3840;
	or.b16  	%rs30, %rs29, %rs27;
	and.b16  	%rs31, %rs30, 13107;
	shl.b16 	%rs32, %rs31, 2;
	shr.u16 	%rs33, %rs30, 2;
	and.b16  	%rs34, %rs33, 13107;
	or.b16  	%rs35, %rs34, %rs32;
	and.b16  	%rs36, %rs35, 21845;
	shl.b16 	%rs37, %rs36, 1;
	shr.u16 	%rs38, %rs35, 1;
	and.b16  	%rs39, %rs38, 21845;
	or.b16  	%rs40, %rs39, %rs37;
	shr.u16 	%rs41, %rs40, 2;
	cvt.u32.u16 	%r56, %rs41;
	add.s32 	%r47, %r2, %r56;
	add.s32 	%r46, %r47, -4;
	// begin inline asm
	ld.shared.f32 %r44, [%r46];
ld.shared.f32 %r45, [%r47];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r48, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r50, %r44, %r35;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r53, %r48, %r35;
	// end inline asm
	st.u32 	[%rd63], %r50;
	st.u32 	[%rd63+4], %r53;
	add.s64 	%rd73, %rd73, -1;
	setp.ne.s64 	%p11, %rd73, 0;
	@%p11 bra 	$L__BB22_7;
	ret;
$L__BB22_4:
	mov.u64 	%rd48, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd49, %rd48;
	mov.u64 	%rd50, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd51, %rd50;
	{ // callseq 78, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd49;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd51;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 78
$L__BB22_1:
	mov.u64 	%rd67, anon_$_9b271fbca47443a0c783d86781d13fba_$_9;
	cvta.global.u64 	%rd68, %rd67;
	{ // callseq 80, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd68;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 80

}
	// .globl	fft_backward_shifted_4096_kernel
.visible .entry fft_backward_shifted_4096_kernel(
	.param .u64 fft_backward_shifted_4096_kernel_param_0
)
{
	.local .align 8 .b8 	__local_depot23[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<12>;
	.reg .b16 	%rs<45>;
	.reg .b32 	%r<59>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<76>;

	mov.u64 	%SPL, __local_depot23;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd32, [fft_backward_shifted_4096_kernel_param_0];
	cvta.to.global.u64 	%rd1, %rd32;
	add.u64 	%rd34, %SP, 16;
	add.u64 	%rd35, %SPL, 16;
	// begin inline asm
	.shared .align 8 .b8 nonphysical[32768];
    mov.u32 %r2, nonphysical;
	// end inline asm
	st.local.u32 	[%rd35], %r2;
	mov.u32 	%r5, %ctaid.x;
	ld.global.nc.u64 	%rd36, [%rd1+8];
	and.b64  	%rd37, %rd36, -4096;
	mul.wide.u32 	%rd6, %r5, 4096;
	setp.lt.u64 	%p2, %rd6, %rd37;
	sub.s64 	%rd38, %rd36, %rd6;
	setp.gt.u64 	%p3, %rd38, 4095;
	and.pred  	%p4, %p2, %p3;
	@%p4 bra 	$L__BB23_2;
	bra.uni 	$L__BB23_1;
$L__BB23_2:
	add.u64 	%rd33, %SP, 0;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r3, %tid.x;
	cvt.u64.u32 	%rd3, %r3;
	mov.u32 	%r4, %ntid.x;
	cvt.u64.u32 	%rd4, %r4;
	cvt.u64.u32 	%rd5, %r5;
	cvt.u32.u64 	%r12, %rd3;
	ld.global.nc.u64 	%rd39, [%rd1];
	shl.b64 	%rd40, %rd6, 3;
	add.s64 	%rd41, %rd39, %rd40;
	add.s64 	%rd7, %rd41, 32768;
	add.s64 	%rd8, %rd4, -1;
	cvt.u16.u64 	%rs1, %rd3;
	xor.b16  	%rs2, %rs1, 4095;
	shl.b64 	%rd42, %rd3, 3;
	add.s64 	%rd9, %rd41, %rd42;
	add.s64 	%rd74, %rd9, 8;
	add.s64 	%rd75, %rd3, 1;
	shl.b32 	%r13, %r12, 3;
	ld.u32 	%r7, [%rd9+4];
	ld.u32 	%r10, [%rd9];
	// begin inline asm
	neg.ftz.f32 %r11, %r7;
	// end inline asm
	add.s32 	%r8, %r2, %r13;
	add.s32 	%r9, %r8, 4;
	// begin inline asm
	st.shared.f32 [%r8], %r10;
st.shared.f32 [%r9], %r11;
	// end inline asm
	cvt.u32.u16 	%r14, %rs2;
	cvt.u32.u64 	%r15, %rd4;
	div.u32 	%r16, %r14, %r15;
	cvt.u64.u32 	%rd71, %r16;
	shl.b64 	%rd43, %rd5, 15;
	shl.b64 	%rd13, %rd4, 3;
	add.s64 	%rd44, %rd43, %rd13;
	add.s64 	%rd45, %rd44, %rd42;
	add.s64 	%rd70, %rd39, %rd45;
	xor.b64  	%rd69, %rd42, 32760;
	mov.u64 	%rd72, %rd75;
$L__BB23_3:
	shr.u64 	%rd46, %rd69, 3;
	setp.gt.u64 	%p5, %rd46, %rd8;
	selp.b64 	%rd47, %rd70, 0, %p5;
	add.s64 	%rd20, %rd72, %rd8;
	ld.f32 	%f1, [%rd47];
	ld.u32 	%r18, [%rd47+4];
	// begin inline asm
	neg.ftz.f32 %r17, %r18;
	// end inline asm
	setp.lt.u64 	%p6, %rd20, 4096;
	@%p6 bra 	$L__BB23_5;
	bra.uni 	$L__BB23_4;
$L__BB23_5:
	setp.lt.u64 	%p1, %rd20, %rd72;
	mov.b32 	%f2, %r17;
	add.s64 	%rd52, %rd72, %rd4;
	selp.b64 	%rd72, 4096, %rd52, %p1;
	cvt.u32.u64 	%r23, %rd20;
	shl.b32 	%r24, %r23, 3;
	add.s32 	%r19, %r24, %r2;
	add.s32 	%r20, %r19, 4;
	mov.b32 	%r21, %f1;
	// begin inline asm
	st.shared.f32 [%r19], %r21;
st.shared.f32 [%r20], %r17;
	// end inline asm
	add.s64 	%rd71, %rd71, -1;
	add.s64 	%rd70, %rd70, %rd13;
	sub.s64 	%rd69, %rd69, %rd13;
	setp.ne.s64 	%p7, %rd71, 0;
	@%p7 bra 	$L__BB23_3;
	ld.global.nc.u64 	%rd53, [%rd1+16];
	ld.global.nc.u64 	%rd54, [%rd1+24];
	st.local.u64 	[%rd2], %rd53;
	st.local.u64 	[%rd2+8], %rd54;
	bar.sync 	0;
	{ // callseq 82, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd33;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd3;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	.param .b64 param3;
	st.param.b64 	[param3+0], %rd34;
	call.uni 
	_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E, 
	(
	param0, 
	param1, 
	param2, 
	param3
	);
	} // callseq 82
	mov.b32 	%r26, 1166016512;
	// begin inline asm
	rcp.approx.ftz.f32 %r35, %r26;
	// end inline asm
	mov.b32 	%f3, %r35;
	bar.sync 	0;
	xor.b32  	%r40, %r12, 4095;
	div.u32 	%r42, %r40, %r15;
	cvt.u64.u32 	%rd73, %r42;
	or.b16  	%rs3, %rs1, 6144;
	shl.b16 	%rs4, %rs1, 8;
	shr.u16 	%rs5, %rs3, 8;
	or.b16  	%rs6, %rs4, %rs5;
	and.b16  	%rs7, %rs6, 3851;
	shl.b16 	%rs8, %rs7, 4;
	shr.u16 	%rs9, %rs6, 4;
	and.b16  	%rs10, %rs9, 3841;
	or.b16  	%rs11, %rs10, %rs8;
	and.b16  	%rs12, %rs11, 13107;
	shl.b16 	%rs13, %rs12, 2;
	shr.u16 	%rs14, %rs11, 2;
	and.b16  	%rs15, %rs14, 13107;
	or.b16  	%rs16, %rs15, %rs13;
	and.b16  	%rs17, %rs16, 21845;
	shl.b16 	%rs18, %rs17, 1;
	shr.u16 	%rs19, %rs16, 1;
	and.b16  	%rs20, %rs19, 21845;
	or.b16  	%rs21, %rs20, %rs18;
	shr.u16 	%rs22, %rs21, 1;
	cvt.u32.u16 	%r43, %rs22;
	add.s32 	%r30, %r2, %r43;
	add.s32 	%r29, %r30, -4;
	// begin inline asm
	ld.shared.f32 %r34, [%r29];
ld.shared.f32 %r32, [%r30];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r37, %r32;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r33, %r34, %r35;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r36, %r37, %r35;
	// end inline asm
	st.u32 	[%rd9], %r33;
	st.u32 	[%rd9+4], %r36;
$L__BB23_7:
	sub.s64 	%rd57, %rd7, %rd74;
	shr.u64 	%rd58, %rd57, 3;
	setp.gt.u64 	%p8, %rd58, %rd8;
	add.s64 	%rd61, %rd74, %rd13;
	add.s64 	%rd62, %rd61, -8;
	selp.b64 	%rd74, %rd61, %rd7, %p8;
	selp.b64 	%rd63, %rd62, 0, %p8;
	add.s64 	%rd64, %rd75, %rd8;
	setp.lt.u64 	%p9, %rd64, %rd75;
	setp.gt.u64 	%p10, %rd64, 4095;
	add.s64 	%rd65, %rd64, 1;
	selp.b64 	%rd66, 4096, %rd65, %p10;
	selp.b64 	%rd75, 4096, %rd66, %p9;
	cvt.u16.u64 	%rs23, %rd64;
	and.b16  	%rs24, %rs23, 3840;
	xor.b16  	%rs25, %rs24, 6144;
	shl.b16 	%rs26, %rs23, 8;
	shr.u16 	%rs27, %rs25, 8;
	or.b16  	%rs28, %rs26, %rs27;
	and.b16  	%rs29, %rs28, 3855;
	shl.b16 	%rs30, %rs29, 4;
	shr.u16 	%rs31, %rs28, 4;
	and.b16  	%rs32, %rs31, 3841;
	or.b16  	%rs33, %rs32, %rs30;
	and.b16  	%rs34, %rs33, 13107;
	shl.b16 	%rs35, %rs34, 2;
	shr.u16 	%rs36, %rs33, 2;
	and.b16  	%rs37, %rs36, 13107;
	or.b16  	%rs38, %rs37, %rs35;
	and.b16  	%rs39, %rs38, 21845;
	shl.b16 	%rs40, %rs39, 1;
	shr.u16 	%rs41, %rs38, 1;
	and.b16  	%rs42, %rs41, 21845;
	or.b16  	%rs43, %rs42, %rs40;
	shr.u16 	%rs44, %rs43, 1;
	cvt.u32.u16 	%r56, %rs44;
	add.s32 	%r47, %r2, %r56;
	add.s32 	%r46, %r47, -4;
	// begin inline asm
	ld.shared.f32 %r44, [%r46];
ld.shared.f32 %r45, [%r47];
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r48, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r50, %r44, %r35;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r53, %r48, %r35;
	// end inline asm
	st.u32 	[%rd63], %r50;
	st.u32 	[%rd63+4], %r53;
	add.s64 	%rd73, %rd73, -1;
	setp.ne.s64 	%p11, %rd73, 0;
	@%p11 bra 	$L__BB23_7;
	ret;
$L__BB23_4:
	mov.u64 	%rd48, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd49, %rd48;
	mov.u64 	%rd50, anon_$_9b271fbca47443a0c783d86781d13fba_$_66;
	cvta.global.u64 	%rd51, %rd50;
	{ // callseq 81, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd49;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd51;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 81
$L__BB23_1:
	mov.u64 	%rd67, anon_$_9b271fbca47443a0c783d86781d13fba_$_9;
	cvta.global.u64 	%rd68, %rd67;
	{ // callseq 83, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd68;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 83

}
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_3
)
{
	.reg .pred 	%p<63>;
	.reg .b32 	%r<399>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<180>;

	ld.param.u64 	%rd57, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_2];
	setp.eq.s64 	%p8, %rd57, 0;
	@%p8 bra 	$L__BB24_5;
	ld.param.u64 	%rd56, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_1];
	ld.param.u64 	%rd58, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_0];
	add.s64 	%rd1, %rd57, -1;
	setp.gt.u64 	%p9, %rd56, 63;
	ld.param.u64 	%rd59, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h0d7c55d795544359E_param_3];
	add.s64 	%rd60, %rd56, 1;
	selp.b64 	%rd61, 64, %rd60, %p9;
	ld.u64 	%rd62, [%rd58];
	ld.u64 	%rd63, [%rd58+8];
	ld.u32 	%r1, [%rd59];
	shl.b64 	%rd64, %rd63, 3;
	add.s64 	%rd2, %rd62, %rd64;
	setp.eq.s64 	%p10, %rd63, 0;
	selp.b64 	%rd3, 0, %rd62, %p10;
	selp.b64 	%rd65, 0, 8, %p10;
	add.s64 	%rd4, %rd62, %rd65;
	add.s64 	%rd66, %rd61, %rd1;
	setp.lt.u64 	%p11, %rd66, %rd61;
	setp.gt.u64 	%p12, %rd66, 63;
	or.pred  	%p1, %p11, %p12;
	mov.u64 	%rd7, 1;
	mov.u64 	%rd8, 7;
	bra.uni 	$L__BB24_2;
$L__BB24_32:
	shl.b64 	%rd7, %rd7, 1;
	bar.sync 	0;
	setp.gt.u64 	%p37, %rd8, 2;
	@%p37 bra 	$L__BB24_2;
	bra.uni 	$L__BB24_33;
$L__BB24_2:
	add.s64 	%rd8, %rd8, -1;
	@%p9 bra 	$L__BB24_32;
	cvt.u32.u64 	%r13, %rd8;
	shr.u64 	%rd69, %rd56, %r13;
	mov.u64 	%rd70, 1;
	shl.b64 	%rd27, %rd70, %r13;
	mov.u64 	%rd71, 2;
	shl.b64 	%rd28, %rd71, %r13;
	add.s64 	%rd29, %rd27, -1;
	and.b64  	%rd172, %rd29, %rd56;
	mul.lo.s64 	%rd72, %rd69, %rd28;
	add.s64 	%rd31, %rd172, %rd72;
	setp.lt.u64 	%p14, %rd31, 128;
	@%p14 bra 	$L__BB24_20;
	bra.uni 	$L__BB24_4;
$L__BB24_20:
	cvt.u32.u64 	%r18, %rd31;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r16, %r1, %r19;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	ld.shared.f32 %r14, [%r16];
ld.shared.f32 %r15, [%r17];
	// end inline asm
	add.s64 	%rd32, %rd31, %rd27;
	setp.lt.u64 	%p15, %rd32, 128;
	@%p15 bra 	$L__BB24_22;
	bra.uni 	$L__BB24_21;
$L__BB24_22:
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r22, %r1, %r25;
	add.s32 	%r23, %r22, 4;
	// begin inline asm
	ld.shared.f32 %r20, [%r22];
ld.shared.f32 %r21, [%r23];
	// end inline asm
	setp.eq.s64 	%p16, %rd172, 0;
	mov.u64 	%rd162, -1;
	mov.u64 	%rd177, %rd3;
	@%p16 bra 	$L__BB24_29;
	mul.lo.s64 	%rd176, %rd172, %rd7;
	mul.hi.u64 	%rd81, %rd172, %rd7;
	setp.eq.s64 	%p17, %rd81, 0;
	mov.u64 	%rd174, %rd4;
	@%p17 bra 	$L__BB24_28;
	mov.u64 	%rd173, %rd7;
	mov.u64 	%rd174, %rd4;
$L__BB24_25:
	setp.eq.s64 	%p18, %rd172, 0;
	@%p18 bra 	$L__BB24_55;
	setp.eq.s64 	%p19, %rd173, 0;
	@%p19 bra 	$L__BB24_56;
	div.u64 	%rd83, %rd162, %rd172;
	div.u64 	%rd84, %rd162, %rd173;
	mul.lo.s64 	%rd85, %rd83, %rd172;
	mul.lo.s64 	%rd86, %rd84, %rd173;
	setp.gt.u64 	%p20, %rd85, %rd86;
	max.u64 	%rd87, %rd85, %rd86;
	selp.b64 	%rd88, %rd83, 0, %p20;
	sub.s64 	%rd173, %rd173, %rd88;
	selp.b64 	%rd89, 0, %rd84, %p20;
	sub.s64 	%rd172, %rd172, %rd89;
	add.s64 	%rd90, %rd87, -1;
	sub.s64 	%rd91, %rd2, %rd174;
	shr.u64 	%rd92, %rd91, 3;
	setp.gt.u64 	%p21, %rd92, %rd90;
	shl.b64 	%rd93, %rd87, 3;
	add.s64 	%rd94, %rd174, %rd93;
	selp.b64 	%rd174, %rd94, %rd2, %p21;
	mul.lo.s64 	%rd176, %rd172, %rd173;
	mul.hi.u64 	%rd95, %rd172, %rd173;
	setp.ne.s64 	%p6, %rd95, 0;
	@%p6 bra 	$L__BB24_25;
$L__BB24_28:
	add.s64 	%rd96, %rd176, -1;
	sub.s64 	%rd97, %rd2, %rd174;
	shr.u64 	%rd98, %rd97, 3;
	setp.gt.u64 	%p22, %rd98, %rd96;
	shl.b64 	%rd99, %rd176, 3;
	add.s64 	%rd100, %rd174, %rd99;
	add.s64 	%rd101, %rd100, -8;
	selp.b64 	%rd177, %rd101, 0, %p22;
$L__BB24_29:
	setp.ne.s64 	%p23, %rd177, 0;
	@%p23 bra 	$L__BB24_31;
	bra.uni 	$L__BB24_30;
$L__BB24_31:
	mov.b32 	%f6, %r15;
	mov.b32 	%f5, %r14;
	mov.b32 	%f8, %r21;
	mov.b32 	%f7, %r20;
	// begin inline asm
	add.rn.ftz.f32 %r26, %r14, %r20;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r29, %r15, %r21;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r16], %r26;
st.shared.f32 [%r17], %r29;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r43, %r14, %r20;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r46, %r15, %r21;
	// end inline asm
	ld.u32 	%r56, [%rd177];
	ld.u32 	%r53, [%rd177+4];
	// begin inline asm
	mul.rn.ftz.f32 %r42, %r43, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r46, %r53;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r48, %r42, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r51, %r43, %r53;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r54, %r46, %r56;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r57, %r51, %r54;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r22], %r48;
st.shared.f32 [%r23], %r57;
	// end inline asm
	mov.u64 	%rd9, %rd66;
	@%p1 bra 	$L__BB24_32;
$L__BB24_7:
	shr.u64 	%rd106, %rd9, %r13;
	and.b64  	%rd166, %rd9, %rd29;
	mul.lo.s64 	%rd107, %rd106, %rd28;
	add.s64 	%rd12, %rd107, %rd166;
	setp.lt.u64 	%p24, %rd12, 128;
	@%p24 bra 	$L__BB24_9;
	bra.uni 	$L__BB24_8;
$L__BB24_9:
	cvt.u32.u64 	%r69, %rd12;
	shl.b32 	%r70, %r69, 3;
	add.s32 	%r67, %r70, %r1;
	add.s32 	%r68, %r67, 4;
	// begin inline asm
	ld.shared.f32 %r65, [%r67];
ld.shared.f32 %r66, [%r68];
	// end inline asm
	add.s64 	%rd13, %rd12, %rd27;
	setp.lt.u64 	%p25, %rd13, 128;
	@%p25 bra 	$L__BB24_11;
	bra.uni 	$L__BB24_10;
$L__BB24_11:
	cvt.u32.u64 	%r75, %rd13;
	shl.b32 	%r76, %r75, 3;
	add.s32 	%r73, %r76, %r1;
	add.s32 	%r74, %r73, 4;
	// begin inline asm
	ld.shared.f32 %r71, [%r73];
ld.shared.f32 %r72, [%r74];
	// end inline asm
	setp.eq.s64 	%p26, %rd166, 0;
	mov.u64 	%rd171, %rd3;
	@%p26 bra 	$L__BB24_18;
	mul.lo.s64 	%rd170, %rd166, %rd7;
	mul.hi.u64 	%rd116, %rd166, %rd7;
	setp.eq.s64 	%p27, %rd116, 0;
	mov.u64 	%rd168, %rd4;
	@%p27 bra 	$L__BB24_17;
	mov.u64 	%rd167, %rd7;
	mov.u64 	%rd168, %rd4;
$L__BB24_14:
	setp.eq.s64 	%p28, %rd166, 0;
	@%p28 bra 	$L__BB24_53;
	setp.eq.s64 	%p29, %rd167, 0;
	@%p29 bra 	$L__BB24_54;
	div.u64 	%rd118, %rd162, %rd166;
	div.u64 	%rd119, %rd162, %rd167;
	mul.lo.s64 	%rd120, %rd118, %rd166;
	mul.lo.s64 	%rd121, %rd119, %rd167;
	setp.gt.u64 	%p30, %rd120, %rd121;
	max.u64 	%rd122, %rd120, %rd121;
	selp.b64 	%rd123, %rd118, 0, %p30;
	sub.s64 	%rd167, %rd167, %rd123;
	selp.b64 	%rd124, 0, %rd119, %p30;
	sub.s64 	%rd166, %rd166, %rd124;
	add.s64 	%rd125, %rd122, -1;
	sub.s64 	%rd126, %rd2, %rd168;
	shr.u64 	%rd127, %rd126, 3;
	setp.gt.u64 	%p31, %rd127, %rd125;
	shl.b64 	%rd128, %rd122, 3;
	add.s64 	%rd129, %rd168, %rd128;
	selp.b64 	%rd168, %rd129, %rd2, %p31;
	mul.lo.s64 	%rd170, %rd166, %rd167;
	mul.hi.u64 	%rd130, %rd166, %rd167;
	setp.ne.s64 	%p3, %rd130, 0;
	@%p3 bra 	$L__BB24_14;
$L__BB24_17:
	add.s64 	%rd131, %rd170, -1;
	sub.s64 	%rd132, %rd2, %rd168;
	shr.u64 	%rd133, %rd132, 3;
	setp.gt.u64 	%p32, %rd133, %rd131;
	shl.b64 	%rd134, %rd170, 3;
	add.s64 	%rd135, %rd168, %rd134;
	add.s64 	%rd136, %rd135, -8;
	selp.b64 	%rd171, %rd136, 0, %p32;
$L__BB24_18:
	setp.ne.s64 	%p33, %rd171, 0;
	@%p33 bra 	$L__BB24_6;
	bra.uni 	$L__BB24_19;
$L__BB24_6:
	add.s64 	%rd10, %rd9, 1;
	mov.b32 	%f2, %r66;
	mov.b32 	%f1, %r65;
	mov.b32 	%f4, %r72;
	mov.b32 	%f3, %r71;
	// begin inline asm
	add.rn.ftz.f32 %r77, %r65, %r71;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r66, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r67], %r77;
st.shared.f32 [%r68], %r80;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r94, %r65, %r71;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r97, %r66, %r72;
	// end inline asm
	ld.u32 	%r107, [%rd171];
	ld.u32 	%r104, [%rd171+4];
	// begin inline asm
	mul.rn.ftz.f32 %r93, %r94, %r107;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r96, %r97, %r104;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r99, %r93, %r96;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r102, %r94, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r105, %r97, %r107;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r108, %r102, %r105;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r73], %r99;
st.shared.f32 [%r74], %r108;
	// end inline asm
	add.s64 	%rd141, %rd10, %rd1;
	setp.lt.u64 	%p34, %rd141, %rd10;
	add.s64 	%rd9, %rd9, %rd57;
	setp.gt.u64 	%p35, %rd9, 63;
	or.pred  	%p36, %p34, %p35;
	@%p36 bra 	$L__BB24_32;
	bra.uni 	$L__BB24_7;
$L__BB24_33:
	@%p9 bra 	$L__BB24_34;
	and.b64  	%rd146, %rd56, 1;
	setp.eq.b64 	%p39, %rd146, 1;
	mov.pred 	%p40, 0;
	xor.pred  	%p41, %p39, %p40;
	cvt.u32.u64 	%r115, %rd56;
	shl.b32 	%r12, %r115, 4;
	@%p41 bra 	$L__BB24_36;
	add.s32 	%r151, %r1, %r12;
	add.s32 	%r152, %r151, 4;
	// begin inline asm
	ld.shared.f32 %r158, [%r151];
ld.shared.f32 %r161, [%r152];
	// end inline asm
	add.s32 	%r173, %r151, 16;
	add.s32 	%r174, %r151, 20;
	// begin inline asm
	ld.shared.f32 %r159, [%r173];
ld.shared.f32 %r162, [%r174];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r165, %r158, %r159;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r166, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r151], %r165;
st.shared.f32 [%r152], %r166;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r175, %r158, %r159;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r176, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r173], %r175;
st.shared.f32 [%r174], %r176;
	// end inline asm
	bra.uni 	$L__BB24_37;
$L__BB24_36:
	and.b32  	%r146, %r12, 992;
	add.s32 	%r147, %r1, %r146;
	add.s32 	%r130, %r147, 8;
	add.s32 	%r131, %r147, 12;
	// begin inline asm
	ld.shared.f32 %r125, [%r130];
ld.shared.f32 %r128, [%r131];
	// end inline asm
	or.b32  	%r148, %r12, 24;
	add.s32 	%r122, %r1, %r148;
	add.s32 	%r123, %r122, 4;
	// begin inline asm
	ld.shared.f32 %r126, [%r122];
ld.shared.f32 %r129, [%r123];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r132, %r125, %r126;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r133, %r128, %r129;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r130], %r132;
st.shared.f32 [%r131], %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r125, %r126;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r128, %r129;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r145, %r141;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r122], %r144;
st.shared.f32 [%r123], %r145;
	// end inline asm
$L__BB24_37:
	@%p1 bra 	$L__BB24_34;
	add.s64 	%rd46, %rd66, 1;
	and.b64  	%rd147, %rd66, 1;
	setp.eq.b64 	%p42, %rd147, 1;
	xor.pred  	%p44, %p42, %p40;
	not.pred 	%p45, %p44;
	cvt.u32.u64 	%r177, %rd66;
	shl.b32 	%r10, %r177, 4;
	@%p45 bra 	$L__BB24_40;
	bra.uni 	$L__BB24_39;
$L__BB24_40:
	add.s32 	%r213, %r1, %r10;
	add.s32 	%r214, %r213, 4;
	// begin inline asm
	ld.shared.f32 %r220, [%r213];
ld.shared.f32 %r223, [%r214];
	// end inline asm
	add.s32 	%r235, %r213, 16;
	add.s32 	%r236, %r213, 20;
	// begin inline asm
	ld.shared.f32 %r221, [%r235];
ld.shared.f32 %r224, [%r236];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r227, %r220, %r221;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r228, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r213], %r227;
st.shared.f32 [%r214], %r228;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r237, %r220, %r221;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r238, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r235], %r237;
st.shared.f32 [%r236], %r238;
	// end inline asm
	bra.uni 	$L__BB24_41;
$L__BB24_39:
	and.b32  	%r208, %r10, 992;
	add.s32 	%r209, %r1, %r208;
	add.s32 	%r192, %r209, 8;
	add.s32 	%r193, %r209, 12;
	// begin inline asm
	ld.shared.f32 %r187, [%r192];
ld.shared.f32 %r190, [%r193];
	// end inline asm
	or.b32  	%r210, %r10, 24;
	add.s32 	%r184, %r1, %r210;
	add.s32 	%r185, %r184, 4;
	// begin inline asm
	ld.shared.f32 %r188, [%r184];
ld.shared.f32 %r191, [%r185];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r194, %r187, %r188;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r195, %r190, %r191;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r192], %r194;
st.shared.f32 [%r193], %r195;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r203, %r187, %r188;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r206, %r190, %r191;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r207, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r184], %r206;
st.shared.f32 [%r185], %r207;
	// end inline asm
$L__BB24_41:
	add.s64 	%rd148, %rd46, %rd1;
	setp.lt.u64 	%p46, %rd148, %rd46;
	add.s64 	%rd178, %rd66, %rd57;
	setp.gt.u64 	%p47, %rd178, 63;
	or.pred  	%p48, %p46, %p47;
	@%p48 bra 	$L__BB24_34;
	bra.uni 	$L__BB24_42;
$L__BB24_34:
	bar.sync 	0;
	@%p9 bra 	$L__BB24_51;
	cvt.u32.u64 	%r329, %rd56;
	shl.b32 	%r330, %r329, 4;
	add.s32 	%r303, %r1, %r330;
	add.s32 	%r304, %r303, 4;
	// begin inline asm
	ld.shared.f32 %r310, [%r303];
ld.shared.f32 %r313, [%r304];
	// end inline asm
	add.s32 	%r307, %r303, 8;
	add.s32 	%r308, %r303, 12;
	// begin inline asm
	ld.shared.f32 %r311, [%r307];
ld.shared.f32 %r314, [%r308];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r309, %r310, %r311;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r312, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r303], %r309;
st.shared.f32 [%r304], %r312;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r319, %r310, %r311;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r322, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r307], %r319;
st.shared.f32 [%r308], %r322;
	// end inline asm
	@!%p1 bra 	$L__BB24_48;
	bra.uni 	$L__BB24_51;
$L__BB24_48:
	shl.b64 	%rd55, %rd66, 1;
	setp.gt.u64 	%p56, %rd55, 127;
	@%p56 bra 	$L__BB24_52;
	add.s64 	%rd54, %rd66, 1;
	cvt.u32.u64 	%r359, %rd55;
	shl.b32 	%r360, %r359, 3;
	add.s32 	%r333, %r1, %r360;
	add.s32 	%r334, %r333, 4;
	// begin inline asm
	ld.shared.f32 %r340, [%r333];
ld.shared.f32 %r343, [%r334];
	// end inline asm
	add.s32 	%r337, %r333, 8;
	add.s32 	%r338, %r333, 12;
	// begin inline asm
	ld.shared.f32 %r341, [%r337];
ld.shared.f32 %r344, [%r338];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r339, %r340, %r341;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r342, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r333], %r339;
st.shared.f32 [%r334], %r342;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r349, %r340, %r341;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r352, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r337], %r349;
st.shared.f32 [%r338], %r352;
	// end inline asm
	add.s64 	%rd155, %rd54, %rd1;
	setp.lt.u64 	%p57, %rd155, %rd54;
	add.s64 	%rd179, %rd66, %rd57;
	setp.gt.u64 	%p58, %rd179, 63;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB24_51;
$L__BB24_50:
	add.s64 	%rd156, %rd179, 1;
	cvt.u32.u64 	%r389, %rd179;
	shl.b32 	%r390, %r389, 4;
	add.s32 	%r363, %r390, %r1;
	add.s32 	%r364, %r363, 4;
	// begin inline asm
	ld.shared.f32 %r370, [%r363];
ld.shared.f32 %r373, [%r364];
	// end inline asm
	add.s32 	%r367, %r363, 8;
	add.s32 	%r368, %r363, 12;
	// begin inline asm
	ld.shared.f32 %r371, [%r367];
ld.shared.f32 %r374, [%r368];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r369, %r370, %r371;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r372, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r363], %r369;
st.shared.f32 [%r364], %r372;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r379, %r370, %r371;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r382, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r367], %r379;
st.shared.f32 [%r368], %r382;
	// end inline asm
	add.s64 	%rd157, %rd156, %rd1;
	setp.lt.u64 	%p60, %rd157, %rd156;
	add.s64 	%rd179, %rd179, %rd57;
	setp.gt.u64 	%p61, %rd179, 63;
	or.pred  	%p62, %p60, %p61;
	@%p62 bra 	$L__BB24_51;
	bra.uni 	$L__BB24_50;
$L__BB24_51:
	ret;
$L__BB24_44:
	and.b32  	%r270, %r11, 992;
	add.s32 	%r271, %r1, %r270;
	add.s32 	%r254, %r271, 8;
	add.s32 	%r255, %r271, 12;
	// begin inline asm
	ld.shared.f32 %r249, [%r254];
ld.shared.f32 %r252, [%r255];
	// end inline asm
	or.b32  	%r272, %r11, 24;
	add.s32 	%r246, %r272, %r1;
	add.s32 	%r247, %r246, 4;
	// begin inline asm
	ld.shared.f32 %r250, [%r246];
ld.shared.f32 %r253, [%r247];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r256, %r249, %r250;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r257, %r252, %r253;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r254], %r256;
st.shared.f32 [%r255], %r257;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r265, %r249, %r250;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r268, %r252, %r253;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r269, %r265;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r246], %r268;
st.shared.f32 [%r247], %r269;
	// end inline asm
$L__BB24_45:
	add.s64 	%rd49, %rd178, 1;
	add.s64 	%rd150, %rd49, %rd1;
	setp.lt.u64 	%p52, %rd150, %rd49;
	add.s64 	%rd178, %rd178, %rd57;
	setp.gt.u64 	%p53, %rd178, 63;
	or.pred  	%p54, %p52, %p53;
	@%p54 bra 	$L__BB24_34;
$L__BB24_42:
	and.b64  	%rd149, %rd178, 1;
	setp.eq.b64 	%p49, %rd149, 1;
	xor.pred  	%p51, %p49, %p40;
	cvt.u32.u64 	%r239, %rd178;
	shl.b32 	%r11, %r239, 4;
	@%p51 bra 	$L__BB24_44;
	add.s32 	%r275, %r11, %r1;
	add.s32 	%r276, %r275, 4;
	// begin inline asm
	ld.shared.f32 %r282, [%r275];
ld.shared.f32 %r285, [%r276];
	// end inline asm
	add.s32 	%r297, %r275, 16;
	add.s32 	%r298, %r275, 20;
	// begin inline asm
	ld.shared.f32 %r283, [%r297];
ld.shared.f32 %r286, [%r298];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r289, %r282, %r283;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r290, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r275], %r289;
st.shared.f32 [%r276], %r290;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r299, %r282, %r283;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r300, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r297], %r299;
st.shared.f32 [%r298], %r300;
	// end inline asm
	bra.uni 	$L__BB24_45;
$L__BB24_54:
	mov.u64 	%rd137, anon_$_9b271fbca47443a0c783d86781d13fba_$_2;
	cvta.global.u64 	%rd138, %rd137;
	{ // callseq 90, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd138;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 90
$L__BB24_53:
	mov.u64 	%rd139, anon_$_9b271fbca47443a0c783d86781d13fba_$_1;
	cvta.global.u64 	%rd140, %rd139;
	{ // callseq 91, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 91
$L__BB24_10:
	mov.u64 	%rd112, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd113, %rd112;
	mov.u64 	%rd114, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd115, %rd114;
	{ // callseq 89, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd113;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd115;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 89
$L__BB24_8:
	mov.u64 	%rd108, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd109, %rd108;
	mov.u64 	%rd110, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd111, %rd110;
	{ // callseq 88, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd109;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd111;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 88
$L__BB24_19:
	mov.u64 	%rd142, anon_$_9b271fbca47443a0c783d86781d13fba_$_10;
	cvta.global.u64 	%rd143, %rd142;
	{ // callseq 92, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd143;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 92
$L__BB24_55:
	mov.u64 	%rd104, anon_$_9b271fbca47443a0c783d86781d13fba_$_1;
	cvta.global.u64 	%rd105, %rd104;
	{ // callseq 87, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd105;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 87
$L__BB24_56:
	mov.u64 	%rd102, anon_$_9b271fbca47443a0c783d86781d13fba_$_2;
	cvta.global.u64 	%rd103, %rd102;
	{ // callseq 86, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd103;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 86
$L__BB24_30:
	mov.u64 	%rd144, anon_$_9b271fbca47443a0c783d86781d13fba_$_10;
	cvta.global.u64 	%rd145, %rd144;
	{ // callseq 93, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd145;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 93
$L__BB24_4:
	mov.u64 	%rd73, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd74, %rd73;
	mov.u64 	%rd75, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd76, %rd75;
	{ // callseq 84, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd74;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd76;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 84
$L__BB24_21:
	mov.u64 	%rd77, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd78, %rd77;
	mov.u64 	%rd79, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd80, %rd79;
	{ // callseq 85, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd80;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 85
$L__BB24_5:
	mov.u64 	%rd158, anon_$_9b271fbca47443a0c783d86781d13fba_$_5;
	cvta.global.u64 	%rd159, %rd158;
	mov.u64 	%rd160, anon_$_9b271fbca47443a0c783d86781d13fba_$_6;
	cvta.global.u64 	%rd161, %rd160;
	{ // callseq 95, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd159;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd161;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 95
$L__BB24_52:
	mov.u64 	%rd151, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd152, %rd151;
	mov.u64 	%rd153, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd154, %rd153;
	{ // callseq 94, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd152;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd154;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 94

}
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_3
)
{
	.reg .pred 	%p<63>;
	.reg .b32 	%r<399>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<180>;

	ld.param.u64 	%rd57, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_2];
	setp.eq.s64 	%p8, %rd57, 0;
	@%p8 bra 	$L__BB25_5;
	ld.param.u64 	%rd56, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_1];
	ld.param.u64 	%rd58, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_0];
	add.s64 	%rd1, %rd57, -1;
	setp.gt.u64 	%p9, %rd56, 2047;
	ld.param.u64 	%rd59, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1b609a61a1f86893E_param_3];
	add.s64 	%rd60, %rd56, 1;
	selp.b64 	%rd61, 2048, %rd60, %p9;
	ld.u64 	%rd62, [%rd58];
	ld.u64 	%rd63, [%rd58+8];
	ld.u32 	%r1, [%rd59];
	shl.b64 	%rd64, %rd63, 3;
	add.s64 	%rd2, %rd62, %rd64;
	setp.eq.s64 	%p10, %rd63, 0;
	selp.b64 	%rd3, 0, %rd62, %p10;
	selp.b64 	%rd65, 0, 8, %p10;
	add.s64 	%rd4, %rd62, %rd65;
	add.s64 	%rd66, %rd61, %rd1;
	setp.lt.u64 	%p11, %rd66, %rd61;
	setp.gt.u64 	%p12, %rd66, 2047;
	or.pred  	%p1, %p11, %p12;
	mov.u64 	%rd7, 1;
	mov.u64 	%rd8, 12;
	bra.uni 	$L__BB25_2;
$L__BB25_32:
	shl.b64 	%rd7, %rd7, 1;
	bar.sync 	0;
	setp.gt.u64 	%p37, %rd8, 2;
	@%p37 bra 	$L__BB25_2;
	bra.uni 	$L__BB25_33;
$L__BB25_2:
	add.s64 	%rd8, %rd8, -1;
	@%p9 bra 	$L__BB25_32;
	cvt.u32.u64 	%r13, %rd8;
	shr.u64 	%rd69, %rd56, %r13;
	mov.u64 	%rd70, 1;
	shl.b64 	%rd27, %rd70, %r13;
	mov.u64 	%rd71, 2;
	shl.b64 	%rd28, %rd71, %r13;
	add.s64 	%rd29, %rd27, -1;
	and.b64  	%rd172, %rd29, %rd56;
	mul.lo.s64 	%rd72, %rd69, %rd28;
	add.s64 	%rd31, %rd172, %rd72;
	setp.lt.u64 	%p14, %rd31, 4096;
	@%p14 bra 	$L__BB25_20;
	bra.uni 	$L__BB25_4;
$L__BB25_20:
	cvt.u32.u64 	%r18, %rd31;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r16, %r1, %r19;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	ld.shared.f32 %r14, [%r16];
ld.shared.f32 %r15, [%r17];
	// end inline asm
	add.s64 	%rd32, %rd31, %rd27;
	setp.lt.u64 	%p15, %rd32, 4096;
	@%p15 bra 	$L__BB25_22;
	bra.uni 	$L__BB25_21;
$L__BB25_22:
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r22, %r1, %r25;
	add.s32 	%r23, %r22, 4;
	// begin inline asm
	ld.shared.f32 %r20, [%r22];
ld.shared.f32 %r21, [%r23];
	// end inline asm
	setp.eq.s64 	%p16, %rd172, 0;
	mov.u64 	%rd162, -1;
	mov.u64 	%rd177, %rd3;
	@%p16 bra 	$L__BB25_29;
	mul.lo.s64 	%rd176, %rd172, %rd7;
	mul.hi.u64 	%rd81, %rd172, %rd7;
	setp.eq.s64 	%p17, %rd81, 0;
	mov.u64 	%rd174, %rd4;
	@%p17 bra 	$L__BB25_28;
	mov.u64 	%rd173, %rd7;
	mov.u64 	%rd174, %rd4;
$L__BB25_25:
	setp.eq.s64 	%p18, %rd172, 0;
	@%p18 bra 	$L__BB25_55;
	setp.eq.s64 	%p19, %rd173, 0;
	@%p19 bra 	$L__BB25_56;
	div.u64 	%rd83, %rd162, %rd172;
	div.u64 	%rd84, %rd162, %rd173;
	mul.lo.s64 	%rd85, %rd83, %rd172;
	mul.lo.s64 	%rd86, %rd84, %rd173;
	setp.gt.u64 	%p20, %rd85, %rd86;
	max.u64 	%rd87, %rd85, %rd86;
	selp.b64 	%rd88, %rd83, 0, %p20;
	sub.s64 	%rd173, %rd173, %rd88;
	selp.b64 	%rd89, 0, %rd84, %p20;
	sub.s64 	%rd172, %rd172, %rd89;
	add.s64 	%rd90, %rd87, -1;
	sub.s64 	%rd91, %rd2, %rd174;
	shr.u64 	%rd92, %rd91, 3;
	setp.gt.u64 	%p21, %rd92, %rd90;
	shl.b64 	%rd93, %rd87, 3;
	add.s64 	%rd94, %rd174, %rd93;
	selp.b64 	%rd174, %rd94, %rd2, %p21;
	mul.lo.s64 	%rd176, %rd172, %rd173;
	mul.hi.u64 	%rd95, %rd172, %rd173;
	setp.ne.s64 	%p6, %rd95, 0;
	@%p6 bra 	$L__BB25_25;
$L__BB25_28:
	add.s64 	%rd96, %rd176, -1;
	sub.s64 	%rd97, %rd2, %rd174;
	shr.u64 	%rd98, %rd97, 3;
	setp.gt.u64 	%p22, %rd98, %rd96;
	shl.b64 	%rd99, %rd176, 3;
	add.s64 	%rd100, %rd174, %rd99;
	add.s64 	%rd101, %rd100, -8;
	selp.b64 	%rd177, %rd101, 0, %p22;
$L__BB25_29:
	setp.ne.s64 	%p23, %rd177, 0;
	@%p23 bra 	$L__BB25_31;
	bra.uni 	$L__BB25_30;
$L__BB25_31:
	mov.b32 	%f6, %r15;
	mov.b32 	%f5, %r14;
	mov.b32 	%f8, %r21;
	mov.b32 	%f7, %r20;
	// begin inline asm
	add.rn.ftz.f32 %r26, %r14, %r20;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r29, %r15, %r21;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r16], %r26;
st.shared.f32 [%r17], %r29;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r43, %r14, %r20;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r46, %r15, %r21;
	// end inline asm
	ld.u32 	%r56, [%rd177];
	ld.u32 	%r53, [%rd177+4];
	// begin inline asm
	mul.rn.ftz.f32 %r42, %r43, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r46, %r53;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r48, %r42, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r51, %r43, %r53;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r54, %r46, %r56;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r57, %r51, %r54;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r22], %r48;
st.shared.f32 [%r23], %r57;
	// end inline asm
	mov.u64 	%rd9, %rd66;
	@%p1 bra 	$L__BB25_32;
$L__BB25_7:
	shr.u64 	%rd106, %rd9, %r13;
	and.b64  	%rd166, %rd9, %rd29;
	mul.lo.s64 	%rd107, %rd106, %rd28;
	add.s64 	%rd12, %rd107, %rd166;
	setp.lt.u64 	%p24, %rd12, 4096;
	@%p24 bra 	$L__BB25_9;
	bra.uni 	$L__BB25_8;
$L__BB25_9:
	cvt.u32.u64 	%r69, %rd12;
	shl.b32 	%r70, %r69, 3;
	add.s32 	%r67, %r70, %r1;
	add.s32 	%r68, %r67, 4;
	// begin inline asm
	ld.shared.f32 %r65, [%r67];
ld.shared.f32 %r66, [%r68];
	// end inline asm
	add.s64 	%rd13, %rd12, %rd27;
	setp.lt.u64 	%p25, %rd13, 4096;
	@%p25 bra 	$L__BB25_11;
	bra.uni 	$L__BB25_10;
$L__BB25_11:
	cvt.u32.u64 	%r75, %rd13;
	shl.b32 	%r76, %r75, 3;
	add.s32 	%r73, %r76, %r1;
	add.s32 	%r74, %r73, 4;
	// begin inline asm
	ld.shared.f32 %r71, [%r73];
ld.shared.f32 %r72, [%r74];
	// end inline asm
	setp.eq.s64 	%p26, %rd166, 0;
	mov.u64 	%rd171, %rd3;
	@%p26 bra 	$L__BB25_18;
	mul.lo.s64 	%rd170, %rd166, %rd7;
	mul.hi.u64 	%rd116, %rd166, %rd7;
	setp.eq.s64 	%p27, %rd116, 0;
	mov.u64 	%rd168, %rd4;
	@%p27 bra 	$L__BB25_17;
	mov.u64 	%rd167, %rd7;
	mov.u64 	%rd168, %rd4;
$L__BB25_14:
	setp.eq.s64 	%p28, %rd166, 0;
	@%p28 bra 	$L__BB25_53;
	setp.eq.s64 	%p29, %rd167, 0;
	@%p29 bra 	$L__BB25_54;
	div.u64 	%rd118, %rd162, %rd166;
	div.u64 	%rd119, %rd162, %rd167;
	mul.lo.s64 	%rd120, %rd118, %rd166;
	mul.lo.s64 	%rd121, %rd119, %rd167;
	setp.gt.u64 	%p30, %rd120, %rd121;
	max.u64 	%rd122, %rd120, %rd121;
	selp.b64 	%rd123, %rd118, 0, %p30;
	sub.s64 	%rd167, %rd167, %rd123;
	selp.b64 	%rd124, 0, %rd119, %p30;
	sub.s64 	%rd166, %rd166, %rd124;
	add.s64 	%rd125, %rd122, -1;
	sub.s64 	%rd126, %rd2, %rd168;
	shr.u64 	%rd127, %rd126, 3;
	setp.gt.u64 	%p31, %rd127, %rd125;
	shl.b64 	%rd128, %rd122, 3;
	add.s64 	%rd129, %rd168, %rd128;
	selp.b64 	%rd168, %rd129, %rd2, %p31;
	mul.lo.s64 	%rd170, %rd166, %rd167;
	mul.hi.u64 	%rd130, %rd166, %rd167;
	setp.ne.s64 	%p3, %rd130, 0;
	@%p3 bra 	$L__BB25_14;
$L__BB25_17:
	add.s64 	%rd131, %rd170, -1;
	sub.s64 	%rd132, %rd2, %rd168;
	shr.u64 	%rd133, %rd132, 3;
	setp.gt.u64 	%p32, %rd133, %rd131;
	shl.b64 	%rd134, %rd170, 3;
	add.s64 	%rd135, %rd168, %rd134;
	add.s64 	%rd136, %rd135, -8;
	selp.b64 	%rd171, %rd136, 0, %p32;
$L__BB25_18:
	setp.ne.s64 	%p33, %rd171, 0;
	@%p33 bra 	$L__BB25_6;
	bra.uni 	$L__BB25_19;
$L__BB25_6:
	add.s64 	%rd10, %rd9, 1;
	mov.b32 	%f2, %r66;
	mov.b32 	%f1, %r65;
	mov.b32 	%f4, %r72;
	mov.b32 	%f3, %r71;
	// begin inline asm
	add.rn.ftz.f32 %r77, %r65, %r71;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r66, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r67], %r77;
st.shared.f32 [%r68], %r80;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r94, %r65, %r71;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r97, %r66, %r72;
	// end inline asm
	ld.u32 	%r107, [%rd171];
	ld.u32 	%r104, [%rd171+4];
	// begin inline asm
	mul.rn.ftz.f32 %r93, %r94, %r107;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r96, %r97, %r104;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r99, %r93, %r96;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r102, %r94, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r105, %r97, %r107;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r108, %r102, %r105;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r73], %r99;
st.shared.f32 [%r74], %r108;
	// end inline asm
	add.s64 	%rd141, %rd10, %rd1;
	setp.lt.u64 	%p34, %rd141, %rd10;
	add.s64 	%rd9, %rd9, %rd57;
	setp.gt.u64 	%p35, %rd9, 2047;
	or.pred  	%p36, %p34, %p35;
	@%p36 bra 	$L__BB25_32;
	bra.uni 	$L__BB25_7;
$L__BB25_33:
	@%p9 bra 	$L__BB25_34;
	and.b64  	%rd146, %rd56, 1;
	setp.eq.b64 	%p39, %rd146, 1;
	mov.pred 	%p40, 0;
	xor.pred  	%p41, %p39, %p40;
	cvt.u32.u64 	%r115, %rd56;
	shl.b32 	%r12, %r115, 4;
	@%p41 bra 	$L__BB25_36;
	add.s32 	%r151, %r1, %r12;
	add.s32 	%r152, %r151, 4;
	// begin inline asm
	ld.shared.f32 %r158, [%r151];
ld.shared.f32 %r161, [%r152];
	// end inline asm
	add.s32 	%r173, %r151, 16;
	add.s32 	%r174, %r151, 20;
	// begin inline asm
	ld.shared.f32 %r159, [%r173];
ld.shared.f32 %r162, [%r174];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r165, %r158, %r159;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r166, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r151], %r165;
st.shared.f32 [%r152], %r166;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r175, %r158, %r159;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r176, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r173], %r175;
st.shared.f32 [%r174], %r176;
	// end inline asm
	bra.uni 	$L__BB25_37;
$L__BB25_36:
	and.b32  	%r146, %r12, 32736;
	add.s32 	%r147, %r1, %r146;
	add.s32 	%r130, %r147, 8;
	add.s32 	%r131, %r147, 12;
	// begin inline asm
	ld.shared.f32 %r125, [%r130];
ld.shared.f32 %r128, [%r131];
	// end inline asm
	or.b32  	%r148, %r12, 24;
	add.s32 	%r122, %r1, %r148;
	add.s32 	%r123, %r122, 4;
	// begin inline asm
	ld.shared.f32 %r126, [%r122];
ld.shared.f32 %r129, [%r123];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r132, %r125, %r126;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r133, %r128, %r129;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r130], %r132;
st.shared.f32 [%r131], %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r125, %r126;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r128, %r129;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r145, %r141;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r122], %r144;
st.shared.f32 [%r123], %r145;
	// end inline asm
$L__BB25_37:
	@%p1 bra 	$L__BB25_34;
	add.s64 	%rd46, %rd66, 1;
	and.b64  	%rd147, %rd66, 1;
	setp.eq.b64 	%p42, %rd147, 1;
	xor.pred  	%p44, %p42, %p40;
	not.pred 	%p45, %p44;
	cvt.u32.u64 	%r177, %rd66;
	shl.b32 	%r10, %r177, 4;
	@%p45 bra 	$L__BB25_40;
	bra.uni 	$L__BB25_39;
$L__BB25_40:
	add.s32 	%r213, %r1, %r10;
	add.s32 	%r214, %r213, 4;
	// begin inline asm
	ld.shared.f32 %r220, [%r213];
ld.shared.f32 %r223, [%r214];
	// end inline asm
	add.s32 	%r235, %r213, 16;
	add.s32 	%r236, %r213, 20;
	// begin inline asm
	ld.shared.f32 %r221, [%r235];
ld.shared.f32 %r224, [%r236];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r227, %r220, %r221;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r228, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r213], %r227;
st.shared.f32 [%r214], %r228;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r237, %r220, %r221;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r238, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r235], %r237;
st.shared.f32 [%r236], %r238;
	// end inline asm
	bra.uni 	$L__BB25_41;
$L__BB25_39:
	and.b32  	%r208, %r10, 32736;
	add.s32 	%r209, %r1, %r208;
	add.s32 	%r192, %r209, 8;
	add.s32 	%r193, %r209, 12;
	// begin inline asm
	ld.shared.f32 %r187, [%r192];
ld.shared.f32 %r190, [%r193];
	// end inline asm
	or.b32  	%r210, %r10, 24;
	add.s32 	%r184, %r1, %r210;
	add.s32 	%r185, %r184, 4;
	// begin inline asm
	ld.shared.f32 %r188, [%r184];
ld.shared.f32 %r191, [%r185];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r194, %r187, %r188;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r195, %r190, %r191;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r192], %r194;
st.shared.f32 [%r193], %r195;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r203, %r187, %r188;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r206, %r190, %r191;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r207, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r184], %r206;
st.shared.f32 [%r185], %r207;
	// end inline asm
$L__BB25_41:
	add.s64 	%rd148, %rd46, %rd1;
	setp.lt.u64 	%p46, %rd148, %rd46;
	add.s64 	%rd178, %rd66, %rd57;
	setp.gt.u64 	%p47, %rd178, 2047;
	or.pred  	%p48, %p46, %p47;
	@%p48 bra 	$L__BB25_34;
	bra.uni 	$L__BB25_42;
$L__BB25_34:
	bar.sync 	0;
	@%p9 bra 	$L__BB25_51;
	cvt.u32.u64 	%r329, %rd56;
	shl.b32 	%r330, %r329, 4;
	add.s32 	%r303, %r1, %r330;
	add.s32 	%r304, %r303, 4;
	// begin inline asm
	ld.shared.f32 %r310, [%r303];
ld.shared.f32 %r313, [%r304];
	// end inline asm
	add.s32 	%r307, %r303, 8;
	add.s32 	%r308, %r303, 12;
	// begin inline asm
	ld.shared.f32 %r311, [%r307];
ld.shared.f32 %r314, [%r308];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r309, %r310, %r311;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r312, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r303], %r309;
st.shared.f32 [%r304], %r312;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r319, %r310, %r311;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r322, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r307], %r319;
st.shared.f32 [%r308], %r322;
	// end inline asm
	@!%p1 bra 	$L__BB25_48;
	bra.uni 	$L__BB25_51;
$L__BB25_48:
	shl.b64 	%rd55, %rd66, 1;
	setp.gt.u64 	%p56, %rd55, 4095;
	@%p56 bra 	$L__BB25_52;
	add.s64 	%rd54, %rd66, 1;
	cvt.u32.u64 	%r359, %rd55;
	shl.b32 	%r360, %r359, 3;
	add.s32 	%r333, %r1, %r360;
	add.s32 	%r334, %r333, 4;
	// begin inline asm
	ld.shared.f32 %r340, [%r333];
ld.shared.f32 %r343, [%r334];
	// end inline asm
	add.s32 	%r337, %r333, 8;
	add.s32 	%r338, %r333, 12;
	// begin inline asm
	ld.shared.f32 %r341, [%r337];
ld.shared.f32 %r344, [%r338];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r339, %r340, %r341;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r342, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r333], %r339;
st.shared.f32 [%r334], %r342;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r349, %r340, %r341;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r352, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r337], %r349;
st.shared.f32 [%r338], %r352;
	// end inline asm
	add.s64 	%rd155, %rd54, %rd1;
	setp.lt.u64 	%p57, %rd155, %rd54;
	add.s64 	%rd179, %rd66, %rd57;
	setp.gt.u64 	%p58, %rd179, 2047;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB25_51;
$L__BB25_50:
	add.s64 	%rd156, %rd179, 1;
	cvt.u32.u64 	%r389, %rd179;
	shl.b32 	%r390, %r389, 4;
	add.s32 	%r363, %r390, %r1;
	add.s32 	%r364, %r363, 4;
	// begin inline asm
	ld.shared.f32 %r370, [%r363];
ld.shared.f32 %r373, [%r364];
	// end inline asm
	add.s32 	%r367, %r363, 8;
	add.s32 	%r368, %r363, 12;
	// begin inline asm
	ld.shared.f32 %r371, [%r367];
ld.shared.f32 %r374, [%r368];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r369, %r370, %r371;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r372, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r363], %r369;
st.shared.f32 [%r364], %r372;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r379, %r370, %r371;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r382, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r367], %r379;
st.shared.f32 [%r368], %r382;
	// end inline asm
	add.s64 	%rd157, %rd156, %rd1;
	setp.lt.u64 	%p60, %rd157, %rd156;
	add.s64 	%rd179, %rd179, %rd57;
	setp.gt.u64 	%p61, %rd179, 2047;
	or.pred  	%p62, %p60, %p61;
	@%p62 bra 	$L__BB25_51;
	bra.uni 	$L__BB25_50;
$L__BB25_51:
	ret;
$L__BB25_44:
	and.b32  	%r270, %r11, 32736;
	add.s32 	%r271, %r1, %r270;
	add.s32 	%r254, %r271, 8;
	add.s32 	%r255, %r271, 12;
	// begin inline asm
	ld.shared.f32 %r249, [%r254];
ld.shared.f32 %r252, [%r255];
	// end inline asm
	or.b32  	%r272, %r11, 24;
	add.s32 	%r246, %r272, %r1;
	add.s32 	%r247, %r246, 4;
	// begin inline asm
	ld.shared.f32 %r250, [%r246];
ld.shared.f32 %r253, [%r247];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r256, %r249, %r250;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r257, %r252, %r253;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r254], %r256;
st.shared.f32 [%r255], %r257;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r265, %r249, %r250;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r268, %r252, %r253;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r269, %r265;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r246], %r268;
st.shared.f32 [%r247], %r269;
	// end inline asm
$L__BB25_45:
	add.s64 	%rd49, %rd178, 1;
	add.s64 	%rd150, %rd49, %rd1;
	setp.lt.u64 	%p52, %rd150, %rd49;
	add.s64 	%rd178, %rd178, %rd57;
	setp.gt.u64 	%p53, %rd178, 2047;
	or.pred  	%p54, %p52, %p53;
	@%p54 bra 	$L__BB25_34;
$L__BB25_42:
	and.b64  	%rd149, %rd178, 1;
	setp.eq.b64 	%p49, %rd149, 1;
	xor.pred  	%p51, %p49, %p40;
	cvt.u32.u64 	%r239, %rd178;
	shl.b32 	%r11, %r239, 4;
	@%p51 bra 	$L__BB25_44;
	add.s32 	%r275, %r11, %r1;
	add.s32 	%r276, %r275, 4;
	// begin inline asm
	ld.shared.f32 %r282, [%r275];
ld.shared.f32 %r285, [%r276];
	// end inline asm
	add.s32 	%r297, %r275, 16;
	add.s32 	%r298, %r275, 20;
	// begin inline asm
	ld.shared.f32 %r283, [%r297];
ld.shared.f32 %r286, [%r298];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r289, %r282, %r283;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r290, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r275], %r289;
st.shared.f32 [%r276], %r290;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r299, %r282, %r283;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r300, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r297], %r299;
st.shared.f32 [%r298], %r300;
	// end inline asm
	bra.uni 	$L__BB25_45;
$L__BB25_54:
	mov.u64 	%rd137, anon_$_9b271fbca47443a0c783d86781d13fba_$_2;
	cvta.global.u64 	%rd138, %rd137;
	{ // callseq 102, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd138;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 102
$L__BB25_53:
	mov.u64 	%rd139, anon_$_9b271fbca47443a0c783d86781d13fba_$_1;
	cvta.global.u64 	%rd140, %rd139;
	{ // callseq 103, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 103
$L__BB25_10:
	mov.u64 	%rd112, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd113, %rd112;
	mov.u64 	%rd114, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd115, %rd114;
	{ // callseq 101, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd113;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd115;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 101
$L__BB25_8:
	mov.u64 	%rd108, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd109, %rd108;
	mov.u64 	%rd110, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd111, %rd110;
	{ // callseq 100, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd109;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd111;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 100
$L__BB25_19:
	mov.u64 	%rd142, anon_$_9b271fbca47443a0c783d86781d13fba_$_10;
	cvta.global.u64 	%rd143, %rd142;
	{ // callseq 104, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd143;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 104
$L__BB25_55:
	mov.u64 	%rd104, anon_$_9b271fbca47443a0c783d86781d13fba_$_1;
	cvta.global.u64 	%rd105, %rd104;
	{ // callseq 99, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd105;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 99
$L__BB25_56:
	mov.u64 	%rd102, anon_$_9b271fbca47443a0c783d86781d13fba_$_2;
	cvta.global.u64 	%rd103, %rd102;
	{ // callseq 98, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd103;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 98
$L__BB25_30:
	mov.u64 	%rd144, anon_$_9b271fbca47443a0c783d86781d13fba_$_10;
	cvta.global.u64 	%rd145, %rd144;
	{ // callseq 105, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd145;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 105
$L__BB25_4:
	mov.u64 	%rd73, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd74, %rd73;
	mov.u64 	%rd75, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd76, %rd75;
	{ // callseq 96, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd74;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd76;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 96
$L__BB25_21:
	mov.u64 	%rd77, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd78, %rd77;
	mov.u64 	%rd79, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd80, %rd79;
	{ // callseq 97, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd80;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 97
$L__BB25_5:
	mov.u64 	%rd158, anon_$_9b271fbca47443a0c783d86781d13fba_$_5;
	cvta.global.u64 	%rd159, %rd158;
	mov.u64 	%rd160, anon_$_9b271fbca47443a0c783d86781d13fba_$_6;
	cvta.global.u64 	%rd161, %rd160;
	{ // callseq 107, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd159;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd161;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 107
$L__BB25_52:
	mov.u64 	%rd151, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd152, %rd151;
	mov.u64 	%rd153, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd154, %rd153;
	{ // callseq 106, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd152;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd154;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 106

}
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_3
)
{
	.reg .pred 	%p<63>;
	.reg .b32 	%r<399>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<180>;

	ld.param.u64 	%rd57, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_2];
	setp.eq.s64 	%p8, %rd57, 0;
	@%p8 bra 	$L__BB26_5;
	ld.param.u64 	%rd56, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_1];
	ld.param.u64 	%rd58, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_0];
	add.s64 	%rd1, %rd57, -1;
	setp.gt.u64 	%p9, %rd56, 1023;
	ld.param.u64 	%rd59, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h1df8ba6c711100c2E_param_3];
	add.s64 	%rd60, %rd56, 1;
	selp.b64 	%rd61, 1024, %rd60, %p9;
	ld.u64 	%rd62, [%rd58];
	ld.u64 	%rd63, [%rd58+8];
	ld.u32 	%r1, [%rd59];
	shl.b64 	%rd64, %rd63, 3;
	add.s64 	%rd2, %rd62, %rd64;
	setp.eq.s64 	%p10, %rd63, 0;
	selp.b64 	%rd3, 0, %rd62, %p10;
	selp.b64 	%rd65, 0, 8, %p10;
	add.s64 	%rd4, %rd62, %rd65;
	add.s64 	%rd66, %rd61, %rd1;
	setp.lt.u64 	%p11, %rd66, %rd61;
	setp.gt.u64 	%p12, %rd66, 1023;
	or.pred  	%p1, %p11, %p12;
	mov.u64 	%rd7, 1;
	mov.u64 	%rd8, 11;
	bra.uni 	$L__BB26_2;
$L__BB26_32:
	shl.b64 	%rd7, %rd7, 1;
	bar.sync 	0;
	setp.gt.u64 	%p37, %rd8, 2;
	@%p37 bra 	$L__BB26_2;
	bra.uni 	$L__BB26_33;
$L__BB26_2:
	add.s64 	%rd8, %rd8, -1;
	@%p9 bra 	$L__BB26_32;
	cvt.u32.u64 	%r13, %rd8;
	shr.u64 	%rd69, %rd56, %r13;
	mov.u64 	%rd70, 1;
	shl.b64 	%rd27, %rd70, %r13;
	mov.u64 	%rd71, 2;
	shl.b64 	%rd28, %rd71, %r13;
	add.s64 	%rd29, %rd27, -1;
	and.b64  	%rd172, %rd29, %rd56;
	mul.lo.s64 	%rd72, %rd69, %rd28;
	add.s64 	%rd31, %rd172, %rd72;
	setp.lt.u64 	%p14, %rd31, 2048;
	@%p14 bra 	$L__BB26_20;
	bra.uni 	$L__BB26_4;
$L__BB26_20:
	cvt.u32.u64 	%r18, %rd31;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r16, %r1, %r19;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	ld.shared.f32 %r14, [%r16];
ld.shared.f32 %r15, [%r17];
	// end inline asm
	add.s64 	%rd32, %rd31, %rd27;
	setp.lt.u64 	%p15, %rd32, 2048;
	@%p15 bra 	$L__BB26_22;
	bra.uni 	$L__BB26_21;
$L__BB26_22:
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r22, %r1, %r25;
	add.s32 	%r23, %r22, 4;
	// begin inline asm
	ld.shared.f32 %r20, [%r22];
ld.shared.f32 %r21, [%r23];
	// end inline asm
	setp.eq.s64 	%p16, %rd172, 0;
	mov.u64 	%rd162, -1;
	mov.u64 	%rd177, %rd3;
	@%p16 bra 	$L__BB26_29;
	mul.lo.s64 	%rd176, %rd172, %rd7;
	mul.hi.u64 	%rd81, %rd172, %rd7;
	setp.eq.s64 	%p17, %rd81, 0;
	mov.u64 	%rd174, %rd4;
	@%p17 bra 	$L__BB26_28;
	mov.u64 	%rd173, %rd7;
	mov.u64 	%rd174, %rd4;
$L__BB26_25:
	setp.eq.s64 	%p18, %rd172, 0;
	@%p18 bra 	$L__BB26_55;
	setp.eq.s64 	%p19, %rd173, 0;
	@%p19 bra 	$L__BB26_56;
	div.u64 	%rd83, %rd162, %rd172;
	div.u64 	%rd84, %rd162, %rd173;
	mul.lo.s64 	%rd85, %rd83, %rd172;
	mul.lo.s64 	%rd86, %rd84, %rd173;
	setp.gt.u64 	%p20, %rd85, %rd86;
	max.u64 	%rd87, %rd85, %rd86;
	selp.b64 	%rd88, %rd83, 0, %p20;
	sub.s64 	%rd173, %rd173, %rd88;
	selp.b64 	%rd89, 0, %rd84, %p20;
	sub.s64 	%rd172, %rd172, %rd89;
	add.s64 	%rd90, %rd87, -1;
	sub.s64 	%rd91, %rd2, %rd174;
	shr.u64 	%rd92, %rd91, 3;
	setp.gt.u64 	%p21, %rd92, %rd90;
	shl.b64 	%rd93, %rd87, 3;
	add.s64 	%rd94, %rd174, %rd93;
	selp.b64 	%rd174, %rd94, %rd2, %p21;
	mul.lo.s64 	%rd176, %rd172, %rd173;
	mul.hi.u64 	%rd95, %rd172, %rd173;
	setp.ne.s64 	%p6, %rd95, 0;
	@%p6 bra 	$L__BB26_25;
$L__BB26_28:
	add.s64 	%rd96, %rd176, -1;
	sub.s64 	%rd97, %rd2, %rd174;
	shr.u64 	%rd98, %rd97, 3;
	setp.gt.u64 	%p22, %rd98, %rd96;
	shl.b64 	%rd99, %rd176, 3;
	add.s64 	%rd100, %rd174, %rd99;
	add.s64 	%rd101, %rd100, -8;
	selp.b64 	%rd177, %rd101, 0, %p22;
$L__BB26_29:
	setp.ne.s64 	%p23, %rd177, 0;
	@%p23 bra 	$L__BB26_31;
	bra.uni 	$L__BB26_30;
$L__BB26_31:
	mov.b32 	%f6, %r15;
	mov.b32 	%f5, %r14;
	mov.b32 	%f8, %r21;
	mov.b32 	%f7, %r20;
	// begin inline asm
	add.rn.ftz.f32 %r26, %r14, %r20;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r29, %r15, %r21;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r16], %r26;
st.shared.f32 [%r17], %r29;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r43, %r14, %r20;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r46, %r15, %r21;
	// end inline asm
	ld.u32 	%r56, [%rd177];
	ld.u32 	%r53, [%rd177+4];
	// begin inline asm
	mul.rn.ftz.f32 %r42, %r43, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r46, %r53;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r48, %r42, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r51, %r43, %r53;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r54, %r46, %r56;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r57, %r51, %r54;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r22], %r48;
st.shared.f32 [%r23], %r57;
	// end inline asm
	mov.u64 	%rd9, %rd66;
	@%p1 bra 	$L__BB26_32;
$L__BB26_7:
	shr.u64 	%rd106, %rd9, %r13;
	and.b64  	%rd166, %rd9, %rd29;
	mul.lo.s64 	%rd107, %rd106, %rd28;
	add.s64 	%rd12, %rd107, %rd166;
	setp.lt.u64 	%p24, %rd12, 2048;
	@%p24 bra 	$L__BB26_9;
	bra.uni 	$L__BB26_8;
$L__BB26_9:
	cvt.u32.u64 	%r69, %rd12;
	shl.b32 	%r70, %r69, 3;
	add.s32 	%r67, %r70, %r1;
	add.s32 	%r68, %r67, 4;
	// begin inline asm
	ld.shared.f32 %r65, [%r67];
ld.shared.f32 %r66, [%r68];
	// end inline asm
	add.s64 	%rd13, %rd12, %rd27;
	setp.lt.u64 	%p25, %rd13, 2048;
	@%p25 bra 	$L__BB26_11;
	bra.uni 	$L__BB26_10;
$L__BB26_11:
	cvt.u32.u64 	%r75, %rd13;
	shl.b32 	%r76, %r75, 3;
	add.s32 	%r73, %r76, %r1;
	add.s32 	%r74, %r73, 4;
	// begin inline asm
	ld.shared.f32 %r71, [%r73];
ld.shared.f32 %r72, [%r74];
	// end inline asm
	setp.eq.s64 	%p26, %rd166, 0;
	mov.u64 	%rd171, %rd3;
	@%p26 bra 	$L__BB26_18;
	mul.lo.s64 	%rd170, %rd166, %rd7;
	mul.hi.u64 	%rd116, %rd166, %rd7;
	setp.eq.s64 	%p27, %rd116, 0;
	mov.u64 	%rd168, %rd4;
	@%p27 bra 	$L__BB26_17;
	mov.u64 	%rd167, %rd7;
	mov.u64 	%rd168, %rd4;
$L__BB26_14:
	setp.eq.s64 	%p28, %rd166, 0;
	@%p28 bra 	$L__BB26_53;
	setp.eq.s64 	%p29, %rd167, 0;
	@%p29 bra 	$L__BB26_54;
	div.u64 	%rd118, %rd162, %rd166;
	div.u64 	%rd119, %rd162, %rd167;
	mul.lo.s64 	%rd120, %rd118, %rd166;
	mul.lo.s64 	%rd121, %rd119, %rd167;
	setp.gt.u64 	%p30, %rd120, %rd121;
	max.u64 	%rd122, %rd120, %rd121;
	selp.b64 	%rd123, %rd118, 0, %p30;
	sub.s64 	%rd167, %rd167, %rd123;
	selp.b64 	%rd124, 0, %rd119, %p30;
	sub.s64 	%rd166, %rd166, %rd124;
	add.s64 	%rd125, %rd122, -1;
	sub.s64 	%rd126, %rd2, %rd168;
	shr.u64 	%rd127, %rd126, 3;
	setp.gt.u64 	%p31, %rd127, %rd125;
	shl.b64 	%rd128, %rd122, 3;
	add.s64 	%rd129, %rd168, %rd128;
	selp.b64 	%rd168, %rd129, %rd2, %p31;
	mul.lo.s64 	%rd170, %rd166, %rd167;
	mul.hi.u64 	%rd130, %rd166, %rd167;
	setp.ne.s64 	%p3, %rd130, 0;
	@%p3 bra 	$L__BB26_14;
$L__BB26_17:
	add.s64 	%rd131, %rd170, -1;
	sub.s64 	%rd132, %rd2, %rd168;
	shr.u64 	%rd133, %rd132, 3;
	setp.gt.u64 	%p32, %rd133, %rd131;
	shl.b64 	%rd134, %rd170, 3;
	add.s64 	%rd135, %rd168, %rd134;
	add.s64 	%rd136, %rd135, -8;
	selp.b64 	%rd171, %rd136, 0, %p32;
$L__BB26_18:
	setp.ne.s64 	%p33, %rd171, 0;
	@%p33 bra 	$L__BB26_6;
	bra.uni 	$L__BB26_19;
$L__BB26_6:
	add.s64 	%rd10, %rd9, 1;
	mov.b32 	%f2, %r66;
	mov.b32 	%f1, %r65;
	mov.b32 	%f4, %r72;
	mov.b32 	%f3, %r71;
	// begin inline asm
	add.rn.ftz.f32 %r77, %r65, %r71;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r66, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r67], %r77;
st.shared.f32 [%r68], %r80;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r94, %r65, %r71;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r97, %r66, %r72;
	// end inline asm
	ld.u32 	%r107, [%rd171];
	ld.u32 	%r104, [%rd171+4];
	// begin inline asm
	mul.rn.ftz.f32 %r93, %r94, %r107;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r96, %r97, %r104;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r99, %r93, %r96;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r102, %r94, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r105, %r97, %r107;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r108, %r102, %r105;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r73], %r99;
st.shared.f32 [%r74], %r108;
	// end inline asm
	add.s64 	%rd141, %rd10, %rd1;
	setp.lt.u64 	%p34, %rd141, %rd10;
	add.s64 	%rd9, %rd9, %rd57;
	setp.gt.u64 	%p35, %rd9, 1023;
	or.pred  	%p36, %p34, %p35;
	@%p36 bra 	$L__BB26_32;
	bra.uni 	$L__BB26_7;
$L__BB26_33:
	@%p9 bra 	$L__BB26_34;
	and.b64  	%rd146, %rd56, 1;
	setp.eq.b64 	%p39, %rd146, 1;
	mov.pred 	%p40, 0;
	xor.pred  	%p41, %p39, %p40;
	cvt.u32.u64 	%r115, %rd56;
	shl.b32 	%r12, %r115, 4;
	@%p41 bra 	$L__BB26_36;
	add.s32 	%r151, %r1, %r12;
	add.s32 	%r152, %r151, 4;
	// begin inline asm
	ld.shared.f32 %r158, [%r151];
ld.shared.f32 %r161, [%r152];
	// end inline asm
	add.s32 	%r173, %r151, 16;
	add.s32 	%r174, %r151, 20;
	// begin inline asm
	ld.shared.f32 %r159, [%r173];
ld.shared.f32 %r162, [%r174];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r165, %r158, %r159;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r166, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r151], %r165;
st.shared.f32 [%r152], %r166;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r175, %r158, %r159;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r176, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r173], %r175;
st.shared.f32 [%r174], %r176;
	// end inline asm
	bra.uni 	$L__BB26_37;
$L__BB26_36:
	and.b32  	%r146, %r12, 16352;
	add.s32 	%r147, %r1, %r146;
	add.s32 	%r130, %r147, 8;
	add.s32 	%r131, %r147, 12;
	// begin inline asm
	ld.shared.f32 %r125, [%r130];
ld.shared.f32 %r128, [%r131];
	// end inline asm
	or.b32  	%r148, %r12, 24;
	add.s32 	%r122, %r1, %r148;
	add.s32 	%r123, %r122, 4;
	// begin inline asm
	ld.shared.f32 %r126, [%r122];
ld.shared.f32 %r129, [%r123];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r132, %r125, %r126;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r133, %r128, %r129;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r130], %r132;
st.shared.f32 [%r131], %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r125, %r126;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r128, %r129;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r145, %r141;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r122], %r144;
st.shared.f32 [%r123], %r145;
	// end inline asm
$L__BB26_37:
	@%p1 bra 	$L__BB26_34;
	add.s64 	%rd46, %rd66, 1;
	and.b64  	%rd147, %rd66, 1;
	setp.eq.b64 	%p42, %rd147, 1;
	xor.pred  	%p44, %p42, %p40;
	not.pred 	%p45, %p44;
	cvt.u32.u64 	%r177, %rd66;
	shl.b32 	%r10, %r177, 4;
	@%p45 bra 	$L__BB26_40;
	bra.uni 	$L__BB26_39;
$L__BB26_40:
	add.s32 	%r213, %r1, %r10;
	add.s32 	%r214, %r213, 4;
	// begin inline asm
	ld.shared.f32 %r220, [%r213];
ld.shared.f32 %r223, [%r214];
	// end inline asm
	add.s32 	%r235, %r213, 16;
	add.s32 	%r236, %r213, 20;
	// begin inline asm
	ld.shared.f32 %r221, [%r235];
ld.shared.f32 %r224, [%r236];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r227, %r220, %r221;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r228, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r213], %r227;
st.shared.f32 [%r214], %r228;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r237, %r220, %r221;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r238, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r235], %r237;
st.shared.f32 [%r236], %r238;
	// end inline asm
	bra.uni 	$L__BB26_41;
$L__BB26_39:
	and.b32  	%r208, %r10, 16352;
	add.s32 	%r209, %r1, %r208;
	add.s32 	%r192, %r209, 8;
	add.s32 	%r193, %r209, 12;
	// begin inline asm
	ld.shared.f32 %r187, [%r192];
ld.shared.f32 %r190, [%r193];
	// end inline asm
	or.b32  	%r210, %r10, 24;
	add.s32 	%r184, %r1, %r210;
	add.s32 	%r185, %r184, 4;
	// begin inline asm
	ld.shared.f32 %r188, [%r184];
ld.shared.f32 %r191, [%r185];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r194, %r187, %r188;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r195, %r190, %r191;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r192], %r194;
st.shared.f32 [%r193], %r195;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r203, %r187, %r188;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r206, %r190, %r191;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r207, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r184], %r206;
st.shared.f32 [%r185], %r207;
	// end inline asm
$L__BB26_41:
	add.s64 	%rd148, %rd46, %rd1;
	setp.lt.u64 	%p46, %rd148, %rd46;
	add.s64 	%rd178, %rd66, %rd57;
	setp.gt.u64 	%p47, %rd178, 1023;
	or.pred  	%p48, %p46, %p47;
	@%p48 bra 	$L__BB26_34;
	bra.uni 	$L__BB26_42;
$L__BB26_34:
	bar.sync 	0;
	@%p9 bra 	$L__BB26_51;
	cvt.u32.u64 	%r329, %rd56;
	shl.b32 	%r330, %r329, 4;
	add.s32 	%r303, %r1, %r330;
	add.s32 	%r304, %r303, 4;
	// begin inline asm
	ld.shared.f32 %r310, [%r303];
ld.shared.f32 %r313, [%r304];
	// end inline asm
	add.s32 	%r307, %r303, 8;
	add.s32 	%r308, %r303, 12;
	// begin inline asm
	ld.shared.f32 %r311, [%r307];
ld.shared.f32 %r314, [%r308];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r309, %r310, %r311;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r312, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r303], %r309;
st.shared.f32 [%r304], %r312;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r319, %r310, %r311;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r322, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r307], %r319;
st.shared.f32 [%r308], %r322;
	// end inline asm
	@!%p1 bra 	$L__BB26_48;
	bra.uni 	$L__BB26_51;
$L__BB26_48:
	shl.b64 	%rd55, %rd66, 1;
	setp.gt.u64 	%p56, %rd55, 2047;
	@%p56 bra 	$L__BB26_52;
	add.s64 	%rd54, %rd66, 1;
	cvt.u32.u64 	%r359, %rd55;
	shl.b32 	%r360, %r359, 3;
	add.s32 	%r333, %r1, %r360;
	add.s32 	%r334, %r333, 4;
	// begin inline asm
	ld.shared.f32 %r340, [%r333];
ld.shared.f32 %r343, [%r334];
	// end inline asm
	add.s32 	%r337, %r333, 8;
	add.s32 	%r338, %r333, 12;
	// begin inline asm
	ld.shared.f32 %r341, [%r337];
ld.shared.f32 %r344, [%r338];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r339, %r340, %r341;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r342, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r333], %r339;
st.shared.f32 [%r334], %r342;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r349, %r340, %r341;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r352, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r337], %r349;
st.shared.f32 [%r338], %r352;
	// end inline asm
	add.s64 	%rd155, %rd54, %rd1;
	setp.lt.u64 	%p57, %rd155, %rd54;
	add.s64 	%rd179, %rd66, %rd57;
	setp.gt.u64 	%p58, %rd179, 1023;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB26_51;
$L__BB26_50:
	add.s64 	%rd156, %rd179, 1;
	cvt.u32.u64 	%r389, %rd179;
	shl.b32 	%r390, %r389, 4;
	add.s32 	%r363, %r390, %r1;
	add.s32 	%r364, %r363, 4;
	// begin inline asm
	ld.shared.f32 %r370, [%r363];
ld.shared.f32 %r373, [%r364];
	// end inline asm
	add.s32 	%r367, %r363, 8;
	add.s32 	%r368, %r363, 12;
	// begin inline asm
	ld.shared.f32 %r371, [%r367];
ld.shared.f32 %r374, [%r368];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r369, %r370, %r371;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r372, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r363], %r369;
st.shared.f32 [%r364], %r372;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r379, %r370, %r371;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r382, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r367], %r379;
st.shared.f32 [%r368], %r382;
	// end inline asm
	add.s64 	%rd157, %rd156, %rd1;
	setp.lt.u64 	%p60, %rd157, %rd156;
	add.s64 	%rd179, %rd179, %rd57;
	setp.gt.u64 	%p61, %rd179, 1023;
	or.pred  	%p62, %p60, %p61;
	@%p62 bra 	$L__BB26_51;
	bra.uni 	$L__BB26_50;
$L__BB26_51:
	ret;
$L__BB26_44:
	and.b32  	%r270, %r11, 16352;
	add.s32 	%r271, %r1, %r270;
	add.s32 	%r254, %r271, 8;
	add.s32 	%r255, %r271, 12;
	// begin inline asm
	ld.shared.f32 %r249, [%r254];
ld.shared.f32 %r252, [%r255];
	// end inline asm
	or.b32  	%r272, %r11, 24;
	add.s32 	%r246, %r272, %r1;
	add.s32 	%r247, %r246, 4;
	// begin inline asm
	ld.shared.f32 %r250, [%r246];
ld.shared.f32 %r253, [%r247];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r256, %r249, %r250;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r257, %r252, %r253;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r254], %r256;
st.shared.f32 [%r255], %r257;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r265, %r249, %r250;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r268, %r252, %r253;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r269, %r265;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r246], %r268;
st.shared.f32 [%r247], %r269;
	// end inline asm
$L__BB26_45:
	add.s64 	%rd49, %rd178, 1;
	add.s64 	%rd150, %rd49, %rd1;
	setp.lt.u64 	%p52, %rd150, %rd49;
	add.s64 	%rd178, %rd178, %rd57;
	setp.gt.u64 	%p53, %rd178, 1023;
	or.pred  	%p54, %p52, %p53;
	@%p54 bra 	$L__BB26_34;
$L__BB26_42:
	and.b64  	%rd149, %rd178, 1;
	setp.eq.b64 	%p49, %rd149, 1;
	xor.pred  	%p51, %p49, %p40;
	cvt.u32.u64 	%r239, %rd178;
	shl.b32 	%r11, %r239, 4;
	@%p51 bra 	$L__BB26_44;
	add.s32 	%r275, %r11, %r1;
	add.s32 	%r276, %r275, 4;
	// begin inline asm
	ld.shared.f32 %r282, [%r275];
ld.shared.f32 %r285, [%r276];
	// end inline asm
	add.s32 	%r297, %r275, 16;
	add.s32 	%r298, %r275, 20;
	// begin inline asm
	ld.shared.f32 %r283, [%r297];
ld.shared.f32 %r286, [%r298];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r289, %r282, %r283;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r290, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r275], %r289;
st.shared.f32 [%r276], %r290;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r299, %r282, %r283;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r300, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r297], %r299;
st.shared.f32 [%r298], %r300;
	// end inline asm
	bra.uni 	$L__BB26_45;
$L__BB26_54:
	mov.u64 	%rd137, anon_$_9b271fbca47443a0c783d86781d13fba_$_2;
	cvta.global.u64 	%rd138, %rd137;
	{ // callseq 114, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd138;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 114
$L__BB26_53:
	mov.u64 	%rd139, anon_$_9b271fbca47443a0c783d86781d13fba_$_1;
	cvta.global.u64 	%rd140, %rd139;
	{ // callseq 115, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 115
$L__BB26_10:
	mov.u64 	%rd112, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd113, %rd112;
	mov.u64 	%rd114, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd115, %rd114;
	{ // callseq 113, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd113;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd115;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 113
$L__BB26_8:
	mov.u64 	%rd108, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd109, %rd108;
	mov.u64 	%rd110, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd111, %rd110;
	{ // callseq 112, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd109;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd111;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 112
$L__BB26_19:
	mov.u64 	%rd142, anon_$_9b271fbca47443a0c783d86781d13fba_$_10;
	cvta.global.u64 	%rd143, %rd142;
	{ // callseq 116, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd143;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 116
$L__BB26_55:
	mov.u64 	%rd104, anon_$_9b271fbca47443a0c783d86781d13fba_$_1;
	cvta.global.u64 	%rd105, %rd104;
	{ // callseq 111, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd105;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 111
$L__BB26_56:
	mov.u64 	%rd102, anon_$_9b271fbca47443a0c783d86781d13fba_$_2;
	cvta.global.u64 	%rd103, %rd102;
	{ // callseq 110, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd103;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 110
$L__BB26_30:
	mov.u64 	%rd144, anon_$_9b271fbca47443a0c783d86781d13fba_$_10;
	cvta.global.u64 	%rd145, %rd144;
	{ // callseq 117, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd145;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 117
$L__BB26_4:
	mov.u64 	%rd73, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd74, %rd73;
	mov.u64 	%rd75, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd76, %rd75;
	{ // callseq 108, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd74;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd76;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 108
$L__BB26_21:
	mov.u64 	%rd77, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd78, %rd77;
	mov.u64 	%rd79, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd80, %rd79;
	{ // callseq 109, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd80;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 109
$L__BB26_5:
	mov.u64 	%rd158, anon_$_9b271fbca47443a0c783d86781d13fba_$_5;
	cvta.global.u64 	%rd159, %rd158;
	mov.u64 	%rd160, anon_$_9b271fbca47443a0c783d86781d13fba_$_6;
	cvta.global.u64 	%rd161, %rd160;
	{ // callseq 119, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd159;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd161;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 119
$L__BB26_52:
	mov.u64 	%rd151, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd152, %rd151;
	mov.u64 	%rd153, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd154, %rd153;
	{ // callseq 118, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd152;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd154;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 118

}
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_3
)
{
	.reg .pred 	%p<63>;
	.reg .b32 	%r<399>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<180>;

	ld.param.u64 	%rd57, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_2];
	setp.eq.s64 	%p8, %rd57, 0;
	@%p8 bra 	$L__BB27_5;
	ld.param.u64 	%rd56, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_1];
	ld.param.u64 	%rd58, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_0];
	add.s64 	%rd1, %rd57, -1;
	setp.gt.u64 	%p9, %rd56, 127;
	ld.param.u64 	%rd59, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h48c3d3230cc21960E_param_3];
	add.s64 	%rd60, %rd56, 1;
	selp.b64 	%rd61, 128, %rd60, %p9;
	ld.u64 	%rd62, [%rd58];
	ld.u64 	%rd63, [%rd58+8];
	ld.u32 	%r1, [%rd59];
	shl.b64 	%rd64, %rd63, 3;
	add.s64 	%rd2, %rd62, %rd64;
	setp.eq.s64 	%p10, %rd63, 0;
	selp.b64 	%rd3, 0, %rd62, %p10;
	selp.b64 	%rd65, 0, 8, %p10;
	add.s64 	%rd4, %rd62, %rd65;
	add.s64 	%rd66, %rd61, %rd1;
	setp.lt.u64 	%p11, %rd66, %rd61;
	setp.gt.u64 	%p12, %rd66, 127;
	or.pred  	%p1, %p11, %p12;
	mov.u64 	%rd7, 1;
	mov.u64 	%rd8, 8;
	bra.uni 	$L__BB27_2;
$L__BB27_32:
	shl.b64 	%rd7, %rd7, 1;
	bar.sync 	0;
	setp.gt.u64 	%p37, %rd8, 2;
	@%p37 bra 	$L__BB27_2;
	bra.uni 	$L__BB27_33;
$L__BB27_2:
	add.s64 	%rd8, %rd8, -1;
	@%p9 bra 	$L__BB27_32;
	cvt.u32.u64 	%r13, %rd8;
	shr.u64 	%rd69, %rd56, %r13;
	mov.u64 	%rd70, 1;
	shl.b64 	%rd27, %rd70, %r13;
	mov.u64 	%rd71, 2;
	shl.b64 	%rd28, %rd71, %r13;
	add.s64 	%rd29, %rd27, -1;
	and.b64  	%rd172, %rd29, %rd56;
	mul.lo.s64 	%rd72, %rd69, %rd28;
	add.s64 	%rd31, %rd172, %rd72;
	setp.lt.u64 	%p14, %rd31, 256;
	@%p14 bra 	$L__BB27_20;
	bra.uni 	$L__BB27_4;
$L__BB27_20:
	cvt.u32.u64 	%r18, %rd31;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r16, %r1, %r19;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	ld.shared.f32 %r14, [%r16];
ld.shared.f32 %r15, [%r17];
	// end inline asm
	add.s64 	%rd32, %rd31, %rd27;
	setp.lt.u64 	%p15, %rd32, 256;
	@%p15 bra 	$L__BB27_22;
	bra.uni 	$L__BB27_21;
$L__BB27_22:
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r22, %r1, %r25;
	add.s32 	%r23, %r22, 4;
	// begin inline asm
	ld.shared.f32 %r20, [%r22];
ld.shared.f32 %r21, [%r23];
	// end inline asm
	setp.eq.s64 	%p16, %rd172, 0;
	mov.u64 	%rd162, -1;
	mov.u64 	%rd177, %rd3;
	@%p16 bra 	$L__BB27_29;
	mul.lo.s64 	%rd176, %rd172, %rd7;
	mul.hi.u64 	%rd81, %rd172, %rd7;
	setp.eq.s64 	%p17, %rd81, 0;
	mov.u64 	%rd174, %rd4;
	@%p17 bra 	$L__BB27_28;
	mov.u64 	%rd173, %rd7;
	mov.u64 	%rd174, %rd4;
$L__BB27_25:
	setp.eq.s64 	%p18, %rd172, 0;
	@%p18 bra 	$L__BB27_55;
	setp.eq.s64 	%p19, %rd173, 0;
	@%p19 bra 	$L__BB27_56;
	div.u64 	%rd83, %rd162, %rd172;
	div.u64 	%rd84, %rd162, %rd173;
	mul.lo.s64 	%rd85, %rd83, %rd172;
	mul.lo.s64 	%rd86, %rd84, %rd173;
	setp.gt.u64 	%p20, %rd85, %rd86;
	max.u64 	%rd87, %rd85, %rd86;
	selp.b64 	%rd88, %rd83, 0, %p20;
	sub.s64 	%rd173, %rd173, %rd88;
	selp.b64 	%rd89, 0, %rd84, %p20;
	sub.s64 	%rd172, %rd172, %rd89;
	add.s64 	%rd90, %rd87, -1;
	sub.s64 	%rd91, %rd2, %rd174;
	shr.u64 	%rd92, %rd91, 3;
	setp.gt.u64 	%p21, %rd92, %rd90;
	shl.b64 	%rd93, %rd87, 3;
	add.s64 	%rd94, %rd174, %rd93;
	selp.b64 	%rd174, %rd94, %rd2, %p21;
	mul.lo.s64 	%rd176, %rd172, %rd173;
	mul.hi.u64 	%rd95, %rd172, %rd173;
	setp.ne.s64 	%p6, %rd95, 0;
	@%p6 bra 	$L__BB27_25;
$L__BB27_28:
	add.s64 	%rd96, %rd176, -1;
	sub.s64 	%rd97, %rd2, %rd174;
	shr.u64 	%rd98, %rd97, 3;
	setp.gt.u64 	%p22, %rd98, %rd96;
	shl.b64 	%rd99, %rd176, 3;
	add.s64 	%rd100, %rd174, %rd99;
	add.s64 	%rd101, %rd100, -8;
	selp.b64 	%rd177, %rd101, 0, %p22;
$L__BB27_29:
	setp.ne.s64 	%p23, %rd177, 0;
	@%p23 bra 	$L__BB27_31;
	bra.uni 	$L__BB27_30;
$L__BB27_31:
	mov.b32 	%f6, %r15;
	mov.b32 	%f5, %r14;
	mov.b32 	%f8, %r21;
	mov.b32 	%f7, %r20;
	// begin inline asm
	add.rn.ftz.f32 %r26, %r14, %r20;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r29, %r15, %r21;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r16], %r26;
st.shared.f32 [%r17], %r29;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r43, %r14, %r20;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r46, %r15, %r21;
	// end inline asm
	ld.u32 	%r56, [%rd177];
	ld.u32 	%r53, [%rd177+4];
	// begin inline asm
	mul.rn.ftz.f32 %r42, %r43, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r46, %r53;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r48, %r42, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r51, %r43, %r53;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r54, %r46, %r56;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r57, %r51, %r54;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r22], %r48;
st.shared.f32 [%r23], %r57;
	// end inline asm
	mov.u64 	%rd9, %rd66;
	@%p1 bra 	$L__BB27_32;
$L__BB27_7:
	shr.u64 	%rd106, %rd9, %r13;
	and.b64  	%rd166, %rd9, %rd29;
	mul.lo.s64 	%rd107, %rd106, %rd28;
	add.s64 	%rd12, %rd107, %rd166;
	setp.lt.u64 	%p24, %rd12, 256;
	@%p24 bra 	$L__BB27_9;
	bra.uni 	$L__BB27_8;
$L__BB27_9:
	cvt.u32.u64 	%r69, %rd12;
	shl.b32 	%r70, %r69, 3;
	add.s32 	%r67, %r70, %r1;
	add.s32 	%r68, %r67, 4;
	// begin inline asm
	ld.shared.f32 %r65, [%r67];
ld.shared.f32 %r66, [%r68];
	// end inline asm
	add.s64 	%rd13, %rd12, %rd27;
	setp.lt.u64 	%p25, %rd13, 256;
	@%p25 bra 	$L__BB27_11;
	bra.uni 	$L__BB27_10;
$L__BB27_11:
	cvt.u32.u64 	%r75, %rd13;
	shl.b32 	%r76, %r75, 3;
	add.s32 	%r73, %r76, %r1;
	add.s32 	%r74, %r73, 4;
	// begin inline asm
	ld.shared.f32 %r71, [%r73];
ld.shared.f32 %r72, [%r74];
	// end inline asm
	setp.eq.s64 	%p26, %rd166, 0;
	mov.u64 	%rd171, %rd3;
	@%p26 bra 	$L__BB27_18;
	mul.lo.s64 	%rd170, %rd166, %rd7;
	mul.hi.u64 	%rd116, %rd166, %rd7;
	setp.eq.s64 	%p27, %rd116, 0;
	mov.u64 	%rd168, %rd4;
	@%p27 bra 	$L__BB27_17;
	mov.u64 	%rd167, %rd7;
	mov.u64 	%rd168, %rd4;
$L__BB27_14:
	setp.eq.s64 	%p28, %rd166, 0;
	@%p28 bra 	$L__BB27_53;
	setp.eq.s64 	%p29, %rd167, 0;
	@%p29 bra 	$L__BB27_54;
	div.u64 	%rd118, %rd162, %rd166;
	div.u64 	%rd119, %rd162, %rd167;
	mul.lo.s64 	%rd120, %rd118, %rd166;
	mul.lo.s64 	%rd121, %rd119, %rd167;
	setp.gt.u64 	%p30, %rd120, %rd121;
	max.u64 	%rd122, %rd120, %rd121;
	selp.b64 	%rd123, %rd118, 0, %p30;
	sub.s64 	%rd167, %rd167, %rd123;
	selp.b64 	%rd124, 0, %rd119, %p30;
	sub.s64 	%rd166, %rd166, %rd124;
	add.s64 	%rd125, %rd122, -1;
	sub.s64 	%rd126, %rd2, %rd168;
	shr.u64 	%rd127, %rd126, 3;
	setp.gt.u64 	%p31, %rd127, %rd125;
	shl.b64 	%rd128, %rd122, 3;
	add.s64 	%rd129, %rd168, %rd128;
	selp.b64 	%rd168, %rd129, %rd2, %p31;
	mul.lo.s64 	%rd170, %rd166, %rd167;
	mul.hi.u64 	%rd130, %rd166, %rd167;
	setp.ne.s64 	%p3, %rd130, 0;
	@%p3 bra 	$L__BB27_14;
$L__BB27_17:
	add.s64 	%rd131, %rd170, -1;
	sub.s64 	%rd132, %rd2, %rd168;
	shr.u64 	%rd133, %rd132, 3;
	setp.gt.u64 	%p32, %rd133, %rd131;
	shl.b64 	%rd134, %rd170, 3;
	add.s64 	%rd135, %rd168, %rd134;
	add.s64 	%rd136, %rd135, -8;
	selp.b64 	%rd171, %rd136, 0, %p32;
$L__BB27_18:
	setp.ne.s64 	%p33, %rd171, 0;
	@%p33 bra 	$L__BB27_6;
	bra.uni 	$L__BB27_19;
$L__BB27_6:
	add.s64 	%rd10, %rd9, 1;
	mov.b32 	%f2, %r66;
	mov.b32 	%f1, %r65;
	mov.b32 	%f4, %r72;
	mov.b32 	%f3, %r71;
	// begin inline asm
	add.rn.ftz.f32 %r77, %r65, %r71;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r66, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r67], %r77;
st.shared.f32 [%r68], %r80;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r94, %r65, %r71;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r97, %r66, %r72;
	// end inline asm
	ld.u32 	%r107, [%rd171];
	ld.u32 	%r104, [%rd171+4];
	// begin inline asm
	mul.rn.ftz.f32 %r93, %r94, %r107;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r96, %r97, %r104;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r99, %r93, %r96;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r102, %r94, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r105, %r97, %r107;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r108, %r102, %r105;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r73], %r99;
st.shared.f32 [%r74], %r108;
	// end inline asm
	add.s64 	%rd141, %rd10, %rd1;
	setp.lt.u64 	%p34, %rd141, %rd10;
	add.s64 	%rd9, %rd9, %rd57;
	setp.gt.u64 	%p35, %rd9, 127;
	or.pred  	%p36, %p34, %p35;
	@%p36 bra 	$L__BB27_32;
	bra.uni 	$L__BB27_7;
$L__BB27_33:
	@%p9 bra 	$L__BB27_34;
	and.b64  	%rd146, %rd56, 1;
	setp.eq.b64 	%p39, %rd146, 1;
	mov.pred 	%p40, 0;
	xor.pred  	%p41, %p39, %p40;
	cvt.u32.u64 	%r115, %rd56;
	shl.b32 	%r12, %r115, 4;
	@%p41 bra 	$L__BB27_36;
	add.s32 	%r151, %r1, %r12;
	add.s32 	%r152, %r151, 4;
	// begin inline asm
	ld.shared.f32 %r158, [%r151];
ld.shared.f32 %r161, [%r152];
	// end inline asm
	add.s32 	%r173, %r151, 16;
	add.s32 	%r174, %r151, 20;
	// begin inline asm
	ld.shared.f32 %r159, [%r173];
ld.shared.f32 %r162, [%r174];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r165, %r158, %r159;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r166, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r151], %r165;
st.shared.f32 [%r152], %r166;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r175, %r158, %r159;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r176, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r173], %r175;
st.shared.f32 [%r174], %r176;
	// end inline asm
	bra.uni 	$L__BB27_37;
$L__BB27_36:
	and.b32  	%r146, %r12, 2016;
	add.s32 	%r147, %r1, %r146;
	add.s32 	%r130, %r147, 8;
	add.s32 	%r131, %r147, 12;
	// begin inline asm
	ld.shared.f32 %r125, [%r130];
ld.shared.f32 %r128, [%r131];
	// end inline asm
	or.b32  	%r148, %r12, 24;
	add.s32 	%r122, %r1, %r148;
	add.s32 	%r123, %r122, 4;
	// begin inline asm
	ld.shared.f32 %r126, [%r122];
ld.shared.f32 %r129, [%r123];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r132, %r125, %r126;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r133, %r128, %r129;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r130], %r132;
st.shared.f32 [%r131], %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r125, %r126;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r128, %r129;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r145, %r141;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r122], %r144;
st.shared.f32 [%r123], %r145;
	// end inline asm
$L__BB27_37:
	@%p1 bra 	$L__BB27_34;
	add.s64 	%rd46, %rd66, 1;
	and.b64  	%rd147, %rd66, 1;
	setp.eq.b64 	%p42, %rd147, 1;
	xor.pred  	%p44, %p42, %p40;
	not.pred 	%p45, %p44;
	cvt.u32.u64 	%r177, %rd66;
	shl.b32 	%r10, %r177, 4;
	@%p45 bra 	$L__BB27_40;
	bra.uni 	$L__BB27_39;
$L__BB27_40:
	add.s32 	%r213, %r1, %r10;
	add.s32 	%r214, %r213, 4;
	// begin inline asm
	ld.shared.f32 %r220, [%r213];
ld.shared.f32 %r223, [%r214];
	// end inline asm
	add.s32 	%r235, %r213, 16;
	add.s32 	%r236, %r213, 20;
	// begin inline asm
	ld.shared.f32 %r221, [%r235];
ld.shared.f32 %r224, [%r236];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r227, %r220, %r221;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r228, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r213], %r227;
st.shared.f32 [%r214], %r228;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r237, %r220, %r221;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r238, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r235], %r237;
st.shared.f32 [%r236], %r238;
	// end inline asm
	bra.uni 	$L__BB27_41;
$L__BB27_39:
	and.b32  	%r208, %r10, 2016;
	add.s32 	%r209, %r1, %r208;
	add.s32 	%r192, %r209, 8;
	add.s32 	%r193, %r209, 12;
	// begin inline asm
	ld.shared.f32 %r187, [%r192];
ld.shared.f32 %r190, [%r193];
	// end inline asm
	or.b32  	%r210, %r10, 24;
	add.s32 	%r184, %r1, %r210;
	add.s32 	%r185, %r184, 4;
	// begin inline asm
	ld.shared.f32 %r188, [%r184];
ld.shared.f32 %r191, [%r185];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r194, %r187, %r188;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r195, %r190, %r191;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r192], %r194;
st.shared.f32 [%r193], %r195;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r203, %r187, %r188;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r206, %r190, %r191;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r207, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r184], %r206;
st.shared.f32 [%r185], %r207;
	// end inline asm
$L__BB27_41:
	add.s64 	%rd148, %rd46, %rd1;
	setp.lt.u64 	%p46, %rd148, %rd46;
	add.s64 	%rd178, %rd66, %rd57;
	setp.gt.u64 	%p47, %rd178, 127;
	or.pred  	%p48, %p46, %p47;
	@%p48 bra 	$L__BB27_34;
	bra.uni 	$L__BB27_42;
$L__BB27_34:
	bar.sync 	0;
	@%p9 bra 	$L__BB27_51;
	cvt.u32.u64 	%r329, %rd56;
	shl.b32 	%r330, %r329, 4;
	add.s32 	%r303, %r1, %r330;
	add.s32 	%r304, %r303, 4;
	// begin inline asm
	ld.shared.f32 %r310, [%r303];
ld.shared.f32 %r313, [%r304];
	// end inline asm
	add.s32 	%r307, %r303, 8;
	add.s32 	%r308, %r303, 12;
	// begin inline asm
	ld.shared.f32 %r311, [%r307];
ld.shared.f32 %r314, [%r308];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r309, %r310, %r311;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r312, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r303], %r309;
st.shared.f32 [%r304], %r312;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r319, %r310, %r311;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r322, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r307], %r319;
st.shared.f32 [%r308], %r322;
	// end inline asm
	@!%p1 bra 	$L__BB27_48;
	bra.uni 	$L__BB27_51;
$L__BB27_48:
	shl.b64 	%rd55, %rd66, 1;
	setp.gt.u64 	%p56, %rd55, 255;
	@%p56 bra 	$L__BB27_52;
	add.s64 	%rd54, %rd66, 1;
	cvt.u32.u64 	%r359, %rd55;
	shl.b32 	%r360, %r359, 3;
	add.s32 	%r333, %r1, %r360;
	add.s32 	%r334, %r333, 4;
	// begin inline asm
	ld.shared.f32 %r340, [%r333];
ld.shared.f32 %r343, [%r334];
	// end inline asm
	add.s32 	%r337, %r333, 8;
	add.s32 	%r338, %r333, 12;
	// begin inline asm
	ld.shared.f32 %r341, [%r337];
ld.shared.f32 %r344, [%r338];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r339, %r340, %r341;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r342, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r333], %r339;
st.shared.f32 [%r334], %r342;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r349, %r340, %r341;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r352, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r337], %r349;
st.shared.f32 [%r338], %r352;
	// end inline asm
	add.s64 	%rd155, %rd54, %rd1;
	setp.lt.u64 	%p57, %rd155, %rd54;
	add.s64 	%rd179, %rd66, %rd57;
	setp.gt.u64 	%p58, %rd179, 127;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB27_51;
$L__BB27_50:
	add.s64 	%rd156, %rd179, 1;
	cvt.u32.u64 	%r389, %rd179;
	shl.b32 	%r390, %r389, 4;
	add.s32 	%r363, %r390, %r1;
	add.s32 	%r364, %r363, 4;
	// begin inline asm
	ld.shared.f32 %r370, [%r363];
ld.shared.f32 %r373, [%r364];
	// end inline asm
	add.s32 	%r367, %r363, 8;
	add.s32 	%r368, %r363, 12;
	// begin inline asm
	ld.shared.f32 %r371, [%r367];
ld.shared.f32 %r374, [%r368];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r369, %r370, %r371;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r372, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r363], %r369;
st.shared.f32 [%r364], %r372;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r379, %r370, %r371;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r382, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r367], %r379;
st.shared.f32 [%r368], %r382;
	// end inline asm
	add.s64 	%rd157, %rd156, %rd1;
	setp.lt.u64 	%p60, %rd157, %rd156;
	add.s64 	%rd179, %rd179, %rd57;
	setp.gt.u64 	%p61, %rd179, 127;
	or.pred  	%p62, %p60, %p61;
	@%p62 bra 	$L__BB27_51;
	bra.uni 	$L__BB27_50;
$L__BB27_51:
	ret;
$L__BB27_44:
	and.b32  	%r270, %r11, 2016;
	add.s32 	%r271, %r1, %r270;
	add.s32 	%r254, %r271, 8;
	add.s32 	%r255, %r271, 12;
	// begin inline asm
	ld.shared.f32 %r249, [%r254];
ld.shared.f32 %r252, [%r255];
	// end inline asm
	or.b32  	%r272, %r11, 24;
	add.s32 	%r246, %r272, %r1;
	add.s32 	%r247, %r246, 4;
	// begin inline asm
	ld.shared.f32 %r250, [%r246];
ld.shared.f32 %r253, [%r247];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r256, %r249, %r250;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r257, %r252, %r253;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r254], %r256;
st.shared.f32 [%r255], %r257;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r265, %r249, %r250;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r268, %r252, %r253;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r269, %r265;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r246], %r268;
st.shared.f32 [%r247], %r269;
	// end inline asm
$L__BB27_45:
	add.s64 	%rd49, %rd178, 1;
	add.s64 	%rd150, %rd49, %rd1;
	setp.lt.u64 	%p52, %rd150, %rd49;
	add.s64 	%rd178, %rd178, %rd57;
	setp.gt.u64 	%p53, %rd178, 127;
	or.pred  	%p54, %p52, %p53;
	@%p54 bra 	$L__BB27_34;
$L__BB27_42:
	and.b64  	%rd149, %rd178, 1;
	setp.eq.b64 	%p49, %rd149, 1;
	xor.pred  	%p51, %p49, %p40;
	cvt.u32.u64 	%r239, %rd178;
	shl.b32 	%r11, %r239, 4;
	@%p51 bra 	$L__BB27_44;
	add.s32 	%r275, %r11, %r1;
	add.s32 	%r276, %r275, 4;
	// begin inline asm
	ld.shared.f32 %r282, [%r275];
ld.shared.f32 %r285, [%r276];
	// end inline asm
	add.s32 	%r297, %r275, 16;
	add.s32 	%r298, %r275, 20;
	// begin inline asm
	ld.shared.f32 %r283, [%r297];
ld.shared.f32 %r286, [%r298];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r289, %r282, %r283;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r290, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r275], %r289;
st.shared.f32 [%r276], %r290;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r299, %r282, %r283;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r300, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r297], %r299;
st.shared.f32 [%r298], %r300;
	// end inline asm
	bra.uni 	$L__BB27_45;
$L__BB27_54:
	mov.u64 	%rd137, anon_$_9b271fbca47443a0c783d86781d13fba_$_2;
	cvta.global.u64 	%rd138, %rd137;
	{ // callseq 126, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd138;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 126
$L__BB27_53:
	mov.u64 	%rd139, anon_$_9b271fbca47443a0c783d86781d13fba_$_1;
	cvta.global.u64 	%rd140, %rd139;
	{ // callseq 127, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 127
$L__BB27_10:
	mov.u64 	%rd112, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd113, %rd112;
	mov.u64 	%rd114, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd115, %rd114;
	{ // callseq 125, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd113;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd115;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 125
$L__BB27_8:
	mov.u64 	%rd108, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd109, %rd108;
	mov.u64 	%rd110, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd111, %rd110;
	{ // callseq 124, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd109;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd111;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 124
$L__BB27_19:
	mov.u64 	%rd142, anon_$_9b271fbca47443a0c783d86781d13fba_$_10;
	cvta.global.u64 	%rd143, %rd142;
	{ // callseq 128, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd143;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 128
$L__BB27_55:
	mov.u64 	%rd104, anon_$_9b271fbca47443a0c783d86781d13fba_$_1;
	cvta.global.u64 	%rd105, %rd104;
	{ // callseq 123, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd105;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 123
$L__BB27_56:
	mov.u64 	%rd102, anon_$_9b271fbca47443a0c783d86781d13fba_$_2;
	cvta.global.u64 	%rd103, %rd102;
	{ // callseq 122, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd103;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 122
$L__BB27_30:
	mov.u64 	%rd144, anon_$_9b271fbca47443a0c783d86781d13fba_$_10;
	cvta.global.u64 	%rd145, %rd144;
	{ // callseq 129, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd145;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 129
$L__BB27_4:
	mov.u64 	%rd73, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd74, %rd73;
	mov.u64 	%rd75, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd76, %rd75;
	{ // callseq 120, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd74;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd76;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 120
$L__BB27_21:
	mov.u64 	%rd77, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd78, %rd77;
	mov.u64 	%rd79, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd80, %rd79;
	{ // callseq 121, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd80;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 121
$L__BB27_5:
	mov.u64 	%rd158, anon_$_9b271fbca47443a0c783d86781d13fba_$_5;
	cvta.global.u64 	%rd159, %rd158;
	mov.u64 	%rd160, anon_$_9b271fbca47443a0c783d86781d13fba_$_6;
	cvta.global.u64 	%rd161, %rd160;
	{ // callseq 131, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd159;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd161;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 131
$L__BB27_52:
	mov.u64 	%rd151, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd152, %rd151;
	mov.u64 	%rd153, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd154, %rd153;
	{ // callseq 130, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd152;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd154;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 130

}
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_3
)
{
	.reg .pred 	%p<63>;
	.reg .b32 	%r<399>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<180>;

	ld.param.u64 	%rd57, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_2];
	setp.eq.s64 	%p8, %rd57, 0;
	@%p8 bra 	$L__BB28_5;
	ld.param.u64 	%rd56, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_1];
	ld.param.u64 	%rd58, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_0];
	add.s64 	%rd1, %rd57, -1;
	setp.gt.u64 	%p9, %rd56, 255;
	ld.param.u64 	%rd59, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h7341df042d37b6f9E_param_3];
	add.s64 	%rd60, %rd56, 1;
	selp.b64 	%rd61, 256, %rd60, %p9;
	ld.u64 	%rd62, [%rd58];
	ld.u64 	%rd63, [%rd58+8];
	ld.u32 	%r1, [%rd59];
	shl.b64 	%rd64, %rd63, 3;
	add.s64 	%rd2, %rd62, %rd64;
	setp.eq.s64 	%p10, %rd63, 0;
	selp.b64 	%rd3, 0, %rd62, %p10;
	selp.b64 	%rd65, 0, 8, %p10;
	add.s64 	%rd4, %rd62, %rd65;
	add.s64 	%rd66, %rd61, %rd1;
	setp.lt.u64 	%p11, %rd66, %rd61;
	setp.gt.u64 	%p12, %rd66, 255;
	or.pred  	%p1, %p11, %p12;
	mov.u64 	%rd7, 1;
	mov.u64 	%rd8, 9;
	bra.uni 	$L__BB28_2;
$L__BB28_32:
	shl.b64 	%rd7, %rd7, 1;
	bar.sync 	0;
	setp.gt.u64 	%p37, %rd8, 2;
	@%p37 bra 	$L__BB28_2;
	bra.uni 	$L__BB28_33;
$L__BB28_2:
	add.s64 	%rd8, %rd8, -1;
	@%p9 bra 	$L__BB28_32;
	cvt.u32.u64 	%r13, %rd8;
	shr.u64 	%rd69, %rd56, %r13;
	mov.u64 	%rd70, 1;
	shl.b64 	%rd27, %rd70, %r13;
	mov.u64 	%rd71, 2;
	shl.b64 	%rd28, %rd71, %r13;
	add.s64 	%rd29, %rd27, -1;
	and.b64  	%rd172, %rd29, %rd56;
	mul.lo.s64 	%rd72, %rd69, %rd28;
	add.s64 	%rd31, %rd172, %rd72;
	setp.lt.u64 	%p14, %rd31, 512;
	@%p14 bra 	$L__BB28_20;
	bra.uni 	$L__BB28_4;
$L__BB28_20:
	cvt.u32.u64 	%r18, %rd31;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r16, %r1, %r19;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	ld.shared.f32 %r14, [%r16];
ld.shared.f32 %r15, [%r17];
	// end inline asm
	add.s64 	%rd32, %rd31, %rd27;
	setp.lt.u64 	%p15, %rd32, 512;
	@%p15 bra 	$L__BB28_22;
	bra.uni 	$L__BB28_21;
$L__BB28_22:
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r22, %r1, %r25;
	add.s32 	%r23, %r22, 4;
	// begin inline asm
	ld.shared.f32 %r20, [%r22];
ld.shared.f32 %r21, [%r23];
	// end inline asm
	setp.eq.s64 	%p16, %rd172, 0;
	mov.u64 	%rd162, -1;
	mov.u64 	%rd177, %rd3;
	@%p16 bra 	$L__BB28_29;
	mul.lo.s64 	%rd176, %rd172, %rd7;
	mul.hi.u64 	%rd81, %rd172, %rd7;
	setp.eq.s64 	%p17, %rd81, 0;
	mov.u64 	%rd174, %rd4;
	@%p17 bra 	$L__BB28_28;
	mov.u64 	%rd173, %rd7;
	mov.u64 	%rd174, %rd4;
$L__BB28_25:
	setp.eq.s64 	%p18, %rd172, 0;
	@%p18 bra 	$L__BB28_55;
	setp.eq.s64 	%p19, %rd173, 0;
	@%p19 bra 	$L__BB28_56;
	div.u64 	%rd83, %rd162, %rd172;
	div.u64 	%rd84, %rd162, %rd173;
	mul.lo.s64 	%rd85, %rd83, %rd172;
	mul.lo.s64 	%rd86, %rd84, %rd173;
	setp.gt.u64 	%p20, %rd85, %rd86;
	max.u64 	%rd87, %rd85, %rd86;
	selp.b64 	%rd88, %rd83, 0, %p20;
	sub.s64 	%rd173, %rd173, %rd88;
	selp.b64 	%rd89, 0, %rd84, %p20;
	sub.s64 	%rd172, %rd172, %rd89;
	add.s64 	%rd90, %rd87, -1;
	sub.s64 	%rd91, %rd2, %rd174;
	shr.u64 	%rd92, %rd91, 3;
	setp.gt.u64 	%p21, %rd92, %rd90;
	shl.b64 	%rd93, %rd87, 3;
	add.s64 	%rd94, %rd174, %rd93;
	selp.b64 	%rd174, %rd94, %rd2, %p21;
	mul.lo.s64 	%rd176, %rd172, %rd173;
	mul.hi.u64 	%rd95, %rd172, %rd173;
	setp.ne.s64 	%p6, %rd95, 0;
	@%p6 bra 	$L__BB28_25;
$L__BB28_28:
	add.s64 	%rd96, %rd176, -1;
	sub.s64 	%rd97, %rd2, %rd174;
	shr.u64 	%rd98, %rd97, 3;
	setp.gt.u64 	%p22, %rd98, %rd96;
	shl.b64 	%rd99, %rd176, 3;
	add.s64 	%rd100, %rd174, %rd99;
	add.s64 	%rd101, %rd100, -8;
	selp.b64 	%rd177, %rd101, 0, %p22;
$L__BB28_29:
	setp.ne.s64 	%p23, %rd177, 0;
	@%p23 bra 	$L__BB28_31;
	bra.uni 	$L__BB28_30;
$L__BB28_31:
	mov.b32 	%f6, %r15;
	mov.b32 	%f5, %r14;
	mov.b32 	%f8, %r21;
	mov.b32 	%f7, %r20;
	// begin inline asm
	add.rn.ftz.f32 %r26, %r14, %r20;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r29, %r15, %r21;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r16], %r26;
st.shared.f32 [%r17], %r29;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r43, %r14, %r20;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r46, %r15, %r21;
	// end inline asm
	ld.u32 	%r56, [%rd177];
	ld.u32 	%r53, [%rd177+4];
	// begin inline asm
	mul.rn.ftz.f32 %r42, %r43, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r46, %r53;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r48, %r42, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r51, %r43, %r53;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r54, %r46, %r56;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r57, %r51, %r54;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r22], %r48;
st.shared.f32 [%r23], %r57;
	// end inline asm
	mov.u64 	%rd9, %rd66;
	@%p1 bra 	$L__BB28_32;
$L__BB28_7:
	shr.u64 	%rd106, %rd9, %r13;
	and.b64  	%rd166, %rd9, %rd29;
	mul.lo.s64 	%rd107, %rd106, %rd28;
	add.s64 	%rd12, %rd107, %rd166;
	setp.lt.u64 	%p24, %rd12, 512;
	@%p24 bra 	$L__BB28_9;
	bra.uni 	$L__BB28_8;
$L__BB28_9:
	cvt.u32.u64 	%r69, %rd12;
	shl.b32 	%r70, %r69, 3;
	add.s32 	%r67, %r70, %r1;
	add.s32 	%r68, %r67, 4;
	// begin inline asm
	ld.shared.f32 %r65, [%r67];
ld.shared.f32 %r66, [%r68];
	// end inline asm
	add.s64 	%rd13, %rd12, %rd27;
	setp.lt.u64 	%p25, %rd13, 512;
	@%p25 bra 	$L__BB28_11;
	bra.uni 	$L__BB28_10;
$L__BB28_11:
	cvt.u32.u64 	%r75, %rd13;
	shl.b32 	%r76, %r75, 3;
	add.s32 	%r73, %r76, %r1;
	add.s32 	%r74, %r73, 4;
	// begin inline asm
	ld.shared.f32 %r71, [%r73];
ld.shared.f32 %r72, [%r74];
	// end inline asm
	setp.eq.s64 	%p26, %rd166, 0;
	mov.u64 	%rd171, %rd3;
	@%p26 bra 	$L__BB28_18;
	mul.lo.s64 	%rd170, %rd166, %rd7;
	mul.hi.u64 	%rd116, %rd166, %rd7;
	setp.eq.s64 	%p27, %rd116, 0;
	mov.u64 	%rd168, %rd4;
	@%p27 bra 	$L__BB28_17;
	mov.u64 	%rd167, %rd7;
	mov.u64 	%rd168, %rd4;
$L__BB28_14:
	setp.eq.s64 	%p28, %rd166, 0;
	@%p28 bra 	$L__BB28_53;
	setp.eq.s64 	%p29, %rd167, 0;
	@%p29 bra 	$L__BB28_54;
	div.u64 	%rd118, %rd162, %rd166;
	div.u64 	%rd119, %rd162, %rd167;
	mul.lo.s64 	%rd120, %rd118, %rd166;
	mul.lo.s64 	%rd121, %rd119, %rd167;
	setp.gt.u64 	%p30, %rd120, %rd121;
	max.u64 	%rd122, %rd120, %rd121;
	selp.b64 	%rd123, %rd118, 0, %p30;
	sub.s64 	%rd167, %rd167, %rd123;
	selp.b64 	%rd124, 0, %rd119, %p30;
	sub.s64 	%rd166, %rd166, %rd124;
	add.s64 	%rd125, %rd122, -1;
	sub.s64 	%rd126, %rd2, %rd168;
	shr.u64 	%rd127, %rd126, 3;
	setp.gt.u64 	%p31, %rd127, %rd125;
	shl.b64 	%rd128, %rd122, 3;
	add.s64 	%rd129, %rd168, %rd128;
	selp.b64 	%rd168, %rd129, %rd2, %p31;
	mul.lo.s64 	%rd170, %rd166, %rd167;
	mul.hi.u64 	%rd130, %rd166, %rd167;
	setp.ne.s64 	%p3, %rd130, 0;
	@%p3 bra 	$L__BB28_14;
$L__BB28_17:
	add.s64 	%rd131, %rd170, -1;
	sub.s64 	%rd132, %rd2, %rd168;
	shr.u64 	%rd133, %rd132, 3;
	setp.gt.u64 	%p32, %rd133, %rd131;
	shl.b64 	%rd134, %rd170, 3;
	add.s64 	%rd135, %rd168, %rd134;
	add.s64 	%rd136, %rd135, -8;
	selp.b64 	%rd171, %rd136, 0, %p32;
$L__BB28_18:
	setp.ne.s64 	%p33, %rd171, 0;
	@%p33 bra 	$L__BB28_6;
	bra.uni 	$L__BB28_19;
$L__BB28_6:
	add.s64 	%rd10, %rd9, 1;
	mov.b32 	%f2, %r66;
	mov.b32 	%f1, %r65;
	mov.b32 	%f4, %r72;
	mov.b32 	%f3, %r71;
	// begin inline asm
	add.rn.ftz.f32 %r77, %r65, %r71;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r66, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r67], %r77;
st.shared.f32 [%r68], %r80;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r94, %r65, %r71;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r97, %r66, %r72;
	// end inline asm
	ld.u32 	%r107, [%rd171];
	ld.u32 	%r104, [%rd171+4];
	// begin inline asm
	mul.rn.ftz.f32 %r93, %r94, %r107;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r96, %r97, %r104;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r99, %r93, %r96;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r102, %r94, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r105, %r97, %r107;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r108, %r102, %r105;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r73], %r99;
st.shared.f32 [%r74], %r108;
	// end inline asm
	add.s64 	%rd141, %rd10, %rd1;
	setp.lt.u64 	%p34, %rd141, %rd10;
	add.s64 	%rd9, %rd9, %rd57;
	setp.gt.u64 	%p35, %rd9, 255;
	or.pred  	%p36, %p34, %p35;
	@%p36 bra 	$L__BB28_32;
	bra.uni 	$L__BB28_7;
$L__BB28_33:
	@%p9 bra 	$L__BB28_34;
	and.b64  	%rd146, %rd56, 1;
	setp.eq.b64 	%p39, %rd146, 1;
	mov.pred 	%p40, 0;
	xor.pred  	%p41, %p39, %p40;
	cvt.u32.u64 	%r115, %rd56;
	shl.b32 	%r12, %r115, 4;
	@%p41 bra 	$L__BB28_36;
	add.s32 	%r151, %r1, %r12;
	add.s32 	%r152, %r151, 4;
	// begin inline asm
	ld.shared.f32 %r158, [%r151];
ld.shared.f32 %r161, [%r152];
	// end inline asm
	add.s32 	%r173, %r151, 16;
	add.s32 	%r174, %r151, 20;
	// begin inline asm
	ld.shared.f32 %r159, [%r173];
ld.shared.f32 %r162, [%r174];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r165, %r158, %r159;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r166, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r151], %r165;
st.shared.f32 [%r152], %r166;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r175, %r158, %r159;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r176, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r173], %r175;
st.shared.f32 [%r174], %r176;
	// end inline asm
	bra.uni 	$L__BB28_37;
$L__BB28_36:
	and.b32  	%r146, %r12, 4064;
	add.s32 	%r147, %r1, %r146;
	add.s32 	%r130, %r147, 8;
	add.s32 	%r131, %r147, 12;
	// begin inline asm
	ld.shared.f32 %r125, [%r130];
ld.shared.f32 %r128, [%r131];
	// end inline asm
	or.b32  	%r148, %r12, 24;
	add.s32 	%r122, %r1, %r148;
	add.s32 	%r123, %r122, 4;
	// begin inline asm
	ld.shared.f32 %r126, [%r122];
ld.shared.f32 %r129, [%r123];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r132, %r125, %r126;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r133, %r128, %r129;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r130], %r132;
st.shared.f32 [%r131], %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r125, %r126;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r128, %r129;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r145, %r141;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r122], %r144;
st.shared.f32 [%r123], %r145;
	// end inline asm
$L__BB28_37:
	@%p1 bra 	$L__BB28_34;
	add.s64 	%rd46, %rd66, 1;
	and.b64  	%rd147, %rd66, 1;
	setp.eq.b64 	%p42, %rd147, 1;
	xor.pred  	%p44, %p42, %p40;
	not.pred 	%p45, %p44;
	cvt.u32.u64 	%r177, %rd66;
	shl.b32 	%r10, %r177, 4;
	@%p45 bra 	$L__BB28_40;
	bra.uni 	$L__BB28_39;
$L__BB28_40:
	add.s32 	%r213, %r1, %r10;
	add.s32 	%r214, %r213, 4;
	// begin inline asm
	ld.shared.f32 %r220, [%r213];
ld.shared.f32 %r223, [%r214];
	// end inline asm
	add.s32 	%r235, %r213, 16;
	add.s32 	%r236, %r213, 20;
	// begin inline asm
	ld.shared.f32 %r221, [%r235];
ld.shared.f32 %r224, [%r236];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r227, %r220, %r221;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r228, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r213], %r227;
st.shared.f32 [%r214], %r228;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r237, %r220, %r221;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r238, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r235], %r237;
st.shared.f32 [%r236], %r238;
	// end inline asm
	bra.uni 	$L__BB28_41;
$L__BB28_39:
	and.b32  	%r208, %r10, 4064;
	add.s32 	%r209, %r1, %r208;
	add.s32 	%r192, %r209, 8;
	add.s32 	%r193, %r209, 12;
	// begin inline asm
	ld.shared.f32 %r187, [%r192];
ld.shared.f32 %r190, [%r193];
	// end inline asm
	or.b32  	%r210, %r10, 24;
	add.s32 	%r184, %r1, %r210;
	add.s32 	%r185, %r184, 4;
	// begin inline asm
	ld.shared.f32 %r188, [%r184];
ld.shared.f32 %r191, [%r185];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r194, %r187, %r188;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r195, %r190, %r191;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r192], %r194;
st.shared.f32 [%r193], %r195;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r203, %r187, %r188;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r206, %r190, %r191;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r207, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r184], %r206;
st.shared.f32 [%r185], %r207;
	// end inline asm
$L__BB28_41:
	add.s64 	%rd148, %rd46, %rd1;
	setp.lt.u64 	%p46, %rd148, %rd46;
	add.s64 	%rd178, %rd66, %rd57;
	setp.gt.u64 	%p47, %rd178, 255;
	or.pred  	%p48, %p46, %p47;
	@%p48 bra 	$L__BB28_34;
	bra.uni 	$L__BB28_42;
$L__BB28_34:
	bar.sync 	0;
	@%p9 bra 	$L__BB28_51;
	cvt.u32.u64 	%r329, %rd56;
	shl.b32 	%r330, %r329, 4;
	add.s32 	%r303, %r1, %r330;
	add.s32 	%r304, %r303, 4;
	// begin inline asm
	ld.shared.f32 %r310, [%r303];
ld.shared.f32 %r313, [%r304];
	// end inline asm
	add.s32 	%r307, %r303, 8;
	add.s32 	%r308, %r303, 12;
	// begin inline asm
	ld.shared.f32 %r311, [%r307];
ld.shared.f32 %r314, [%r308];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r309, %r310, %r311;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r312, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r303], %r309;
st.shared.f32 [%r304], %r312;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r319, %r310, %r311;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r322, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r307], %r319;
st.shared.f32 [%r308], %r322;
	// end inline asm
	@!%p1 bra 	$L__BB28_48;
	bra.uni 	$L__BB28_51;
$L__BB28_48:
	shl.b64 	%rd55, %rd66, 1;
	setp.gt.u64 	%p56, %rd55, 511;
	@%p56 bra 	$L__BB28_52;
	add.s64 	%rd54, %rd66, 1;
	cvt.u32.u64 	%r359, %rd55;
	shl.b32 	%r360, %r359, 3;
	add.s32 	%r333, %r1, %r360;
	add.s32 	%r334, %r333, 4;
	// begin inline asm
	ld.shared.f32 %r340, [%r333];
ld.shared.f32 %r343, [%r334];
	// end inline asm
	add.s32 	%r337, %r333, 8;
	add.s32 	%r338, %r333, 12;
	// begin inline asm
	ld.shared.f32 %r341, [%r337];
ld.shared.f32 %r344, [%r338];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r339, %r340, %r341;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r342, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r333], %r339;
st.shared.f32 [%r334], %r342;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r349, %r340, %r341;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r352, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r337], %r349;
st.shared.f32 [%r338], %r352;
	// end inline asm
	add.s64 	%rd155, %rd54, %rd1;
	setp.lt.u64 	%p57, %rd155, %rd54;
	add.s64 	%rd179, %rd66, %rd57;
	setp.gt.u64 	%p58, %rd179, 255;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB28_51;
$L__BB28_50:
	add.s64 	%rd156, %rd179, 1;
	cvt.u32.u64 	%r389, %rd179;
	shl.b32 	%r390, %r389, 4;
	add.s32 	%r363, %r390, %r1;
	add.s32 	%r364, %r363, 4;
	// begin inline asm
	ld.shared.f32 %r370, [%r363];
ld.shared.f32 %r373, [%r364];
	// end inline asm
	add.s32 	%r367, %r363, 8;
	add.s32 	%r368, %r363, 12;
	// begin inline asm
	ld.shared.f32 %r371, [%r367];
ld.shared.f32 %r374, [%r368];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r369, %r370, %r371;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r372, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r363], %r369;
st.shared.f32 [%r364], %r372;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r379, %r370, %r371;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r382, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r367], %r379;
st.shared.f32 [%r368], %r382;
	// end inline asm
	add.s64 	%rd157, %rd156, %rd1;
	setp.lt.u64 	%p60, %rd157, %rd156;
	add.s64 	%rd179, %rd179, %rd57;
	setp.gt.u64 	%p61, %rd179, 255;
	or.pred  	%p62, %p60, %p61;
	@%p62 bra 	$L__BB28_51;
	bra.uni 	$L__BB28_50;
$L__BB28_51:
	ret;
$L__BB28_44:
	and.b32  	%r270, %r11, 4064;
	add.s32 	%r271, %r1, %r270;
	add.s32 	%r254, %r271, 8;
	add.s32 	%r255, %r271, 12;
	// begin inline asm
	ld.shared.f32 %r249, [%r254];
ld.shared.f32 %r252, [%r255];
	// end inline asm
	or.b32  	%r272, %r11, 24;
	add.s32 	%r246, %r272, %r1;
	add.s32 	%r247, %r246, 4;
	// begin inline asm
	ld.shared.f32 %r250, [%r246];
ld.shared.f32 %r253, [%r247];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r256, %r249, %r250;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r257, %r252, %r253;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r254], %r256;
st.shared.f32 [%r255], %r257;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r265, %r249, %r250;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r268, %r252, %r253;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r269, %r265;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r246], %r268;
st.shared.f32 [%r247], %r269;
	// end inline asm
$L__BB28_45:
	add.s64 	%rd49, %rd178, 1;
	add.s64 	%rd150, %rd49, %rd1;
	setp.lt.u64 	%p52, %rd150, %rd49;
	add.s64 	%rd178, %rd178, %rd57;
	setp.gt.u64 	%p53, %rd178, 255;
	or.pred  	%p54, %p52, %p53;
	@%p54 bra 	$L__BB28_34;
$L__BB28_42:
	and.b64  	%rd149, %rd178, 1;
	setp.eq.b64 	%p49, %rd149, 1;
	xor.pred  	%p51, %p49, %p40;
	cvt.u32.u64 	%r239, %rd178;
	shl.b32 	%r11, %r239, 4;
	@%p51 bra 	$L__BB28_44;
	add.s32 	%r275, %r11, %r1;
	add.s32 	%r276, %r275, 4;
	// begin inline asm
	ld.shared.f32 %r282, [%r275];
ld.shared.f32 %r285, [%r276];
	// end inline asm
	add.s32 	%r297, %r275, 16;
	add.s32 	%r298, %r275, 20;
	// begin inline asm
	ld.shared.f32 %r283, [%r297];
ld.shared.f32 %r286, [%r298];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r289, %r282, %r283;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r290, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r275], %r289;
st.shared.f32 [%r276], %r290;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r299, %r282, %r283;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r300, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r297], %r299;
st.shared.f32 [%r298], %r300;
	// end inline asm
	bra.uni 	$L__BB28_45;
$L__BB28_54:
	mov.u64 	%rd137, anon_$_9b271fbca47443a0c783d86781d13fba_$_2;
	cvta.global.u64 	%rd138, %rd137;
	{ // callseq 138, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd138;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 138
$L__BB28_53:
	mov.u64 	%rd139, anon_$_9b271fbca47443a0c783d86781d13fba_$_1;
	cvta.global.u64 	%rd140, %rd139;
	{ // callseq 139, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 139
$L__BB28_10:
	mov.u64 	%rd112, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd113, %rd112;
	mov.u64 	%rd114, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd115, %rd114;
	{ // callseq 137, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd113;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd115;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 137
$L__BB28_8:
	mov.u64 	%rd108, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd109, %rd108;
	mov.u64 	%rd110, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd111, %rd110;
	{ // callseq 136, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd109;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd111;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 136
$L__BB28_19:
	mov.u64 	%rd142, anon_$_9b271fbca47443a0c783d86781d13fba_$_10;
	cvta.global.u64 	%rd143, %rd142;
	{ // callseq 140, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd143;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 140
$L__BB28_55:
	mov.u64 	%rd104, anon_$_9b271fbca47443a0c783d86781d13fba_$_1;
	cvta.global.u64 	%rd105, %rd104;
	{ // callseq 135, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd105;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 135
$L__BB28_56:
	mov.u64 	%rd102, anon_$_9b271fbca47443a0c783d86781d13fba_$_2;
	cvta.global.u64 	%rd103, %rd102;
	{ // callseq 134, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd103;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 134
$L__BB28_30:
	mov.u64 	%rd144, anon_$_9b271fbca47443a0c783d86781d13fba_$_10;
	cvta.global.u64 	%rd145, %rd144;
	{ // callseq 141, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd145;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 141
$L__BB28_4:
	mov.u64 	%rd73, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd74, %rd73;
	mov.u64 	%rd75, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd76, %rd75;
	{ // callseq 132, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd74;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd76;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 132
$L__BB28_21:
	mov.u64 	%rd77, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd78, %rd77;
	mov.u64 	%rd79, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd80, %rd79;
	{ // callseq 133, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd80;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 133
$L__BB28_5:
	mov.u64 	%rd158, anon_$_9b271fbca47443a0c783d86781d13fba_$_5;
	cvta.global.u64 	%rd159, %rd158;
	mov.u64 	%rd160, anon_$_9b271fbca47443a0c783d86781d13fba_$_6;
	cvta.global.u64 	%rd161, %rd160;
	{ // callseq 143, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd159;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd161;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 143
$L__BB28_52:
	mov.u64 	%rd151, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd152, %rd151;
	mov.u64 	%rd153, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd154, %rd153;
	{ // callseq 142, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd152;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd154;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 142

}
.func _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE(
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_0,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_1,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_2,
	.param .b64 _ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_3
)
{
	.reg .pred 	%p<63>;
	.reg .b32 	%r<399>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<180>;

	ld.param.u64 	%rd57, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_2];
	setp.eq.s64 	%p8, %rd57, 0;
	@%p8 bra 	$L__BB29_5;
	ld.param.u64 	%rd56, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_1];
	ld.param.u64 	%rd58, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_0];
	add.s64 	%rd1, %rd57, -1;
	setp.gt.u64 	%p9, %rd56, 511;
	ld.param.u64 	%rd59, [_ZN15nonphysical_ptx6signal7fourier11ptx_fourier35ComplexFourierTransformPtx$LT$_$GT$8core_fft17h80fd7a5a5e5a5e3eE_param_3];
	add.s64 	%rd60, %rd56, 1;
	selp.b64 	%rd61, 512, %rd60, %p9;
	ld.u64 	%rd62, [%rd58];
	ld.u64 	%rd63, [%rd58+8];
	ld.u32 	%r1, [%rd59];
	shl.b64 	%rd64, %rd63, 3;
	add.s64 	%rd2, %rd62, %rd64;
	setp.eq.s64 	%p10, %rd63, 0;
	selp.b64 	%rd3, 0, %rd62, %p10;
	selp.b64 	%rd65, 0, 8, %p10;
	add.s64 	%rd4, %rd62, %rd65;
	add.s64 	%rd66, %rd61, %rd1;
	setp.lt.u64 	%p11, %rd66, %rd61;
	setp.gt.u64 	%p12, %rd66, 511;
	or.pred  	%p1, %p11, %p12;
	mov.u64 	%rd7, 1;
	mov.u64 	%rd8, 10;
	bra.uni 	$L__BB29_2;
$L__BB29_32:
	shl.b64 	%rd7, %rd7, 1;
	bar.sync 	0;
	setp.gt.u64 	%p37, %rd8, 2;
	@%p37 bra 	$L__BB29_2;
	bra.uni 	$L__BB29_33;
$L__BB29_2:
	add.s64 	%rd8, %rd8, -1;
	@%p9 bra 	$L__BB29_32;
	cvt.u32.u64 	%r13, %rd8;
	shr.u64 	%rd69, %rd56, %r13;
	mov.u64 	%rd70, 1;
	shl.b64 	%rd27, %rd70, %r13;
	mov.u64 	%rd71, 2;
	shl.b64 	%rd28, %rd71, %r13;
	add.s64 	%rd29, %rd27, -1;
	and.b64  	%rd172, %rd29, %rd56;
	mul.lo.s64 	%rd72, %rd69, %rd28;
	add.s64 	%rd31, %rd172, %rd72;
	setp.lt.u64 	%p14, %rd31, 1024;
	@%p14 bra 	$L__BB29_20;
	bra.uni 	$L__BB29_4;
$L__BB29_20:
	cvt.u32.u64 	%r18, %rd31;
	shl.b32 	%r19, %r18, 3;
	add.s32 	%r16, %r1, %r19;
	add.s32 	%r17, %r16, 4;
	// begin inline asm
	ld.shared.f32 %r14, [%r16];
ld.shared.f32 %r15, [%r17];
	// end inline asm
	add.s64 	%rd32, %rd31, %rd27;
	setp.lt.u64 	%p15, %rd32, 1024;
	@%p15 bra 	$L__BB29_22;
	bra.uni 	$L__BB29_21;
$L__BB29_22:
	cvt.u32.u64 	%r24, %rd32;
	shl.b32 	%r25, %r24, 3;
	add.s32 	%r22, %r1, %r25;
	add.s32 	%r23, %r22, 4;
	// begin inline asm
	ld.shared.f32 %r20, [%r22];
ld.shared.f32 %r21, [%r23];
	// end inline asm
	setp.eq.s64 	%p16, %rd172, 0;
	mov.u64 	%rd162, -1;
	mov.u64 	%rd177, %rd3;
	@%p16 bra 	$L__BB29_29;
	mul.lo.s64 	%rd176, %rd172, %rd7;
	mul.hi.u64 	%rd81, %rd172, %rd7;
	setp.eq.s64 	%p17, %rd81, 0;
	mov.u64 	%rd174, %rd4;
	@%p17 bra 	$L__BB29_28;
	mov.u64 	%rd173, %rd7;
	mov.u64 	%rd174, %rd4;
$L__BB29_25:
	setp.eq.s64 	%p18, %rd172, 0;
	@%p18 bra 	$L__BB29_55;
	setp.eq.s64 	%p19, %rd173, 0;
	@%p19 bra 	$L__BB29_56;
	div.u64 	%rd83, %rd162, %rd172;
	div.u64 	%rd84, %rd162, %rd173;
	mul.lo.s64 	%rd85, %rd83, %rd172;
	mul.lo.s64 	%rd86, %rd84, %rd173;
	setp.gt.u64 	%p20, %rd85, %rd86;
	max.u64 	%rd87, %rd85, %rd86;
	selp.b64 	%rd88, %rd83, 0, %p20;
	sub.s64 	%rd173, %rd173, %rd88;
	selp.b64 	%rd89, 0, %rd84, %p20;
	sub.s64 	%rd172, %rd172, %rd89;
	add.s64 	%rd90, %rd87, -1;
	sub.s64 	%rd91, %rd2, %rd174;
	shr.u64 	%rd92, %rd91, 3;
	setp.gt.u64 	%p21, %rd92, %rd90;
	shl.b64 	%rd93, %rd87, 3;
	add.s64 	%rd94, %rd174, %rd93;
	selp.b64 	%rd174, %rd94, %rd2, %p21;
	mul.lo.s64 	%rd176, %rd172, %rd173;
	mul.hi.u64 	%rd95, %rd172, %rd173;
	setp.ne.s64 	%p6, %rd95, 0;
	@%p6 bra 	$L__BB29_25;
$L__BB29_28:
	add.s64 	%rd96, %rd176, -1;
	sub.s64 	%rd97, %rd2, %rd174;
	shr.u64 	%rd98, %rd97, 3;
	setp.gt.u64 	%p22, %rd98, %rd96;
	shl.b64 	%rd99, %rd176, 3;
	add.s64 	%rd100, %rd174, %rd99;
	add.s64 	%rd101, %rd100, -8;
	selp.b64 	%rd177, %rd101, 0, %p22;
$L__BB29_29:
	setp.ne.s64 	%p23, %rd177, 0;
	@%p23 bra 	$L__BB29_31;
	bra.uni 	$L__BB29_30;
$L__BB29_31:
	mov.b32 	%f6, %r15;
	mov.b32 	%f5, %r14;
	mov.b32 	%f8, %r21;
	mov.b32 	%f7, %r20;
	// begin inline asm
	add.rn.ftz.f32 %r26, %r14, %r20;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r29, %r15, %r21;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r16], %r26;
st.shared.f32 [%r17], %r29;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r43, %r14, %r20;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r46, %r15, %r21;
	// end inline asm
	ld.u32 	%r56, [%rd177];
	ld.u32 	%r53, [%rd177+4];
	// begin inline asm
	mul.rn.ftz.f32 %r42, %r43, %r56;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r45, %r46, %r53;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r48, %r42, %r45;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r51, %r43, %r53;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r54, %r46, %r56;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r57, %r51, %r54;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r22], %r48;
st.shared.f32 [%r23], %r57;
	// end inline asm
	mov.u64 	%rd9, %rd66;
	@%p1 bra 	$L__BB29_32;
$L__BB29_7:
	shr.u64 	%rd106, %rd9, %r13;
	and.b64  	%rd166, %rd9, %rd29;
	mul.lo.s64 	%rd107, %rd106, %rd28;
	add.s64 	%rd12, %rd107, %rd166;
	setp.lt.u64 	%p24, %rd12, 1024;
	@%p24 bra 	$L__BB29_9;
	bra.uni 	$L__BB29_8;
$L__BB29_9:
	cvt.u32.u64 	%r69, %rd12;
	shl.b32 	%r70, %r69, 3;
	add.s32 	%r67, %r70, %r1;
	add.s32 	%r68, %r67, 4;
	// begin inline asm
	ld.shared.f32 %r65, [%r67];
ld.shared.f32 %r66, [%r68];
	// end inline asm
	add.s64 	%rd13, %rd12, %rd27;
	setp.lt.u64 	%p25, %rd13, 1024;
	@%p25 bra 	$L__BB29_11;
	bra.uni 	$L__BB29_10;
$L__BB29_11:
	cvt.u32.u64 	%r75, %rd13;
	shl.b32 	%r76, %r75, 3;
	add.s32 	%r73, %r76, %r1;
	add.s32 	%r74, %r73, 4;
	// begin inline asm
	ld.shared.f32 %r71, [%r73];
ld.shared.f32 %r72, [%r74];
	// end inline asm
	setp.eq.s64 	%p26, %rd166, 0;
	mov.u64 	%rd171, %rd3;
	@%p26 bra 	$L__BB29_18;
	mul.lo.s64 	%rd170, %rd166, %rd7;
	mul.hi.u64 	%rd116, %rd166, %rd7;
	setp.eq.s64 	%p27, %rd116, 0;
	mov.u64 	%rd168, %rd4;
	@%p27 bra 	$L__BB29_17;
	mov.u64 	%rd167, %rd7;
	mov.u64 	%rd168, %rd4;
$L__BB29_14:
	setp.eq.s64 	%p28, %rd166, 0;
	@%p28 bra 	$L__BB29_53;
	setp.eq.s64 	%p29, %rd167, 0;
	@%p29 bra 	$L__BB29_54;
	div.u64 	%rd118, %rd162, %rd166;
	div.u64 	%rd119, %rd162, %rd167;
	mul.lo.s64 	%rd120, %rd118, %rd166;
	mul.lo.s64 	%rd121, %rd119, %rd167;
	setp.gt.u64 	%p30, %rd120, %rd121;
	max.u64 	%rd122, %rd120, %rd121;
	selp.b64 	%rd123, %rd118, 0, %p30;
	sub.s64 	%rd167, %rd167, %rd123;
	selp.b64 	%rd124, 0, %rd119, %p30;
	sub.s64 	%rd166, %rd166, %rd124;
	add.s64 	%rd125, %rd122, -1;
	sub.s64 	%rd126, %rd2, %rd168;
	shr.u64 	%rd127, %rd126, 3;
	setp.gt.u64 	%p31, %rd127, %rd125;
	shl.b64 	%rd128, %rd122, 3;
	add.s64 	%rd129, %rd168, %rd128;
	selp.b64 	%rd168, %rd129, %rd2, %p31;
	mul.lo.s64 	%rd170, %rd166, %rd167;
	mul.hi.u64 	%rd130, %rd166, %rd167;
	setp.ne.s64 	%p3, %rd130, 0;
	@%p3 bra 	$L__BB29_14;
$L__BB29_17:
	add.s64 	%rd131, %rd170, -1;
	sub.s64 	%rd132, %rd2, %rd168;
	shr.u64 	%rd133, %rd132, 3;
	setp.gt.u64 	%p32, %rd133, %rd131;
	shl.b64 	%rd134, %rd170, 3;
	add.s64 	%rd135, %rd168, %rd134;
	add.s64 	%rd136, %rd135, -8;
	selp.b64 	%rd171, %rd136, 0, %p32;
$L__BB29_18:
	setp.ne.s64 	%p33, %rd171, 0;
	@%p33 bra 	$L__BB29_6;
	bra.uni 	$L__BB29_19;
$L__BB29_6:
	add.s64 	%rd10, %rd9, 1;
	mov.b32 	%f2, %r66;
	mov.b32 	%f1, %r65;
	mov.b32 	%f4, %r72;
	mov.b32 	%f3, %r71;
	// begin inline asm
	add.rn.ftz.f32 %r77, %r65, %r71;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r80, %r66, %r72;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r67], %r77;
st.shared.f32 [%r68], %r80;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r94, %r65, %r71;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r97, %r66, %r72;
	// end inline asm
	ld.u32 	%r107, [%rd171];
	ld.u32 	%r104, [%rd171+4];
	// begin inline asm
	mul.rn.ftz.f32 %r93, %r94, %r107;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r96, %r97, %r104;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r99, %r93, %r96;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r102, %r94, %r104;
	// end inline asm
	// begin inline asm
	mul.rn.ftz.f32 %r105, %r97, %r107;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r108, %r102, %r105;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r73], %r99;
st.shared.f32 [%r74], %r108;
	// end inline asm
	add.s64 	%rd141, %rd10, %rd1;
	setp.lt.u64 	%p34, %rd141, %rd10;
	add.s64 	%rd9, %rd9, %rd57;
	setp.gt.u64 	%p35, %rd9, 511;
	or.pred  	%p36, %p34, %p35;
	@%p36 bra 	$L__BB29_32;
	bra.uni 	$L__BB29_7;
$L__BB29_33:
	@%p9 bra 	$L__BB29_34;
	and.b64  	%rd146, %rd56, 1;
	setp.eq.b64 	%p39, %rd146, 1;
	mov.pred 	%p40, 0;
	xor.pred  	%p41, %p39, %p40;
	cvt.u32.u64 	%r115, %rd56;
	shl.b32 	%r12, %r115, 4;
	@%p41 bra 	$L__BB29_36;
	add.s32 	%r151, %r1, %r12;
	add.s32 	%r152, %r151, 4;
	// begin inline asm
	ld.shared.f32 %r158, [%r151];
ld.shared.f32 %r161, [%r152];
	// end inline asm
	add.s32 	%r173, %r151, 16;
	add.s32 	%r174, %r151, 20;
	// begin inline asm
	ld.shared.f32 %r159, [%r173];
ld.shared.f32 %r162, [%r174];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r165, %r158, %r159;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r166, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r151], %r165;
st.shared.f32 [%r152], %r166;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r175, %r158, %r159;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r176, %r161, %r162;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r173], %r175;
st.shared.f32 [%r174], %r176;
	// end inline asm
	bra.uni 	$L__BB29_37;
$L__BB29_36:
	and.b32  	%r146, %r12, 8160;
	add.s32 	%r147, %r1, %r146;
	add.s32 	%r130, %r147, 8;
	add.s32 	%r131, %r147, 12;
	// begin inline asm
	ld.shared.f32 %r125, [%r130];
ld.shared.f32 %r128, [%r131];
	// end inline asm
	or.b32  	%r148, %r12, 24;
	add.s32 	%r122, %r1, %r148;
	add.s32 	%r123, %r122, 4;
	// begin inline asm
	ld.shared.f32 %r126, [%r122];
ld.shared.f32 %r129, [%r123];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r132, %r125, %r126;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r133, %r128, %r129;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r130], %r132;
st.shared.f32 [%r131], %r133;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r141, %r125, %r126;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r144, %r128, %r129;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r145, %r141;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r122], %r144;
st.shared.f32 [%r123], %r145;
	// end inline asm
$L__BB29_37:
	@%p1 bra 	$L__BB29_34;
	add.s64 	%rd46, %rd66, 1;
	and.b64  	%rd147, %rd66, 1;
	setp.eq.b64 	%p42, %rd147, 1;
	xor.pred  	%p44, %p42, %p40;
	not.pred 	%p45, %p44;
	cvt.u32.u64 	%r177, %rd66;
	shl.b32 	%r10, %r177, 4;
	@%p45 bra 	$L__BB29_40;
	bra.uni 	$L__BB29_39;
$L__BB29_40:
	add.s32 	%r213, %r1, %r10;
	add.s32 	%r214, %r213, 4;
	// begin inline asm
	ld.shared.f32 %r220, [%r213];
ld.shared.f32 %r223, [%r214];
	// end inline asm
	add.s32 	%r235, %r213, 16;
	add.s32 	%r236, %r213, 20;
	// begin inline asm
	ld.shared.f32 %r221, [%r235];
ld.shared.f32 %r224, [%r236];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r227, %r220, %r221;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r228, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r213], %r227;
st.shared.f32 [%r214], %r228;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r237, %r220, %r221;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r238, %r223, %r224;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r235], %r237;
st.shared.f32 [%r236], %r238;
	// end inline asm
	bra.uni 	$L__BB29_41;
$L__BB29_39:
	and.b32  	%r208, %r10, 8160;
	add.s32 	%r209, %r1, %r208;
	add.s32 	%r192, %r209, 8;
	add.s32 	%r193, %r209, 12;
	// begin inline asm
	ld.shared.f32 %r187, [%r192];
ld.shared.f32 %r190, [%r193];
	// end inline asm
	or.b32  	%r210, %r10, 24;
	add.s32 	%r184, %r1, %r210;
	add.s32 	%r185, %r184, 4;
	// begin inline asm
	ld.shared.f32 %r188, [%r184];
ld.shared.f32 %r191, [%r185];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r194, %r187, %r188;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r195, %r190, %r191;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r192], %r194;
st.shared.f32 [%r193], %r195;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r203, %r187, %r188;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r206, %r190, %r191;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r207, %r203;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r184], %r206;
st.shared.f32 [%r185], %r207;
	// end inline asm
$L__BB29_41:
	add.s64 	%rd148, %rd46, %rd1;
	setp.lt.u64 	%p46, %rd148, %rd46;
	add.s64 	%rd178, %rd66, %rd57;
	setp.gt.u64 	%p47, %rd178, 511;
	or.pred  	%p48, %p46, %p47;
	@%p48 bra 	$L__BB29_34;
	bra.uni 	$L__BB29_42;
$L__BB29_34:
	bar.sync 	0;
	@%p9 bra 	$L__BB29_51;
	cvt.u32.u64 	%r329, %rd56;
	shl.b32 	%r330, %r329, 4;
	add.s32 	%r303, %r1, %r330;
	add.s32 	%r304, %r303, 4;
	// begin inline asm
	ld.shared.f32 %r310, [%r303];
ld.shared.f32 %r313, [%r304];
	// end inline asm
	add.s32 	%r307, %r303, 8;
	add.s32 	%r308, %r303, 12;
	// begin inline asm
	ld.shared.f32 %r311, [%r307];
ld.shared.f32 %r314, [%r308];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r309, %r310, %r311;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r312, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r303], %r309;
st.shared.f32 [%r304], %r312;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r319, %r310, %r311;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r322, %r313, %r314;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r307], %r319;
st.shared.f32 [%r308], %r322;
	// end inline asm
	@!%p1 bra 	$L__BB29_48;
	bra.uni 	$L__BB29_51;
$L__BB29_48:
	shl.b64 	%rd55, %rd66, 1;
	setp.gt.u64 	%p56, %rd55, 1023;
	@%p56 bra 	$L__BB29_52;
	add.s64 	%rd54, %rd66, 1;
	cvt.u32.u64 	%r359, %rd55;
	shl.b32 	%r360, %r359, 3;
	add.s32 	%r333, %r1, %r360;
	add.s32 	%r334, %r333, 4;
	// begin inline asm
	ld.shared.f32 %r340, [%r333];
ld.shared.f32 %r343, [%r334];
	// end inline asm
	add.s32 	%r337, %r333, 8;
	add.s32 	%r338, %r333, 12;
	// begin inline asm
	ld.shared.f32 %r341, [%r337];
ld.shared.f32 %r344, [%r338];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r339, %r340, %r341;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r342, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r333], %r339;
st.shared.f32 [%r334], %r342;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r349, %r340, %r341;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r352, %r343, %r344;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r337], %r349;
st.shared.f32 [%r338], %r352;
	// end inline asm
	add.s64 	%rd155, %rd54, %rd1;
	setp.lt.u64 	%p57, %rd155, %rd54;
	add.s64 	%rd179, %rd66, %rd57;
	setp.gt.u64 	%p58, %rd179, 511;
	or.pred  	%p59, %p57, %p58;
	@%p59 bra 	$L__BB29_51;
$L__BB29_50:
	add.s64 	%rd156, %rd179, 1;
	cvt.u32.u64 	%r389, %rd179;
	shl.b32 	%r390, %r389, 4;
	add.s32 	%r363, %r390, %r1;
	add.s32 	%r364, %r363, 4;
	// begin inline asm
	ld.shared.f32 %r370, [%r363];
ld.shared.f32 %r373, [%r364];
	// end inline asm
	add.s32 	%r367, %r363, 8;
	add.s32 	%r368, %r363, 12;
	// begin inline asm
	ld.shared.f32 %r371, [%r367];
ld.shared.f32 %r374, [%r368];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r369, %r370, %r371;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r372, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r363], %r369;
st.shared.f32 [%r364], %r372;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r379, %r370, %r371;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r382, %r373, %r374;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r367], %r379;
st.shared.f32 [%r368], %r382;
	// end inline asm
	add.s64 	%rd157, %rd156, %rd1;
	setp.lt.u64 	%p60, %rd157, %rd156;
	add.s64 	%rd179, %rd179, %rd57;
	setp.gt.u64 	%p61, %rd179, 511;
	or.pred  	%p62, %p60, %p61;
	@%p62 bra 	$L__BB29_51;
	bra.uni 	$L__BB29_50;
$L__BB29_51:
	ret;
$L__BB29_44:
	and.b32  	%r270, %r11, 8160;
	add.s32 	%r271, %r1, %r270;
	add.s32 	%r254, %r271, 8;
	add.s32 	%r255, %r271, 12;
	// begin inline asm
	ld.shared.f32 %r249, [%r254];
ld.shared.f32 %r252, [%r255];
	// end inline asm
	or.b32  	%r272, %r11, 24;
	add.s32 	%r246, %r272, %r1;
	add.s32 	%r247, %r246, 4;
	// begin inline asm
	ld.shared.f32 %r250, [%r246];
ld.shared.f32 %r253, [%r247];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r256, %r249, %r250;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r257, %r252, %r253;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r254], %r256;
st.shared.f32 [%r255], %r257;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r265, %r249, %r250;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r268, %r252, %r253;
	// end inline asm
	// begin inline asm
	neg.ftz.f32 %r269, %r265;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r246], %r268;
st.shared.f32 [%r247], %r269;
	// end inline asm
$L__BB29_45:
	add.s64 	%rd49, %rd178, 1;
	add.s64 	%rd150, %rd49, %rd1;
	setp.lt.u64 	%p52, %rd150, %rd49;
	add.s64 	%rd178, %rd178, %rd57;
	setp.gt.u64 	%p53, %rd178, 511;
	or.pred  	%p54, %p52, %p53;
	@%p54 bra 	$L__BB29_34;
$L__BB29_42:
	and.b64  	%rd149, %rd178, 1;
	setp.eq.b64 	%p49, %rd149, 1;
	xor.pred  	%p51, %p49, %p40;
	cvt.u32.u64 	%r239, %rd178;
	shl.b32 	%r11, %r239, 4;
	@%p51 bra 	$L__BB29_44;
	add.s32 	%r275, %r11, %r1;
	add.s32 	%r276, %r275, 4;
	// begin inline asm
	ld.shared.f32 %r282, [%r275];
ld.shared.f32 %r285, [%r276];
	// end inline asm
	add.s32 	%r297, %r275, 16;
	add.s32 	%r298, %r275, 20;
	// begin inline asm
	ld.shared.f32 %r283, [%r297];
ld.shared.f32 %r286, [%r298];
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r289, %r282, %r283;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r290, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r275], %r289;
st.shared.f32 [%r276], %r290;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r299, %r282, %r283;
	// end inline asm
	// begin inline asm
	sub.rn.ftz.f32 %r300, %r285, %r286;
	// end inline asm
	// begin inline asm
	st.shared.f32 [%r297], %r299;
st.shared.f32 [%r298], %r300;
	// end inline asm
	bra.uni 	$L__BB29_45;
$L__BB29_54:
	mov.u64 	%rd137, anon_$_9b271fbca47443a0c783d86781d13fba_$_2;
	cvta.global.u64 	%rd138, %rd137;
	{ // callseq 150, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd138;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 150
$L__BB29_53:
	mov.u64 	%rd139, anon_$_9b271fbca47443a0c783d86781d13fba_$_1;
	cvta.global.u64 	%rd140, %rd139;
	{ // callseq 151, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd140;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 151
$L__BB29_10:
	mov.u64 	%rd112, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd113, %rd112;
	mov.u64 	%rd114, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd115, %rd114;
	{ // callseq 149, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd113;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd115;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 149
$L__BB29_8:
	mov.u64 	%rd108, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd109, %rd108;
	mov.u64 	%rd110, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd111, %rd110;
	{ // callseq 148, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd109;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd111;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 148
$L__BB29_19:
	mov.u64 	%rd142, anon_$_9b271fbca47443a0c783d86781d13fba_$_10;
	cvta.global.u64 	%rd143, %rd142;
	{ // callseq 152, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd143;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 152
$L__BB29_55:
	mov.u64 	%rd104, anon_$_9b271fbca47443a0c783d86781d13fba_$_1;
	cvta.global.u64 	%rd105, %rd104;
	{ // callseq 147, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd105;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 147
$L__BB29_56:
	mov.u64 	%rd102, anon_$_9b271fbca47443a0c783d86781d13fba_$_2;
	cvta.global.u64 	%rd103, %rd102;
	{ // callseq 146, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd103;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 146
$L__BB29_30:
	mov.u64 	%rd144, anon_$_9b271fbca47443a0c783d86781d13fba_$_10;
	cvta.global.u64 	%rd145, %rd144;
	{ // callseq 153, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd145;
	call.uni 
	_ZN4core6option13unwrap_failed17hfcad5e6976a1bc89E, 
	(
	param0
	);
	} // callseq 153
$L__BB29_4:
	mov.u64 	%rd73, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd74, %rd73;
	mov.u64 	%rd75, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd76, %rd75;
	{ // callseq 144, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd74;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd76;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 144
$L__BB29_21:
	mov.u64 	%rd77, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd78, %rd77;
	mov.u64 	%rd79, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd80, %rd79;
	{ // callseq 145, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd78;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd80;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 145
$L__BB29_5:
	mov.u64 	%rd158, anon_$_9b271fbca47443a0c783d86781d13fba_$_5;
	cvta.global.u64 	%rd159, %rd158;
	mov.u64 	%rd160, anon_$_9b271fbca47443a0c783d86781d13fba_$_6;
	cvta.global.u64 	%rd161, %rd160;
	{ // callseq 155, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd159;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd161;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 155
$L__BB29_52:
	mov.u64 	%rd151, anon_$_9b271fbca47443a0c783d86781d13fba_$_63;
	cvta.global.u64 	%rd152, %rd151;
	mov.u64 	%rd153, anon_$_9b271fbca47443a0c783d86781d13fba_$_65;
	cvta.global.u64 	%rd154, %rd153;
	{ // callseq 154, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd152;
	.param .b64 param1;
	st.param.b64 	[param1+0], 27;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd154;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 154

}
	// .globl	_ZN81_$LT$nonphysical_ptx$$shared$$primitive$$F32$u20$as$u20$core$$ops$$arith$$Rem$GT$3rem17h47656ded776ad7efE
.visible .func  (.param .b32 func_retval0) _ZN81_$LT$nonphysical_ptx$$shared$$primitive$$F32$u20$as$u20$core$$ops$$arith$$Rem$GT$3rem17h47656ded776ad7efE(
	.param .b32 _ZN81_$LT$nonphysical_ptx$$shared$$primitive$$F32$u20$as$u20$core$$ops$$arith$$Rem$GT$3rem17h47656ded776ad7efE_param_0,
	.param .b32 _ZN81_$LT$nonphysical_ptx$$shared$$primitive$$F32$u20$as$u20$core$$ops$$arith$$Rem$GT$3rem17h47656ded776ad7efE_param_1
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9b271fbca47443a0c783d86781d13fba_$_20;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 156, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 156

}
	// .globl	_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E
.visible .func  (.param .b32 func_retval0) _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E(
	.param .b32 _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E_param_0,
	.param .b32 _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<3>;

	ld.param.u32 	%r2, [_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E_param_1];
	setp.eq.s32 	%p1, %r2, 0;
	@%p1 bra 	$L__BB31_2;
	ld.param.u32 	%r1, [_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$Div$GT$3div17ha4b5946742d66902E_param_0];
	div.u32 	%r3, %r1, %r2;
	st.param.b32 	[func_retval0+0], %r3;
	ret;
$L__BB31_2:
	mov.u64 	%rd1, anon_$_9b271fbca47443a0c783d86781d13fba_$_22;
	cvta.global.u64 	%rd2, %rd1;
	{ // callseq 157, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 157

}
	// .globl	_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E
.visible .func _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E(
	.param .b64 _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E_param_0,
	.param .b32 _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<4>;

	ld.param.u32 	%r1, [_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E_param_1];
	setp.eq.s32 	%p1, %r1, 0;
	@%p1 bra 	$L__BB32_2;
	ld.param.u64 	%rd1, [_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U32$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h4dc25cac39cb0540E_param_0];
	ld.u32 	%r2, [%rd1];
	div.u32 	%r3, %r2, %r1;
	st.u32 	[%rd1], %r3;
	ret;
$L__BB32_2:
	mov.u64 	%rd2, anon_$_9b271fbca47443a0c783d86781d13fba_$_23;
	cvta.global.u64 	%rd3, %rd2;
	{ // callseq 158, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd3;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 158

}
	// .globl	_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE
.visible .func  (.param .b64 func_retval0) _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE(
	.param .b64 _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE_param_0,
	.param .b64 _ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd5, [_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE_param_1];
	setp.eq.s64 	%p1, %rd5, 0;
	@%p1 bra 	$L__BB33_5;
	ld.param.u64 	%rd4, [_ZN80_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$Div$GT$3div17haa5dc319ce36635aE_param_0];
	or.b64  	%rd6, %rd4, %rd5;
	and.b64  	%rd7, %rd6, -4294967296;
	setp.ne.s64 	%p2, %rd7, 0;
	@%p2 bra 	$L__BB33_3;
	bra.uni 	$L__BB33_2;
$L__BB33_3:
	div.u64 	%rd10, %rd4, %rd5;
	bra.uni 	$L__BB33_4;
$L__BB33_2:
	cvt.u32.u64 	%r1, %rd5;
	cvt.u32.u64 	%r2, %rd4;
	div.u32 	%r3, %r2, %r1;
	cvt.u64.u32 	%rd10, %r3;
$L__BB33_4:
	st.param.b64 	[func_retval0+0], %rd10;
	ret;
$L__BB33_5:
	mov.u64 	%rd8, anon_$_9b271fbca47443a0c783d86781d13fba_$_24;
	cvta.global.u64 	%rd9, %rd8;
	{ // callseq 159, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd9;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 159

}
	// .globl	_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE
.visible .func _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE(
	.param .b64 _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE_param_0,
	.param .b64 _ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE_param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<12>;

	ld.param.u64 	%rd6, [_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE_param_1];
	setp.eq.s64 	%p1, %rd6, 0;
	@%p1 bra 	$L__BB34_5;
	ld.param.u64 	%rd5, [_ZN86_$LT$nonphysical_ptx$$shared$$unsigned$$U64$u20$as$u20$core$$ops$$arith$$DivAssign$GT$10div_assign17h92ce44bc0f4327cdE_param_0];
	ld.u64 	%rd1, [%rd5];
	or.b64  	%rd7, %rd1, %rd6;
	and.b64  	%rd8, %rd7, -4294967296;
	setp.ne.s64 	%p2, %rd8, 0;
	@%p2 bra 	$L__BB34_3;
	bra.uni 	$L__BB34_2;
$L__BB34_3:
	div.u64 	%rd11, %rd1, %rd6;
	bra.uni 	$L__BB34_4;
$L__BB34_2:
	cvt.u32.u64 	%r1, %rd6;
	cvt.u32.u64 	%r2, %rd1;
	div.u32 	%r3, %r2, %r1;
	cvt.u64.u32 	%rd11, %r3;
$L__BB34_4:
	st.u64 	[%rd5], %rd11;
	ret;
$L__BB34_5:
	mov.u64 	%rd9, anon_$_9b271fbca47443a0c783d86781d13fba_$_25;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 160, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd10;
	call.uni 
	_ZN4core9panicking11panic_const23panic_const_div_by_zero17hbcf7982255ac5ef9E, 
	(
	param0
	);
	} // callseq 160

}
	// .globl	vector_sum_f32
.visible .entry vector_sum_f32(
	.param .u64 vector_sum_f32_param_0
)
{
	.reg .pred 	%p<13>;
	.reg .b32 	%r<107>;
	.reg .f32 	%f<16>;
	.reg .b64 	%rd<54>;

	ld.param.u64 	%rd23, [vector_sum_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd23;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	mov.u32 	%r6, %tid.x;
	cvt.u64.u32 	%rd4, %r6;
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	shl.b64 	%rd24, %rd3, 2;
	add.s64 	%rd5, %rd2, %rd24;
	mul.wide.u32 	%rd25, %r7, %r1;
	add.s64 	%rd6, %rd25, %rd4;
	// begin inline asm
	.shared .align 4 .b8 nonphysical[128];
    mov.u32 %r5, nonphysical;
	// end inline asm
	setp.eq.s64 	%p1, %rd6, 0;
	@%p1 bra 	$L__BB35_1;
	setp.gt.u64 	%p2, %rd3, %rd6;
	shl.b64 	%rd26, %rd6, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd52, %rd28, %rd5, %p2;
	selp.b64 	%rd53, %rd27, 0, %p2;
	bra.uni 	$L__BB35_6;
$L__BB35_1:
	setp.eq.s64 	%p3, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p3;
	add.s64 	%rd52, %rd2, %rd29;
	selp.b64 	%rd53, 0, %rd2, %p3;
$L__BB35_6:
	setp.eq.s64 	%p4, %rd53, 0;
	mov.f32 	%f13, 0f00000000;
	@%p4 bra 	$L__BB35_8;
	mov.u32 	%r8, %nctaid.x;
	mul.wide.u32 	%rd7, %r1, %r8;
	add.s64 	%rd8, %rd7, -1;
	ld.u32 	%r11, [%rd53];
	mov.b32 	%r10, 0;
	// begin inline asm
	add.rn.ftz.f32 %r9, %r10, %r11;
	// end inline asm
	mov.b32 	%f13, %r9;
	sub.s64 	%rd30, %rd5, %rd52;
	shr.u64 	%rd31, %rd30, 2;
	setp.gt.u64 	%p5, %rd31, %rd8;
	@%p5 bra 	$L__BB35_2;
	bra.uni 	$L__BB35_8;
$L__BB35_2:
	shl.b64 	%rd32, %rd7, 2;
	add.s64 	%rd33, %rd52, %rd32;
	add.s64 	%rd34, %rd33, -4;
	ld.u32 	%r14, [%rd33+-4];
	// begin inline asm
	add.rn.ftz.f32 %r106, %r9, %r14;
	// end inline asm
	mov.b32 	%f13, %r106;
	sub.s64 	%rd35, %rd5, %rd34;
	add.s64 	%rd36, %rd35, -4;
	shr.u64 	%rd37, %rd36, 2;
	setp.le.u64 	%p6, %rd37, %rd8;
	@%p6 bra 	$L__BB35_8;
	shl.b64 	%rd40, %rd7, 3;
	sub.s64 	%rd41, %rd5, %rd40;
	sub.s64 	%rd51, %rd41, %rd52;
	add.s64 	%rd42, %rd40, %rd52;
	add.s64 	%rd50, %rd42, -4;
$L__BB35_4:
	ld.u32 	%r17, [%rd50];
	// begin inline asm
	add.rn.ftz.f32 %r15, %r106, %r17;
	// end inline asm
	mov.b32 	%f13, %r15;
	shr.u64 	%rd43, %rd51, 2;
	setp.gt.u64 	%p7, %rd43, %rd8;
	sub.s64 	%rd51, %rd51, %rd32;
	add.s64 	%rd50, %rd50, %rd32;
	mov.u32 	%r106, %r15;
	@%p7 bra 	$L__BB35_4;
$L__BB35_8:
	cvt.u32.u64 	%r53, %rd4;
	mov.b32 	%r21, %f13;
	mov.b32 	%r19, 16;
	mov.b32 	%r67, 31;
	// begin inline asm
	shfl.sync.bfly.b32 %r18,%r21, %r19, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r28, %r21, %r18;
	// end inline asm
	mov.b32 	%r26, 8;
	// begin inline asm
	shfl.sync.bfly.b32 %r25,%r28, %r26, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r35, %r28, %r25;
	// end inline asm
	mov.b32 	%r33, 4;
	// begin inline asm
	shfl.sync.bfly.b32 %r32,%r35, %r33, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r42, %r35, %r32;
	// end inline asm
	mov.b32 	%r40, 2;
	// begin inline asm
	shfl.sync.bfly.b32 %r39,%r42, %r40, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r49, %r42, %r39;
	// end inline asm
	mov.b32 	%r47, 1;
	// begin inline asm
	shfl.sync.bfly.b32 %r46,%r49, %r47, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r104, %r49, %r46;
	// end inline asm
	and.b64  	%rd22, %rd4, 31;
	setp.ne.s64 	%p8, %rd22, 0;
	@%p8 bra 	$L__BB35_10;
	mov.b32 	%f6, %r104;
	shr.u32 	%r54, %r53, 3;
	and.b32  	%r55, %r54, 124;
	add.s32 	%r56, %r5, %r55;
	// begin inline asm
	st.shared.f32 [%r56], %r104;
	// end inline asm
$L__BB35_10:
	bar.sync 	0;
	shr.u32 	%r59, %r1, 5;
	setp.ge.u32 	%p9, %r53, %r59;
	mov.f32 	%f14, 0f00000000;
	@%p9 bra 	$L__BB35_12;
	cvt.u32.u64 	%r60, %rd22;
	shl.b32 	%r61, %r60, 2;
	add.s32 	%r63, %r5, %r61;
	// begin inline asm
	ld.shared.f32 %r62, [%r63];
	// end inline asm
	mov.b32 	%f14, %r62;
$L__BB35_12:
	setp.gt.u32 	%p10, %r53, 31;
	@%p10 bra 	$L__BB35_14;
	setp.eq.s64 	%p11, %rd22, 0;
	mov.b32 	%r68, %f14;
	// begin inline asm
	shfl.sync.bfly.b32 %r65,%r68, %r19, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r75, %r68, %r65;
	// end inline asm
	// begin inline asm
	shfl.sync.bfly.b32 %r72,%r75, %r26, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r82, %r75, %r72;
	// end inline asm
	// begin inline asm
	shfl.sync.bfly.b32 %r79,%r82, %r33, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r89, %r82, %r79;
	// end inline asm
	// begin inline asm
	shfl.sync.bfly.b32 %r86,%r89, %r40, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r96, %r89, %r86;
	// end inline asm
	// begin inline asm
	shfl.sync.bfly.b32 %r93,%r96, %r47, %r67, 4294967295;
	// end inline asm
	// begin inline asm
	add.rn.ftz.f32 %r97, %r96, %r93;
	// end inline asm
	@%p11 bra 	$L__BB35_15;
	bra.uni 	$L__BB35_14;
$L__BB35_15:
	ld.global.nc.u64 	%rd44, [%rd1+24];
	setp.ne.s64 	%p12, %rd44, 0;
	@%p12 bra 	$L__BB35_17;
	bra.uni 	$L__BB35_16;
$L__BB35_17:
	mov.b32 	%f9, %r97;
	ld.global.nc.u64 	%rd45, [%rd1+16];
	// begin inline asm
	red.global.add.f32 [%rd45], %r97;
	// end inline asm
$L__BB35_14:
	ret;
$L__BB35_16:
	mov.u64 	%rd46, anon_$_9b271fbca47443a0c783d86781d13fba_$_26;
	cvta.global.u64 	%rd47, %rd46;
	mov.u64 	%rd48, anon_$_9b271fbca47443a0c783d86781d13fba_$_28;
	cvta.global.u64 	%rd49, %rd48;
	{ // callseq 161, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd47;
	.param .b64 param1;
	st.param.b64 	[param1+0], 40;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd49;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 161

}
	// .globl	vector_product_f32
.visible .entry vector_product_f32(
	.param .u64 vector_product_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9b271fbca47443a0c783d86781d13fba_$_30;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 162, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 162

}
	// .globl	vector_greater_f32
.visible .entry vector_greater_f32(
	.param .u64 vector_greater_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9b271fbca47443a0c783d86781d13fba_$_31;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 163, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 163

}
	// .globl	vector_lesser_f32
.visible .entry vector_lesser_f32(
	.param .u64 vector_lesser_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9b271fbca47443a0c783d86781d13fba_$_32;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 164, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 164

}
	// .globl	vector_mean_f32
.visible .entry vector_mean_f32(
	.param .u64 vector_mean_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9b271fbca47443a0c783d86781d13fba_$_33;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 165, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 165

}
	// .globl	vector_variance_f32
.visible .entry vector_variance_f32(
	.param .u64 vector_variance_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9b271fbca47443a0c783d86781d13fba_$_34;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 166, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 166

}
	// .globl	vector_deviation_f32
.visible .entry vector_deviation_f32(
	.param .u64 vector_deviation_f32_param_0
)
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9b271fbca47443a0c783d86781d13fba_$_35;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 167, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 167

}
	// .globl	vector_add_f32
.visible .entry vector_add_f32(
	.param .u64 vector_add_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_add_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB42_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB42_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB42_4;
	bra.uni 	$L__BB42_3;
$L__BB42_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB42_5;
$L__BB42_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB42_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB42_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB42_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB42_9;
	bra.uni 	$L__BB42_8;
$L__BB42_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB42_10;
$L__BB42_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB42_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB42_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB42_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB42_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB42_15;
$L__BB42_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB42_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB42_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB42_18;
$L__BB42_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB42_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB42_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	add.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB42_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB42_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB42_28;
	bra.uni 	$L__BB42_22;
$L__BB42_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB42_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB42_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	add.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB42_31:
	st.f32 	[%rd53], %f17;
$L__BB42_32:
	ret;
$L__BB42_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB42_23;
$L__BB42_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB42_23;
	bra.uni 	$L__BB42_28;
$L__BB42_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB42_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	add.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB42_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB42_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	add.rn.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB42_27;
$L__BB42_33:
	mov.u64 	%rd99, anon_$_9b271fbca47443a0c783d86781d13fba_$_36;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 168, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 168

}
	// .globl	vector_sub_f32
.visible .entry vector_sub_f32(
	.param .u64 vector_sub_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_sub_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB43_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB43_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB43_4;
	bra.uni 	$L__BB43_3;
$L__BB43_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB43_5;
$L__BB43_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB43_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB43_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB43_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB43_9;
	bra.uni 	$L__BB43_8;
$L__BB43_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB43_10;
$L__BB43_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB43_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB43_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB43_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB43_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB43_15;
$L__BB43_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB43_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB43_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB43_18;
$L__BB43_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB43_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB43_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	sub.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB43_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB43_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB43_28;
	bra.uni 	$L__BB43_22;
$L__BB43_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB43_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB43_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	sub.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB43_31:
	st.f32 	[%rd53], %f17;
$L__BB43_32:
	ret;
$L__BB43_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB43_23;
$L__BB43_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB43_23;
	bra.uni 	$L__BB43_28;
$L__BB43_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB43_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	sub.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB43_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB43_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	sub.rn.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB43_27;
$L__BB43_33:
	mov.u64 	%rd99, anon_$_9b271fbca47443a0c783d86781d13fba_$_37;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 169, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 169

}
	// .globl	vector_mul_f32
.visible .entry vector_mul_f32(
	.param .u64 vector_mul_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_mul_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB44_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB44_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB44_4;
	bra.uni 	$L__BB44_3;
$L__BB44_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB44_5;
$L__BB44_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB44_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB44_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB44_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB44_9;
	bra.uni 	$L__BB44_8;
$L__BB44_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB44_10;
$L__BB44_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB44_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB44_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB44_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB44_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB44_15;
$L__BB44_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB44_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB44_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB44_18;
$L__BB44_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB44_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB44_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB44_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB44_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB44_28;
	bra.uni 	$L__BB44_22;
$L__BB44_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB44_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB44_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB44_31:
	st.f32 	[%rd53], %f17;
$L__BB44_32:
	ret;
$L__BB44_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB44_23;
$L__BB44_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB44_23;
	bra.uni 	$L__BB44_28;
$L__BB44_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB44_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB44_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB44_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB44_27;
$L__BB44_33:
	mov.u64 	%rd99, anon_$_9b271fbca47443a0c783d86781d13fba_$_38;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 170, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 170

}
	// .globl	vector_div_f32
.visible .entry vector_div_f32(
	.param .u64 vector_div_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_div_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB45_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB45_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB45_4;
	bra.uni 	$L__BB45_3;
$L__BB45_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB45_5;
$L__BB45_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB45_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB45_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB45_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB45_9;
	bra.uni 	$L__BB45_8;
$L__BB45_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB45_10;
$L__BB45_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB45_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB45_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB45_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB45_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB45_15;
$L__BB45_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB45_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB45_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB45_18;
$L__BB45_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB45_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB45_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB45_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB45_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB45_28;
	bra.uni 	$L__BB45_22;
$L__BB45_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB45_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB45_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB45_31:
	st.f32 	[%rd53], %f17;
$L__BB45_32:
	ret;
$L__BB45_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB45_23;
$L__BB45_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB45_23;
	bra.uni 	$L__BB45_28;
$L__BB45_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB45_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB45_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB45_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB45_27;
$L__BB45_33:
	mov.u64 	%rd99, anon_$_9b271fbca47443a0c783d86781d13fba_$_39;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 171, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 171

}
	// .globl	vector_neg_f32
.visible .entry vector_neg_f32(
	.param .u64 vector_neg_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_neg_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB46_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB46_3;
	bra.uni 	$L__BB46_2;
$L__BB46_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB46_4;
$L__BB46_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB46_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB46_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB46_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB46_8;
	bra.uni 	$L__BB46_7;
$L__BB46_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB46_9;
$L__BB46_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB46_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB46_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB46_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB46_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB46_14;
$L__BB46_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB46_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB46_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB46_17;
$L__BB46_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB46_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB46_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	neg.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB46_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB46_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB46_27;
	bra.uni 	$L__BB46_21;
$L__BB46_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB46_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB46_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	neg.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB46_30:
	st.f32 	[%rd52], %f16;
$L__BB46_31:
	ret;
$L__BB46_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB46_22;
$L__BB46_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB46_22;
	bra.uni 	$L__BB46_27;
$L__BB46_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB46_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	neg.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB46_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB46_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	neg.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB46_26;

}
	// .globl	vector_scale_f32
.visible .entry vector_scale_f32(
	.param .u64 vector_scale_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_scale_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB47_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB47_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB47_4;
	bra.uni 	$L__BB47_3;
$L__BB47_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB47_5;
$L__BB47_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB47_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB47_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB47_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB47_9;
	bra.uni 	$L__BB47_8;
$L__BB47_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB47_10;
$L__BB47_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB47_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB47_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB47_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB47_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB47_15;
$L__BB47_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB47_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB47_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB47_18;
$L__BB47_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB47_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB47_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB47_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB47_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB47_28;
	bra.uni 	$L__BB47_22;
$L__BB47_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB47_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB47_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB47_31:
	st.f32 	[%rd53], %f17;
$L__BB47_32:
	ret;
$L__BB47_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB47_23;
$L__BB47_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB47_23;
	bra.uni 	$L__BB47_28;
$L__BB47_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB47_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB47_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB47_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB47_27;
$L__BB47_33:
	mov.u64 	%rd99, anon_$_9b271fbca47443a0c783d86781d13fba_$_40;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 172, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 172

}
	// .globl	vector_descale_f32
.visible .entry vector_descale_f32(
	.param .u64 vector_descale_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_descale_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB48_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB48_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB48_4;
	bra.uni 	$L__BB48_3;
$L__BB48_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB48_5;
$L__BB48_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB48_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB48_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB48_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB48_9;
	bra.uni 	$L__BB48_8;
$L__BB48_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB48_10;
$L__BB48_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB48_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB48_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB48_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB48_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB48_15;
$L__BB48_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB48_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB48_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB48_18;
$L__BB48_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB48_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB48_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB48_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB48_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB48_28;
	bra.uni 	$L__BB48_22;
$L__BB48_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB48_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB48_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB48_31:
	st.f32 	[%rd53], %f17;
$L__BB48_32:
	ret;
$L__BB48_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB48_23;
$L__BB48_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB48_23;
	bra.uni 	$L__BB48_28;
$L__BB48_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB48_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB48_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB48_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r16;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB48_27;
$L__BB48_33:
	mov.u64 	%rd99, anon_$_9b271fbca47443a0c783d86781d13fba_$_41;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 173, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 173

}
	// .globl	vector_powf_f32
.visible .entry vector_powf_f32(
	.param .u64 vector_powf_f32_param_0
)
{
	.reg .pred 	%p<25>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<18>;
	.reg .b64 	%rd<116>;

	ld.param.u64 	%rd55, [vector_powf_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd55;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd59, [%rd1+40];
	setp.eq.s64 	%p1, %rd59, 0;
	@%p1 bra 	$L__BB49_33;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd56, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd58, %r3, %r2;
	add.s64 	%rd5, %rd58, %rd56;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	ld.global.nc.u64 	%rd61, [%rd1+32];
	ld.global.nc.u64 	%rd8, [%rd1+16];
	ld.global.nc.u64 	%rd9, [%rd1+24];
	setp.le.u64 	%p2, %rd9, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd106, 0;
	mov.u64 	%rd104, %rd106;
	@%p2 bra 	$L__BB49_6;
	max.u64 	%rd63, %rd9, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p3, %rd66, 0;
	@%p3 bra 	$L__BB49_4;
	bra.uni 	$L__BB49_3;
$L__BB49_4:
	div.u64 	%rd103, %rd11, %rd6;
	bra.uni 	$L__BB49_5;
$L__BB49_3:
	cvt.u32.u64 	%r5, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd103, %r7;
$L__BB49_5:
	add.s64 	%rd104, %rd103, 1;
$L__BB49_6:
	setp.le.u64 	%p4, %rd3, %rd5;
	@%p4 bra 	$L__BB49_11;
	max.u64 	%rd68, %rd3, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p5, %rd71, 0;
	@%p5 bra 	$L__BB49_9;
	bra.uni 	$L__BB49_8;
$L__BB49_9:
	div.u64 	%rd105, %rd17, %rd6;
	bra.uni 	$L__BB49_10;
$L__BB49_8:
	cvt.u32.u64 	%r8, %rd6;
	cvt.u32.u64 	%r9, %rd17;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd105, %r10;
$L__BB49_10:
	add.s64 	%rd106, %rd105, 1;
$L__BB49_11:
	min.u64 	%rd23, %rd104, %rd106;
	setp.eq.s64 	%p6, %rd23, 0;
	@%p6 bra 	$L__BB49_32;
	shl.b64 	%rd57, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd10, %rd8, %rd62;
	setp.eq.s64 	%p7, %rd5, 0;
	shl.b64 	%rd101, %rd5, 2;
	@%p7 bra 	$L__BB49_14;
	setp.gt.u64 	%p8, %rd9, %rd5;
	add.s64 	%rd73, %rd8, %rd101;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd115, %rd74, %rd10, %p8;
	selp.b64 	%rd108, %rd73, 0, %p8;
	bra.uni 	$L__BB49_15;
$L__BB49_14:
	setp.eq.s64 	%p9, %rd9, 0;
	selp.b64 	%rd75, 0, 4, %p9;
	add.s64 	%rd115, %rd8, %rd75;
	selp.b64 	%rd108, 0, %rd8, %p9;
$L__BB49_15:
	add.s64 	%rd4, %rd2, %rd57;
	@%p7 bra 	$L__BB49_17;
	setp.gt.u64 	%p11, %rd3, %rd5;
	add.s64 	%rd77, %rd2, %rd101;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd114, %rd78, %rd4, %p11;
	selp.b64 	%rd110, %rd77, 0, %p11;
	bra.uni 	$L__BB49_18;
$L__BB49_17:
	setp.eq.s64 	%p12, %rd3, 0;
	selp.b64 	%rd79, 0, 4, %p12;
	add.s64 	%rd114, %rd2, %rd79;
	selp.b64 	%rd110, 0, %rd2, %p12;
$L__BB49_18:
	ld.f32 	%f1, [%rd61];
	setp.eq.s64 	%p13, %rd110, 0;
	@%p13 bra 	$L__BB49_20;
	ld.u32 	%r12, [%rd110];
	mov.b32 	%r13, %f1;
	// begin inline asm
	lg2.approx.ftz.f32 %r11, %r12;
    mul.rn.ftz.f32 %r11, %r11, %r13;
    ex2.approx.ftz.f32 %r11, %r11;
	// end inline asm
	mov.b32 	%f14, %r11;
$L__BB49_20:
	st.f32 	[%rd108], %f14;
	setp.eq.s64 	%p14, %rd23, 1;
	@%p14 bra 	$L__BB49_32;
	add.s64 	%rd7, %rd6, -1;
	add.s64 	%rd36, %rd23, -1;
	and.b64  	%rd37, %rd36, 1;
	setp.eq.s64 	%p15, %rd23, 2;
	shl.b64 	%rd102, %rd6, 2;
	@%p15 bra 	$L__BB49_28;
	bra.uni 	$L__BB49_22;
$L__BB49_28:
	setp.eq.s64 	%p22, %rd37, 0;
	@%p22 bra 	$L__BB49_32;
	sub.s64 	%rd92, %rd10, %rd115;
	shr.u64 	%rd93, %rd92, 2;
	setp.gt.u64 	%p23, %rd93, %rd7;
	add.s64 	%rd95, %rd115, %rd102;
	add.s64 	%rd96, %rd95, -4;
	selp.b64 	%rd53, %rd96, 0, %p23;
	sub.s64 	%rd97, %rd4, %rd114;
	shr.u64 	%rd98, %rd97, 2;
	setp.le.u64 	%p24, %rd98, %rd7;
	@%p24 bra 	$L__BB49_31;
	add.s64 	%rd54, %rd114, %rd102;
	ld.u32 	%r21, [%rd54+-4];
	mov.b32 	%r22, %f1;
	// begin inline asm
	lg2.approx.ftz.f32 %r20, %r21;
    mul.rn.ftz.f32 %r20, %r20, %r22;
    ex2.approx.ftz.f32 %r20, %r20;
	// end inline asm
	mov.b32 	%f17, %r20;
$L__BB49_31:
	st.f32 	[%rd53], %f17;
$L__BB49_32:
	ret;
$L__BB49_22:
	and.b64  	%rd111, %rd36, -2;
	mov.b32 	%r16, %f1;
	bra.uni 	$L__BB49_23;
$L__BB49_27:
	add.s64 	%rd115, %rd42, %rd102;
	add.s64 	%rd46, %rd115, -4;
	setp.gt.u64 	%p20, %rd90, %rd7;
	selp.b64 	%rd114, %rd91, %rd4, %p20;
	st.f32 	[%rd46], %f16;
	add.s64 	%rd111, %rd111, -2;
	setp.ne.s64 	%p21, %rd111, 0;
	@%p21 bra 	$L__BB49_23;
	bra.uni 	$L__BB49_28;
$L__BB49_23:
	sub.s64 	%rd85, %rd4, %rd114;
	shr.u64 	%rd86, %rd85, 2;
	setp.le.u64 	%p17, %rd86, %rd7;
	add.s64 	%rd87, %rd114, %rd102;
	@%p17 bra 	$L__BB49_25;
	add.s64 	%rd44, %rd87, -4;
	ld.u32 	%r15, [%rd44];
	// begin inline asm
	lg2.approx.ftz.f32 %r14, %r15;
    mul.rn.ftz.f32 %r14, %r14, %r16;
    ex2.approx.ftz.f32 %r14, %r14;
	// end inline asm
	mov.b32 	%f15, %r14;
$L__BB49_25:
	sub.s64 	%rd80, %rd10, %rd115;
	shr.u64 	%rd81, %rd80, 2;
	setp.gt.u64 	%p16, %rd81, %rd7;
	add.s64 	%rd83, %rd115, %rd102;
	add.s64 	%rd84, %rd83, -4;
	selp.b64 	%rd42, %rd83, %rd10, %p16;
	selp.b64 	%rd43, %rd84, 0, %p16;
	setp.gt.u64 	%p18, %rd86, %rd7;
	selp.b64 	%rd45, %rd87, %rd4, %p18;
	st.f32 	[%rd43], %f15;
	sub.s64 	%rd89, %rd4, %rd45;
	shr.u64 	%rd90, %rd89, 2;
	setp.le.u64 	%p19, %rd90, %rd7;
	add.s64 	%rd91, %rd45, %rd102;
	@%p19 bra 	$L__BB49_27;
	add.s64 	%rd48, %rd91, -4;
	ld.u32 	%r18, [%rd48];
	// begin inline asm
	lg2.approx.ftz.f32 %r17, %r18;
    mul.rn.ftz.f32 %r17, %r17, %r16;
    ex2.approx.ftz.f32 %r17, %r17;
	// end inline asm
	mov.b32 	%f16, %r17;
	bra.uni 	$L__BB49_27;
$L__BB49_33:
	mov.u64 	%rd99, anon_$_9b271fbca47443a0c783d86781d13fba_$_42;
	cvta.global.u64 	%rd100, %rd99;
	{ // callseq 174, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd100;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 174

}
	// .globl	vector_ln_f32
.visible .entry vector_ln_f32(
	.param .u64 vector_ln_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_ln_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB50_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB50_3;
	bra.uni 	$L__BB50_2;
$L__BB50_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB50_4;
$L__BB50_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB50_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB50_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB50_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB50_8;
	bra.uni 	$L__BB50_7;
$L__BB50_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB50_9;
$L__BB50_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB50_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB50_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB50_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB50_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB50_14;
$L__BB50_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB50_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB50_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB50_17;
$L__BB50_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB50_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB50_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	lg2.approx.ftz.f32 %r11, %r12;
    mul.rn.ftz.f32 %r11, %r11, 0f3F317218;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB50_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB50_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB50_27;
	bra.uni 	$L__BB50_21;
$L__BB50_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB50_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB50_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	lg2.approx.ftz.f32 %r17, %r18;
    mul.rn.ftz.f32 %r17, %r17, 0f3F317218;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB50_30:
	st.f32 	[%rd52], %f16;
$L__BB50_31:
	ret;
$L__BB50_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB50_22;
$L__BB50_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB50_22;
	bra.uni 	$L__BB50_27;
$L__BB50_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB50_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	lg2.approx.ftz.f32 %r13, %r14;
    mul.rn.ftz.f32 %r13, %r13, 0f3F317218;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB50_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB50_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	lg2.approx.ftz.f32 %r15, %r16;
    mul.rn.ftz.f32 %r15, %r15, 0f3F317218;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB50_26;

}
	// .globl	vector_log2_f32
.visible .entry vector_log2_f32(
	.param .u64 vector_log2_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_log2_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB51_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB51_3;
	bra.uni 	$L__BB51_2;
$L__BB51_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB51_4;
$L__BB51_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB51_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB51_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB51_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB51_8;
	bra.uni 	$L__BB51_7;
$L__BB51_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB51_9;
$L__BB51_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB51_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB51_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB51_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB51_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB51_14;
$L__BB51_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB51_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB51_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB51_17;
$L__BB51_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB51_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB51_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	lg2.approx.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB51_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB51_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB51_27;
	bra.uni 	$L__BB51_21;
$L__BB51_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB51_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB51_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	lg2.approx.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB51_30:
	st.f32 	[%rd52], %f16;
$L__BB51_31:
	ret;
$L__BB51_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB51_22;
$L__BB51_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB51_22;
	bra.uni 	$L__BB51_27;
$L__BB51_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB51_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	lg2.approx.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB51_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB51_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	lg2.approx.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB51_26;

}
	// .globl	vector_exp_f32
.visible .entry vector_exp_f32(
	.param .u64 vector_exp_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_exp_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB52_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB52_3;
	bra.uni 	$L__BB52_2;
$L__BB52_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB52_4;
$L__BB52_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB52_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB52_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB52_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB52_8;
	bra.uni 	$L__BB52_7;
$L__BB52_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB52_9;
$L__BB52_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB52_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB52_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB52_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB52_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB52_14;
$L__BB52_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB52_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB52_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB52_17;
$L__BB52_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB52_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB52_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r11, %r11;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB52_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB52_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB52_27;
	bra.uni 	$L__BB52_21;
$L__BB52_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB52_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB52_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r17, %r17;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB52_30:
	st.f32 	[%rd52], %f16;
$L__BB52_31:
	ret;
$L__BB52_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB52_22;
$L__BB52_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB52_22;
	bra.uni 	$L__BB52_27;
$L__BB52_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB52_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	mul.rn.ftz.f32 %r13, %r14, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r13, %r13;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB52_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB52_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	mul.rn.ftz.f32 %r15, %r16, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r15, %r15;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB52_26;

}
	// .globl	vector_exp2_f32
.visible .entry vector_exp2_f32(
	.param .u64 vector_exp2_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_exp2_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB53_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB53_3;
	bra.uni 	$L__BB53_2;
$L__BB53_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB53_4;
$L__BB53_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB53_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB53_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB53_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB53_8;
	bra.uni 	$L__BB53_7;
$L__BB53_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB53_9;
$L__BB53_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB53_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB53_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB53_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB53_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB53_14;
$L__BB53_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB53_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB53_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB53_17;
$L__BB53_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB53_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB53_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	ex2.approx.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB53_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB53_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB53_27;
	bra.uni 	$L__BB53_21;
$L__BB53_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB53_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB53_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	ex2.approx.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB53_30:
	st.f32 	[%rd52], %f16;
$L__BB53_31:
	ret;
$L__BB53_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB53_22;
$L__BB53_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB53_22;
	bra.uni 	$L__BB53_27;
$L__BB53_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB53_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	ex2.approx.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB53_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB53_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	ex2.approx.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB53_26;

}
	// .globl	vector_recip_f32
.visible .entry vector_recip_f32(
	.param .u64 vector_recip_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_recip_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB54_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB54_3;
	bra.uni 	$L__BB54_2;
$L__BB54_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB54_4;
$L__BB54_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB54_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB54_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB54_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB54_8;
	bra.uni 	$L__BB54_7;
$L__BB54_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB54_9;
$L__BB54_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB54_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB54_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB54_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB54_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB54_14;
$L__BB54_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB54_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB54_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB54_17;
$L__BB54_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB54_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB54_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	rcp.approx.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB54_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB54_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB54_27;
	bra.uni 	$L__BB54_21;
$L__BB54_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB54_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB54_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	rcp.approx.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB54_30:
	st.f32 	[%rd52], %f16;
$L__BB54_31:
	ret;
$L__BB54_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB54_22;
$L__BB54_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB54_22;
	bra.uni 	$L__BB54_27;
$L__BB54_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB54_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	rcp.approx.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB54_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB54_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	rcp.approx.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB54_26;

}
	// .globl	vector_sin_f32
.visible .entry vector_sin_f32(
	.param .u64 vector_sin_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_sin_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB55_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB55_3;
	bra.uni 	$L__BB55_2;
$L__BB55_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB55_4;
$L__BB55_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB55_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB55_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB55_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB55_8;
	bra.uni 	$L__BB55_7;
$L__BB55_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB55_9;
$L__BB55_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB55_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB55_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB55_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB55_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB55_14;
$L__BB55_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB55_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB55_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB55_17;
$L__BB55_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB55_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB55_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	sin.approx.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB55_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB55_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB55_27;
	bra.uni 	$L__BB55_21;
$L__BB55_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB55_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB55_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	sin.approx.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB55_30:
	st.f32 	[%rd52], %f16;
$L__BB55_31:
	ret;
$L__BB55_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB55_22;
$L__BB55_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB55_22;
	bra.uni 	$L__BB55_27;
$L__BB55_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB55_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	sin.approx.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB55_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB55_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	sin.approx.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB55_26;

}
	// .globl	vector_cos_f32
.visible .entry vector_cos_f32(
	.param .u64 vector_cos_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_cos_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB56_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB56_3;
	bra.uni 	$L__BB56_2;
$L__BB56_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB56_4;
$L__BB56_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB56_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB56_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB56_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB56_8;
	bra.uni 	$L__BB56_7;
$L__BB56_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB56_9;
$L__BB56_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB56_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB56_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB56_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB56_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB56_14;
$L__BB56_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB56_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB56_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB56_17;
$L__BB56_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB56_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB56_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	cos.approx.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB56_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB56_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB56_27;
	bra.uni 	$L__BB56_21;
$L__BB56_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB56_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB56_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	cos.approx.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB56_30:
	st.f32 	[%rd52], %f16;
$L__BB56_31:
	ret;
$L__BB56_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB56_22;
$L__BB56_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB56_22;
	bra.uni 	$L__BB56_27;
$L__BB56_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB56_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	cos.approx.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB56_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB56_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	cos.approx.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB56_26;

}
	// .globl	vector_tan_f32
.visible .entry vector_tan_f32(
	.param .u64 vector_tan_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_tan_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB57_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB57_3;
	bra.uni 	$L__BB57_2;
$L__BB57_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB57_4;
$L__BB57_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB57_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB57_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB57_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB57_8;
	bra.uni 	$L__BB57_7;
$L__BB57_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB57_9;
$L__BB57_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB57_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB57_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB57_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB57_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB57_14;
$L__BB57_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB57_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB57_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB57_17;
$L__BB57_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB57_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB57_19;
	ld.u32 	%r13, [%rd106];
	// begin inline asm
	sin.approx.ftz.f32 %r12, %r13;
    cos.approx.ftz.f32 %r11, %r13;
    div.approx.ftz.f32 %r12, %r12, %r11;
	// end inline asm
	mov.b32 	%f13, %r12;
$L__BB57_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB57_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB57_27;
	bra.uni 	$L__BB57_21;
$L__BB57_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB57_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB57_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r22, [%rd53+-4];
	// begin inline asm
	sin.approx.ftz.f32 %r21, %r22;
    cos.approx.ftz.f32 %r20, %r22;
    div.approx.ftz.f32 %r21, %r21, %r20;
	// end inline asm
	mov.b32 	%f16, %r21;
$L__BB57_30:
	st.f32 	[%rd52], %f16;
$L__BB57_31:
	ret;
$L__BB57_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB57_22;
$L__BB57_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB57_22;
	bra.uni 	$L__BB57_27;
$L__BB57_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB57_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r16, [%rd43];
	// begin inline asm
	sin.approx.ftz.f32 %r15, %r16;
    cos.approx.ftz.f32 %r14, %r16;
    div.approx.ftz.f32 %r15, %r15, %r14;
	// end inline asm
	mov.b32 	%f14, %r15;
$L__BB57_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB57_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r19, [%rd47];
	// begin inline asm
	sin.approx.ftz.f32 %r18, %r19;
    cos.approx.ftz.f32 %r17, %r19;
    div.approx.ftz.f32 %r18, %r18, %r17;
	// end inline asm
	mov.b32 	%f15, %r18;
	bra.uni 	$L__BB57_26;

}
	// .globl	vector_asin_f32
.visible .entry vector_asin_f32(
	.param .u64 vector_asin_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_asin_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB58_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB58_3;
	bra.uni 	$L__BB58_2;
$L__BB58_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB58_4;
$L__BB58_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB58_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB58_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB58_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB58_8;
	bra.uni 	$L__BB58_7;
$L__BB58_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB58_9;
$L__BB58_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB58_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB58_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB58_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB58_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB58_16;
	bra.uni 	$L__BB58_14;
$L__BB58_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB58_16;
	bra.uni 	$L__BB58_14;
$L__BB58_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB58_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB58_17;
	bra.uni 	$L__BB58_16;
$L__BB58_17:
	ret;
$L__BB58_16:
	mov.u64 	%rd45, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_9b271fbca47443a0c783d86781d13fba_$_13;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 175, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 175

}
	// .globl	vector_acos_f32
.visible .entry vector_acos_f32(
	.param .u64 vector_acos_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_acos_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB59_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB59_3;
	bra.uni 	$L__BB59_2;
$L__BB59_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB59_4;
$L__BB59_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB59_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB59_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB59_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB59_8;
	bra.uni 	$L__BB59_7;
$L__BB59_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB59_9;
$L__BB59_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB59_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB59_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB59_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB59_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB59_16;
	bra.uni 	$L__BB59_14;
$L__BB59_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB59_16;
	bra.uni 	$L__BB59_14;
$L__BB59_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB59_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB59_17;
	bra.uni 	$L__BB59_16;
$L__BB59_17:
	ret;
$L__BB59_16:
	mov.u64 	%rd45, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_9b271fbca47443a0c783d86781d13fba_$_14;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 176, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 176

}
	// .globl	vector_atan_f32
.visible .entry vector_atan_f32(
	.param .u64 vector_atan_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_atan_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB60_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB60_3;
	bra.uni 	$L__BB60_2;
$L__BB60_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB60_4;
$L__BB60_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB60_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB60_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB60_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB60_8;
	bra.uni 	$L__BB60_7;
$L__BB60_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB60_9;
$L__BB60_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB60_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB60_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB60_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB60_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB60_16;
	bra.uni 	$L__BB60_14;
$L__BB60_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB60_16;
	bra.uni 	$L__BB60_14;
$L__BB60_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB60_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB60_17;
	bra.uni 	$L__BB60_16;
$L__BB60_17:
	ret;
$L__BB60_16:
	mov.u64 	%rd45, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_9b271fbca47443a0c783d86781d13fba_$_15;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 177, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 177

}
	// .globl	vector_sinh_f32
.visible .entry vector_sinh_f32(
	.param .u64 vector_sinh_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_sinh_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB61_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB61_3;
	bra.uni 	$L__BB61_2;
$L__BB61_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB61_4;
$L__BB61_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB61_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB61_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB61_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB61_8;
	bra.uni 	$L__BB61_7;
$L__BB61_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB61_9;
$L__BB61_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB61_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB61_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB61_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB61_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB61_14;
$L__BB61_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB61_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB61_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB61_17;
$L__BB61_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB61_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB61_19;
	ld.u32 	%r13, [%rd106];
	// begin inline asm
	mul.rn.ftz.f32 %r12, %r13, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r12, %r12;
    neg.ftz.f32 %r11, %r13;
    mul.rn.ftz.f32 %r11, %r11, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r11, %r11;
    sub.rn.ftz.f32 %r12, %r12, %r11;
    div.approx.ftz.f32 %r12, %r12, 0f40000000;
	// end inline asm
	mov.b32 	%f13, %r12;
$L__BB61_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB61_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB61_27;
	bra.uni 	$L__BB61_21;
$L__BB61_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB61_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB61_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r22, [%rd53+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r21, %r22, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r21, %r21;
    neg.ftz.f32 %r20, %r22;
    mul.rn.ftz.f32 %r20, %r20, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r20, %r20;
    sub.rn.ftz.f32 %r21, %r21, %r20;
    div.approx.ftz.f32 %r21, %r21, 0f40000000;
	// end inline asm
	mov.b32 	%f16, %r21;
$L__BB61_30:
	st.f32 	[%rd52], %f16;
$L__BB61_31:
	ret;
$L__BB61_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB61_22;
$L__BB61_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB61_22;
	bra.uni 	$L__BB61_27;
$L__BB61_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB61_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r16, [%rd43];
	// begin inline asm
	mul.rn.ftz.f32 %r15, %r16, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r15, %r15;
    neg.ftz.f32 %r14, %r16;
    mul.rn.ftz.f32 %r14, %r14, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r14, %r14;
    sub.rn.ftz.f32 %r15, %r15, %r14;
    div.approx.ftz.f32 %r15, %r15, 0f40000000;
	// end inline asm
	mov.b32 	%f14, %r15;
$L__BB61_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB61_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r19, [%rd47];
	// begin inline asm
	mul.rn.ftz.f32 %r18, %r19, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r18, %r18;
    neg.ftz.f32 %r17, %r19;
    mul.rn.ftz.f32 %r17, %r17, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r17, %r17;
    sub.rn.ftz.f32 %r18, %r18, %r17;
    div.approx.ftz.f32 %r18, %r18, 0f40000000;
	// end inline asm
	mov.b32 	%f15, %r18;
	bra.uni 	$L__BB61_26;

}
	// .globl	vector_cosh_f32
.visible .entry vector_cosh_f32(
	.param .u64 vector_cosh_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<23>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_cosh_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB62_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB62_3;
	bra.uni 	$L__BB62_2;
$L__BB62_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB62_4;
$L__BB62_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB62_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB62_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB62_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB62_8;
	bra.uni 	$L__BB62_7;
$L__BB62_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB62_9;
$L__BB62_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB62_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB62_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB62_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB62_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB62_14;
$L__BB62_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB62_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB62_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB62_17;
$L__BB62_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB62_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB62_19;
	ld.u32 	%r13, [%rd106];
	// begin inline asm
	mul.rn.ftz.f32 %r12, %r13, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r12, %r12;
    neg.ftz.f32 %r11, %r13;
    mul.rn.ftz.f32 %r11, %r11, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r11, %r11;
    add.rn.ftz.f32 %r12, %r12, %r11;
    div.approx.ftz.f32 %r12, %r12, 0f40000000;
	// end inline asm
	mov.b32 	%f13, %r12;
$L__BB62_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB62_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB62_27;
	bra.uni 	$L__BB62_21;
$L__BB62_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB62_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB62_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r22, [%rd53+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r21, %r22, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r21, %r21;
    neg.ftz.f32 %r20, %r22;
    mul.rn.ftz.f32 %r20, %r20, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r20, %r20;
    add.rn.ftz.f32 %r21, %r21, %r20;
    div.approx.ftz.f32 %r21, %r21, 0f40000000;
	// end inline asm
	mov.b32 	%f16, %r21;
$L__BB62_30:
	st.f32 	[%rd52], %f16;
$L__BB62_31:
	ret;
$L__BB62_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB62_22;
$L__BB62_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB62_22;
	bra.uni 	$L__BB62_27;
$L__BB62_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB62_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r16, [%rd43];
	// begin inline asm
	mul.rn.ftz.f32 %r15, %r16, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r15, %r15;
    neg.ftz.f32 %r14, %r16;
    mul.rn.ftz.f32 %r14, %r14, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r14, %r14;
    add.rn.ftz.f32 %r15, %r15, %r14;
    div.approx.ftz.f32 %r15, %r15, 0f40000000;
	// end inline asm
	mov.b32 	%f14, %r15;
$L__BB62_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB62_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r19, [%rd47];
	// begin inline asm
	mul.rn.ftz.f32 %r18, %r19, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r18, %r18;
    neg.ftz.f32 %r17, %r19;
    mul.rn.ftz.f32 %r17, %r17, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r17, %r17;
    add.rn.ftz.f32 %r18, %r18, %r17;
    div.approx.ftz.f32 %r18, %r18, 0f40000000;
	// end inline asm
	mov.b32 	%f15, %r18;
	bra.uni 	$L__BB62_26;

}
	// .globl	vector_tanh_f32
.visible .entry vector_tanh_f32(
	.param .u64 vector_tanh_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_tanh_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB63_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB63_3;
	bra.uni 	$L__BB63_2;
$L__BB63_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB63_4;
$L__BB63_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB63_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB63_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB63_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB63_8;
	bra.uni 	$L__BB63_7;
$L__BB63_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB63_9;
$L__BB63_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB63_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB63_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB63_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB63_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB63_14;
$L__BB63_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB63_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB63_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB63_17;
$L__BB63_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB63_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB63_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	tanh.approx.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB63_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB63_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB63_27;
	bra.uni 	$L__BB63_21;
$L__BB63_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB63_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB63_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	tanh.approx.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB63_30:
	st.f32 	[%rd52], %f16;
$L__BB63_31:
	ret;
$L__BB63_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB63_22;
$L__BB63_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB63_22;
	bra.uni 	$L__BB63_27;
$L__BB63_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB63_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	tanh.approx.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB63_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB63_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	tanh.approx.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB63_26;

}
	// .globl	vector_asinh_f32
.visible .entry vector_asinh_f32(
	.param .u64 vector_asinh_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_asinh_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB64_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB64_3;
	bra.uni 	$L__BB64_2;
$L__BB64_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB64_4;
$L__BB64_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB64_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB64_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB64_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB64_8;
	bra.uni 	$L__BB64_7;
$L__BB64_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB64_9;
$L__BB64_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB64_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB64_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB64_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB64_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB64_16;
	bra.uni 	$L__BB64_14;
$L__BB64_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB64_16;
	bra.uni 	$L__BB64_14;
$L__BB64_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB64_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB64_17;
	bra.uni 	$L__BB64_16;
$L__BB64_17:
	ret;
$L__BB64_16:
	mov.u64 	%rd45, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_9b271fbca47443a0c783d86781d13fba_$_16;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 178, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 178

}
	// .globl	vector_acosh_f32
.visible .entry vector_acosh_f32(
	.param .u64 vector_acosh_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_acosh_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB65_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB65_3;
	bra.uni 	$L__BB65_2;
$L__BB65_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB65_4;
$L__BB65_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB65_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB65_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB65_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB65_8;
	bra.uni 	$L__BB65_7;
$L__BB65_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB65_9;
$L__BB65_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB65_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB65_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB65_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB65_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB65_16;
	bra.uni 	$L__BB65_14;
$L__BB65_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB65_16;
	bra.uni 	$L__BB65_14;
$L__BB65_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB65_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB65_17;
	bra.uni 	$L__BB65_16;
$L__BB65_17:
	ret;
$L__BB65_16:
	mov.u64 	%rd45, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_9b271fbca47443a0c783d86781d13fba_$_17;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 179, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 179

}
	// .globl	vector_atanh_f32
.visible .entry vector_atanh_f32(
	.param .u64 vector_atanh_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<57>;

	ld.param.u64 	%rd24, [vector_atanh_f32_param_0];
	cvta.to.global.u64 	%rd25, %rd24;
	ld.global.nc.u64 	%rd1, [%rd25];
	ld.global.nc.u64 	%rd2, [%rd25+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd26, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd28, %r3, %r2;
	add.s64 	%rd4, %rd28, %rd26;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd29, [%rd25+24];
	setp.le.u64 	%p4, %rd29, %rd4;
	not.b64 	%rd31, %rd4;
	mov.u64 	%rd55, 0;
	mov.u64 	%rd53, %rd55;
	@%p4 bra 	$L__BB66_5;
	max.u64 	%rd30, %rd29, %rd4;
	add.s64 	%rd7, %rd31, %rd30;
	or.b64  	%rd32, %rd7, %rd5;
	and.b64  	%rd33, %rd32, -4294967296;
	setp.ne.s64 	%p5, %rd33, 0;
	@%p5 bra 	$L__BB66_3;
	bra.uni 	$L__BB66_2;
$L__BB66_3:
	div.u64 	%rd52, %rd7, %rd5;
	bra.uni 	$L__BB66_4;
$L__BB66_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd7;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd52, %r7;
$L__BB66_4:
	add.s64 	%rd53, %rd52, 1;
$L__BB66_5:
	setp.le.u64 	%p6, %rd2, %rd4;
	@%p6 bra 	$L__BB66_10;
	max.u64 	%rd35, %rd2, %rd4;
	add.s64 	%rd13, %rd31, %rd35;
	or.b64  	%rd37, %rd13, %rd5;
	and.b64  	%rd38, %rd37, -4294967296;
	setp.ne.s64 	%p7, %rd38, 0;
	@%p7 bra 	$L__BB66_8;
	bra.uni 	$L__BB66_7;
$L__BB66_8:
	div.u64 	%rd54, %rd13, %rd5;
	bra.uni 	$L__BB66_9;
$L__BB66_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd13;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd54, %r10;
$L__BB66_9:
	add.s64 	%rd55, %rd54, 1;
$L__BB66_10:
	min.u64 	%rd19, %rd53, %rd55;
	setp.eq.s64 	%p8, %rd19, 0;
	@%p8 bra 	$L__BB66_17;
	shl.b64 	%rd27, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd27;
	setp.eq.s64 	%p9, %rd4, 0;
	@%p9 bra 	$L__BB66_13;
	shl.b64 	%rd39, %rd4, 2;
	add.s64 	%rd40, %rd1, %rd39;
	add.s64 	%rd41, %rd40, 4;
	selp.b64 	%rd56, %rd3, %rd41, %p6;
	@!%p6 bra 	$L__BB66_16;
	bra.uni 	$L__BB66_14;
$L__BB66_13:
	setp.eq.s64 	%p2, %rd2, 0;
	selp.b64 	%rd42, 0, 4, %p2;
	add.s64 	%rd56, %rd1, %rd42;
	@!%p2 bra 	$L__BB66_16;
	bra.uni 	$L__BB66_14;
$L__BB66_14:
	setp.eq.s64 	%p10, %rd19, 1;
	@%p10 bra 	$L__BB66_17;
	add.s64 	%rd6, %rd5, -1;
	sub.s64 	%rd43, %rd3, %rd56;
	shr.u64 	%rd44, %rd43, 2;
	setp.le.u64 	%p11, %rd44, %rd6;
	@%p11 bra 	$L__BB66_17;
	bra.uni 	$L__BB66_16;
$L__BB66_17:
	ret;
$L__BB66_16:
	mov.u64 	%rd45, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd46, %rd45;
	mov.u64 	%rd47, anon_$_9b271fbca47443a0c783d86781d13fba_$_18;
	cvta.global.u64 	%rd48, %rd47;
	{ // callseq 180, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd46;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd48;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 180

}
	// .globl	vector_l1_norm_f32
.visible .entry vector_l1_norm_f32(
	.param .u64 vector_l1_norm_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_l1_norm_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB67_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB67_3;
	bra.uni 	$L__BB67_2;
$L__BB67_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB67_4;
$L__BB67_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB67_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB67_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB67_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB67_8;
	bra.uni 	$L__BB67_7;
$L__BB67_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB67_9;
$L__BB67_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB67_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB67_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB67_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB67_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB67_14;
$L__BB67_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB67_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB67_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB67_17;
$L__BB67_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB67_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB67_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	abs.ftz.f32 %r11, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB67_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB67_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB67_27;
	bra.uni 	$L__BB67_21;
$L__BB67_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB67_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB67_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	abs.ftz.f32 %r17, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB67_30:
	st.f32 	[%rd52], %f16;
$L__BB67_31:
	ret;
$L__BB67_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB67_22;
$L__BB67_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB67_22;
	bra.uni 	$L__BB67_27;
$L__BB67_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB67_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	abs.ftz.f32 %r13, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB67_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB67_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	abs.ftz.f32 %r15, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB67_26;

}
	// .globl	vector_l2_norm_f32
.visible .entry vector_l2_norm_f32(
	.param .u64 vector_l2_norm_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<19>;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<112>;

	ld.param.u64 	%rd55, [vector_l2_norm_f32_param_0];
	cvta.to.global.u64 	%rd56, %rd55;
	ld.global.nc.u64 	%rd1, [%rd56];
	ld.global.nc.u64 	%rd2, [%rd56+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd57, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd59, %r3, %r2;
	add.s64 	%rd4, %rd59, %rd57;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd56+16];
	ld.global.nc.u64 	%rd8, [%rd56+24];
	setp.le.u64 	%p1, %rd8, %rd4;
	not.b64 	%rd62, %rd4;
	mov.u64 	%rd102, 0;
	mov.u64 	%rd100, %rd102;
	@%p1 bra 	$L__BB68_5;
	max.u64 	%rd61, %rd8, %rd4;
	add.s64 	%rd10, %rd62, %rd61;
	or.b64  	%rd63, %rd10, %rd5;
	and.b64  	%rd64, %rd63, -4294967296;
	setp.ne.s64 	%p2, %rd64, 0;
	@%p2 bra 	$L__BB68_3;
	bra.uni 	$L__BB68_2;
$L__BB68_3:
	div.u64 	%rd99, %rd10, %rd5;
	bra.uni 	$L__BB68_4;
$L__BB68_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd99, %r7;
$L__BB68_4:
	add.s64 	%rd100, %rd99, 1;
$L__BB68_5:
	setp.le.u64 	%p3, %rd2, %rd4;
	@%p3 bra 	$L__BB68_10;
	max.u64 	%rd66, %rd2, %rd4;
	add.s64 	%rd16, %rd62, %rd66;
	or.b64  	%rd68, %rd16, %rd5;
	and.b64  	%rd69, %rd68, -4294967296;
	setp.ne.s64 	%p4, %rd69, 0;
	@%p4 bra 	$L__BB68_8;
	bra.uni 	$L__BB68_7;
$L__BB68_8:
	div.u64 	%rd101, %rd16, %rd5;
	bra.uni 	$L__BB68_9;
$L__BB68_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd101, %r10;
$L__BB68_9:
	add.s64 	%rd102, %rd101, 1;
$L__BB68_10:
	min.u64 	%rd22, %rd100, %rd102;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB68_31;
	shl.b64 	%rd58, %rd2, 2;
	shl.b64 	%rd60, %rd8, 2;
	add.s64 	%rd9, %rd7, %rd60;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd97, %rd4, 2;
	@%p6 bra 	$L__BB68_13;
	setp.gt.u64 	%p7, %rd8, %rd4;
	add.s64 	%rd71, %rd7, %rd97;
	add.s64 	%rd72, %rd71, 4;
	selp.b64 	%rd110, %rd72, %rd9, %p7;
	selp.b64 	%rd104, %rd71, 0, %p7;
	bra.uni 	$L__BB68_14;
$L__BB68_13:
	setp.eq.s64 	%p8, %rd8, 0;
	selp.b64 	%rd73, 0, 4, %p8;
	add.s64 	%rd110, %rd7, %rd73;
	selp.b64 	%rd104, 0, %rd7, %p8;
$L__BB68_14:
	add.s64 	%rd3, %rd1, %rd58;
	@%p6 bra 	$L__BB68_16;
	setp.gt.u64 	%p10, %rd2, %rd4;
	add.s64 	%rd75, %rd1, %rd97;
	add.s64 	%rd76, %rd75, 4;
	selp.b64 	%rd111, %rd76, %rd3, %p10;
	selp.b64 	%rd106, %rd75, 0, %p10;
	bra.uni 	$L__BB68_17;
$L__BB68_16:
	setp.eq.s64 	%p11, %rd2, 0;
	selp.b64 	%rd77, 0, 4, %p11;
	add.s64 	%rd111, %rd1, %rd77;
	selp.b64 	%rd106, 0, %rd1, %p11;
$L__BB68_17:
	setp.eq.s64 	%p12, %rd106, 0;
	@%p12 bra 	$L__BB68_19;
	ld.u32 	%r12, [%rd106];
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, %r12;
	// end inline asm
	mov.b32 	%f13, %r11;
$L__BB68_19:
	st.f32 	[%rd104], %f13;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB68_31;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd98, %rd5, 2;
	@%p14 bra 	$L__BB68_27;
	bra.uni 	$L__BB68_21;
$L__BB68_27:
	setp.eq.s64 	%p21, %rd36, 0;
	@%p21 bra 	$L__BB68_31;
	sub.s64 	%rd90, %rd9, %rd110;
	shr.u64 	%rd91, %rd90, 2;
	setp.gt.u64 	%p22, %rd91, %rd6;
	add.s64 	%rd93, %rd110, %rd98;
	add.s64 	%rd94, %rd93, -4;
	selp.b64 	%rd52, %rd94, 0, %p22;
	sub.s64 	%rd95, %rd3, %rd111;
	shr.u64 	%rd96, %rd95, 2;
	setp.le.u64 	%p23, %rd96, %rd6;
	@%p23 bra 	$L__BB68_30;
	add.s64 	%rd53, %rd111, %rd98;
	ld.u32 	%r18, [%rd53+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r18;
	// end inline asm
	mov.b32 	%f16, %r17;
$L__BB68_30:
	st.f32 	[%rd52], %f16;
$L__BB68_31:
	ret;
$L__BB68_21:
	and.b64  	%rd107, %rd35, -2;
	bra.uni 	$L__BB68_22;
$L__BB68_26:
	add.s64 	%rd110, %rd41, %rd98;
	add.s64 	%rd45, %rd110, -4;
	setp.gt.u64 	%p19, %rd88, %rd6;
	selp.b64 	%rd111, %rd89, %rd3, %p19;
	st.f32 	[%rd45], %f15;
	add.s64 	%rd107, %rd107, -2;
	setp.ne.s64 	%p20, %rd107, 0;
	@%p20 bra 	$L__BB68_22;
	bra.uni 	$L__BB68_27;
$L__BB68_22:
	sub.s64 	%rd83, %rd3, %rd111;
	shr.u64 	%rd84, %rd83, 2;
	setp.le.u64 	%p16, %rd84, %rd6;
	add.s64 	%rd85, %rd111, %rd98;
	@%p16 bra 	$L__BB68_24;
	add.s64 	%rd43, %rd85, -4;
	ld.u32 	%r14, [%rd43];
	// begin inline asm
	mul.rn.ftz.f32 %r13, %r14, %r14;
	// end inline asm
	mov.b32 	%f14, %r13;
$L__BB68_24:
	sub.s64 	%rd78, %rd9, %rd110;
	shr.u64 	%rd79, %rd78, 2;
	setp.gt.u64 	%p15, %rd79, %rd6;
	add.s64 	%rd81, %rd110, %rd98;
	add.s64 	%rd82, %rd81, -4;
	selp.b64 	%rd41, %rd81, %rd9, %p15;
	selp.b64 	%rd42, %rd82, 0, %p15;
	setp.gt.u64 	%p17, %rd84, %rd6;
	selp.b64 	%rd44, %rd85, %rd3, %p17;
	st.f32 	[%rd42], %f14;
	sub.s64 	%rd87, %rd3, %rd44;
	shr.u64 	%rd88, %rd87, 2;
	setp.le.u64 	%p18, %rd88, %rd6;
	add.s64 	%rd89, %rd44, %rd98;
	@%p18 bra 	$L__BB68_26;
	add.s64 	%rd47, %rd89, -4;
	ld.u32 	%r16, [%rd47];
	// begin inline asm
	mul.rn.ftz.f32 %r15, %r16, %r16;
	// end inline asm
	mov.b32 	%f15, %r15;
	bra.uni 	$L__BB68_26;

}
	// .globl	vector_add_ref_f32
.visible .entry vector_add_ref_f32(
	.param .u64 vector_add_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_add_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB69_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB69_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB69_4;
$L__BB69_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB69_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB69_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	add.rn.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB69_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB69_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	add.rn.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB69_7;
$L__BB69_8:
	ret;
$L__BB69_9:
	mov.u64 	%rd37, anon_$_9b271fbca47443a0c783d86781d13fba_$_43;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 181, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 181

}
	// .globl	vector_sub_ref_f32
.visible .entry vector_sub_ref_f32(
	.param .u64 vector_sub_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_sub_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB70_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB70_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB70_4;
$L__BB70_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB70_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB70_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	sub.rn.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB70_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB70_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	sub.rn.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB70_7;
$L__BB70_8:
	ret;
$L__BB70_9:
	mov.u64 	%rd37, anon_$_9b271fbca47443a0c783d86781d13fba_$_44;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 182, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 182

}
	// .globl	vector_mul_ref_f32
.visible .entry vector_mul_ref_f32(
	.param .u64 vector_mul_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_mul_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB71_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB71_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB71_4;
$L__BB71_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB71_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB71_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB71_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB71_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	mul.rn.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB71_7;
$L__BB71_8:
	ret;
$L__BB71_9:
	mov.u64 	%rd37, anon_$_9b271fbca47443a0c783d86781d13fba_$_45;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 183, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 183

}
	// .globl	vector_div_ref_f32
.visible .entry vector_div_ref_f32(
	.param .u64 vector_div_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_div_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB72_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB72_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB72_4;
$L__BB72_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB72_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB72_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB72_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB72_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	div.approx.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB72_7;
$L__BB72_8:
	ret;
$L__BB72_9:
	mov.u64 	%rd37, anon_$_9b271fbca47443a0c783d86781d13fba_$_46;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 184, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 184

}
	// .globl	vector_neg_ref_f32
.visible .entry vector_neg_ref_f32(
	.param .u64 vector_neg_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_neg_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB73_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB73_3;
$L__BB73_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB73_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB73_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	neg.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB73_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB73_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	neg.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB73_6;
$L__BB73_7:
	ret;

}
	// .globl	vector_scale_ref_f32
.visible .entry vector_scale_ref_f32(
	.param .u64 vector_scale_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_scale_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB74_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB74_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB74_4;
$L__BB74_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB74_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB74_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	mul.rn.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB74_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB74_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	mul.rn.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB74_7;
$L__BB74_8:
	ret;
$L__BB74_9:
	mov.u64 	%rd37, anon_$_9b271fbca47443a0c783d86781d13fba_$_47;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 185, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 185

}
	// .globl	vector_descale_ref_f32
.visible .entry vector_descale_ref_f32(
	.param .u64 vector_descale_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_descale_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB75_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB75_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB75_4;
$L__BB75_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB75_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB75_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	div.approx.ftz.f32 %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB75_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB75_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	div.approx.ftz.f32 %r8, %r9, %r7;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB75_7;
$L__BB75_8:
	ret;
$L__BB75_9:
	mov.u64 	%rd37, anon_$_9b271fbca47443a0c783d86781d13fba_$_48;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 186, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 186

}
	// .globl	vector_powf_ref_f32
.visible .entry vector_powf_ref_f32(
	.param .u64 vector_powf_ref_f32_param_0
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<2>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd20, [vector_powf_ref_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd20;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	ld.global.nc.u64 	%rd24, [%rd1+24];
	setp.eq.s64 	%p1, %rd24, 0;
	@%p1 bra 	$L__BB76_9;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd5, %rd23, %rd21;
	ld.global.nc.u64 	%rd25, [%rd1+16];
	setp.eq.s64 	%p2, %rd5, 0;
	@%p2 bra 	$L__BB76_2;
	setp.gt.u64 	%p3, %rd3, %rd5;
	shl.b64 	%rd26, %rd5, 2;
	add.s64 	%rd27, %rd2, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd41, %rd28, %rd4, %p3;
	selp.b64 	%rd42, %rd27, 0, %p3;
	bra.uni 	$L__BB76_4;
$L__BB76_2:
	setp.eq.s64 	%p4, %rd3, 0;
	selp.b64 	%rd29, 0, 4, %p4;
	add.s64 	%rd41, %rd2, %rd29;
	selp.b64 	%rd42, 0, %rd2, %p4;
$L__BB76_4:
	setp.eq.s64 	%p5, %rd42, 0;
	@%p5 bra 	$L__BB76_8;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd6, %r2, %r4;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f1, [%rd25];
	ld.u32 	%r6, [%rd42];
	mov.b32 	%r7, %f1;
	// begin inline asm
	lg2.approx.ftz.f32 %r5, %r6;
    mul.rn.ftz.f32 %r5, %r5, %r7;
    ex2.approx.ftz.f32 %r5, %r5;
	// end inline asm
	st.u32 	[%rd42], %r5;
	sub.s64 	%rd30, %rd4, %rd41;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd7;
	@%p6 bra 	$L__BB76_8;
	shl.b64 	%rd32, %rd6, 2;
	add.s64 	%rd40, %rd41, %rd32;
	add.s64 	%rd39, %rd40, -4;
$L__BB76_7:
	ld.u32 	%r9, [%rd39];
	// begin inline asm
	lg2.approx.ftz.f32 %r8, %r9;
    mul.rn.ftz.f32 %r8, %r8, %r7;
    ex2.approx.ftz.f32 %r8, %r8;
	// end inline asm
	st.u32 	[%rd39], %r8;
	sub.s64 	%rd33, %rd4, %rd40;
	shr.u64 	%rd34, %rd33, 2;
	setp.gt.u64 	%p7, %rd34, %rd7;
	add.s64 	%rd36, %rd40, %rd32;
	add.s64 	%rd39, %rd36, -4;
	selp.b64 	%rd40, %rd36, %rd4, %p7;
	@%p7 bra 	$L__BB76_7;
$L__BB76_8:
	ret;
$L__BB76_9:
	mov.u64 	%rd37, anon_$_9b271fbca47443a0c783d86781d13fba_$_49;
	cvta.global.u64 	%rd38, %rd37;
	{ // callseq 187, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], 0;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd38;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 187

}
	// .globl	vector_ln_ref_f32
.visible .entry vector_ln_ref_f32(
	.param .u64 vector_ln_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_ln_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB77_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB77_3;
$L__BB77_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB77_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB77_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	lg2.approx.ftz.f32 %r5, %r6;
    mul.rn.ftz.f32 %r5, %r5, 0f3F317218;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB77_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB77_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	lg2.approx.ftz.f32 %r7, %r8;
    mul.rn.ftz.f32 %r7, %r7, 0f3F317218;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB77_6;
$L__BB77_7:
	ret;

}
	// .globl	vector_log2_ref_f32
.visible .entry vector_log2_ref_f32(
	.param .u64 vector_log2_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_log2_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB78_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB78_3;
$L__BB78_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB78_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB78_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	lg2.approx.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB78_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB78_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	lg2.approx.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB78_6;
$L__BB78_7:
	ret;

}
	// .globl	vector_exp_ref_f32
.visible .entry vector_exp_ref_f32(
	.param .u64 vector_exp_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_exp_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB79_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB79_3;
$L__BB79_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB79_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB79_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	mul.rn.ftz.f32 %r5, %r6, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r5, %r5;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB79_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB79_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	mul.rn.ftz.f32 %r7, %r8, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r7, %r7;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB79_6;
$L__BB79_7:
	ret;

}
	// .globl	vector_exp2_ref_f32
.visible .entry vector_exp2_ref_f32(
	.param .u64 vector_exp2_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_exp2_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB80_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB80_3;
$L__BB80_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB80_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB80_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	ex2.approx.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB80_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB80_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	ex2.approx.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB80_6;
$L__BB80_7:
	ret;

}
	// .globl	vector_recip_ref_f32
.visible .entry vector_recip_ref_f32(
	.param .u64 vector_recip_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_recip_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB81_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB81_3;
$L__BB81_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB81_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB81_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	rcp.approx.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB81_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB81_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	rcp.approx.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB81_6;
$L__BB81_7:
	ret;

}
	// .globl	vector_sin_ref_f32
.visible .entry vector_sin_ref_f32(
	.param .u64 vector_sin_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_sin_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB82_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB82_3;
$L__BB82_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB82_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB82_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	sin.approx.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB82_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB82_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	sin.approx.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB82_6;
$L__BB82_7:
	ret;

}
	// .globl	vector_cos_ref_f32
.visible .entry vector_cos_ref_f32(
	.param .u64 vector_cos_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_cos_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB83_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB83_3;
$L__BB83_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB83_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB83_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	cos.approx.ftz.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB83_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB83_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	cos.approx.ftz.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB83_6;
$L__BB83_7:
	ret;

}
	// .globl	vector_tan_ref_f32
.visible .entry vector_tan_ref_f32(
	.param .u64 vector_tan_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_tan_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB84_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB84_3;
$L__BB84_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB84_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB84_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r7, [%rd38];
	// begin inline asm
	sin.approx.ftz.f32 %r6, %r7;
    cos.approx.ftz.f32 %r5, %r7;
    div.approx.ftz.f32 %r6, %r6, %r5;
	// end inline asm
	st.u32 	[%rd38], %r6;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB84_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB84_6:
	ld.u32 	%r10, [%rd35];
	// begin inline asm
	sin.approx.ftz.f32 %r9, %r10;
    cos.approx.ftz.f32 %r8, %r10;
    div.approx.ftz.f32 %r9, %r9, %r8;
	// end inline asm
	st.u32 	[%rd35], %r9;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB84_6;
$L__BB84_7:
	ret;

}
	// .globl	vector_asin_ref_f32
.visible .entry vector_asin_ref_f32(
	.param .u64 vector_asin_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_asin_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB85_3;
	bra.uni 	$L__BB85_1;
$L__BB85_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB85_2;
	bra.uni 	$L__BB85_4;
$L__BB85_2:
	ret;
$L__BB85_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB85_2;
$L__BB85_4:
	mov.u64 	%rd7, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_9b271fbca47443a0c783d86781d13fba_$_13;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 188, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 188

}
	// .globl	vector_acos_ref_f32
.visible .entry vector_acos_ref_f32(
	.param .u64 vector_acos_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_acos_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB86_3;
	bra.uni 	$L__BB86_1;
$L__BB86_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB86_2;
	bra.uni 	$L__BB86_4;
$L__BB86_2:
	ret;
$L__BB86_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB86_2;
$L__BB86_4:
	mov.u64 	%rd7, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_9b271fbca47443a0c783d86781d13fba_$_14;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 189, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 189

}
	// .globl	vector_atan_ref_f32
.visible .entry vector_atan_ref_f32(
	.param .u64 vector_atan_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_atan_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB87_3;
	bra.uni 	$L__BB87_1;
$L__BB87_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB87_2;
	bra.uni 	$L__BB87_4;
$L__BB87_2:
	ret;
$L__BB87_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB87_2;
$L__BB87_4:
	mov.u64 	%rd7, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_9b271fbca47443a0c783d86781d13fba_$_15;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 190, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 190

}
	// .globl	vector_sinh_ref_f32
.visible .entry vector_sinh_ref_f32(
	.param .u64 vector_sinh_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_sinh_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB88_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB88_3;
$L__BB88_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB88_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB88_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r7, [%rd38];
	// begin inline asm
	mul.rn.ftz.f32 %r6, %r7, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r6, %r6;
    neg.ftz.f32 %r5, %r7;
    mul.rn.ftz.f32 %r5, %r5, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r5, %r5;
    sub.rn.ftz.f32 %r6, %r6, %r5;
    div.approx.ftz.f32 %r6, %r6, 0f40000000;
	// end inline asm
	st.u32 	[%rd38], %r6;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB88_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB88_6:
	ld.u32 	%r10, [%rd35];
	// begin inline asm
	mul.rn.ftz.f32 %r9, %r10, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r9, %r9;
    neg.ftz.f32 %r8, %r10;
    mul.rn.ftz.f32 %r8, %r8, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r8, %r8;
    sub.rn.ftz.f32 %r9, %r9, %r8;
    div.approx.ftz.f32 %r9, %r9, 0f40000000;
	// end inline asm
	st.u32 	[%rd35], %r9;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB88_6;
$L__BB88_7:
	ret;

}
	// .globl	vector_cosh_ref_f32
.visible .entry vector_cosh_ref_f32(
	.param .u64 vector_cosh_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_cosh_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB89_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB89_3;
$L__BB89_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB89_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB89_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r7, [%rd38];
	// begin inline asm
	mul.rn.ftz.f32 %r6, %r7, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r6, %r6;
    neg.ftz.f32 %r5, %r7;
    mul.rn.ftz.f32 %r5, %r5, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r5, %r5;
    add.rn.ftz.f32 %r6, %r6, %r5;
    div.approx.ftz.f32 %r6, %r6, 0f40000000;
	// end inline asm
	st.u32 	[%rd38], %r6;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB89_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB89_6:
	ld.u32 	%r10, [%rd35];
	// begin inline asm
	mul.rn.ftz.f32 %r9, %r10, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r9, %r9;
    neg.ftz.f32 %r8, %r10;
    mul.rn.ftz.f32 %r8, %r8, 0f3FB8AA3B;
    ex2.approx.ftz.f32 %r8, %r8;
    add.rn.ftz.f32 %r9, %r9, %r8;
    div.approx.ftz.f32 %r9, %r9, 0f40000000;
	// end inline asm
	st.u32 	[%rd35], %r9;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB89_6;
$L__BB89_7:
	ret;

}
	// .globl	vector_tanh_ref_f32
.visible .entry vector_tanh_ref_f32(
	.param .u64 vector_tanh_ref_f32_param_0
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<39>;

	ld.param.u64 	%rd19, [vector_tanh_ref_f32_param_0];
	cvta.to.global.u64 	%rd20, %rd19;
	ld.global.nc.u64 	%rd1, [%rd20];
	ld.global.nc.u64 	%rd2, [%rd20+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd21, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	shl.b64 	%rd22, %rd2, 2;
	add.s64 	%rd3, %rd1, %rd22;
	mul.wide.u32 	%rd23, %r3, %r2;
	add.s64 	%rd4, %rd23, %rd21;
	setp.eq.s64 	%p1, %rd4, 0;
	@%p1 bra 	$L__BB90_1;
	setp.gt.u64 	%p2, %rd2, %rd4;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd25, %rd1, %rd24;
	add.s64 	%rd26, %rd25, 4;
	selp.b64 	%rd37, %rd26, %rd3, %p2;
	selp.b64 	%rd38, %rd25, 0, %p2;
	bra.uni 	$L__BB90_3;
$L__BB90_1:
	setp.eq.s64 	%p3, %rd2, 0;
	selp.b64 	%rd27, 0, 4, %p3;
	add.s64 	%rd37, %rd1, %rd27;
	selp.b64 	%rd38, 0, %rd1, %p3;
$L__BB90_3:
	setp.eq.s64 	%p4, %rd38, 0;
	@%p4 bra 	$L__BB90_7;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	add.s64 	%rd6, %rd5, -1;
	ld.u32 	%r6, [%rd38];
	// begin inline asm
	tanh.approx.f32 %r5, %r6;
	// end inline asm
	st.u32 	[%rd38], %r5;
	sub.s64 	%rd28, %rd3, %rd37;
	shr.u64 	%rd29, %rd28, 2;
	setp.le.u64 	%p5, %rd29, %rd6;
	@%p5 bra 	$L__BB90_7;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd36, %rd37, %rd30;
	add.s64 	%rd35, %rd36, -4;
$L__BB90_6:
	ld.u32 	%r8, [%rd35];
	// begin inline asm
	tanh.approx.f32 %r7, %r8;
	// end inline asm
	st.u32 	[%rd35], %r7;
	sub.s64 	%rd31, %rd3, %rd36;
	shr.u64 	%rd32, %rd31, 2;
	setp.gt.u64 	%p6, %rd32, %rd6;
	add.s64 	%rd34, %rd36, %rd30;
	add.s64 	%rd35, %rd34, -4;
	selp.b64 	%rd36, %rd34, %rd3, %p6;
	@%p6 bra 	$L__BB90_6;
$L__BB90_7:
	ret;

}
	// .globl	vector_asinh_ref_f32
.visible .entry vector_asinh_ref_f32(
	.param .u64 vector_asinh_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_asinh_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB91_3;
	bra.uni 	$L__BB91_1;
$L__BB91_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB91_2;
	bra.uni 	$L__BB91_4;
$L__BB91_2:
	ret;
$L__BB91_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB91_2;
$L__BB91_4:
	mov.u64 	%rd7, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_9b271fbca47443a0c783d86781d13fba_$_16;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 191, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 191

}
	// .globl	vector_acosh_ref_f32
.visible .entry vector_acosh_ref_f32(
	.param .u64 vector_acosh_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_acosh_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB92_3;
	bra.uni 	$L__BB92_1;
$L__BB92_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB92_2;
	bra.uni 	$L__BB92_4;
$L__BB92_2:
	ret;
$L__BB92_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB92_2;
$L__BB92_4:
	mov.u64 	%rd7, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_9b271fbca47443a0c783d86781d13fba_$_17;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 192, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 192

}
	// .globl	vector_atanh_ref_f32
.visible .entry vector_atanh_ref_f32(
	.param .u64 vector_atanh_ref_f32_param_0
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd3, [vector_atanh_ref_f32_param_0];
	cvta.to.global.u64 	%rd4, %rd3;
	ld.global.nc.u64 	%rd1, [%rd4+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd5, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd6, %r3, %r2;
	add.s64 	%rd2, %rd6, %rd5;
	setp.ne.s64 	%p1, %rd2, 0;
	@%p1 bra 	$L__BB93_3;
	bra.uni 	$L__BB93_1;
$L__BB93_3:
	setp.le.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$L__BB93_2;
	bra.uni 	$L__BB93_4;
$L__BB93_2:
	ret;
$L__BB93_1:
	setp.eq.s64 	%p3, %rd1, 0;
	@%p3 bra 	$L__BB93_2;
$L__BB93_4:
	mov.u64 	%rd7, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd8, %rd7;
	mov.u64 	%rd9, anon_$_9b271fbca47443a0c783d86781d13fba_$_18;
	cvta.global.u64 	%rd10, %rd9;
	{ // callseq 193, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd8;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd10;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 193

}
	// .globl	vector_add_vec_f32
.visible .entry vector_add_vec_f32(
	.param .u64 vector_add_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_add_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB94_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB94_3;
	bra.uni 	$L__BB94_2;
$L__BB94_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB94_4;
$L__BB94_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB94_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB94_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB94_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB94_8;
	bra.uni 	$L__BB94_7;
$L__BB94_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB94_9;
$L__BB94_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB94_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB94_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB94_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB94_13;
	bra.uni 	$L__BB94_12;
$L__BB94_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB94_14;
$L__BB94_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB94_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB94_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB94_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB94_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB94_19;
$L__BB94_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB94_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB94_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB94_22;
$L__BB94_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB94_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB94_27;
	@%p11 bra 	$L__BB94_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB94_26;
$L__BB94_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB94_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB94_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB94_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	add.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB94_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB94_38;
	bra.uni 	$L__BB94_30;
$L__BB94_38:
	ret;
$L__BB94_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB94_31;
$L__BB94_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB94_31;
	bra.uni 	$L__BB94_38;
$L__BB94_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB94_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB94_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB94_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB94_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB94_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	add.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB94_37;

}
	// .globl	vector_sub_vec_f32
.visible .entry vector_sub_vec_f32(
	.param .u64 vector_sub_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_sub_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB95_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB95_3;
	bra.uni 	$L__BB95_2;
$L__BB95_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB95_4;
$L__BB95_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB95_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB95_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB95_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB95_8;
	bra.uni 	$L__BB95_7;
$L__BB95_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB95_9;
$L__BB95_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB95_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB95_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB95_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB95_13;
	bra.uni 	$L__BB95_12;
$L__BB95_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB95_14;
$L__BB95_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB95_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB95_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB95_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB95_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB95_19;
$L__BB95_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB95_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB95_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB95_22;
$L__BB95_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB95_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB95_27;
	@%p11 bra 	$L__BB95_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB95_26;
$L__BB95_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB95_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB95_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB95_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	sub.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB95_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB95_38;
	bra.uni 	$L__BB95_30;
$L__BB95_38:
	ret;
$L__BB95_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB95_31;
$L__BB95_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB95_31;
	bra.uni 	$L__BB95_38;
$L__BB95_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB95_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB95_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB95_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB95_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB95_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	sub.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB95_37;

}
	// .globl	vector_mul_vec_f32
.visible .entry vector_mul_vec_f32(
	.param .u64 vector_mul_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_mul_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB96_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB96_3;
	bra.uni 	$L__BB96_2;
$L__BB96_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB96_4;
$L__BB96_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB96_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB96_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB96_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB96_8;
	bra.uni 	$L__BB96_7;
$L__BB96_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB96_9;
$L__BB96_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB96_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB96_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB96_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB96_13;
	bra.uni 	$L__BB96_12;
$L__BB96_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB96_14;
$L__BB96_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB96_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB96_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB96_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB96_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB96_19;
$L__BB96_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB96_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB96_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB96_22;
$L__BB96_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB96_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB96_27;
	@%p11 bra 	$L__BB96_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB96_26;
$L__BB96_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB96_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB96_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB96_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB96_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB96_38;
	bra.uni 	$L__BB96_30;
$L__BB96_38:
	ret;
$L__BB96_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB96_31;
$L__BB96_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB96_31;
	bra.uni 	$L__BB96_38;
$L__BB96_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB96_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB96_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB96_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB96_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB96_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB96_37;

}
	// .globl	vector_div_vec_f32
.visible .entry vector_div_vec_f32(
	.param .u64 vector_div_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_div_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB97_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB97_3;
	bra.uni 	$L__BB97_2;
$L__BB97_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB97_4;
$L__BB97_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB97_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB97_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB97_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB97_8;
	bra.uni 	$L__BB97_7;
$L__BB97_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB97_9;
$L__BB97_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB97_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB97_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB97_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB97_13;
	bra.uni 	$L__BB97_12;
$L__BB97_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB97_14;
$L__BB97_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB97_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB97_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB97_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB97_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB97_19;
$L__BB97_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB97_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB97_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB97_22;
$L__BB97_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB97_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB97_27;
	@%p11 bra 	$L__BB97_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB97_26;
$L__BB97_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB97_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB97_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB97_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB97_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB97_38;
	bra.uni 	$L__BB97_30;
$L__BB97_38:
	ret;
$L__BB97_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB97_31;
$L__BB97_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB97_31;
	bra.uni 	$L__BB97_38;
$L__BB97_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB97_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB97_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB97_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB97_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB97_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB97_37;

}
	// .globl	vector_scale_vec_f32
.visible .entry vector_scale_vec_f32(
	.param .u64 vector_scale_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_scale_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB98_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB98_3;
	bra.uni 	$L__BB98_2;
$L__BB98_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB98_4;
$L__BB98_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB98_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB98_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB98_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB98_8;
	bra.uni 	$L__BB98_7;
$L__BB98_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB98_9;
$L__BB98_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB98_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB98_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB98_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB98_13;
	bra.uni 	$L__BB98_12;
$L__BB98_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB98_14;
$L__BB98_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB98_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB98_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB98_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB98_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB98_19;
$L__BB98_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB98_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB98_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB98_22;
$L__BB98_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB98_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB98_27;
	@%p11 bra 	$L__BB98_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB98_26;
$L__BB98_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB98_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB98_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB98_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB98_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB98_38;
	bra.uni 	$L__BB98_30;
$L__BB98_38:
	ret;
$L__BB98_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB98_31;
$L__BB98_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB98_31;
	bra.uni 	$L__BB98_38;
$L__BB98_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB98_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB98_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB98_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB98_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB98_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB98_37;

}
	// .globl	vector_descale_vec_f32
.visible .entry vector_descale_vec_f32(
	.param .u64 vector_descale_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_descale_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB99_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB99_3;
	bra.uni 	$L__BB99_2;
$L__BB99_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB99_4;
$L__BB99_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB99_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB99_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB99_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB99_8;
	bra.uni 	$L__BB99_7;
$L__BB99_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB99_9;
$L__BB99_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB99_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB99_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB99_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB99_13;
	bra.uni 	$L__BB99_12;
$L__BB99_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB99_14;
$L__BB99_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB99_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB99_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB99_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB99_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB99_19;
$L__BB99_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB99_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB99_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB99_22;
$L__BB99_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB99_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB99_27;
	@%p11 bra 	$L__BB99_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB99_26;
$L__BB99_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB99_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB99_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB99_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB99_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB99_38;
	bra.uni 	$L__BB99_30;
$L__BB99_38:
	ret;
$L__BB99_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB99_31;
$L__BB99_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB99_31;
	bra.uni 	$L__BB99_38;
$L__BB99_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB99_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB99_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB99_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB99_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB99_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB99_37;

}
	// .globl	vector_powf_vec_f32
.visible .entry vector_powf_vec_f32(
	.param .u64 vector_powf_vec_f32_param_0
)
{
	.reg .pred 	%p<34>;
	.reg .b32 	%r<20>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_powf_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB100_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB100_3;
	bra.uni 	$L__BB100_2;
$L__BB100_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB100_4;
$L__BB100_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB100_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB100_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB100_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB100_8;
	bra.uni 	$L__BB100_7;
$L__BB100_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB100_9;
$L__BB100_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB100_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB100_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB100_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB100_13;
	bra.uni 	$L__BB100_12;
$L__BB100_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB100_14;
$L__BB100_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB100_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB100_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB100_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB100_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB100_19;
$L__BB100_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB100_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB100_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB100_22;
$L__BB100_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB100_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p33, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p33 bra 	$L__BB100_27;
	@%p11 bra 	$L__BB100_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB100_26;
$L__BB100_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB100_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB100_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB100_29;
	ld.u32 	%r16, [%rd145];
	ld.u32 	%r15, [%rd146];
	// begin inline asm
	lg2.approx.ftz.f32 %r14, %r15;
    mul.rn.ftz.f32 %r14, %r14, %r16;
    ex2.approx.ftz.f32 %r14, %r14;
	// end inline asm
	mov.b32 	%f7, %r14;
$L__BB100_29:
	st.f32 	[%rd138], %f7;
	setp.eq.s64 	%p22, %rd31, 1;
	@%p22 bra 	$L__BB100_38;
	bra.uni 	$L__BB100_30;
$L__BB100_38:
	ret;
$L__BB100_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p27, 0;
	bra.uni 	$L__BB100_31;
$L__BB100_37:
	selp.b64 	%rd148, %rd119, %rd12, %p23;
	selp.b64 	%rd62, %rd120, 0, %p23;
	selp.b64 	%rd149, %rd123, %rd3, %p25;
	st.f32 	[%rd62], %f8;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p31, %rd147, 0;
	@%p31 bra 	$L__BB100_31;
	bra.uni 	$L__BB100_38;
$L__BB100_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p24, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p24 bra 	$L__BB100_35;
	selp.b64 	%rd152, 0, %rd6, %p33;
	setp.eq.s64 	%p26, %rd154, 0;
	@%p26 bra 	$L__BB100_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB100_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p28, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p28;
	selp.b64 	%rd155, %rd128, 0, %p28;
	setp.eq.s64 	%p29, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p29;
	mov.u64 	%rd154, 0;
	mov.pred 	%p33, %p27;
$L__BB100_35:
	setp.gt.u64 	%p23, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p25, %rd122, %rd6;
	setp.eq.s64 	%p30, %rd156, 0;
	@%p30 bra 	$L__BB100_37;
	ld.u32 	%r19, [%rd155];
	ld.u32 	%r18, [%rd156];
	// begin inline asm
	lg2.approx.ftz.f32 %r17, %r18;
    mul.rn.ftz.f32 %r17, %r17, %r19;
    ex2.approx.ftz.f32 %r17, %r17;
	// end inline asm
	mov.b32 	%f8, %r17;
	bra.uni 	$L__BB100_37;

}
	// .globl	vector_greater_vec_f32
.visible .entry vector_greater_vec_f32(
	.param .u64 vector_greater_vec_f32_param_0
)
{
	.reg .pred 	%p<36>;
	.reg .b32 	%r<14>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_greater_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB101_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB101_3;
	bra.uni 	$L__BB101_2;
$L__BB101_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB101_4;
$L__BB101_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB101_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB101_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB101_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB101_8;
	bra.uni 	$L__BB101_7;
$L__BB101_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB101_9;
$L__BB101_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB101_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB101_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB101_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB101_13;
	bra.uni 	$L__BB101_12;
$L__BB101_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB101_14;
$L__BB101_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB101_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB101_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB101_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB101_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB101_19;
$L__BB101_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB101_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB101_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB101_22;
$L__BB101_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB101_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p35, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p35 bra 	$L__BB101_27;
	@%p11 bra 	$L__BB101_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB101_26;
$L__BB101_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB101_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB101_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB101_29;
	ld.f32 	%f6, [%rd146];
	ld.f32 	%f7, [%rd145];
	setp.gt.f32 	%p22, %f6, %f7;
	selp.f32 	%f11, %f6, %f7, %p22;
$L__BB101_29:
	st.f32 	[%rd138], %f11;
	setp.eq.s64 	%p23, %rd31, 1;
	@%p23 bra 	$L__BB101_38;
	bra.uni 	$L__BB101_30;
$L__BB101_38:
	ret;
$L__BB101_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p28, 0;
	bra.uni 	$L__BB101_31;
$L__BB101_37:
	selp.b64 	%rd148, %rd119, %rd12, %p24;
	selp.b64 	%rd62, %rd120, 0, %p24;
	selp.b64 	%rd149, %rd123, %rd3, %p26;
	st.f32 	[%rd62], %f12;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p33, %rd147, 0;
	@%p33 bra 	$L__BB101_31;
	bra.uni 	$L__BB101_38;
$L__BB101_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p25, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p25 bra 	$L__BB101_35;
	selp.b64 	%rd152, 0, %rd6, %p35;
	setp.eq.s64 	%p27, %rd154, 0;
	@%p27 bra 	$L__BB101_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB101_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p29, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p29;
	selp.b64 	%rd155, %rd128, 0, %p29;
	setp.eq.s64 	%p30, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p30;
	mov.u64 	%rd154, 0;
	mov.pred 	%p35, %p28;
$L__BB101_35:
	setp.gt.u64 	%p24, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p26, %rd122, %rd6;
	setp.eq.s64 	%p31, %rd156, 0;
	@%p31 bra 	$L__BB101_37;
	ld.f32 	%f9, [%rd156];
	ld.f32 	%f10, [%rd155];
	setp.gt.f32 	%p32, %f9, %f10;
	selp.f32 	%f12, %f9, %f10, %p32;
	bra.uni 	$L__BB101_37;

}
	// .globl	vector_lesser_vec_f32
.visible .entry vector_lesser_vec_f32(
	.param .u64 vector_lesser_vec_f32_param_0
)
{
	.reg .pred 	%p<36>;
	.reg .b32 	%r<14>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<157>;

	ld.param.u64 	%rd77, [vector_lesser_vec_f32_param_0];
	cvta.to.global.u64 	%rd78, %rd77;
	ld.global.nc.u64 	%rd1, [%rd78];
	ld.global.nc.u64 	%rd2, [%rd78+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd79, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd81, %r3, %r2;
	add.s64 	%rd154, %rd81, %rd79;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd143, [%rd78+32];
	ld.global.nc.u64 	%rd8, [%rd78+40];
	ld.global.nc.u64 	%rd10, [%rd78+16];
	ld.global.nc.u64 	%rd11, [%rd78+24];
	setp.le.u64 	%p4, %rd11, %rd154;
	not.b64 	%rd85, %rd154;
	mov.u64 	%rd136, 0;
	mov.u64 	%rd132, %rd136;
	@%p4 bra 	$L__BB102_5;
	max.u64 	%rd84, %rd11, %rd154;
	add.s64 	%rd13, %rd85, %rd84;
	or.b64  	%rd86, %rd13, %rd5;
	and.b64  	%rd87, %rd86, -4294967296;
	setp.ne.s64 	%p5, %rd87, 0;
	@%p5 bra 	$L__BB102_3;
	bra.uni 	$L__BB102_2;
$L__BB102_3:
	div.u64 	%rd131, %rd13, %rd5;
	bra.uni 	$L__BB102_4;
$L__BB102_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd13;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd131, %r7;
$L__BB102_4:
	add.s64 	%rd132, %rd131, 1;
$L__BB102_5:
	setp.le.u64 	%p6, %rd2, %rd154;
	mov.u64 	%rd134, %rd136;
	@%p6 bra 	$L__BB102_10;
	max.u64 	%rd89, %rd2, %rd154;
	add.s64 	%rd19, %rd85, %rd89;
	or.b64  	%rd91, %rd19, %rd5;
	and.b64  	%rd92, %rd91, -4294967296;
	setp.ne.s64 	%p7, %rd92, 0;
	@%p7 bra 	$L__BB102_8;
	bra.uni 	$L__BB102_7;
$L__BB102_8:
	div.u64 	%rd133, %rd19, %rd5;
	bra.uni 	$L__BB102_9;
$L__BB102_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd19;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd133, %r10;
$L__BB102_9:
	add.s64 	%rd134, %rd133, 1;
$L__BB102_10:
	setp.le.u64 	%p8, %rd8, %rd154;
	@%p8 bra 	$L__BB102_15;
	max.u64 	%rd94, %rd8, %rd154;
	add.s64 	%rd25, %rd85, %rd94;
	or.b64  	%rd96, %rd25, %rd5;
	and.b64  	%rd97, %rd96, -4294967296;
	setp.ne.s64 	%p9, %rd97, 0;
	@%p9 bra 	$L__BB102_13;
	bra.uni 	$L__BB102_12;
$L__BB102_13:
	div.u64 	%rd135, %rd25, %rd5;
	bra.uni 	$L__BB102_14;
$L__BB102_12:
	cvt.u32.u64 	%r11, %rd5;
	cvt.u32.u64 	%r12, %rd25;
	div.u32 	%r13, %r12, %r11;
	cvt.u64.u32 	%rd135, %r13;
$L__BB102_14:
	add.s64 	%rd136, %rd135, 1;
$L__BB102_15:
	min.u64 	%rd98, %rd134, %rd136;
	min.u64 	%rd31, %rd132, %rd98;
	setp.eq.s64 	%p10, %rd31, 0;
	@%p10 bra 	$L__BB102_38;
	shl.b64 	%rd80, %rd2, 2;
	shl.b64 	%rd83, %rd11, 2;
	add.s64 	%rd12, %rd10, %rd83;
	setp.eq.s64 	%p11, %rd154, 0;
	shl.b64 	%rd130, %rd154, 2;
	@%p11 bra 	$L__BB102_18;
	setp.gt.u64 	%p12, %rd11, %rd154;
	add.s64 	%rd100, %rd10, %rd130;
	add.s64 	%rd101, %rd100, 4;
	selp.b64 	%rd148, %rd101, %rd12, %p12;
	selp.b64 	%rd138, %rd100, 0, %p12;
	bra.uni 	$L__BB102_19;
$L__BB102_18:
	setp.eq.s64 	%p13, %rd11, 0;
	selp.b64 	%rd102, 0, 4, %p13;
	add.s64 	%rd148, %rd10, %rd102;
	selp.b64 	%rd138, 0, %rd10, %p13;
$L__BB102_19:
	shl.b64 	%rd82, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd80;
	@%p11 bra 	$L__BB102_21;
	setp.gt.u64 	%p15, %rd2, %rd154;
	add.s64 	%rd104, %rd1, %rd130;
	add.s64 	%rd105, %rd104, 4;
	selp.b64 	%rd149, %rd105, %rd3, %p15;
	selp.b64 	%rd140, %rd104, 0, %p15;
	bra.uni 	$L__BB102_22;
$L__BB102_21:
	setp.eq.s64 	%p16, %rd2, 0;
	selp.b64 	%rd106, 0, 4, %p16;
	add.s64 	%rd149, %rd1, %rd106;
	selp.b64 	%rd140, 0, %rd1, %p16;
$L__BB102_22:
	add.s64 	%rd9, %rd143, %rd82;
	setp.eq.s64 	%p35, %rd140, 0;
	mov.u64 	%rd146, 0;
	@%p35 bra 	$L__BB102_27;
	@%p11 bra 	$L__BB102_25;
	setp.gt.u64 	%p18, %rd8, %rd154;
	add.s64 	%rd110, %rd143, %rd130;
	add.s64 	%rd111, %rd110, 4;
	selp.b64 	%rd143, %rd111, %rd9, %p18;
	selp.b64 	%rd145, %rd110, 0, %p18;
	bra.uni 	$L__BB102_26;
$L__BB102_25:
	setp.eq.s64 	%p19, %rd8, 0;
	selp.b64 	%rd112, 0, 4, %p19;
	add.s64 	%rd46, %rd143, %rd112;
	selp.b64 	%rd145, 0, %rd143, %p19;
	mov.u64 	%rd143, %rd46;
$L__BB102_26:
	setp.eq.s64 	%p20, %rd145, 0;
	selp.b64 	%rd146, 0, %rd140, %p20;
	mov.u64 	%rd154, 0;
$L__BB102_27:
	setp.eq.s64 	%p21, %rd146, 0;
	@%p21 bra 	$L__BB102_29;
	ld.f32 	%f6, [%rd146];
	ld.f32 	%f7, [%rd145];
	setp.lt.f32 	%p22, %f6, %f7;
	selp.f32 	%f11, %f6, %f7, %p22;
$L__BB102_29:
	st.f32 	[%rd138], %f11;
	setp.eq.s64 	%p23, %rd31, 1;
	@%p23 bra 	$L__BB102_38;
	bra.uni 	$L__BB102_30;
$L__BB102_38:
	ret;
$L__BB102_30:
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd147, %rd31, -1;
	shl.b64 	%rd118, %rd5, 2;
	mov.u64 	%rd115, 0;
	mov.pred 	%p28, 0;
	bra.uni 	$L__BB102_31;
$L__BB102_37:
	selp.b64 	%rd148, %rd119, %rd12, %p24;
	selp.b64 	%rd62, %rd120, 0, %p24;
	selp.b64 	%rd149, %rd123, %rd3, %p26;
	st.f32 	[%rd62], %f12;
	add.s64 	%rd147, %rd147, -1;
	setp.ne.s64 	%p33, %rd147, 0;
	@%p33 bra 	$L__BB102_31;
	bra.uni 	$L__BB102_38;
$L__BB102_31:
	sub.s64 	%rd116, %rd12, %rd148;
	shr.u64 	%rd117, %rd116, 2;
	sub.s64 	%rd121, %rd3, %rd149;
	shr.u64 	%rd122, %rd121, 2;
	setp.le.u64 	%p25, %rd122, %rd6;
	add.s64 	%rd123, %rd149, %rd118;
	mov.u64 	%rd156, %rd115;
	@%p25 bra 	$L__BB102_35;
	selp.b64 	%rd152, 0, %rd6, %p35;
	setp.eq.s64 	%p27, %rd154, 0;
	@%p27 bra 	$L__BB102_34;
	add.s64 	%rd152, %rd152, %rd154;
$L__BB102_34:
	add.s64 	%rd63, %rd123, -4;
	sub.s64 	%rd125, %rd9, %rd143;
	shr.u64 	%rd126, %rd125, 2;
	setp.gt.u64 	%p29, %rd126, %rd152;
	shl.b64 	%rd127, %rd152, 2;
	add.s64 	%rd128, %rd143, %rd127;
	add.s64 	%rd129, %rd128, 4;
	selp.b64 	%rd143, %rd129, %rd9, %p29;
	selp.b64 	%rd155, %rd128, 0, %p29;
	setp.eq.s64 	%p30, %rd155, 0;
	selp.b64 	%rd156, 0, %rd63, %p30;
	mov.u64 	%rd154, 0;
	mov.pred 	%p35, %p28;
$L__BB102_35:
	setp.gt.u64 	%p24, %rd117, %rd6;
	add.s64 	%rd119, %rd148, %rd118;
	add.s64 	%rd120, %rd119, -4;
	setp.gt.u64 	%p26, %rd122, %rd6;
	setp.eq.s64 	%p31, %rd156, 0;
	@%p31 bra 	$L__BB102_37;
	ld.f32 	%f9, [%rd156];
	ld.f32 	%f10, [%rd155];
	setp.lt.f32 	%p32, %f9, %f10;
	selp.f32 	%f12, %f9, %f10, %p32;
	bra.uni 	$L__BB102_37;

}
	// .globl	vector_dot_f32
.visible .entry vector_dot_f32()
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9b271fbca47443a0c783d86781d13fba_$_50;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 194, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 194

}
	// .globl	vector_quote_f32
.visible .entry vector_quote_f32()
{
	.reg .b64 	%rd<5>;

	mov.u64 	%rd1, anon_$_9b271fbca47443a0c783d86781d13fba_$_11;
	cvta.global.u64 	%rd2, %rd1;
	mov.u64 	%rd3, anon_$_9b271fbca47443a0c783d86781d13fba_$_51;
	cvta.global.u64 	%rd4, %rd3;
	{ // callseq 195, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd2;
	.param .b64 param1;
	st.param.b64 	[param1+0], 19;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd4;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 195

}
	// .globl	vector_add_vec_ref_f32
.visible .entry vector_add_vec_ref_f32(
	.param .u64 vector_add_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_add_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB105_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB105_3;
	bra.uni 	$L__BB105_2;
$L__BB105_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB105_4;
$L__BB105_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB105_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB105_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB105_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB105_8;
	bra.uni 	$L__BB105_7;
$L__BB105_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB105_9;
$L__BB105_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB105_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB105_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB105_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB105_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB105_14;
$L__BB105_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB105_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB105_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB105_17;
$L__BB105_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB105_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	add.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB105_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB105_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB105_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	add.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	add.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB105_20;
$L__BB105_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB105_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	add.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB105_23:
	ret;

}
	// .globl	vector_sub_vec_ref_f32
.visible .entry vector_sub_vec_ref_f32(
	.param .u64 vector_sub_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_sub_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB106_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB106_3;
	bra.uni 	$L__BB106_2;
$L__BB106_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB106_4;
$L__BB106_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB106_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB106_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB106_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB106_8;
	bra.uni 	$L__BB106_7;
$L__BB106_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB106_9;
$L__BB106_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB106_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB106_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB106_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB106_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB106_14;
$L__BB106_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB106_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB106_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB106_17;
$L__BB106_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB106_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	sub.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB106_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB106_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB106_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	sub.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	sub.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB106_20;
$L__BB106_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB106_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	sub.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB106_23:
	ret;

}
	// .globl	vector_mul_vec_ref_f32
.visible .entry vector_mul_vec_ref_f32(
	.param .u64 vector_mul_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_mul_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB107_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB107_3;
	bra.uni 	$L__BB107_2;
$L__BB107_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB107_4;
$L__BB107_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB107_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB107_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB107_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB107_8;
	bra.uni 	$L__BB107_7;
$L__BB107_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB107_9;
$L__BB107_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB107_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB107_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB107_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB107_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB107_14;
$L__BB107_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB107_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB107_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB107_17;
$L__BB107_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB107_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB107_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB107_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB107_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB107_20;
$L__BB107_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB107_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	mul.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB107_23:
	ret;

}
	// .globl	vector_div_vec_ref_f32
.visible .entry vector_div_vec_ref_f32(
	.param .u64 vector_div_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_div_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB108_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB108_3;
	bra.uni 	$L__BB108_2;
$L__BB108_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB108_4;
$L__BB108_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB108_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB108_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB108_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB108_8;
	bra.uni 	$L__BB108_7;
$L__BB108_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB108_9;
$L__BB108_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB108_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB108_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB108_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB108_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB108_14;
$L__BB108_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB108_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB108_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB108_17;
$L__BB108_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB108_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	div.approx.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB108_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB108_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB108_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB108_20;
$L__BB108_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB108_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	div.approx.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB108_23:
	ret;

}
	// .globl	vector_scale_vec_ref_f32
.visible .entry vector_scale_vec_ref_f32(
	.param .u64 vector_scale_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_scale_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB109_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB109_3;
	bra.uni 	$L__BB109_2;
$L__BB109_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB109_4;
$L__BB109_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB109_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB109_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB109_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB109_8;
	bra.uni 	$L__BB109_7;
$L__BB109_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB109_9;
$L__BB109_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB109_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB109_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB109_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB109_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB109_14;
$L__BB109_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB109_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB109_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB109_17;
$L__BB109_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB109_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	mul.rn.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB109_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB109_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB109_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	mul.rn.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	mul.rn.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB109_20;
$L__BB109_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB109_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	mul.rn.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB109_23:
	ret;

}
	// .globl	vector_descale_vec_ref_f32
.visible .entry vector_descale_vec_ref_f32(
	.param .u64 vector_descale_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_descale_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB110_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB110_3;
	bra.uni 	$L__BB110_2;
$L__BB110_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB110_4;
$L__BB110_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB110_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB110_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB110_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB110_8;
	bra.uni 	$L__BB110_7;
$L__BB110_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB110_9;
$L__BB110_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB110_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB110_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB110_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB110_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB110_14;
$L__BB110_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB110_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB110_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB110_17;
$L__BB110_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB110_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	div.approx.ftz.f32 %r11, %r12, %r13;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB110_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB110_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB110_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	div.approx.ftz.f32 %r14, %r15, %r16;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	div.approx.ftz.f32 %r17, %r18, %r19;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB110_20;
$L__BB110_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB110_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	div.approx.ftz.f32 %r20, %r21, %r22;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB110_23:
	ret;

}
	// .globl	vector_powf_vec_ref_f32
.visible .entry vector_powf_vec_ref_f32(
	.param .u64 vector_powf_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<23>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_powf_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB111_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB111_3;
	bra.uni 	$L__BB111_2;
$L__BB111_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB111_4;
$L__BB111_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB111_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB111_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB111_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB111_8;
	bra.uni 	$L__BB111_7;
$L__BB111_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB111_9;
$L__BB111_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB111_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB111_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB111_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB111_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB111_14;
$L__BB111_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB111_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB111_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB111_17;
$L__BB111_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB111_17:
	ld.u32 	%r12, [%rd101];
	ld.u32 	%r13, [%rd103];
	// begin inline asm
	lg2.approx.ftz.f32 %r11, %r12;
    mul.rn.ftz.f32 %r11, %r11, %r13;
    ex2.approx.ftz.f32 %r11, %r11;
	// end inline asm
	st.u32 	[%rd101], %r11;
	setp.eq.s64 	%p12, %rd22, 1;
	@%p12 bra 	$L__BB111_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p13, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p13 bra 	$L__BB111_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB111_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p14, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p14;
	selp.b64 	%rd76, %rd74, 0, %p14;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p15, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd80, 0, %p15;
	selp.b64 	%rd82, %rd79, %rd9, %p15;
	ld.u32 	%r15, [%rd76];
	ld.u32 	%r16, [%rd81];
	// begin inline asm
	lg2.approx.ftz.f32 %r14, %r15;
    mul.rn.ftz.f32 %r14, %r14, %r16;
    ex2.approx.ftz.f32 %r14, %r14;
	// end inline asm
	st.u32 	[%rd76], %r14;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd82, %rd95;
	ld.u32 	%r19, [%rd107+-4];
	ld.u32 	%r18, [%rd108+-4];
	// begin inline asm
	lg2.approx.ftz.f32 %r17, %r18;
    mul.rn.ftz.f32 %r17, %r17, %r19;
    ex2.approx.ftz.f32 %r17, %r17;
	// end inline asm
	st.u32 	[%rd108+-4], %r17;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p16, %rd104, 0;
	@%p16 bra 	$L__BB111_20;
$L__BB111_21:
	setp.eq.s64 	%p17, %rd36, 0;
	@%p17 bra 	$L__BB111_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p18, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p18;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p19, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p19;
	ld.u32 	%r21, [%rd88];
	ld.u32 	%r22, [%rd93];
	// begin inline asm
	lg2.approx.ftz.f32 %r20, %r21;
    mul.rn.ftz.f32 %r20, %r20, %r22;
    ex2.approx.ftz.f32 %r20, %r20;
	// end inline asm
	st.u32 	[%rd88], %r20;
$L__BB111_23:
	ret;

}
	// .globl	vector_greater_vec_ref_f32
.visible .entry vector_greater_vec_ref_f32(
	.param .u64 vector_greater_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_greater_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB112_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB112_3;
	bra.uni 	$L__BB112_2;
$L__BB112_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB112_4;
$L__BB112_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB112_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB112_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB112_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB112_8;
	bra.uni 	$L__BB112_7;
$L__BB112_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB112_9;
$L__BB112_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB112_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB112_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB112_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB112_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB112_14;
$L__BB112_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB112_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB112_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB112_17;
$L__BB112_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB112_17:
	ld.f32 	%f1, [%rd103];
	ld.f32 	%f2, [%rd101];
	setp.gt.f32 	%p12, %f2, %f1;
	selp.f32 	%f3, %f2, %f1, %p12;
	st.f32 	[%rd101], %f3;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB112_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p14 bra 	$L__BB112_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB112_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p15, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p15;
	selp.b64 	%rd76, %rd74, 0, %p15;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p16, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd79, %rd9, %p16;
	selp.b64 	%rd82, %rd80, 0, %p16;
	ld.f32 	%f4, [%rd82];
	ld.f32 	%f5, [%rd76];
	setp.gt.f32 	%p17, %f5, %f4;
	selp.f32 	%f6, %f5, %f4, %p17;
	st.f32 	[%rd76], %f6;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd81, %rd95;
	ld.f32 	%f7, [%rd107+-4];
	ld.f32 	%f8, [%rd108+-4];
	setp.gt.f32 	%p18, %f8, %f7;
	selp.f32 	%f9, %f8, %f7, %p18;
	st.f32 	[%rd108+-4], %f9;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p19, %rd104, 0;
	@%p19 bra 	$L__BB112_20;
$L__BB112_21:
	setp.eq.s64 	%p20, %rd36, 0;
	@%p20 bra 	$L__BB112_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p21, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p21;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p22, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p22;
	ld.f32 	%f10, [%rd93];
	ld.f32 	%f11, [%rd88];
	setp.gt.f32 	%p23, %f11, %f10;
	selp.f32 	%f12, %f11, %f10, %p23;
	st.f32 	[%rd88], %f12;
$L__BB112_23:
	ret;

}
	// .globl	vector_lesser_vec_ref_f32
.visible .entry vector_lesser_vec_ref_f32(
	.param .u64 vector_lesser_vec_ref_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<13>;
	.reg .b64 	%rd<109>;

	ld.param.u64 	%rd47, [vector_lesser_vec_ref_f32_param_0];
	cvta.to.global.u64 	%rd48, %rd47;
	ld.global.nc.u64 	%rd1, [%rd48];
	ld.global.nc.u64 	%rd2, [%rd48+8];
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd49, %r1;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mul.wide.u32 	%rd51, %r3, %r2;
	add.s64 	%rd4, %rd51, %rd49;
	mov.u32 	%r4, %nctaid.x;
	mul.wide.u32 	%rd5, %r2, %r4;
	ld.global.nc.u64 	%rd7, [%rd48+16];
	ld.global.nc.u64 	%rd8, [%rd48+24];
	setp.le.u64 	%p1, %rd2, %rd4;
	not.b64 	%rd54, %rd4;
	mov.u64 	%rd99, 0;
	mov.u64 	%rd97, %rd99;
	@%p1 bra 	$L__BB113_5;
	max.u64 	%rd53, %rd2, %rd4;
	add.s64 	%rd10, %rd54, %rd53;
	or.b64  	%rd55, %rd10, %rd5;
	and.b64  	%rd56, %rd55, -4294967296;
	setp.ne.s64 	%p2, %rd56, 0;
	@%p2 bra 	$L__BB113_3;
	bra.uni 	$L__BB113_2;
$L__BB113_3:
	div.u64 	%rd96, %rd10, %rd5;
	bra.uni 	$L__BB113_4;
$L__BB113_2:
	cvt.u32.u64 	%r5, %rd5;
	cvt.u32.u64 	%r6, %rd10;
	div.u32 	%r7, %r6, %r5;
	cvt.u64.u32 	%rd96, %r7;
$L__BB113_4:
	add.s64 	%rd97, %rd96, 1;
$L__BB113_5:
	setp.le.u64 	%p3, %rd8, %rd4;
	@%p3 bra 	$L__BB113_10;
	max.u64 	%rd58, %rd8, %rd4;
	add.s64 	%rd16, %rd54, %rd58;
	or.b64  	%rd60, %rd16, %rd5;
	and.b64  	%rd61, %rd60, -4294967296;
	setp.ne.s64 	%p4, %rd61, 0;
	@%p4 bra 	$L__BB113_8;
	bra.uni 	$L__BB113_7;
$L__BB113_8:
	div.u64 	%rd98, %rd16, %rd5;
	bra.uni 	$L__BB113_9;
$L__BB113_7:
	cvt.u32.u64 	%r8, %rd5;
	cvt.u32.u64 	%r9, %rd16;
	div.u32 	%r10, %r9, %r8;
	cvt.u64.u32 	%rd98, %r10;
$L__BB113_9:
	add.s64 	%rd99, %rd98, 1;
$L__BB113_10:
	min.u64 	%rd22, %rd97, %rd99;
	setp.eq.s64 	%p5, %rd22, 0;
	@%p5 bra 	$L__BB113_23;
	shl.b64 	%rd50, %rd2, 2;
	shl.b64 	%rd52, %rd8, 2;
	add.s64 	%rd3, %rd1, %rd50;
	setp.eq.s64 	%p6, %rd4, 0;
	shl.b64 	%rd94, %rd4, 2;
	@%p6 bra 	$L__BB113_13;
	setp.gt.u64 	%p7, %rd2, %rd4;
	add.s64 	%rd63, %rd1, %rd94;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd108, %rd64, %rd3, %p7;
	selp.b64 	%rd101, %rd63, 0, %p7;
	bra.uni 	$L__BB113_14;
$L__BB113_13:
	setp.eq.s64 	%p8, %rd2, 0;
	selp.b64 	%rd65, 0, 4, %p8;
	add.s64 	%rd108, %rd1, %rd65;
	selp.b64 	%rd101, 0, %rd1, %p8;
$L__BB113_14:
	add.s64 	%rd9, %rd7, %rd52;
	@%p6 bra 	$L__BB113_16;
	setp.gt.u64 	%p10, %rd8, %rd4;
	add.s64 	%rd67, %rd7, %rd94;
	add.s64 	%rd68, %rd67, 4;
	selp.b64 	%rd107, %rd68, %rd9, %p10;
	selp.b64 	%rd103, %rd67, 0, %p10;
	bra.uni 	$L__BB113_17;
$L__BB113_16:
	setp.eq.s64 	%p11, %rd8, 0;
	selp.b64 	%rd69, 0, 4, %p11;
	add.s64 	%rd107, %rd7, %rd69;
	selp.b64 	%rd103, 0, %rd7, %p11;
$L__BB113_17:
	ld.f32 	%f1, [%rd103];
	ld.f32 	%f2, [%rd101];
	setp.lt.f32 	%p12, %f2, %f1;
	selp.f32 	%f3, %f2, %f1, %p12;
	st.f32 	[%rd101], %f3;
	setp.eq.s64 	%p13, %rd22, 1;
	@%p13 bra 	$L__BB113_23;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd35, %rd22, -1;
	and.b64  	%rd36, %rd35, 1;
	setp.eq.s64 	%p14, %rd22, 2;
	shl.b64 	%rd95, %rd5, 2;
	@%p14 bra 	$L__BB113_21;
	and.b64  	%rd104, %rd35, -2;
$L__BB113_20:
	sub.s64 	%rd70, %rd3, %rd108;
	shr.u64 	%rd71, %rd70, 2;
	setp.gt.u64 	%p15, %rd71, %rd6;
	add.s64 	%rd73, %rd108, %rd95;
	add.s64 	%rd74, %rd73, -4;
	selp.b64 	%rd75, %rd73, %rd3, %p15;
	selp.b64 	%rd76, %rd74, 0, %p15;
	sub.s64 	%rd77, %rd9, %rd107;
	shr.u64 	%rd78, %rd77, 2;
	setp.gt.u64 	%p16, %rd78, %rd6;
	add.s64 	%rd79, %rd107, %rd95;
	add.s64 	%rd80, %rd79, -4;
	selp.b64 	%rd81, %rd79, %rd9, %p16;
	selp.b64 	%rd82, %rd80, 0, %p16;
	ld.f32 	%f4, [%rd82];
	ld.f32 	%f5, [%rd76];
	setp.lt.f32 	%p17, %f5, %f4;
	selp.f32 	%f6, %f5, %f4, %p17;
	st.f32 	[%rd76], %f6;
	add.s64 	%rd108, %rd75, %rd95;
	add.s64 	%rd107, %rd81, %rd95;
	ld.f32 	%f7, [%rd107+-4];
	ld.f32 	%f8, [%rd108+-4];
	setp.lt.f32 	%p18, %f8, %f7;
	selp.f32 	%f9, %f8, %f7, %p18;
	st.f32 	[%rd108+-4], %f9;
	add.s64 	%rd104, %rd104, -2;
	setp.ne.s64 	%p19, %rd104, 0;
	@%p19 bra 	$L__BB113_20;
$L__BB113_21:
	setp.eq.s64 	%p20, %rd36, 0;
	@%p20 bra 	$L__BB113_23;
	sub.s64 	%rd83, %rd3, %rd108;
	shr.u64 	%rd84, %rd83, 2;
	setp.gt.u64 	%p21, %rd84, %rd6;
	add.s64 	%rd86, %rd108, %rd95;
	add.s64 	%rd87, %rd86, -4;
	selp.b64 	%rd88, %rd87, 0, %p21;
	sub.s64 	%rd89, %rd9, %rd107;
	shr.u64 	%rd90, %rd89, 2;
	setp.gt.u64 	%p22, %rd90, %rd6;
	add.s64 	%rd91, %rd107, %rd95;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p22;
	ld.f32 	%f10, [%rd93];
	ld.f32 	%f11, [%rd88];
	setp.lt.f32 	%p23, %f11, %f10;
	selp.f32 	%f12, %f11, %f10, %p23;
	st.f32 	[%rd88], %f12;
$L__BB113_23:
	ret;

}
	// .globl	insert_hash_table_f32
.visible .entry insert_hash_table_f32(
	.param .u64 insert_hash_table_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<31>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<85>;

	ld.param.u64 	%rd37, [insert_hash_table_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd37;
	ld.global.nc.u64 	%rd83, [%rd1];
	ld.global.nc.u64 	%rd38, [%rd1+8];
	mov.u32 	%r5, %tid.x;
	cvt.u64.u32 	%rd39, %r5;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mul.wide.u32 	%rd41, %r7, %r6;
	add.s64 	%rd80, %rd41, %rd39;
	mov.u32 	%r8, %nctaid.x;
	mul.wide.u32 	%rd5, %r6, %r8;
	ld.global.nc.u64 	%rd84, [%rd1+16];
	ld.global.nc.u64 	%rd8, [%rd1+24];
	setp.le.u64 	%p2, %rd38, %rd80;
	not.b64 	%rd44, %rd80;
	mov.u64 	%rd79, 0;
	mov.u64 	%rd77, %rd79;
	@%p2 bra 	$L__BB114_5;
	max.u64 	%rd43, %rd38, %rd80;
	add.s64 	%rd10, %rd44, %rd43;
	or.b64  	%rd45, %rd10, %rd5;
	and.b64  	%rd46, %rd45, -4294967296;
	setp.ne.s64 	%p3, %rd46, 0;
	@%p3 bra 	$L__BB114_3;
	bra.uni 	$L__BB114_2;
$L__BB114_3:
	div.u64 	%rd76, %rd10, %rd5;
	bra.uni 	$L__BB114_4;
$L__BB114_2:
	cvt.u32.u64 	%r9, %rd5;
	cvt.u32.u64 	%r10, %rd10;
	div.u32 	%r11, %r10, %r9;
	cvt.u64.u32 	%rd76, %r11;
$L__BB114_4:
	add.s64 	%rd77, %rd76, 1;
$L__BB114_5:
	setp.le.u64 	%p4, %rd8, %rd80;
	@%p4 bra 	$L__BB114_10;
	max.u64 	%rd48, %rd8, %rd80;
	add.s64 	%rd16, %rd44, %rd48;
	or.b64  	%rd50, %rd16, %rd5;
	and.b64  	%rd51, %rd50, -4294967296;
	setp.ne.s64 	%p5, %rd51, 0;
	@%p5 bra 	$L__BB114_8;
	bra.uni 	$L__BB114_7;
$L__BB114_8:
	div.u64 	%rd78, %rd16, %rd5;
	bra.uni 	$L__BB114_9;
$L__BB114_7:
	cvt.u32.u64 	%r12, %rd5;
	cvt.u32.u64 	%r13, %rd16;
	div.u32 	%r14, %r13, %r12;
	cvt.u64.u32 	%rd78, %r14;
$L__BB114_9:
	add.s64 	%rd79, %rd78, 1;
$L__BB114_10:
	min.u64 	%rd22, %rd77, %rd79;
	setp.eq.s64 	%p6, %rd22, 0;
	@%p6 bra 	$L__BB114_21;
	shl.b64 	%rd40, %rd38, 2;
	shl.b64 	%rd42, %rd8, 2;
	add.s64 	%rd3, %rd83, %rd40;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd9, %rd84, %rd42;
	ld.global.nc.u64 	%rd23, [%rd1+40];
	ld.global.nc.u64 	%rd24, [%rd1+32];
	ld.global.nc.u64 	%rd25, [%rd1+56];
	ld.global.nc.u64 	%rd26, [%rd1+48];
	mov.u64 	%rd81, 0;
	mov.b32 	%r26, 2139095039;
	mov.u64 	%rd82, %rd81;
	bra.uni 	$L__BB114_14;
$L__BB114_13:
	setp.eq.s64 	%p13, %rd82, %rd22;
	mov.u64 	%rd80, 0;
	mov.u64 	%rd81, %rd6;
	@%p13 bra 	$L__BB114_21;
$L__BB114_14:
	add.s64 	%rd82, %rd82, 1;
	add.s64 	%rd53, %rd81, %rd80;
	sub.s64 	%rd54, %rd3, %rd83;
	shr.u64 	%rd55, %rd54, 2;
	setp.gt.u64 	%p7, %rd55, %rd53;
	shl.b64 	%rd56, %rd53, 2;
	add.s64 	%rd57, %rd83, %rd56;
	add.s64 	%rd58, %rd57, 4;
	selp.b64 	%rd83, %rd58, %rd3, %p7;
	selp.b64 	%rd59, %rd57, 0, %p7;
	sub.s64 	%rd60, %rd9, %rd84;
	shr.u64 	%rd61, %rd60, 2;
	setp.gt.u64 	%p8, %rd61, %rd53;
	add.s64 	%rd62, %rd84, %rd56;
	add.s64 	%rd63, %rd62, 4;
	selp.b64 	%rd84, %rd63, %rd9, %p8;
	selp.b64 	%rd64, %rd62, 0, %p8;
	ld.f32 	%f1, [%rd59];
	ld.f32 	%f2, [%rd64];
	mov.b32 	%r15, %f1;
	shr.u32 	%r16, %r15, 16;
	xor.b32  	%r17, %r16, %r15;
	mul.lo.s32 	%r18, %r17, -2048144789;
	shr.u32 	%r19, %r18, 13;
	xor.b32  	%r20, %r19, %r18;
	mul.lo.s32 	%r21, %r20, -1028477387;
	shr.u32 	%r22, %r21, 16;
	xor.b32  	%r23, %r22, %r21;
	and.b32  	%r30, %r23, 1023;
	bra.uni 	$L__BB114_15;
$L__BB114_12:
	add.s32 	%r27, %r30, 1;
	and.b32  	%r30, %r27, 134217727;
	@%p1 bra 	$L__BB114_15;
	bra.uni 	$L__BB114_13;
$L__BB114_15:
	cvt.u64.u32 	%rd35, %r30;
	setp.gt.u64 	%p9, %rd23, %rd35;
	@%p9 bra 	$L__BB114_17;
	bra.uni 	$L__BB114_16;
$L__BB114_17:
	shl.b64 	%rd70, %rd35, 2;
	add.s64 	%rd69, %rd70, %rd24;
	// begin inline asm
	atom.global.cas.b32 %r24,[%rd69], %r26, %r15;
	// end inline asm
	mov.b32 	%f3, %r24;
	setp.neu.f32 	%p10, %f3, 0f7F7FFFFF;
	setp.neu.f32 	%p11, %f3, %f1;
	and.pred  	%p1, %p10, %p11;
	@%p1 bra 	$L__BB114_12;
	setp.gt.u64 	%p12, %rd25, %rd35;
	@%p12 bra 	$L__BB114_19;
	bra.uni 	$L__BB114_20;
$L__BB114_19:
	add.s64 	%rd74, %rd26, %rd70;
	st.f32 	[%rd74], %f2;
	@%p1 bra 	$L__BB114_15;
	bra.uni 	$L__BB114_13;
$L__BB114_21:
	ret;
$L__BB114_16:
	mov.u64 	%rd65, anon_$_9b271fbca47443a0c783d86781d13fba_$_26;
	cvta.global.u64 	%rd66, %rd65;
	mov.u64 	%rd67, anon_$_9b271fbca47443a0c783d86781d13fba_$_62;
	cvta.global.u64 	%rd68, %rd67;
	{ // callseq 196, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd66;
	.param .b64 param1;
	st.param.b64 	[param1+0], 40;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd68;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 196
$L__BB114_20:
	mov.u64 	%rd71, anon_$_9b271fbca47443a0c783d86781d13fba_$_53;
	cvta.global.u64 	%rd72, %rd71;
	{ // callseq 197, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd35;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd25;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd72;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 197

}
	// .globl	lookup_hash_table_f32
.visible .entry lookup_hash_table_f32(
	.param .u64 lookup_hash_table_f32_param_0
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<30>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<83>;

	ld.param.u64 	%rd38, [lookup_hash_table_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd38;
	ld.global.nc.u64 	%rd78, [%rd1];
	ld.global.nc.u64 	%rd39, [%rd1+8];
	mov.u32 	%r5, %tid.x;
	cvt.u64.u32 	%rd40, %r5;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mul.wide.u32 	%rd42, %r7, %r6;
	add.s64 	%rd79, %rd42, %rd40;
	mov.u32 	%r8, %nctaid.x;
	mul.wide.u32 	%rd5, %r6, %r8;
	ld.global.nc.u64 	%rd81, [%rd1+16];
	ld.global.nc.u64 	%rd8, [%rd1+24];
	setp.le.u64 	%p2, %rd39, %rd79;
	not.b64 	%rd45, %rd79;
	mov.u64 	%rd77, 0;
	mov.u64 	%rd75, %rd77;
	@%p2 bra 	$L__BB115_5;
	max.u64 	%rd44, %rd39, %rd79;
	add.s64 	%rd10, %rd45, %rd44;
	or.b64  	%rd46, %rd10, %rd5;
	and.b64  	%rd47, %rd46, -4294967296;
	setp.ne.s64 	%p3, %rd47, 0;
	@%p3 bra 	$L__BB115_3;
	bra.uni 	$L__BB115_2;
$L__BB115_3:
	div.u64 	%rd74, %rd10, %rd5;
	bra.uni 	$L__BB115_4;
$L__BB115_2:
	cvt.u32.u64 	%r9, %rd5;
	cvt.u32.u64 	%r10, %rd10;
	div.u32 	%r11, %r10, %r9;
	cvt.u64.u32 	%rd74, %r11;
$L__BB115_4:
	add.s64 	%rd75, %rd74, 1;
$L__BB115_5:
	setp.le.u64 	%p4, %rd8, %rd79;
	@%p4 bra 	$L__BB115_10;
	max.u64 	%rd49, %rd8, %rd79;
	add.s64 	%rd16, %rd45, %rd49;
	or.b64  	%rd51, %rd16, %rd5;
	and.b64  	%rd52, %rd51, -4294967296;
	setp.ne.s64 	%p5, %rd52, 0;
	@%p5 bra 	$L__BB115_8;
	bra.uni 	$L__BB115_7;
$L__BB115_8:
	div.u64 	%rd76, %rd16, %rd5;
	bra.uni 	$L__BB115_9;
$L__BB115_7:
	cvt.u32.u64 	%r12, %rd5;
	cvt.u32.u64 	%r13, %rd16;
	div.u32 	%r14, %r13, %r12;
	cvt.u64.u32 	%rd76, %r14;
$L__BB115_9:
	add.s64 	%rd77, %rd76, 1;
$L__BB115_10:
	min.u64 	%rd22, %rd75, %rd77;
	setp.eq.s64 	%p6, %rd22, 0;
	@%p6 bra 	$L__BB115_22;
	shl.b64 	%rd41, %rd39, 2;
	shl.b64 	%rd43, %rd8, 2;
	add.s64 	%rd3, %rd78, %rd41;
	add.s64 	%rd6, %rd5, -1;
	add.s64 	%rd9, %rd81, %rd43;
	ld.global.nc.u64 	%rd23, [%rd1+40];
	ld.global.nc.u64 	%rd24, [%rd1+32];
	ld.global.nc.u64 	%rd25, [%rd1+56];
	ld.global.nc.u64 	%rd26, [%rd1+48];
	mov.u64 	%rd80, 0;
	mov.pred 	%p15, 0;
	mov.pred 	%p12, -1;
	mov.u64 	%rd82, %rd80;
	bra.uni 	$L__BB115_14;
$L__BB115_13:
	setp.eq.s64 	%p16, %rd82, %rd22;
	mov.u64 	%rd79, 0;
	mov.u64 	%rd80, %rd6;
	@%p16 bra 	$L__BB115_22;
$L__BB115_14:
	add.s64 	%rd82, %rd82, 1;
	add.s64 	%rd54, %rd80, %rd79;
	sub.s64 	%rd55, %rd3, %rd78;
	shr.u64 	%rd56, %rd55, 2;
	setp.gt.u64 	%p7, %rd56, %rd54;
	shl.b64 	%rd57, %rd54, 2;
	add.s64 	%rd58, %rd78, %rd57;
	add.s64 	%rd59, %rd58, 4;
	selp.b64 	%rd78, %rd59, %rd3, %p7;
	selp.b64 	%rd60, %rd58, 0, %p7;
	sub.s64 	%rd61, %rd9, %rd81;
	shr.u64 	%rd62, %rd61, 2;
	setp.gt.u64 	%p8, %rd62, %rd54;
	add.s64 	%rd63, %rd81, %rd57;
	add.s64 	%rd64, %rd63, 4;
	selp.b64 	%rd81, %rd64, %rd9, %p8;
	selp.b64 	%rd35, %rd63, 0, %p8;
	ld.f32 	%f1, [%rd60];
	mov.b32 	%r15, %f1;
	shr.u32 	%r16, %r15, 16;
	xor.b32  	%r17, %r16, %r15;
	mul.lo.s32 	%r18, %r17, -2048144789;
	shr.u32 	%r19, %r18, 13;
	xor.b32  	%r20, %r19, %r18;
	mul.lo.s32 	%r21, %r20, -1028477387;
	shr.u32 	%r22, %r21, 16;
	xor.b32  	%r23, %r22, %r21;
	and.b32  	%r29, %r23, 1023;
	bra.uni 	$L__BB115_15;
$L__BB115_20:
	add.s32 	%r24, %r29, 1;
	and.b32  	%r29, %r24, 134217727;
	@%p12 bra 	$L__BB115_15;
	bra.uni 	$L__BB115_13;
$L__BB115_15:
	cvt.u64.u32 	%rd36, %r29;
	setp.le.u64 	%p9, %rd23, %rd36;
	@%p9 bra 	$L__BB115_23;
	shl.b64 	%rd67, %rd36, 2;
	add.s64 	%rd68, %rd24, %rd67;
	ld.f32 	%f2, [%rd68];
	setp.eq.f32 	%p10, %f2, %f1;
	@%p10 bra 	$L__BB115_17;
	bra.uni 	$L__BB115_19;
$L__BB115_17:
	setp.gt.u64 	%p14, %rd25, %rd36;
	@%p14 bra 	$L__BB115_18;
	bra.uni 	$L__BB115_21;
$L__BB115_18:
	add.s64 	%rd72, %rd26, %rd67;
	ld.f32 	%f3, [%rd72];
	st.f32 	[%rd35], %f3;
	@%p15 bra 	$L__BB115_15;
	bra.uni 	$L__BB115_13;
$L__BB115_19:
	setp.eq.f32 	%p11, %f2, 0f7F7FFFFF;
	@%p11 bra 	$L__BB115_12;
	bra.uni 	$L__BB115_20;
$L__BB115_12:
	mov.b32 	%r25, 0;
	st.u32 	[%rd35], %r25;
	@%p15 bra 	$L__BB115_15;
	bra.uni 	$L__BB115_13;
$L__BB115_22:
	ret;
$L__BB115_23:
	mov.u64 	%rd65, anon_$_9b271fbca47443a0c783d86781d13fba_$_54;
	cvta.global.u64 	%rd66, %rd65;
	{ // callseq 198, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd36;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd23;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd66;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 198
$L__BB115_21:
	mov.u64 	%rd69, anon_$_9b271fbca47443a0c783d86781d13fba_$_55;
	cvta.global.u64 	%rd70, %rd69;
	{ // callseq 199, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd36;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd25;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd70;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 199

}
	// .globl	delete_hash_table_f32
.visible .entry delete_hash_table_f32(
	.param .u64 delete_hash_table_f32_param_0
)
{
	.reg .pred 	%p<26>;
	.reg .b32 	%r<39>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<61>;

	ld.param.u64 	%rd26, [delete_hash_table_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd26;
	ld.global.nc.u64 	%rd2, [%rd1];
	ld.global.nc.u64 	%rd3, [%rd1+8];
	mov.u32 	%r9, %tid.x;
	cvt.u64.u32 	%rd27, %r9;
	mov.u32 	%r10, %ntid.x;
	mov.u32 	%r11, %ctaid.x;
	shl.b64 	%rd28, %rd3, 2;
	add.s64 	%rd4, %rd2, %rd28;
	mul.wide.u32 	%rd29, %r11, %r10;
	add.s64 	%rd5, %rd29, %rd27;
	setp.eq.s64 	%p3, %rd5, 0;
	@%p3 bra 	$L__BB116_1;
	setp.gt.u64 	%p4, %rd3, %rd5;
	shl.b64 	%rd30, %rd5, 2;
	add.s64 	%rd31, %rd2, %rd30;
	add.s64 	%rd32, %rd31, 4;
	selp.b64 	%rd19, %rd32, %rd4, %p4;
	selp.b64 	%rd60, %rd31, 0, %p4;
	bra.uni 	$L__BB116_15;
$L__BB116_1:
	setp.eq.s64 	%p5, %rd3, 0;
	selp.b64 	%rd33, 0, 4, %p5;
	add.s64 	%rd19, %rd2, %rd33;
	selp.b64 	%rd60, 0, %rd2, %p5;
$L__BB116_15:
	setp.eq.s64 	%p6, %rd60, 0;
	@%p6 bra 	$L__BB116_25;
	mov.u32 	%r12, %nctaid.x;
	mul.wide.u32 	%rd6, %r10, %r12;
	add.s64 	%rd7, %rd6, -1;
	ld.f32 	%f3, [%rd60];
	mov.b32 	%r13, %f3;
	shr.u32 	%r14, %r13, 16;
	xor.b32  	%r15, %r14, %r13;
	mul.lo.s32 	%r16, %r15, -2048144789;
	shr.u32 	%r17, %r16, 13;
	xor.b32  	%r18, %r17, %r16;
	mul.lo.s32 	%r19, %r18, -1028477387;
	shr.u32 	%r20, %r19, 16;
	xor.b32  	%r21, %r20, %r19;
	and.b32  	%r38, %r21, 1023;
	ld.global.nc.u64 	%rd21, [%rd1+40];
	ld.global.nc.u64 	%rd22, [%rd1+32];
	ld.global.nc.u64 	%rd23, [%rd1+56];
	ld.global.nc.u64 	%rd24, [%rd1+48];
	mov.b32 	%r23, 2139095039;
	mov.pred 	%p13, 0;
	mov.pred 	%p11, -1;
	bra.uni 	$L__BB116_17;
$L__BB116_21:
	setp.eq.f32 	%p10, %f4, 0f00000000;
	mov.pred 	%p25, %p13;
	@%p10 bra 	$L__BB116_23;
	bra.uni 	$L__BB116_22;
$L__BB116_23:
	@%p25 bra 	$L__BB116_17;
	bra.uni 	$L__BB116_2;
$L__BB116_17:
	cvt.u64.u32 	%rd25, %r38;
	setp.le.u64 	%p7, %rd21, %rd25;
	@%p7 bra 	$L__BB116_27;
	shl.b64 	%rd36, %rd25, 2;
	add.s64 	%rd37, %rd22, %rd36;
	ld.f32 	%f4, [%rd37];
	setp.eq.f32 	%p8, %f4, %f3;
	@%p8 bra 	$L__BB116_19;
	bra.uni 	$L__BB116_21;
$L__BB116_19:
	setp.gt.u64 	%p12, %rd23, %rd25;
	@%p12 bra 	$L__BB116_20;
	bra.uni 	$L__BB116_24;
$L__BB116_20:
	add.s64 	%rd41, %rd24, %rd36;
	st.u32 	[%rd41], %r23;
	mov.pred 	%p25, %p13;
	bra.uni 	$L__BB116_23;
$L__BB116_22:
	add.s32 	%r22, %r38, 1;
	and.b32  	%r38, %r22, 134217727;
	mov.pred 	%p25, %p11;
	bra.uni 	$L__BB116_23;
$L__BB116_2:
	sub.s64 	%rd42, %rd4, %rd19;
	shr.u64 	%rd43, %rd42, 2;
	setp.le.u64 	%p14, %rd43, %rd7;
	@%p14 bra 	$L__BB116_25;
	shl.b64 	%rd44, %rd6, 2;
	add.s64 	%rd13, %rd19, %rd44;
	add.s64 	%rd57, %rd13, -4;
	mov.pred 	%p21, 0;
	mov.pred 	%p19, -1;
	bra.uni 	$L__BB116_5;
$L__BB116_4:
	sub.s64 	%rd53, %rd4, %rd13;
	shr.u64 	%rd54, %rd53, 2;
	setp.le.u64 	%p22, %rd54, %rd7;
	setp.gt.u64 	%p23, %rd54, %rd7;
	add.s64 	%rd56, %rd13, %rd44;
	add.s64 	%rd57, %rd56, -4;
	selp.b64 	%rd13, %rd56, %rd4, %p23;
	@%p22 bra 	$L__BB116_25;
$L__BB116_5:
	ld.f32 	%f1, [%rd57];
	mov.b32 	%r24, %f1;
	shr.u32 	%r25, %r24, 16;
	xor.b32  	%r26, %r25, %r24;
	mul.lo.s32 	%r27, %r26, -2048144789;
	shr.u32 	%r28, %r27, 13;
	xor.b32  	%r29, %r28, %r27;
	mul.lo.s32 	%r30, %r29, -1028477387;
	shr.u32 	%r31, %r30, 16;
	xor.b32  	%r32, %r31, %r30;
	and.b32  	%r36, %r32, 1023;
	bra.uni 	$L__BB116_6;
$L__BB116_10:
	setp.eq.f32 	%p18, %f2, 0f00000000;
	mov.pred 	%p24, %p21;
	@%p18 bra 	$L__BB116_12;
	bra.uni 	$L__BB116_11;
$L__BB116_12:
	@%p24 bra 	$L__BB116_6;
	bra.uni 	$L__BB116_4;
$L__BB116_6:
	cvt.u64.u32 	%rd14, %r36;
	setp.le.u64 	%p15, %rd21, %rd14;
	@%p15 bra 	$L__BB116_26;
	shl.b64 	%rd47, %rd14, 2;
	add.s64 	%rd48, %rd22, %rd47;
	ld.f32 	%f2, [%rd48];
	setp.eq.f32 	%p16, %f2, %f1;
	@%p16 bra 	$L__BB116_8;
	bra.uni 	$L__BB116_10;
$L__BB116_8:
	setp.gt.u64 	%p20, %rd23, %rd14;
	@%p20 bra 	$L__BB116_9;
	bra.uni 	$L__BB116_13;
$L__BB116_9:
	add.s64 	%rd52, %rd24, %rd47;
	st.u32 	[%rd52], %r23;
	mov.pred 	%p24, %p21;
	bra.uni 	$L__BB116_12;
$L__BB116_11:
	add.s32 	%r33, %r36, 1;
	and.b32  	%r36, %r33, 134217727;
	mov.pred 	%p24, %p19;
	bra.uni 	$L__BB116_12;
$L__BB116_25:
	ret;
$L__BB116_26:
	mov.u64 	%rd45, anon_$_9b271fbca47443a0c783d86781d13fba_$_56;
	cvta.global.u64 	%rd46, %rd45;
	{ // callseq 202, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd14;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd21;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd46;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 202
$L__BB116_13:
	mov.u64 	%rd49, anon_$_9b271fbca47443a0c783d86781d13fba_$_57;
	cvta.global.u64 	%rd50, %rd49;
	{ // callseq 203, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd14;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd23;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd50;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 203
$L__BB116_27:
	mov.u64 	%rd34, anon_$_9b271fbca47443a0c783d86781d13fba_$_56;
	cvta.global.u64 	%rd35, %rd34;
	{ // callseq 200, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd25;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd21;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd35;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 200
$L__BB116_24:
	mov.u64 	%rd38, anon_$_9b271fbca47443a0c783d86781d13fba_$_57;
	cvta.global.u64 	%rd39, %rd38;
	{ // callseq 201, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd25;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd23;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd39;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 201

}
	// .globl	count_hash_table_f32
.visible .entry count_hash_table_f32(
	.param .u64 count_hash_table_f32_param_0
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<73>;
	.reg .f32 	%f<3>;
	.reg .b64 	%rd<47>;

	ld.param.u64 	%rd22, [count_hash_table_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd22;
	// begin inline asm
	.shared .align 4 .b8 nonphysical[128];
    mov.u32 %r14, nonphysical;
	// end inline asm
	mov.u32 	%r2, %tid.x;
	cvt.u64.u32 	%rd23, %r2;
	mov.u32 	%r3, %ntid.x;
	ld.global.nc.u64 	%rd3, [%rd1+32];
	ld.global.nc.u64 	%rd4, [%rd1+40];
	mov.u32 	%r15, %ctaid.x;
	shl.b64 	%rd24, %rd4, 2;
	add.s64 	%rd5, %rd3, %rd24;
	mul.wide.u32 	%rd25, %r15, %r3;
	add.s64 	%rd6, %rd25, %rd23;
	setp.eq.s64 	%p1, %rd6, 0;
	@%p1 bra 	$L__BB117_1;
	setp.gt.u64 	%p2, %rd4, %rd6;
	shl.b64 	%rd26, %rd6, 2;
	add.s64 	%rd27, %rd3, %rd26;
	add.s64 	%rd28, %rd27, 4;
	selp.b64 	%rd45, %rd28, %rd5, %p2;
	selp.b64 	%rd46, %rd27, 0, %p2;
	bra.uni 	$L__BB117_3;
$L__BB117_1:
	setp.eq.s64 	%p3, %rd4, 0;
	selp.b64 	%rd29, 0, 4, %p3;
	add.s64 	%rd45, %rd3, %rd29;
	selp.b64 	%rd46, 0, %rd3, %p3;
$L__BB117_3:
	and.b64  	%rd2, %rd23, 31;
	setp.eq.s64 	%p4, %rd46, 0;
	mov.b32 	%r71, 0;
	@%p4 bra 	$L__BB117_7;
	mov.u32 	%r16, %nctaid.x;
	mul.wide.u32 	%rd7, %r3, %r16;
	add.s64 	%rd8, %rd7, -1;
	ld.f32 	%f1, [%rd46];
	setp.neu.f32 	%p5, %f1, 0f7F7FFFFF;
	selp.u32 	%r71, 1, 0, %p5;
	sub.s64 	%rd30, %rd5, %rd45;
	shr.u64 	%rd31, %rd30, 2;
	setp.le.u64 	%p6, %rd31, %rd8;
	@%p6 bra 	$L__BB117_7;
	shl.b64 	%rd19, %rd7, 2;
	sub.s64 	%rd34, %rd5, %rd19;
	sub.s64 	%rd44, %rd34, %rd45;
	add.s64 	%rd35, %rd19, %rd45;
	add.s64 	%rd43, %rd35, -4;
$L__BB117_6:
	ld.f32 	%f2, [%rd43];
	setp.neu.f32 	%p7, %f2, 0f7F7FFFFF;
	selp.u32 	%r18, 1, 0, %p7;
	add.s32 	%r71, %r71, %r18;
	shr.u64 	%rd36, %rd44, 2;
	setp.gt.u64 	%p8, %rd36, %rd8;
	sub.s64 	%rd44, %rd44, %rd19;
	add.s64 	%rd43, %rd43, %rd19;
	@%p8 bra 	$L__BB117_6;
$L__BB117_7:
	mov.b32 	%r20, 16;
	mov.b32 	%r51, 31;
	// begin inline asm
	shfl.sync.bfly.b32 %r19,%r71, %r20, %r51, 4294967295;
	// end inline asm
	add.s32 	%r26, %r19, %r71;
	mov.b32 	%r24, 8;
	// begin inline asm
	shfl.sync.bfly.b32 %r23,%r26, %r24, %r51, 4294967295;
	// end inline asm
	add.s32 	%r30, %r23, %r26;
	mov.b32 	%r28, 4;
	// begin inline asm
	shfl.sync.bfly.b32 %r27,%r30, %r28, %r51, 4294967295;
	// end inline asm
	add.s32 	%r34, %r27, %r30;
	mov.b32 	%r32, 2;
	// begin inline asm
	shfl.sync.bfly.b32 %r31,%r34, %r32, %r51, 4294967295;
	// end inline asm
	add.s32 	%r38, %r31, %r34;
	mov.b32 	%r36, 1;
	// begin inline asm
	shfl.sync.bfly.b32 %r35,%r38, %r36, %r51, 4294967295;
	// end inline asm
	setp.ne.s64 	%p9, %rd2, 0;
	@%p9 bra 	$L__BB117_9;
	add.s32 	%r42, %r35, %r38;
	shr.u32 	%r39, %r2, 3;
	and.b32  	%r40, %r39, 124;
	add.s32 	%r41, %r14, %r40;
	// begin inline asm
	st.shared.u32 [%r41], %r42;
	// end inline asm
$L__BB117_9:
	bar.sync 	0;
	shr.u32 	%r44, %r3, 5;
	setp.ge.u32 	%p10, %r2, %r44;
	mov.b32 	%r72, 0;
	@%p10 bra 	$L__BB117_11;
	cvt.u32.u64 	%r45, %rd2;
	shl.b32 	%r46, %r45, 2;
	add.s32 	%r48, %r14, %r46;
	// begin inline asm
	ld.shared.u32 %r72, [%r48];
	// end inline asm
$L__BB117_11:
	setp.gt.u32 	%p11, %r2, 31;
	@%p11 bra 	$L__BB117_13;
	setp.eq.s64 	%p12, %rd2, 0;
	// begin inline asm
	shfl.sync.bfly.b32 %r49,%r72, %r20, %r51, 4294967295;
	// end inline asm
	add.s32 	%r56, %r49, %r72;
	// begin inline asm
	shfl.sync.bfly.b32 %r53,%r56, %r24, %r51, 4294967295;
	// end inline asm
	add.s32 	%r60, %r53, %r56;
	// begin inline asm
	shfl.sync.bfly.b32 %r57,%r60, %r28, %r51, 4294967295;
	// end inline asm
	add.s32 	%r64, %r57, %r60;
	// begin inline asm
	shfl.sync.bfly.b32 %r61,%r64, %r32, %r51, 4294967295;
	// end inline asm
	add.s32 	%r68, %r61, %r64;
	// begin inline asm
	shfl.sync.bfly.b32 %r65,%r68, %r36, %r51, 4294967295;
	// end inline asm
	@%p12 bra 	$L__BB117_14;
	bra.uni 	$L__BB117_13;
$L__BB117_14:
	ld.global.nc.u64 	%rd37, [%rd1+72];
	setp.ne.s64 	%p13, %rd37, 0;
	@%p13 bra 	$L__BB117_16;
	bra.uni 	$L__BB117_15;
$L__BB117_16:
	add.s32 	%r69, %r65, %r68;
	ld.global.nc.u64 	%rd38, [%rd1+64];
	// begin inline asm
	red.global.add.u32 [%rd38], %r69;
	// end inline asm
$L__BB117_13:
	ret;
$L__BB117_15:
	mov.u64 	%rd39, anon_$_9b271fbca47443a0c783d86781d13fba_$_26;
	cvta.global.u64 	%rd40, %rd39;
	mov.u64 	%rd41, anon_$_9b271fbca47443a0c783d86781d13fba_$_58;
	cvta.global.u64 	%rd42, %rd41;
	{ // callseq 204, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd40;
	.param .b64 param1;
	st.param.b64 	[param1+0], 40;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd42;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 204

}
	// .globl	iterate_hash_table_f32
.visible .entry iterate_hash_table_f32(
	.param .u64 iterate_hash_table_f32_param_0
)
{
	.reg .pred 	%p<24>;
	.reg .b32 	%r<16>;
	.reg .f32 	%f<5>;
	.reg .b64 	%rd<128>;

	ld.param.u64 	%rd58, [iterate_hash_table_f32_param_0];
	cvta.to.global.u64 	%rd1, %rd58;
	ld.global.nc.u64 	%rd2, [%rd1+32];
	ld.global.nc.u64 	%rd3, [%rd1+40];
	mov.u32 	%r2, %tid.x;
	cvt.u64.u32 	%rd59, %r2;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mul.wide.u32 	%rd61, %r4, %r3;
	add.s64 	%rd5, %rd61, %rd59;
	mov.u32 	%r5, %nctaid.x;
	mul.wide.u32 	%rd6, %r3, %r5;
	ld.global.nc.u64 	%rd8, [%rd1+48];
	ld.global.nc.u64 	%rd9, [%rd1+56];
	setp.le.u64 	%p1, %rd3, %rd5;
	not.b64 	%rd64, %rd5;
	mov.u64 	%rd116, 0;
	mov.u64 	%rd114, %rd116;
	@%p1 bra 	$L__BB118_5;
	max.u64 	%rd63, %rd3, %rd5;
	add.s64 	%rd11, %rd64, %rd63;
	or.b64  	%rd65, %rd11, %rd6;
	and.b64  	%rd66, %rd65, -4294967296;
	setp.ne.s64 	%p2, %rd66, 0;
	@%p2 bra 	$L__BB118_3;
	bra.uni 	$L__BB118_2;
$L__BB118_3:
	div.u64 	%rd113, %rd11, %rd6;
	bra.uni 	$L__BB118_4;
$L__BB118_2:
	cvt.u32.u64 	%r6, %rd6;
	cvt.u32.u64 	%r7, %rd11;
	div.u32 	%r8, %r7, %r6;
	cvt.u64.u32 	%rd113, %r8;
$L__BB118_4:
	add.s64 	%rd114, %rd113, 1;
$L__BB118_5:
	setp.le.u64 	%p3, %rd9, %rd5;
	@%p3 bra 	$L__BB118_10;
	max.u64 	%rd68, %rd9, %rd5;
	add.s64 	%rd17, %rd64, %rd68;
	or.b64  	%rd70, %rd17, %rd6;
	and.b64  	%rd71, %rd70, -4294967296;
	setp.ne.s64 	%p4, %rd71, 0;
	@%p4 bra 	$L__BB118_8;
	bra.uni 	$L__BB118_7;
$L__BB118_8:
	div.u64 	%rd115, %rd17, %rd6;
	bra.uni 	$L__BB118_9;
$L__BB118_7:
	cvt.u32.u64 	%r9, %rd6;
	cvt.u32.u64 	%r10, %rd17;
	div.u32 	%r11, %r10, %r9;
	cvt.u64.u32 	%rd115, %r11;
$L__BB118_9:
	add.s64 	%rd116, %rd115, 1;
$L__BB118_10:
	min.u64 	%rd23, %rd114, %rd116;
	setp.eq.s64 	%p5, %rd23, 0;
	@%p5 bra 	$L__BB118_31;
	shl.b64 	%rd60, %rd3, 2;
	shl.b64 	%rd62, %rd9, 2;
	add.s64 	%rd4, %rd2, %rd60;
	setp.eq.s64 	%p6, %rd5, 0;
	shl.b64 	%rd112, %rd5, 2;
	@%p6 bra 	$L__BB118_13;
	setp.gt.u64 	%p7, %rd3, %rd5;
	add.s64 	%rd73, %rd2, %rd112;
	add.s64 	%rd74, %rd73, 4;
	selp.b64 	%rd122, %rd74, %rd4, %p7;
	selp.b64 	%rd118, %rd73, 0, %p7;
	bra.uni 	$L__BB118_14;
$L__BB118_13:
	setp.eq.s64 	%p8, %rd3, 0;
	selp.b64 	%rd75, 0, 4, %p8;
	add.s64 	%rd122, %rd2, %rd75;
	selp.b64 	%rd118, 0, %rd2, %p8;
$L__BB118_14:
	add.s64 	%rd10, %rd8, %rd62;
	@%p6 bra 	$L__BB118_16;
	setp.gt.u64 	%p10, %rd9, %rd5;
	add.s64 	%rd77, %rd8, %rd112;
	add.s64 	%rd78, %rd77, 4;
	selp.b64 	%rd123, %rd78, %rd10, %p10;
	selp.b64 	%rd120, %rd77, 0, %p10;
	bra.uni 	$L__BB118_17;
$L__BB118_16:
	setp.eq.s64 	%p11, %rd9, 0;
	selp.b64 	%rd79, 0, 4, %p11;
	add.s64 	%rd123, %rd8, %rd79;
	selp.b64 	%rd120, 0, %rd8, %p11;
$L__BB118_17:
	ld.f32 	%f1, [%rd118];
	setp.eq.f32 	%p12, %f1, 0f7F7FFFFF;
	@%p12 bra 	$L__BB118_22;
	ld.global.nc.u64 	%rd80, [%rd1+72];
	setp.eq.s64 	%p13, %rd80, 0;
	@%p13 bra 	$L__BB118_26;
	ld.f32 	%f2, [%rd120];
	ld.global.nc.u64 	%rd40, [%rd1+8];
	cvt.u32.u64 	%r13, %rd40;
	ld.global.nc.u64 	%rd81, [%rd1+64];
	// begin inline asm
	atom.global.inc.u32 %r12,[%rd81], %r13;
	// end inline asm
	cvt.u64.u32 	%rd127, %r12;
	setp.le.u64 	%p14, %rd40, %rd127;
	@%p14 bra 	$L__BB118_32;
	ld.global.nc.u64 	%rd82, [%rd1];
	shl.b64 	%rd83, %rd127, 2;
	add.s64 	%rd84, %rd82, %rd83;
	st.f32 	[%rd84], %f1;
	ld.global.nc.u64 	%rd43, [%rd1+24];
	setp.le.u64 	%p15, %rd43, %rd127;
	@%p15 bra 	$L__BB118_33;
	ld.global.nc.u64 	%rd85, [%rd1+16];
	add.s64 	%rd87, %rd85, %rd83;
	st.f32 	[%rd87], %f2;
$L__BB118_22:
	setp.eq.s64 	%p16, %rd23, 1;
	@%p16 bra 	$L__BB118_31;
	bra.uni 	$L__BB118_23;
$L__BB118_31:
	ret;
$L__BB118_23:
	add.s64 	%rd7, %rd6, -1;
	ld.global.nc.u64 	%rd39, [%rd1+72];
	ld.global.nc.u64 	%rd40, [%rd1+8];
	cvt.u32.u64 	%r15, %rd40;
	ld.global.nc.u64 	%rd99, [%rd1+64];
	ld.global.nc.u64 	%rd42, [%rd1];
	ld.global.nc.u64 	%rd43, [%rd1+24];
	ld.global.nc.u64 	%rd44, [%rd1+16];
	add.s64 	%rd46, %rd23, -1;
	setp.ne.s64 	%p20, %rd39, 0;
	bra.uni 	$L__BB118_24;
$L__BB118_30:
	selp.b64 	%rd122, %rd91, %rd4, %p17;
	selp.b64 	%rd123, %rd96, %rd10, %p18;
	add.s64 	%rd46, %rd46, -1;
	setp.ne.s64 	%p23, %rd46, 0;
	@%p23 bra 	$L__BB118_24;
	bra.uni 	$L__BB118_31;
$L__BB118_24:
	sub.s64 	%rd88, %rd4, %rd122;
	shr.u64 	%rd89, %rd88, 2;
	setp.gt.u64 	%p17, %rd89, %rd7;
	shl.b64 	%rd90, %rd6, 2;
	add.s64 	%rd91, %rd122, %rd90;
	add.s64 	%rd92, %rd91, -4;
	selp.b64 	%rd93, %rd92, 0, %p17;
	sub.s64 	%rd94, %rd10, %rd123;
	shr.u64 	%rd95, %rd94, 2;
	setp.gt.u64 	%p18, %rd95, %rd7;
	add.s64 	%rd96, %rd123, %rd90;
	ld.f32 	%f3, [%rd93];
	setp.eq.f32 	%p19, %f3, 0f7F7FFFFF;
	@%p19 bra 	$L__BB118_30;
	@%p20 bra 	$L__BB118_27;
	bra.uni 	$L__BB118_26;
$L__BB118_27:
	add.s64 	%rd97, %rd96, -4;
	selp.b64 	%rd98, %rd97, 0, %p18;
	ld.f32 	%f4, [%rd98];
	// begin inline asm
	atom.global.inc.u32 %r14,[%rd99], %r15;
	// end inline asm
	cvt.u64.u32 	%rd127, %r14;
	setp.le.u64 	%p21, %rd40, %rd127;
	@%p21 bra 	$L__BB118_32;
	shl.b64 	%rd102, %rd127, 2;
	add.s64 	%rd103, %rd42, %rd102;
	st.f32 	[%rd103], %f3;
	setp.gt.u64 	%p22, %rd43, %rd127;
	@%p22 bra 	$L__BB118_29;
	bra.uni 	$L__BB118_33;
$L__BB118_29:
	add.s64 	%rd107, %rd44, %rd102;
	st.f32 	[%rd107], %f4;
	bra.uni 	$L__BB118_30;
$L__BB118_33:
	mov.u64 	%rd104, anon_$_9b271fbca47443a0c783d86781d13fba_$_61;
	cvta.global.u64 	%rd105, %rd104;
	{ // callseq 206, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd127;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd43;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd105;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 206
$L__BB118_32:
	mov.u64 	%rd100, anon_$_9b271fbca47443a0c783d86781d13fba_$_60;
	cvta.global.u64 	%rd101, %rd100;
	{ // callseq 205, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd127;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd40;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd101;
	call.uni 
	_ZN4core9panicking18panic_bounds_check17h4e5c286e8d48ca34E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 205
$L__BB118_26:
	mov.u64 	%rd108, anon_$_9b271fbca47443a0c783d86781d13fba_$_26;
	cvta.global.u64 	%rd109, %rd108;
	mov.u64 	%rd110, anon_$_9b271fbca47443a0c783d86781d13fba_$_59;
	cvta.global.u64 	%rd111, %rd110;
	{ // callseq 207, 0
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd109;
	.param .b64 param1;
	st.param.b64 	[param1+0], 40;
	.param .b64 param2;
	st.param.b64 	[param2+0], %rd111;
	call.uni 
	_ZN4core9panicking5panic17hc7c8a74e6511bb99E, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 207

}